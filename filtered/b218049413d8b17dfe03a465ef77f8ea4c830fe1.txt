b218049413d8b17dfe03a465ef77f8ea4c830fe1

Heinz U Lemke Christos  Angelopoulos Leonard  Berliner Hubertus  Feussner Franziska  Schweikert Kensaku  Mori Hiroyuki  Yoshida 


The overarching purpose of the scholarly publication and communication process of IJCARS in the context of the CARS congress could be defined as: ''To enable the exchange/communication of R&D ideas by means of verbal and written statements made by responsible authors, scrutinized by informed reviewers and utilized by an open-minded audience, with the aim to stimulate complimentary thoughts and actions within the given domain of discourse by all parties involved in the scientific/medical communication process''. The traditional platforms of CARS Congresses for the scholarly publication and communication process for the presentation of R&D ideas were congress centers or hotels, typically hosting 600-800 participants. In the times of COVID-19 overshadowing CARS 2020 and what the future may hold, a CARS meeting with these numbers of participants is not feasible anymore and new ways have to be explored to still fulfill the overarching purpose concerning scholarly communication. A hybrid (analogue and digital) CARS 2020 has therefore been envisaged to take place at the University Hospital in Munich, with a balanced combination of analogue/personal and digital presentations and discussions. With an increasing general demand and pressure on CARS to also go fully digital in the long term, many members of the CARS Congress Organizing Committee, however, are more cautious and convinced that one of the outstanding features of CARS is the personal (analogue) interaction of participants from different disciplines and cultures. Aiming to stimulate complimentary thoughts and actions on what is being presented at CARS, implies a number of enabling variables for optimal analogue scholarly communication, such as (examples given are in no order of preference): Int J CARS (2020) 15 (Suppl 1):S1-S214 As a member of the CARS Community, we hope you can associate yourself with one or more of the above enabling variables. If you have some thoughts on this, please let us know. This is the time to give them your attention. The hybrid CARS 2020 Congress envisaged at this point in time (beginning of May 2020) tries to satisfy the scholarly communication target as stated at the beginning of this preface, while at the same time with the analogue component of the Congress, trying to be inducive to individuals and society as a whole to work on recovering from the effect of the COVID-19 pandemic and maintaining the human touch of personal communication in science, engineering and medicine. Finally, we should like to thank the enablers of the analogue and digital CARS 2020 Congress, in particular our Munich colleagues, Dirk Wilhelm, Nassir Navab and Daniel Ostler, but also the authors who submitted a digital version of their presentations and above all, the presenters and participants who plan to come and finally attend the meeting in Munich. Hubertus Feussner, MD and Heinz U. Lemke, PhD Munich, May 2020 S2 Int J CARS (2020) 15 (Suppl 1):S1-S214 Computer Assisted Radiology and Surgery Proceedings of the 34th International Congress and Exhibition Munich, Germany, June 23-27, 2020  Chairs: Ulrich Bick, MD (DE) Hubertus Feussner, MD (DE) S10 Int J CARS (2020) 15 (Suppl 1):S1-S214 Purpose Dual-energy computed tomography (DECT) can obtain images of two different X-ray spectra, and it provides material-specific information, such as an iodine perfusion map (IM), an electron density map by post-processing. Although the DECT image is useful in several cases, the DECT scanner is not as widely installed as the single-energy CT (SECT) scanner. For the SECT scanner, it is difficult to obtain the DECT image owing to problems of the machine performance and the imaging dose of additional examinations. Recently, deep learning techniques have been extensively applied in medical image processing [1] . The deep learning technique can provide new information from existing information. Therefore, the pseudo-DECT (DECT pseudo ) image may be able to obtain from a SECT image by means of deep learning techniques. If the DECT pseudo image can only be obtained from a SECT image, the SECT scanner may be able to overcome the above limitations. The objectives of this study were to develop an algorithm to generate a DECT pseudo image from a SECT image based on the convolutional neural network, and to generate a DECT pseudo -derived IM for thoracic and abdominal regions. Furthermore, the generation accuracy of DECTpseudo and DECTpseudo-derived IM was evaluated. Methods DECT image acquisition In this study, contrast-enhanced DECT images were used for training and evaluation of the proposed algorithm. All DECT images were acquired using a DECT scanner (SOMATOM Force; Siemens Healthineers, Forchheim, Germany). For the thoracic region, 20 patients who underwent DECT examination were enrolled. Ten of these patients were used for training; the remaining patients were employed for the algorithm evaluation. For the abdominal region, 24 patients were enrolled. Fourteen of these patients were used for training; the remaining patients were employed for the algorithm evaluation. Algorithm implementation details Figure 1 shows a schematic of the developed algorithm. The algorithm was constructed in accordance with the following two U-net models [2] . (1) The U-net model for the whole-body CT image: The wholebody low-energy CT (CT low ) and high-energy CT (CT high ) images were used as input and output data. (2) The U-net model for organ CT images, such as lung images for the thoracic region and liver images for the abdominal region: The organ CT low and CT high images were used as input and output data. The whole-body CT low image was inputted to U-net model 1, and the whole-body pseudo-high-energy CT (CT pseudo_high ) image was obtained as output data. The organ CT low image was segmented from the whole-body CT low image using 3D Slicer (ver. 4.7.0; Brigham and Women's Hospital, MA, USA) and MATLAB software (MathWorks, MA, USA). The organ CT low image was inputted to U-net model 2, and the organ CT pseudo_high image was obtained as output data. The CT pseu-do_high image was generated by replacing the organ region of the whole-body CT pseudo_high image with the organ CT pseudo_high image. Furthermore, to obtain the iodine-specific information, the pseudo-IM (IM pseudo ) image was generated based on a three-materials decomposition method using the organ CT pseudo_high and corresponding CT low images. The U-net model was implemented in Keras (ver. 2.2.4). The mean absolute error (MAE) was used as the loss function. The U-net models were trained on a GPU (NVIDIA GeForce GTX 1080Ti). For the evaluation, the CT low image of each of the ten patients was used as the algorithm input, and the CT pseudo_high image was generated from the CT low image. To evaluate the algorithm, the differences of CT values between the CT pseudo_high image and corresponding CT high image were measured. For the thoracic region, the CT values were measured for the pulmonary artery, pulmonary vein, ascending aorta, lung, and spine. For the abdominal region, the CT values were measured for the abdominal aorta, portal vein, liver, spleen, and spine. Furthermore, MAEs between the CT pseudo_high image and the CT high image were calculated for the whole body, lung, liver and bone. To evaluate the accuracy of the CT values of IM, the MAEs between the IM pseudo and ground truth IM (IM truth ) were calculated. Results Figure 2 shows examples of CT and IM images for the thoracic and abdominal region. For both regions, the CT pseudo_high image is in good agreement with the CT high image except for regions with a high iodine concentration. The image noise of CT pseudo_high is improved compared to that of CT high . For the thoracic region, the mean ± standard deviation (SD) of differences of CT values is 3.1 ± 6.9, 0.9 ± 9.6, 2.9 ± 12.0, -6.1 ± 7.1, and -7.6 ± 9.5 HU for the pulmonary artery, pulmonary vein, ascending aorta, lung, and spine, respectively. The mean ± SD of MAEs is 29.0 ± 4.4, 24.0 ± 3.5, and 30.6 ± 3.5 HU for the whole body, lung, and bone, respectively. For the abdominal region, the mean ± SD of CT values is 2.9 ± 4.0, 0.3 ± 5.9, 1.5 ± 5.5, 2.0 ± 5.1, and -2.0 ± 8.4 HU for the abdominal aorta, portal vein, liver, spleen, and spine, respectively. The mean ± SD of MAEs is 23.6 ± 3.5, 19.4 ± 2.2, and 33.9 ± 4.2 HU for the whole body, liver, and bone, respectively. For the IM, the mean ± SD of MAEs is 18.5 ± 4.3 and 11.3 ± 2.4 HU for thoracic and abdominal regions, respectively. Conclusion In this study, an algorithm was developed to generate a DECT pseudo image from a SECT image. The CT values of the CT pseudo_high image were comparable to those of the CT high image. The IM pseudo image could be generated in a manner similar to IM truth using the DECT pseudo images. The study results suggest that the DECT pseudo -derived IM can be used as well as the truth DECTderived IM. Purpose Twin-to-twin Transfusion Syndrome (TTTS) is a complication that affects around 10-15% of monochorionic identical twin pregnancies, where two or more fetuses share the same placenta. The syndrome is originated by small anastomoses that connect the blood flow of both foetuses in the placental vasculature. This causes an imbalance blood distribution between the twins and it is generally fatal if left untreated. The first line of treatment of severe TTTS is fetoscopic laser photocoagulation between 16 and 26 gestation weeks, which is associated with improved survival rates for at least one of the twins. However, in up to 33% of treated cases, some anastomoses are missed which cause a recurrence of the syndrome in later gestation weeks [1] . In previous work [2] , we successfully explored the use of a detailed 3D model of the placenta and the vasculature to guide the choice of the correct entry point increasing the probability of success of the intervention. A previous limitation was that the information of the vessel was not detailed enough in the 3D pre-operative model based only on MRI scan processing. In this contribution, we present our initial results in combining information from MRI and US images to create a more detailed vessel map on top of the existing 3D model of the placenta. Database: A multimodal database has been provided by Fetal i + D. Four patients of singleton pregnancies between 25 and 37 weeks of gestation were scanned as part of routine follow-up procedure and MRI and 3D US sequences collected. All MRI scans were acquired using a Half-Fourier-Acquired Single-shot Turbo spin Echo (HASTE) sequence, which usually improves the image quality and usability. Three volumetric and isotropic scans projected on different orientations (axial, sagittal and coronal views) have been performed to each fetus with slice dimension of 256 9 208, voxel spacing 1.4 mm 9 1.4 mm and thickness equal to 3.5 mm. Moreover, the 3D US sequences of the same subjects were collected using a GE Voluson E10 with a curved electronic matrix 4D probe transducer. The spatial resolution of the video captures is 1456 9 666 9 228. Registration: Our pipeline for multimodal image registration has several steps. First, we obtain appropriate segmentation for MRI and US images. For additional details we refer the interested reader to previous works in [2] . Second, we register those models using affine and bspline registration. We tested several possible registration strategies and found that the best results could be obtained by applying a distance transform to the segmentation masks of both the MRI and US placentas. This is because the distance transform creates a gradient that can be followed by the registration algorithm. We present in Fig. 1 the results of the MRI-US registration pipeline. We can see that the method was successful into bringing both modalities in the same subspace. However, vessels (in red in the image) could be significatively distorted due to the use of the deformable bspline method. We empirically found that using an energy bending penalization term gives a good compromise between the accuracy of the registration and the absence of significant vessel deformation. We can see in the first row of Fig. 1 the resulting vessel deformation without the energy bending penalization term and in the following rows how the results improve once the term is included. In Table 1 we report the quantitative results of the Dice Coefficient (DC) and Hausdorff Distance (HD) for the four patients shown in Fig. 1 . We see that the DC is never completely maximized, this is because many times the placenta in US is smaller than the corresponding one in MRI. In this work we presented our initial results on MRI and US vascular fusion. We performed registration on real patients' multimodal image. Future works will center on transforming the vascular data into a graph format and using a matching algorithm. Int J CARS (2020) 15 (Suppl 1):S1-S214 Purpose SPECT is a tomography to obtain tomographic images showing the distribution of radiopharmaceutical agent. SPECT images are obtained by image reconstruction processing using projection images that are acquired by gamma camera from outside of patients. The matrix size of present SPECT projection images should be small, typically 64 9 64 or 128 9 128, because the number of counts of projection image is relatively low. However, high-resolution SPECT images with large matrix size will be enabled more accurate diagnosis since the details in SPECT images can be seen. In general, to obtain high resolution SPECT image with large matrix size, matrix size of projection image is also required to be large. However, projection image noise will be increased at that acquisition. To reduce the noise, there are two ways, increasing the amounts of radiopharmaceutical agents and/or increasing acquisition time. However, if the amounts of the radiopharmaceutical agent to patients is increased, the dose for patient increases. On the other hand, if the acquisition time is increased, the examination time will be increase. Both methods are not be available in clinical situation. Therefore, a method to realize obtaining high-resolution SPECT images reconstructed from low-resolution projection images acquired with small matrix size will be required. The method will also contribute shorter acquisition time with lower amounts of radiopharmaceutical agents. The purpose of this study is to develop and evaluate a method for obtaining 4 times high-resolution SPECT images from low-resolution projection images acquired with 64 9 64 matrix using super-resolution processing using deep learning technique. We used PC (Core i7-6800 K CPU, 16 GB memory) equipped with GPU (GeForce GTX TITAN X) for super-resolution processing. Chainer ver.6.4 (Preferred Networks, Tokyo) was used as a framework for buliding and training deep learning models. For image reconstruction processing, ''Prominence Processor'' [1] was used. Image processing such as ''resizing'' are carried out by ImageJ ver.1.52n. In this study, we adopted the three types of 4-times up-sampling processing. TYPE 1 is upsampling 64 9 64 projection images into 256 9 256 projection images. TYPE 2 is consisted from 2 steps: STEP 1 for up-sampling from 64 9 64 projection images to 128 9 128 projection images and STEP 2 for up-sampling from 128 9 128 SPECT reconstructed images to 256 9 256 SPECT reconstructed images. TYPE 3 is upsampling 64 9 64 SPECT images into 256 9 256 SPECT images. We created the super-resolution model for three types, which are based on SRCNN [2] composed of one up-sampling layer and three convolutional layers. In the training process of the model, loss function was mean squared error and optimization method was Adam (alpha = 0.001, bera1 = 0.9, beta2 = 0.999). Activation function was ReLU (rectified linear unit) for two convolutional layers, and Sigmoid function was used only for the final layer. Training was carried out 10,000 image pair set derived from Ima-geNet 2012 as training data. All images were converted to monochrome, 8-bit depth, and resized to 64 9 64, 128 9 128, and 256 9 256. The training of model for TYPE1 was carried out with a combination of 64 9 64 input images and 256 9 256 teaching images. The training of model for STEP1 of TYPE2 was carried out with a combination of 64 9 64 input images and 128 9 128 teaching images. The model for STEP2 of TYPE2 was also trained with a combination of 128 9 128 input images and 256 9 256 teaching images. The training of model for TYPE3 was carried out with a combination of 64 9 64 input images and 256 9 256 teaching images. The number of epochs and batch size was 1000 and 10, respectively. We evaluated SPECT images from three points of view, spatial resolution, noise, and image quality. For evaluation of spatial resolution, we used line source numerical phantom that were generated by Monte Carlo simulation. We measured the FWHM of 256 9 256 SPECT images processed by each types super-resolution processing from 64 9 64 linear source images. For evaluation of image noise, uniform phantom that was created by the Monte Carlo simulation was used. We measured the standard deviation of 256 9 256 SPECT images of the uniform phantom images. Image quality evaluation was carried out by measuring PSNR and SSIM of head phantom images acquired by gamma camera (GCA-7200A, Toshiba, Tokyo). SPECT images of 256 9 256 head phantom were also generated using each types of super-resolution processing. For comparison, 256 9 256 SPECT images of each phantom images using up-sampling processing by bilinear method were used. Results Table 1 shows the results of FWHM, SD, PSNR, and SSIM. FWHMs measured on images processed by three types of deep learning super resolution method were lower than those by the bilinear method. SD tended to be higher in the image of deep learning than in the image of bilinear method, Figs. 1 and 2. In both PSNR and SSIM, the values were higher in the deep learning method than in the bilinear method. The image quality evaluation values were higher in the order of TYPE 3, TYPE 2, and TYPE 1. SPECT images up-sampled by super-resolution processing by deep learning contributed to improve spatial resolution, compared to images up-sampled using the bilinear method. On the other hand, the image noise was deteriorated. However, image quality by our methods was superior compared to the bilinear method. The results of image quality evaluation were good in the order of TYPE 1, TYPE 2, and TYPE 3. The dataset used for training in this study was consisted from natural images. Natural images have several different features from SPECT images because they have higher spatial resolution, higher contrast, and less image noise than SPECT images. Therefore, it will be necessary to prepare a data set having similar characteristics to SPECT images for further study. In addition, by optimizing the model and training data, super-resolution processing using deep learning could be contribute to SPECT study. Methods Calibration A voxel usVox v of a 3D US image is transformed into the tracking space as follows: where a v is a point in space a, and a T b 2 < 4Â4 and a A b 2 < 4Â4 are a homogenous rigid and scaling transformation, respectively, that map from space a onto space b. The terms tr, pr, us, usVox refer to tracking, US probe, US volume (mm) and US volume (voxels), respectively. As illustrated in Fig. 1 , the unknown transformation is calculated in the calibration step with pr T us ¼ tr T pr À Á À1 Á tr T ph Á ph T ct Á ct T us where ph and ct refer to phantom and computed tomography (CT), respectively. A landmark-based registration provides tr T ph Á ph T ct , as the optical markers of the phantom are obtained in both tracking and CT spaces. A surface matching algorithm of the segmented geometrical shapes of the phantom in the CT and US spaces provides ct T us . The inputs of the developed 3D Slicer module are a sequence of N US volumes, the corresponding N tracked poses of the US probe, the surface model created from CT data, fiducials (coordinates of the optical marker centroids) in tracking (F tr ) and in CT (F ct ) spaces, and a linear transformation to store pr T us . In addition, adjustable parameters are start and end indices of the US volumes used, and the variance, lower and upper thresholds used for the Canny edge detection to segment the geometrical shapes in the US volumes. The module first calculates tr T ct based on F tr and F ct . Then, the module obtains a surface model from each US volume after the Canny edge detection. Having surface models in both spaces, the iterative closest point algorithm is applied to find ct T us with 200 randomly sampled points of each model. pr T us is obtained for each US volume since all transformations are determined. An average of N pr T us is calculated for final pr T us . Evaluation A 3D US probe (X6-1, Philips Healthcare) was used and volumes (matrix size 416 9 312 9 256, voxel size 0.59 9 0.53 9 0.93 mm 3 ) were recorded from the US station (EPIQ 7, Philips Healthcare) via the PLUS Toolkit in 3D Slicer. An optical tracking system (Atracsys fusionTrack 500) with passive markers was used to track the US probe and phantom. The multimodality calibration phantom (CIRS Inc.) for the Clarity radiotherapy positioning system (Elekta) containing ten cylinders and two spheres was used for imaging. The US probe was placed on the phantom in four different poses and ten volumes (N) were acquired at each pose. Cropping the US volumes and pre-aligning them to the CT surface model within 3D Slicer was necessary to provide only the geometrical shapes of the phantom in a proper initial alignment. Then, four ( pr T us ) i were calculated based on the ten volumes of each pose i. For a first evaluation metric, a tracked stylus with a 2-mm lead bead at its tip was immersed in a water tank at room temperature after calibrating the stylus to obtain the tip position in the tracking space (reference). Six different positions of the tip were acquired with the tracking system. For each position, an US volume containing the lead bead was recorded and the tip was manually selected. For each ( pr T us ) i and tip position, the distance between the tip position (US volume) transformed into the tracking space and its corresponding reference was calculated. For a cross-validation (second metric), each ( pr T us ) i was used to place three US surface models (each corresponded to the first volume of the other three poses used for calibration) in the tracking space. The Hausdorff distance between each US surface model (approximately 25,000 vertices) and CT surface model, both transformed into the tracking space, provided three distance measurements for each pose. Results Figure 2 shows the module and the registered surface models for checking the calibration. The root mean square (RMS) and maximum values of the stylus tip distances and Hausdorff distances (surfaces) for each ( pr T us ) i are shown in Table 1 . The RMS distance of the stylus tip is around three times higher than that of the surfaces. There are multiple possible error sources. First, pivoting of the stylus to find its tip and the manual selection of the tip in the US volume will cause significant error. Additionally, the experiment was performed in a water tank and thus the different speed of sound can cause a nonlinear error of around 4% [1] . These error sources are not applicable for the first metric. Nevertheless, the RMS error of the stylus tip in this 3D US probe calibration study was similar to the mean error of a previous study [2] . The open source 3D Slicer module provides a user-friendly method for 3D US probe calibration and its approach is feasible even when using only one pose of the probe. Future work will focus on extending Int J CARS (2020) 15 (Suppl 1):S1-S214 S15 the module by including other surface matching algorithms, an automated preprocessing step and directly calculating the calibration accuracy. Purpose Digital Subtraction Angiography (DSA) is established as imaging technology for vessels with background removal technique in interventional X-ray angiography. In DSA, mask images which are X-ray images without contrast medium in vessels are subtracted from live images which are X-ray images with contrast medium. As a result, vessel images without background structures such as bone and/or other organs are obtained. Advantages of DSA technique are as follows. (1) Clear visibility of vessels, (2) more visibility of complex vascular structures and/or micro vessels with high contrast, (3) amounts of contrast medium will be reduced. On the other hand, the disadvantage is (1) motion artifact caused by patient movement and/or X-ray tube or detectors are easily occurred, (2) patients must stop breathing during relatively long exposure time to avoid organ moving especially for abdominal DSA. Motion artifacts of DSA images are sensitive and affect DSA image quality. In abdominal DSA, motion artifacts are caused by heart beating, breathing, peristaltic motion of an intestinal tract, and patient body movement. In particular, it will be difficult to avoid peristaltic motion during DSA examination. During past 30 years, several techniques to reduce artifacts of DSA images were proposed. For example, ''re-masking'' technique is selecting another image instead of first live image as ''mask image''. ''pixel shift'' technique is re-masking technique using manual rigid shifting of the mask image. However, the effect for reducing artifacts is limited since abdominal images involving complex movements such as peristalsis. The purpose of this study is to develop a new method to generate clear DSA images directly from live images without subtraction processing. We applied the semantic segmentation models which are used for medical image segmentation in the research area of deep learning to generate DSA images. In this study, we used PC (Intel Xeon E5-1650, 32 MB memory) equipped with GPU (NVIDIA TATIN X), and Chainer ver.6.4 (Preferred Research inc., Tokyo, JAPAN) for modeling and training as deep learning framework. We built two types of semantic segmentation model. One is ''SegNet'' based model, and the other is ''U-Net'' based model. We modified the output layer of both models, so that final layer could output pixels of image data [1, 2] . To train both models, we employed 10 live angiographic images (800 9 800 or 1000 9 1000, 8bits) as input data and its DSA images (800 9 800 or 1000 9 1000, 8bits) as teaching image data. These images were divided into 64 9 64 small image (image patch), and training was carried out employing these image patches. Since the number of image patches made from 10 training images was small (1845), we carried out data augmentation using image rotation (0°, 90°and 270°), image flip vertically, image mirroring. As the result, 11,070 image patches were obtained for training. We employed ''Adam'' and ''absolute mean error'' as optimization method and loss function, respectively. We set 10,000 for the epoch number of training. By inputting other 8 series of angiographic images (live images) to the trained model, DSA images were obtained as outputs. To evaluate our method, conventional DSA images for 8 series of angiographic images were also obtained using subtraction processing. In this study, DSA images were subjectively evaluated by four medical radiological technologists with a focus on artifacts by respiratory movements of the organs, movements of bowel, and beats of heart. In conventional DSA images, artifacts were heavily visible. The artifact was greatly reduced in DSA images generated by ''SegNet'' based model and ''U-Net'' based model, compared to conventional DSA images as shown in Fig. 1 . However, high quality of DSA images was realized using ''U-Net'' based model than that of ''SegNet'' based model. Vessel images of DSA image using ''Seg-Net'' based model were lower quality than that of ''U-Net'' based model. DSA images with few artifacts were generated directly by applying modified semantic segmentation model such as ''SegNet'' and ''U-Net'' to DSA image generation. By applying our method, patients will not be required stop breathing or moving.  With the rapid development of deep learning, Fully Convolutional Networks (FCNs) were widely utilized on semantic segmentation in medical tasks. Automated abdominal organ segmentation is challenging, not only because of the large individual differences between patients' in appearance, but also for the different scan conditions. The accurate segmentation results are expected to apply on the Computer Assisted Surgery and Computer Aided Diagnosis systems. Computer tomography is one of the most common clinical modalities used for diagnosis before surgery. Recently, many researches have been focused on thin-slice CT volumes, which with slice thickness of about 0.5 mm. However, the thick-slice CT volumes with around 5 mm thickness are also widely utilized in clinical. Ö zgün et al. [1] shows that 3D extension achieved more reasonable performance on medical tasks than 2D, because the network could also learn from the adjacent slices. When training with 3D FCNs, more attentions should be paid to resolution of CT volumes. Resolution difference makes the direct application of the network trained by thin-slice CT volumes on thick-slice CT volumes difficult. Moreover, it is quite expensive and time consuming to generate a new annotated thick-slice CT data set. This work effectively generates an annotated thick-slice CT dataset with our existed thin-slice CT data. Here, we adopt transfer learning method to improve the segmentation performance of abdominal multi-class organ segmentation from thickslice CT volumes. We validate our segmentation results both quantitatively and qualitatively. With the development of FCNs, its 3D extension like 3D U-Net [1] is proved to achieve higher performance on 3D volumes. However, when training with 3D network architecture, the resolution of volumes should be careful considered for its influence on segmentation performance. Thin-slice CT volumes contains more information than thick-slice CT, which could be beneficial in training. Transfer learning allows to extract knowledge learned from another task, which exceeds training from scratch. In this work, we adapt transfer learning to extract the features from thin-slice to thick-slice CT volumes [MK1] . When training with deep convolutional network, generic features learnt from earlier layers could be similar in many tasks and later layers focus more on specific features. Fine-tuning is an alternative to train with the features extracted from other tasks. In order to keep the both generic and specific features, we fine-tune all the layers of FCNs from thin-slice CT volumes. Firstly, we train with thin-slice CT volumes and obtain a pre-trained model. Then, the weights of pre-trained model are employed as the initial weights when train with thick-slice CT volumes. In the predict phase, we both used our generated thick-slice CT volumes and unseen data with thickslice. Figure 1 shows the overview of this work. The FCN architecture and parameter settings are based on our previous work [2] . Our network is trained in end-to-end manner without any preprocessing or post-processing phase. Mathematical linear interpolation method was utilized to virtually generate the thick-slice CT volumes from thinslice CT volume. Manual annotation label images are interpolated by nearest neighbor interpolation. Our experiments were implemented in Keras with Tensorflow backend. The training data was randomly cropped sub-volumes with the mini-batch size of three. The initial leering rate is 0.0001 and the network is trained with the adaptive moment estimation (Adam) for optimization. Our generated thick-slice CT data consisted of 415 abdominal clinical CT volumes in portal-venous contrast phase. We randomly divided it into 380 training set and 35 testing set. All of the thick-slice CT volumes are interpolated from their corresponding thin-slice CT volume. Seven abdominal organs including artery, portal vein, liver, spleen, stomach, gallbladder and pancreas were manually annotated by using semi-automated segmentation tools. The original thin-slice CT data contain 460-1177 slices with 512 9 512 pixels per slice. We interpolate the CT volumes as well as ground truth to thick-slice CT with the thickness of 5 mm. The input and output size of our network is 64 9 64 9 32. Our segmentation results were quantitatively evaluated by computing the Dice similarity scores. The computed Dice scores are shown in Table 1 . The average Dice score increase from 83.3 to 86.6% by adapting transfer learning method. Especially for the pancreases, the segmentation accuracy improved 7.4%. Qualitative evaluation results are shown in Fig. 2 . It can be seen that the segmentation performance improved by applying transfer learning. We have to mention that, we also applied our trained model on the unseen dataset with thick-slice from different hospitals and scanners. The segmentation results are acceptable by observation. However, the performance is difficult to quantitatively evaluate since these data do not have ground truth. We generated an annotated thick-slice CT dataset by using mathematical interpolation methods. Moreover, the segmentation model for Fig. 1 The overview of our method. Fully convolutional network architecture and parameter settings are based on our previous work [2] thick-slice CT volumes was effectively generated by applying transfer learning methods. Our experiment indicated that transfer learning from thin-slice CT volume to thick-slice CT volume significantly increase the segmentation accuracy, especially on organs with smaller voxel size. Future work will include extending our method on the thick-slice non-enhancement CT volumes. Moreover, the adaptation from CT volumes to other modalities, such as the MR and ultrasound images, could also be considered. Purpose A common issue in medical deep learning research is the creation of dataset for training the neural networks. Medical data collection is also tied-up by privacy laws and even if a lot of medical data are available, often their elaboration can be time demanding. This problem can be avoided using neural networks architectures that can achieve a good predicting precision with few images (e.g. U-Net). In the case of semantic segmentation, the dataset generation is even more cumbersome since it requires the creation of segmentation masks manually. Some automatic ground-truth creation techniques may be employed like filtering, thresholding and Self Organized Maps [1] (SOM). These automatic methods can be very powerful and useful, but they always have a bottle-neck phase: data validation. Due to algorithm reliability (that sometimes can fail), data needs to be validated manually before they can be included in a dataset for training. In this work, we propose a method to automatize this phase by moving manual intervention to an easier task: instead of creating masks and then validate them manually, we train a convolutional neural network to classify segmentation quality. Therefore, the validation is performed automatically. An initial manual phase is still required, but the classification task requires a smaller number of elements in the dataset that will feed a network employed for classification. After this phase, similar dataset creations will require less effort. This procedure is based on the fact that to obtain a high classification precision, fewer data are required than the data that are needed to obtain high precision in semantic segmentation. High classification score, can automatize validation procedure in dataset creation, being able to discard failure case in dataset creation. Being able to produce bigger dataset in less time can led to higher precision in semantic segmentation. Methods Automatic dataset generation for semantic segmentation (ADAGSS) method is divided in steps. Although this method can be applied generally in every segmentation situation, our test case is the segmentation of the prostate in ultrasound images taken from a multimodal phantom (CIRS 070, Computerized Imaging Reference Systems, Inc. (CIRS), Norfolk, Virginia, US). We tested the method on coronal images acquired with a convex probe and on sagittal images taken with a linear probe. Phase one consists in the segmentation of the dataset. Segmentation is achieved by applying the following methods: spatial filtering (median, top-hat and closing), thresholding (threshold depends on data and can be tuned manually). Threshold operation produces a preliminary binarization version of the input image. At this point, binarization is refined by feeding binarized images to a Self-Organizing Map (SOM) network that perform pixel-clustering, refining binarization boundaries. After this, morphological transformations such as opening and closing are applied. Final step is de-blobbing in which eventual outliers (blobs of pixel residual of segmentation) are removed. De-blobbing is performed applying the heuristic that assumes that prostate blob is the most centered one. Blob centroid distance from the center of the image is computed and only the most central one remains. After this step, segmentation is done. If fine-tuned, this method grants a satisfying result in most of the samples. Sometimes, noise is too much and prevent segmentation to produce precise masks and/or sometimes deblobbing lead to blank images. After segmentation has been made, a preliminary manual classification is required. Using a user interface with 3 buttons (Fig. 1) it's possible to manually classify segmentation masks in three semantic classes: valid segmentations, segmentation to be discarded (e.g. blank ones most of the times) and fixable segmentation. Fixable segmentations are, in proportion, very less than the previous two: they are the segmentation masks that are not correct, but with a very quick and easy manual intervention can be made ''valid'' and added to the dataset. An example is when de-blobbing phase fails, and a blob still has to be deleted. This manual classification is the base to a second dataset of labeled images which will be used to train a second neural network. The second network is a classifier and will be trained to classify segmentations mask, going to automatize the manual process previously described. The classifier has a typical convolutional neural network structure for classification task (Fig. 2) . Output layer take a Softmax layer as input to have a multiclass classification of images. After having achieved a classification accuracy that is high enough ([ 90%), dataset generation can be fully automatized by applying the classifier to validate generated data. After an initial effort, automatic dataset creation can lead to far more big dataset generations than the one achieved by manual intervention. We have applied this method in 2 case scenarios: one with a high number of training data (around 75% of the total) and one with only a few training images (around 2.5% of the total). The first one achieved 92% precision in classification task of ground-truth masks classification of 908 coronal plane images. Network has been trained on a dataset of 3630 labeled images. By using data augmentation, the Fig. 2 Comparison of segmentation Results with applying transfer learning (TL) and without TL in 3D rendering. The region in black box show that when applying transfer learning, the artery region has better performance S18 Int J CARS (2020) 15 (Suppl 1):S1-S214 result reaches an accuracy of 96%. The second one achieved 79% precision (82% with some data-augmentation) in classifying 3630 coronal plane images. The network has been trained on 100 manually classified images, see Table 1 . The use of neural networks for medical image segmentation is a powerful tool but its use is limited by the necessity to have big dataset as ground-truth. The creation of the ground-truth is most of the time manual. Automatic tools are also available, and they are based on classical image processing algorithms. Their result requires always manual supervision, which is again time consuming for large sets of data. We propose here a convolutional network to supervise the automatic generation of ground-truth data. The results of this method (automatic segmentation plus automatic validation) can be itself a segmentation method, but, used in conjunction with a semantic segmentation neural network (e.g. U-net) can be even a more powerful tool, considering that the neural network allows for a more accurate segmentation obtained in less time. We proved here that with a low number of input images for a classification neural network (2.5% of the total) we can achieve a high volume of data validated as ground-truth dataset for a semantic segmentation neural network. Procedure can be re-iterated until a higher precision is reached. Keywords Pediatrics, Liver segmentation, Fully convolutional network, Statistical shape model Purpose Automated organ segmentation from pediatric computed tomography (CT) poses a challenging task compared with segmentation for adult patients. First, because child organs are still developing, there are large variations of shape, position, and size, depending on individual and age. Second, pediatric CT volumes may have lower resolution and signal-to-noise ratios because it is necessary to reduce radiation doses to safe range for children. Furthermore, motion-related artifacts and the use of oral contrast media can further increase variability. Segmentation algorithms designed for pediatric patients have not yet been reported, except for the preliminary study of our research group [1] . For that, we employed a maximum a posteriori (MAP)based segmentation method with a patient's age-specific probabilistic atlas derived from conditional statistical shape model (SSM). However, the probabilistic atlas faced limitations when describing complex liver structure. Thus, details were lost in the segmentation results. In recent studies of image segmentation, many have been based on fully convolutional neural networks (FCN) with encoder-decoder architectures, such as U-Net. FCN has been successfully applied for learning complex features with multiple convolutional layers. However, most conventional FCNs did not employ prior shape knowledge, thus yielding anatomically inappropriate segmentation results. In [2] , a shape model was learned via convolutional auto encoding and incorporated into an FCN segmentation framework. Inspired by the works above, we now propose an FCN-based algorithm for liver segmentation from pediatric CT images. The proposed FCN was trained using loss functions with a regularization term derived from conditional SSM (C-SSM) [1] . Methods An SSM of the liver was constructed based on the level-set shape representation in which the shapes were defined as a zero-level set of a signed distance maps. Given N training data, u n [ R M (1 B n B N) denotes a signed distance map of the liver contour of the nth case defined on a discrete image domain (M = M x 9 M y 9 M z ). Applying principal component analysis to N training distance maps, we obtain low dimensional shape features, x i = f(u i ), where f:R M ? R D denotes the projection onto the subspace of the SSM. C-SSM provides a conditional probability distribution of the shape feature. Let y n [ R C be the condition vector associated with the nth patient. We assume that random variables of shape feature X and condition Y have Gaussian distributions with mean vectors, l X and l Y , and covariance matrices, R XX and R XX , respectively. With a mutual covariance matrix R XY , given a condition, Y = y, the conditional average and the covariance of the shape feature can then be written as l X|y = l X ? R XY R YY -1 (y -l Y ) and R XX|y = R XX -R XY R YY -1 R XY T , respectively. In this study, a 3-dimensional (3D) U-Net architecture was adopted for liver segmentation using three max-pooling layers (kernel size: 2 3 ) and three deconvolution layers (kernel size: 2 3 ). The input and output of the network was the down-sampled CT volume and the signed distance map of the liver, respectively. Because of limitations of graphical processing memory, we inputted 4 9 4 9 4 = 64 downsampled images. The resulting multiple low-resolution outputs were combined with a pixel-shuffling operator to generate an up-sampled image. Figure 1 illustrates the training process of the proposed U-Net with a C-SSM-based hybrid loss function comprising of three terms: Diceðw i ; u i Þ þ ðk 1 =DÞ f ðw i Þ À x i 2 þk 2 jD M ðf ðw i Þjy i Þj 2 h i : The first term is the Dice loss function was calculated from two shapes associated with the two distance maps, w i and u i . The second term was the shape prior term, or a Euclidean distance of the shape feature vectors in the eigenshape space. The third was the condition term defined as the conditional Mahalanobis distance, D M (x|y) = (x -l X|y ) t R XX|y -1 (x -l X|y ). These two terms acted as the shape regularization terms. The 42 CT volumes of pediatric patients having ages ranging between 16 and 2912 days were acquired from the Children's National Medical Center (Washington DC, US). Using manually identified four landmarks on bone, images were aligned into a common image frame having dimension of 256 3 voxels and a voxel size of 1.0 3 mm 3 /voxel. For C-SSM, only one condition (i.e., patient's age in days) was used. Three-fold cross-validation was performed to evaluate segmentation performance. The weighting parameter for the proposed loss function was determined experimentally and set as (k 1 , k 2 ) = (5 9 10 -8 , 1 9 10 -14 ). To measure the effect of using C-SSM, we compared the proposed U-Net with one trained without shape regularization [i.e., (k 1 , k 2 ) = (0, 0)]. The performance was also compared with the conventional MAP-based method [1] . Note that, only for the MAPbased method, we employed leave-one-out cross-validation. The segmentation performance is summarized in Table 1 . The proposed U-Net with C-SSM showed the best performance in terms of Dice score for the number of false-positive (FP) and false-negative (FN) voxels. Two-tailed Wilcoxon signed rank testing was performed with a null hypothesis stating that ''the two methods would have the same performance.'' We found that the Dice score of the U-Net with C-SSM was significantly different from that without C-SSM and that of the MAP-based method [1] (p \ 0.01), e.g., tip of left lobe and bottom tip of right lobe. Examples of two typical segmentation results are shown in Fig. 2 . Overall, the proposed method improved segmentation accuracy, as indicated by the red arrows. Compared to the MAP-based method [1] , U-Net successfully segmented the sharp structures as indicated by the red arrows in Fig. 2  We proposed a U-Net-based model for liver segmentation from a pediatric CT volume, where we employed the loss function with a regularization term derived from C-SSM constructed for each patient's age. The algorithm was tested using 42 pediatric CT volumes, and the proposed U-Net with C-SMM showed the best Dice score (0.9361) on average.  In this paper, we propose a method for detecting obstruction points of ileus patients by virtual cleansing of intestines on CT volumes. Diagnosis of ileus from CT volumes is difficult for non-experts. Therefore, a computer aided detection system for ileus is desired. Intestine segmentation is promising for finding obstructions as endpoints of segmented regions. However, intestines (including both small and large intestines) segmentation is difficult because fecal tagging is infeasible on emergency ileus diagnosis. Especially on infants' CT volumes, intensity difference between intestinal walls and contents like fluid is very low. We introduce a CycleGAN-based virtual cleansing for replacing fluid with intensity of the air. This allows us to segment intestines easily by applying a region growing. Methods This method firstly performs (1) virtual cleansing of intestines on the input CT volumes, and then performs (2) intestine segmentation for finding obstructions as endpoints of segmentation results. (1) Virtual cleansing of intestines Our virtual cleansing scheme is for replacing the fluid inside intestines with intensities of the air. This fluid-air replacement is performed based on CycleGAN, which is one of the image-to-image translation method. We translate CT volumes with fluid inside intestines to CT volumes without fluid inside intestines using CycleGAN as illustrated in Fig. 1 . This method processes 2D patch images. Patch images for training are generated as follows: We manually separate a training dataset into high-contrast and low-contrast groups based on contrast condition of intestinal wall. Patch images of fluid are cropped from axial slices of all CT volumes. On the other hand, patch images of air regions are cropped from the CT volumes in the high-contrast group whose intensities in the intenstines are replaced with the intensity of the air (k [H.U.]). To perform intensity replacement, intestine regions are manually segmented for CT volumes of the training dataset. CycleGAN is trained using both fluid and air patch images. On the inference process of a testing volume, we input patch images clipped from testing volume to trained CycleGAN. We generate a CT volume by reconstructing the output of the trained CycleGAN. In the resulting CT volume, intensities in the intestines are replaced with that of air. (2) Intestine segmentation for finding obstructions as endpoints Intestine segmentation from the translated CT volume is conducted by a region growing. From a manually-clicked seed point, the region growing extracts an air-like region in the intestine. We detect two obstruction points by analyzing the extracted intestine. The obstruction points are defined as the endpoints of the centerline in the extracted intestine. If either detected obstruction points are not true obstructions, another seed point should be clicked in the different intestine section. We developed a graphical user interface for those operations. Distance from seed points in intensities are visualized intuitively by coloring based on the distance. Obstruction points becomes red and the clicked seed point becomes blue. We applied the proposed virtual cleansing and obstruction detection to 7 CT volumes of child ileus patients. We performed a three folds cross validation. For comparison, we also performed virtual cleansing using pix2pix on behalf of CycleGAN. The air intensity k described in the Method section, input patch size, mini-batch size, and number of echoes were set as -950, 128 9 128, 8, and 200, respectively. Figure 2 shows results of our proposed method and pix2pix. Compared to pix2pix (Fig. 2b), CycleGAN (Fig. 2c) well preserved intestinal walls. This allowed accurate obstruction detection (Fig. 2d) . The proposed method detected 7 out of 14 intestinal obstruction points from 7 cases by removing residual materials in CT volumes. In contrast, only 2 obstruction points were detected when virtual cleansing was performed by pix2pix. In this paper, we proposed a method for detecting obstruction points of ileus patients by the virtual cleansing of intestines on CT volumes. CycleGAN was introduced for virtual cleansing of intestines on CT volumes of ileus patients. Compared to pix2pix, CycleGAN preserved intestinal walls well, and prevented leakage across intestinal walls on  Purpose One of surgical treatments for bladder tumors is transurethral resection of bladder tumor (TURBT) which is a procedure for removing the tumors from a bladder wall. In TURBT, a urologist inserts a cystoscopy into the bladder and detects tumors on the bladder wall by the cystoscopic images. Since there is a risk of the tumor dissemination, to avoid the tumor reoccurrence, the urologist needs to detect and resect all tumors. The tumor detection is based on the appearance and shape differences between normal mucous membranes and abnormal membranes with tumors. Here, there are two types of the tumors shapes: flat and elevated lesions. The tumors with elevated lesions can be found because of their distinctive shapes compared with flat lesions. On the contrary, the tumors with flat lesions tend to be small, and have similar appearances to mucous membranes with inflammatory response. These factors make it difficult to segment the tumors with flat lesions from cystoscopic images. Although several methods [1, 2] for detecting the bladder tumors have been developed, there are few methods for segmenting the bladder tumors from cystoscopic images. In this paper, to achieve a complete transurethral resection, we propose a deep learning-based system for segmenting bladder tumors from cystoscopic images. Methods Figure 1 shows the architecture of the proposed system for detecting the bladder tumors. The proposed system is based on U-Net which achieves a good performance in biomedical image segmentation. The U-Net uses local image features to segment target regions. On the contrary, bladder tumors have various appearances. Especially, the appearance of the part of the tumor is sometimes similar with those of normal membranes. This characteristics of the bladder tumors leads the U-Net to fail in segmenting the tumors with similar appearances of normal membranes. To solve this problem, we incorporate dilated convolution layers (the blue parts in Fig. 1 ) into the encoder network of the U-Net. Since the dilated convolution captures the image feature from a wider area, the proposed system segments the bladder tumors stably by using various range image features. To validate the applicability of the proposed method, we made experiments using our dataset of cystoscopic images acquired from 90 patients with bladder cancers. For each image with 128 9 128 [pixel] in the dataset, the extracted tumors by one experienced urologist are used as the ground-truth of the tumors. The dataset is divided into four sub-datasets, each of which contains 22 patient images on average. Therefore, 4-fold cross validation is applied to evaluate the proposed system: 3 sub-datasets are used for training while the rest sub-dataset is employed for testing. In the experiment, the proposed system is compared with the U-Net with an original architecture. When a given cystoscopic image shown in Fig. 2a is given, Fig. 2c and € shows the segmented tumor by the proposed method (Fig. 2c ) and U-Net (Fig. 2e ) from the cystoscopic image. Moreover, Fig. 2d , f indicate the matching between the segmented tumor region by each network and its ground-truth. The blue area in Fig. 2d , f is the overlapping region between the segmented region and its ground-truth while the red area is the region which each network can not extract from the cystoscopic image. The green regions in Fig. 2d , f show the tumor regions that each network can not detect. From the results in Fig. 2 , the segmented region by the proposed method is close to the ground-truth compared with U-Net. The two systems are evaluated by six metrics: precision, recall, accuracy, specificity, Dice and Jaccard index. Table 1 shows the results of the six metrics obtained by the proposed system and the U-Net. Compared with the U-Net, the proposed system improve the recall of the segmentation while keeping the precision rate. Generally, Int J CARS (2020) 15 (Suppl 1):S1-S214 when a system is applied to segment tumors from given images, both the high recall and precision for the system are desirable to segment the tumors reliably. Considering the requirement, from Table 1 , the proposed system achieves a good performance of segmenting bladder tumors. In this paper, we proposed a new deep learning-based system for segmenting bladder tumors from cystoscopic images. From the experimental results, the proposed system can segment the tumors stably and reliably compared with the U-Net. Purpose Bone scintigraphy is an effective imaging technique to detect hot spots of bone metastatic lesions. However, it is difficult for physicians except radiologists to differentiate them from hot spots of non-malignant lesions, resulting in variations in findings and interpretations among physicians except radiologists. Computer-aided diagnosis systems [1, 2] have been developed to support the detection of hot spots of bone metastatic lesions and to evaluate bone scans per the bone scan index (BSI). BSI is computed from regions of skeleton and hot spots of bone metastatic lesions. BONENAVI [1] performed atlas-based skeleton segmentation followed by hot-spot classification using an artificial neural network. However, this technique suffered from low performance in segmentation and classification. A deep-learning-based skeleton segmentation and hot-spot extraction was proposed in [2] . This process consisted of a butterfly-type network (BtrflyNet) whose input was a pair of anterior and posterior bone scintigrams, resulting in high performance and high consistency between the anterior and posterior outputs. The method, however, suffered from inconsistencies between skeleton segmentation and hot-spot extraction, because the two Btr-flyNets were independent of each other. One remedy was to simply trim the extracted hot-spot regions by the extracted skeleton regions, which could, unfortunately lead to additional errors in hot-spot extraction. This paper proposes a method to train the two BtrflyNets simultaneously to resolve the inconsistency problem. We show the results of applying the proposed method to 246 cases to demonstrate its effectiveness. Methods Figure 1 presents a flowchart of the proposed network for test cases. First, spatial and gray-scale normalization is applied to an input bone scintigram and trimmed to 256 9 576 pixels. Second, the normalized image is forwarded to two BtrflyNets having the same number of parameters for skeleton segmentation and hot-spot extraction. Note that the BtrflyNets input is the same as that for skeleton segmentation to maintain consistency between network outputs, while the input of the previous method [2] is a patch image. Third, an integration process is carried out to fuse BtrflyNets outputs, where the probability of a hot spot of bone metastatic lesion is compared to that of the skeleton and replaced by the minimum of the two probabilities. Note that the minimum operation avoids inconsistencies between skeleton segmentation and hot-spot extraction. During the training process, pre-training of both BtrflyNets is carried out independently by minimizing a dice loss of skeletons and that of hot spots. Subsequently, fine tuning of the whole network is performed by minimizing the proposed new loss functions below to maintain consistency between hot-spot extraction and skeleton segmentation. loss hot spots ¼ Dice pred updated hot spots À Á þ w 1 Dice pred hot spots À Á ð1Þ loss skeleton ¼ Dice pred updated hot spots À Á þ w 2 Dice pred skeleton ð Þ ð 2Þ where Dice(pred updated hot spots ) is the dice loss computed using the predicted probability of the hot spot updated during the integration process. Dice(pred skeleton ) and Dice(pred hot spots ) are dice losses of skeletons and hot spots, which are the same as those used during the pre-training process. Symbols w 1 and w 2 are weights. Anterior and posterior bone scintigrams of 246 cases were used to assess usefulness of our proposed method. The number of skeleton labels was 12 at the anterior, and 11 at the posterior, respectively [2] . Hot-spot labels included bone metastatic lesions and nonmalignant lesions [2] . Three-fold cross validation was used, where 164 cases were used for training, 41 cases were for validation, and 41 cases were for testing. The maximum number of training iterations was 3,000, and the mini-batch size was 6. Parameters of Adam were set as a = 0.0001, b 1 = 0.9, and b 2 = 0.999, and the learning rate was reduced to 1/10 for every 100 epochs. The weights of the loss function, 1 and 2 , were set to 0.1 and 0.5, respectively. Figure 2 presents an example of results using the proposed method, where false positives (FPs) of the bladder were eliminated by fine tuning with the proposed loss functions. Table 1 summarizes the performance indices of hot spots of bone metastatic lesions. Dice score (DS) of posterior images were increased by 2.8 points with statistically significant differences from pre-training, whereas the improvement was 1.4 points by simply restricting hot-spot regions using extracted skeleton regions. The main reason for the improvement was the decrease in FP pixels and connected components, as shown in Table 1 . Moreover, the inconsistency index, which evaluates the pixel number of hot spots extracted outside the skeleton, was dramatically reduced by fine tuning. For the anterior image, the numbers of pixels and connected components of FPs and the inconsistency index were improved. Int J CARS (2020) 15 (Suppl 1):S1-S214 S23 Average DS was improved, but no statistically significant difference was found, because of the slight increase in the pixel number of false negatives, which remains to be investigated in a future work. Although the aim of this study was to reduce FPs by maintaining the consistency between hot-spot extractions and skeleton segmentations, it is worth mentioning that the performance of skeleton segmentation after the fine tuning was almost the same as that of pretraining. The average DS was 0.8847-0.8846 for anterior and 0.9177-0.9176 for posterior, where all changes were not statistically significant. We proposed a method to resolve the inconsistency problem between skeleton segmentation and hot-spot extraction. An integration process was introduced to fuse the two BtrflyNets, and the whole network was fine-tuned using the proposed loss function. The effectiveness of the proposed method was demonstrated in terms of DS, FP, and an inconsistency index that counts number of pixels extracted outside the skeleton. The Hebrew University of Jerusalem, School of Computer Science and Engineering, Jerusalem, Israel 2 Tel Aviv Sourasky Medical Center, Sagol Brain Institute, Tel Aviv, Israel Keywords Fetal MRI, segmentation, neural networks, deep learning Purpose Magnetic Resonance Imaging (MRI) of the fetus has recently been introduced as a radiological method for the identification of fetal developmental disorders with respect to gestational age and for the detection of fetal anomalies [1, 2] . Timely, accurate and reliable detection and follow-up are essential to reduce short and long-term risks to the fetus and mother. Quantitative fetal evaluation requires accurate identification of the contours of fetal structures, e.g. the fetal body and brain. Manual delineation of these contours is a very laborious and time-consuming task that is impractical in clinical practice. Automatic model-based segmentation methods have been shown to be of limited use. Recent deep learning methods are promising but require large annotated datasets generated by experts with a very tedious and time-consuming process whose results require validation. We have developed a new method for end-to-end automatic volumetric segmentation of fetal structures in MRI scans. The method is based on custom deep learning networks trained with very few annotated scans. It consists of three main components: (1) automatic structure segmentation, (2) segmentation error estimation, and (3) segmentation error correction. The automatic structure segmentation component is composed of two networks (Fig. 1) : the first computes a 3D region of interest (ROI) with a coarse segmentation network on a downscaled scan; the second computes a final segmentation on the cropped ROI with a fine segmentation network (Fig. 1) . The custom 3D U-Net networks perform non-isotropic down-sampling and are trained with the Dice loss function and use [1] . The segmentation error estimation component uses the resulting segmentation from the previous step and additional segmentations automatically computed with spatial and intensity prediction-time augmentations of the input scan. The slice-wise and scan-wise segmentation error estimations are then derived from these segmentations with 2D and 3D U-Nets, respectively. In both cases, a probability value is assigned to each of the original input scan voxels based on all the segmentations results of the augmentations after they have been aligned (registered) to the input scan. The segmentation uncertainty of each voxel v is the sum of the voxel segmentation binary entropy of its probability value P(v): Larger voxel uncertainty values correspond to the higher uncertainty of the voxel to be in the final segmentation. The segmentation error correction component then uses the segmentation error estimation to locate the most error-prone slices and to correct the segmentations in those slices based on validated adjacent slices. The inputs are the slice in which the segmentation errors are to be corrected (manually or automatically), several adjacent slices around the slice, and an adjacent slice with a correct segmentation. It uses a 2D U-Net network trained on ground truth segmentations and on segmentation error estimations to generate a corrected segmentation. We collected two datasets of fetal brain and fetal body MRI scans (64 cases, 9 used for training, 55 for testing) and fetal brain coronal MRI scans (42 cases, 8 used for training, 34 for testing) acquired with the volumetric FIESTA and 3D fast imaging TrueFISP protocols, respectively. Expert-validated ground-truth segmentations of the fetal body and fetal brain scans were created by manual delineation (13 fetal body and 8 fetal brain scans) and manual correction (51 fetal body and 34 fetal brain scans) of the automatic segmentation method results. The intra and inter-observer manual delineation variability was computed for 13 scans assessed by two radiologists. Table 1 shows the manual delineation observer variability measures and the results of the automatic segmentation method, of three of its variants and of two other methods (L-Net and 2D U-Net). The automatic segmentation of the fetal body and fetal brain yield a mean VOD (Volume Overlap Difference) of 6.8 (std = 2.4) % and 6.7 (std = 3.1) %, and a mean ASSD (Average Symmetric Surface Distance) of 0.74 (std = 0.51) mm and 0.19 (std = 0.1) mm, respectively, both below the intra-and inter-observer manual delineation variability measures. The online runtime performance of our automatic segmentation method on a standard GPU is We quantified the benefit of prioritizing the segmentation error correction with the estimated segmentation error measures. In this setup, the segmentation error estimation is used to refer the radiologist directly to slices that may require manual segmentation error correction. When the slices are corrected in descending order of their estimated segmentation error value, observer variability performance is achieved with the correction of 12% of the slices instead of 33% with random prioritization. Table 1 Results of six architectures and methods on the fetal body dataset The first two rows list the manual segmentation observer variability and serve as the reference for evaluating the automatic segmentation methods results. The next rows indicate the network architectures and methods. The columns indicate the segmentation metric scores (mean and std) over all the test cases. The best scores are highlighted in blue Purpose This paper proposes an abdominal contrast-CT volume estimation method from non-contrast abdominal CT volume using an unpaired image estimation method with a loss function for tiny image difference evaluation. Contrast CT (CT angiography) images are commonly used to diagnose blood vessels and abdominal organs. Use of contrast agents improves visibility of blood and organs. Contrast agents are intravenously injected to a patient to take contrast CT volumes. This process may have side effects including itching, dyspnea, and cardiac arrest [1] . If we can remove use of contrast agents, risk of the side effects can be greatly decreased. We try to estimate contrast CT volumes from non-contrast CT volumes to remove use of contrast agents. In this paper, we propose an estimation method of contrast abdominal CT volumes from non-contrast CT volumes. Our original contributions are: (1) use of a deep learning-based image estimation method trained using unpaired images and (2) proposal of a loss function for tiny image difference evaluation. Because preparation of paired image data is difficult, unpaired training method is important to increase number of training data. Estimation of images are performed by a fully convolutional network (FCN). During training of the FCN, a loss function is used to evaluate differences between estimation results and ground truth. However, because differences between contrast and non-contrast CT volumes are very small (only blood vessel regions), existing loss functions such as the mean absolute error (MAE) cannot evaluate their differences. We propose a new loss function for evaluating tiny image differences (TinyLoss). By using the proposed method, we accurately estimate contrast CT volumes. Methods Preprocessing Non-contrast and contrast abdominal CT volumes are used to train our method. Because we employ image estimation method trainable from unpaired images, these volumes can be taken from different patients. Axial slices which are scaled to 256 9 256 pixels are obtained from the CT volumes to process. Estimation network training using TinyLoss We use a U-Net as an image estimation FCN. The U-Net is used as a generator and trained in the CycleGAN [1] framework. During training in CycleGAN framework, an objective function L ¼ L gan1 þ L gan2 þ kL cyc is minimized [3] . L gan1 and L gan2 evaluate image similarity in two generative adversarial networks (GANs). The cycle consistency loss L cyc evaluates similarity between images translated by two GANs. Because differences between contrast and non-contrast CT images are very small, we propose the TinyLoss. In the TinyLoss calculation, by using estimated and ground truth (GT) images, we separate pixels in the images to high difference and low difference pixels based on absolute differences between the estimated and GT images. MAEs of high and low difference pixels are represented as M h and M l . The TinyLoss is defined as By using this TinyLoss, we can modify degree of contributions of high difference pixels (blood vessel regions) and low difference pixels (other organ regions) to loss value. The TinyLoss is used as cycle consistency loss in our method. We estimate contrast axial CT images using the trained U-Net. Noncontrast axial CT images for testing are given to the trained U-Net to obtain estimation results. We evaluated the proposed method on 29 non-contrast and 29 contrast abdominal CT volumes. Each 26 volumes from them were used for training and each 3 volumes were used for testing. k = 10 and a = 0.05 were used. Specifications of the CT volumes are: image size was 512 9 512 pixels, slice number was 41-96, pixel spacing was 0.586-0.782 mm, and slice spacing was 5.0 mm. Examples of non-contrast (input of the method), estimated contrast [outputs obtained using the TinyLoss and without using the TinyLoss (MAE was used)], and contrast CT images are shown in Fig. 1 . In the non-contrast CT images, intensities of artery and kidney regions were low. In the estimated contrast CT images, artery, heart, and kidney regions had relatively higher intensity values than the surrounding tissues. Especially, the result obtained using TinyLoss was quite close to the contrast images. The artery and kidneys were clearly observed in the result. On the other hand, the result obtained without using the TinyLoss was not good. The artery and kidneys were not well enhanced in the result. These results proof the TinyLoss is effective in image estimation processes using FCNs. Because our method can be trained using unpaired image data, many images stored in medical institutions can be used to train our method. We employed 2D image process on axial slices. Therefore, discontinuity of intensities on coronal slices are observed. Utilization of 3D information will solve this problem. We proposed a contrast abdominal CT volume estimation method from non-contrast CT volume using unpaired image-based training framework. Because differences of contrast and non-contrast images are very small, we proposed the TinyLoss to effectively reconstruct blood vessels in estimation results. Experimental results showed promising results. Future work includes use of 3D information and improvement of estimation accuracy on CT volumes having abnormal structures. . This is based on a Cycle-GAN and introduces pixelshuffling [1] . SR-CycleGAN [2] is a state-of-the-art unpaired SR approaches. Its ability is demonstrated in SR from clinical CT-level to lCT-level. This process is trained by unpaired high-and low-resolution datasets. However, SR-CycleGAN has several drawbacks such as (1) strong lCT-like artifact, (2) blurred results on anatomically important tissues such as bronchi and (3) low performance in image quality measure. This paper attacks these problems by improving SR-CycleGAN architecture. Major improvements in the SR-CycleGAN V2 compared to SR-CycleGAN is to (1) introducing pixel-shuffling layer and (2) removing downsampling layers. We name the proposed method as SR-CycleGAN V2. Methods Overview Network structure of proposed SR-CycleGAN V2 is basically following SR-CycleGAN. SR-CycleGAN has a fully convolutional network (FCN)-based generator to perform SR from LR images to HR images. SR-CycleGAN also has another FCN-based generator that converts HR images to LR images. FCN-based discriminator is implemented to classify a given image is a real HR image or a fake HR image (SR result from LR image.) Another discriminator is also implemented to classify a given image into a real LR image or a fake LR image generated from an HR image. We modify the network structure of SR-CycleGAN's generator in two points: (1) replacing transposed convolution by pixel-shuffling and 2) removing downsampling layers in SR-CycleGAN's generator. (1) Replacing transposed convolution by pixel-shuffling Pixel-shuffling layer [1] is newly introduced in this paper. Pixel shuffling is an approach that learns an array of upscaling filters to upscale the final LR features into HR output. It is reported [1] that it has potential performance to make SR significantly better than the transposed convolution in FCN-based SR network. In SR-CycleGAN, we used transposed convolution in the last layer of the generator to enlarge the size of image to the same as desired (width and height are 8-times of input image). In SR-CycleGAN V2, we replaced transposed convolution layer of the previously proposed SR-CycleGAN's generator [2] with pixel-shuffling layer. (2) Removing downsampling from SR-CycleGAN's generator Typical SR approaches based on FCNs (e.g. SRGAN) do not have downsampling layers in their generator networks. Downsampling layers in the SR network might destroy detailed information of appearances when making feature maps smaller. We removed downsampling and upsampling layers (which rebounds features maps to its original size after downsampling) from the conventional SR-CycleGAN generator. Application to SR of clinical CT-level image into lCT-level image In our experiments to perform SR of clinical CT-level image into lCT-level image, five clinical CT and five lCT volumes are used for training (unpaired dataset). We conducted 8-times SR of clinical CTlevel image in resolution. We set the width and the height of the network's output are 8-times of those of inputs. The input is a clinical 2D CT patch cropped from input clinical CT volumes. The output is a high-resolution lCT-like patch. We evaluated the proposed method on five clinical CT volumes and one lCT volume for lung cancer specimens obtained after lung resection surgeries. The resolution of the clinical CT volumes were 0.625 9 0.625 9 0.6 mm 3 /voxel. lCT volume resolutions were in range of 42-52 lm 3 /voxel. For qualitative evaluation, we input patches cropped from clinical CT volume into the trained model. Figure 1 shows SR result of clinical CT volumes produced by proposed SR-CycleGAN V2 and conventional SR-CycleGAN. In results of the SR-CycleGAN V2, the boundary of veins and bronchus became clearer. Furthermore, artifacts that are observed on lCT images were well reduced. Those artifacts make images blurred. Generated SR results by the conventional SR-CycleGAN contained similar artifacts to those of real lCT. For quantitative evaluation, we used a lCT volume as alternative of clinical CT volumes. Slices of the lCT volume were 1/8-times downsampled by bicubic interpolation. Those downsampled slices were used as inputs to the network of super-resolution. This procedure evaluates how SR results of downsampled lCT volume are similar to the original lCT volume. Results of SR-CycleGAN and SR-Cy-cleGAN V2 for downsampled lCT volumes are shown in Fig. 2 . PSNR (peak signal-to-noise ratio) of the results of SR-CycleGAN V2 was 12.73, which was better than 11.95 of conventional SR-CycleGAN. Removing downsample layer and adding pixel-shuffling layer inside SR-CycleGAN's generator enhanced both quantitative and qualitative performances. Our proposed SR-CycleGAN V2 successfully performed SR of clinical CT images to lCT scale with unpaired training datasets. Current clinical practice recommends performing series of lung CT scans at different time-points in order to have a closer follow-up of a patient's malignant nodules. This means that radiologists have to detect and match nodules in different CT scans of the lung, as well as to evaluate their changes in size, density or morphology. One of the most crucial and cumbersome tasks is to re-identify nodules detected in the current study on previous CT scans. Currently, automatic tools that address this task align lung CT images using registration techniques. Although these techniques, especially the non-linear ones, report accurate CT alignments, they are still slow and introduce some distortions in the intrinsic structure of the lung, hindering their wide clinical acceptance. Therefore, more research is still needed to reliably include the nodule matching in automated tools. To address this problem, we propose building a siamese neural network (SNN) [2] . An SNN is composed of a feature extraction component in which two sub-networks (with shared architecture and weights) process a pair of images at a time to produce two embedding feature vectors directly from the images. A second component (the classification head) aims to classify whether the two embedding feature arrays are similar or not. Currently, obtaining enough annotated series of lung CTs to properly train an SNN from scratch is hard and expensive. Therefore, we propose configuring the sibling networks of the SNN with a pretrained convolution neural network from a previous study [1] , aimed at classifying pulmonary nodules. In the present work, we retrained and improved that network using a 3D ResNet-34. As described in the original paper, the network expects patches of 32 9 32 9 32, cropped around the center of the annotated nodules. This network was trained from scratch using a large number of nodule candidates ([ 750 K) from the LUNA-16 challenge. For this study, we froze the weights and removed the fully connected (FC) layers of the pre-trained network to use it as the backbone of the sibling networks of the feature extraction component of the SNN. Since it is hard to know a priory which layer from the pretrained network can provide the most appropriate features, we configured different SNNs using different features maps (from the last layer of each of the 4 convolution blocks that form the pre-trained network). Regarding the classification head component of the SSN, we configured it with an L1-pairwise distance, a flatten, and an FC block layer. The FC block comprises an FC layer (with 64 units), a batch norm, a ReLU, a dropout layer and a final FC layer (with one unit). Figure 1 shows the SNN architecture for the nodule re-identification problem. To allow a fair comparison between the different SNNs we trained them using the same settings. Thus, binary cross-entropy was set as the loss function, the number of epochs was set to 150, the learning rate to 1e -4, the batch size to 8, dropout to 0.3, the early stopping to 10 epochs, and Adam was used for optimization. Moreover, random rotation, flip, and zoom were applied for data augmentation. A dataset consisting of 151 pairs of thoracic CT scans (cases) taken at two different time-points (T1 and T2) were collected from the Vall Int J CARS (2020) 15 (Suppl 1):S1-S214 d'Hebron University Hospital (Barcelona) and were annotated by two different specialists. Per each case, radiologists detected and matched the most relevant nodule and annotated its location and diameter at both time points. To train the SNNs, we built a balanced dataset using as positive cases (label = 1) all the annotated locations of the matching nodules at T1 and T2. As negative cases (label = 0), we used the nodule locations at T1 together with a random nodule location of the annotated nodule locations at T2 (avoiding to select correct nodule location). Random stratified sampling was used to partition the data into training (75%) and testing sets. Also, we optimized the different SNNs with the training data using stratified 10-fold cross-validation, and we tested them with the testing set. The best SNN, the one using features from block1, achieved in the test set an accuracy of 0.921, a precision of 0.905, and a recall of 0.944, in less than 0.1 s, Table 1 . Different factors may explain the successful performances reported by this method. First, the SNN architecture itself, as it is specifically designed to compute an accurate similarity-based function relying on features extracted automatically from the network. Second, the use of transfer learning, as it enables to properly initialize the weights of the SNN, avoiding the need of having a large training set. Third, the configuration of different SNNs using feature maps from different layers, as it allowed discovering which one worked better. Finally, the use of 3D convolutional layers, as, in contrast to 2D layers, it permits to encode more spatial information of the nodules and, therefore, to re-identify them more effectively. In this article, we propose a novel, accurate and fast approach for pulmonary nodule matching relying on SNNs. Performances achieved by the best model are in concordance with the state-of-the-art. The advantage of this approach is that it does not require a prior registration process, avoiding any deformations in the lung images, and completing the re-identification process in a time-efficient manner.  Since a surgeon requires a thorough understanding of the patientspecific anatomy prior to surgery in order to minimize the risk of positive surgical margins and other surgical complications, nephronsparing surgery (NSS) is used for Wilms' tumor (WT) patients in a limited number of cases. 3-Dimensional (3D) anatomical modeling is increasingly used to improve preoperative planning of NSS for WT patients. However, current 3D imaging techniques for renal surgery have limitations in image quality, usability, duration, and costs. Through a collaboration between the radiology and pediatric surgery departments and 3D imaging specialists, a workflow was developed to overcome these limitations. The 3D planning workflow combines a specific imaging sequence, a standardized segmentation protocol and Augmented Reality (AR) visualization additional to in-house 3D printing for fast, high usability and cheap 3D preoperative planning. A high-resolution non-contrast enhanced magnetic resonance angiography scan was added to the magnetic resonance imaging (MRI) protocol. The MRI sequences were segmented with the use of a standardized segmentation protocol in an open-source software package. The resulting 3D models were visualized in AR through a head-mounted display (HoloLens) and 3D printing with the use of an in-house 3D printer, Fig. 1 . Five WT patients considered eligible for NSS were preoperatively planned through the proposed workflow. The models were computed and visualized within a day after the preoperative MRI. The AR visualization software was fast, free to use and allowed adequate handling of the 3D holograms. The 3D printed models were regarded as convenient and practical for intraoperative orientation. The patientfriendly, fast and low-cost 3D imaging workflow was easily implemented and appeared to be of additional value for the preparation of NSS. Keywords fMRI, Human brain decoding, Domain adaptation, CNN Decoding human mental content (e.g., selecting ''cat'' from multiple categories when a subject is actually seeing a cat on a screen) solely from fMRI brain activity is an important objective for the neuroscience and brain-machine interface fields. However, because individual brains differ significantly, it is extremely difficult to interpret the mental content of one subject based on data from another subject. Domain adaptation (DA) is a transfer learning method in which a model trained on a source distribution is applied in the context of a different target distribution. In this study, we first constructed a deep learning model that can decode the mental contents of an individual and then examined the applicability of DA. We assumed that there are no labels in the target data, which is a difficult but realistic setting defined as unsupervised DA. In this setting, we have source data (i.e., fMRI brain activity data and corresponded labels) and target fMRI data without any labels. To analyze the performance of unsupervised DA, we compared it to other methods without DA, namely a linear support vector machine (SVM) and convolutional neural network (CNN) devised for predicting categories. To determine if unsupervised DA is applicable to fMRI brain decoding, we adopted the method proposed by Saito et al. [1] . This method attempts to align source and target distributions by utilizing task-specific decision boundaries. Task-specific classifiers are used as discriminators that attempt to detect target samples that are located far from the support of source samples. A feature generator learns to generate target features close to this support to fool the classifiers. Because the generator uses feedback from task-specific classifiers, it can avoid generating target features that are close to class boundaries. In this study, we applied our method to an fMRI brain dataset. Figure 1 presents the scheme for applying unsupervised DA based on subject A to subject B. First, we train a CNN model using source data from subject A, then train the model to adapt to the data from subject B. Ideally, this framework enables a CNN classifier to predict ''cat,'' even if the data labels from subject B are missing. We implemented a CNN model architecture to process fMRI 3D volumes. The network contains five convolutional layers and one fully connected layer. It has a max pooling layer after the first convolutional layer and an average pooling layer after the last convolutional layer. In this study, to train the CNN and applying DA, we adopted the following hyperparameters: Note that we used source data from five subjects to prevent overfitting and used the rest of the data (i.e., one subject) as a target (i.e., subjects 1, 2, 3, 4, and 5 as source data and subject 6 as target data). For comparison, we tested a multi-class linear SVM and CNN with the same network structure and hyperparameters as the DA CNN. We used the Haxby fMRI dataset [2] for our experiments. This is a block-design fMRI dataset from a study on the representation of faces and objects in the human ventral temporal cortex. • Six subjects viewed objects belonging to eight classes (bottles, cats, chairs, scrambled images, faces, houses, scissors, shoes) • Each subject viewed 864 stimuli images, where the number of fMRI volumes corresponded to the total number of images (12 sessions). We used 720 fMRI volumes (10 sessions) for training and 144 volumes (2 sessions) for testing. • The brain activities in the fMRI volumes from the subjects are represented by 40 9 64 9 64 3D voxels. To prevent overfitting, we selected voxels from the full ventral stream, where voxels were contained for at least one subject, resulting in the selection of 1963 voxels. For each subject, we normalized their brain activities to obtain a Z-score. We present the prediction accuracy results in Table 1 and Fig. 2 . SX-Y-[ SZ indicates that subjects from SX to SY are grouped as the source and subject SZ is the target for DA. Note that the chance rate is 12.5%, and asterisks indicate that a classifier outperforms the other classifiers with a significance value of at least 1% after ten trials. As shown in the table, the DA CNN outperforms the other classifiers in most cases with a significance level of at least 1%. It is difficult to decode the mental contents of a human brain based on data from another human brain owing to differing brain activities among individuals. To address this problem, we examined the applicability of DA to fMRI brain activities, where no labels were given for a target subject. We first constructed a deep learning model to decode the mental contents of individuals. We then examined the applicability of DA by comparing the developed model to other methods without DA, namely a linear SVM and CNN. With accuracy exceeding the chance rate, we observed that most DA subjects significantly outperformed non-DA subjects. Int J CARS (2020) 15 (Suppl 1):S1-S214 However, better accuracies have been reported previously. Based on a lack of training data, the unsupervised DA was unable to obtain high performance. As noted in [1] , most datasets for DA are very large (e.g., photo images), allowing CNNs to achieve better performance than that achieved for the dataset considered herein. Future work will include applying unsupervised DA to larger fMRI datasets.  The purpose of this paper is to present the first version of a statistical atlas of lung lesions. The lesions considered in this study are of different nature and different biological substrates. Despite these lesion types are very common, they may be partly associated with lung tuberculosis because they were detected, isolated, and segmented on lung CT images of tuberculosis (TB) patients. The ultimate goal of this study and related free electronic resources is to provide the relevant part of the society for 3D statistical maps of the frequency of benign lung lesions of different kinds. A total of 388 CT images of lung tuberculosis patients aged from 13 to 87, mean age 42.2, STD 15.5 years treated in a national tuberculosis center were used in this study. The following six lesion types have been considered: Foci including infiltrations and their mixes (381), Caverns (164), Fibrosis (122), Pleurisy (22), Atelectasis (10), and Pneumothorax (9). The numbers given in parentheses correspond to the number of patients in which at least one lesion of the given type is presented. It should be noted that it is a reasonably common situation when several lesions of the same type or different types are presented in one single patient simultaneously. The lesions in all CT images were labeled manually by a chief radiologist. The pipeline of creation of the statistical atlas of lung lesions assumes selection of the ''most typical'' (reference) CT image followed by non-rigid registration of all the rest CTs to the reference image. Finally, the calculated non-rigid transformations are applied to the lesion masks. Overall, the process of generation of the statistical lesion atlas includes the following steps. In addition, the positions of CT scans in the feature space together with the centroid and the reference patient were plotted for visual examination and validation purposes. This was done by way of projecting N-dimensional space to 2D using the Multi-Dimensional Scaling (MDS) and t-SNE methods. As a result of the implementation of the pipeline steps aiming at the selection of a reference subject, it was found that the visual examination of the resultant CT image of the automatically selected reference patient is well agreed with the expectations of medical and image analysis experts. The t-SNE visualization method fails to produce convincing results (e.g., it places the centroid and its closes CT scan to a border of actual CT image cloud) whereas the MDS produces a very realistic picture (Fig. 1) . Figure 2 illustrates the resultant statistical atlases of different lesion types which are represented here in the form of corresponding heat maps. Detailed information is freely available for download and validation from a dedicated web site [2] in the form of animated pictures as well as the 3D probability maps. Conclusion Results obtained in this study allow to draw the following conclusions. 1. To our best knowledge, the version of the lung lesion atlas resulted from this study is the first public electronic resource of its sort. 2. The location and the frequency of appearance of lung lesions of specific types documented by the atlas are agreed with the opinion of the experts. Finally, it should be noted that in this paper we reporting our work-in-progress. It is planned that the amount of original image data will be extended further and the necessary comparisons with alternative methods would be provided. Keywords Echocardiography, Image segmentation, Cardiac surgery, Surgical navigation Purpose Segmentation of echocardiography, or a cardiac ultrasound sequence, is a difficult problem due in part to ultrasonic reflections being highly noisy, subject to dropout of signal, and tissue movement over time. Existing methods have focused primarily on diagnostic uses in transthoracic echocardiography (TTE), which are not directly applicable to intraoperative transesophageal echocardiography (TEE). Recent work demonstrated that an accurate segmentation of the cardiac tissue in routine intraoperative TEE can be registered to preoperative cardiac computed tomography (CT) [1] , which provided a visual alignment that could be valuable to surgeons. That method for cardiac TEE segmentation required a manual initialization each time that error rates were intolerably high or when the ultrasound probe was physically moved; this manual work is intolerable in a surgical setting. We propose a method that takes advantage of the typical cardiac structure seen in TEE to eliminate the need for manual initialization. Our study used intraoperative TEE echocardiography sequences acquired from three patients, having all four cardiac chambers visible in each sequence. For each image in a given sequence, we computed a noisy estimate of pixels on chamber boundaries by thresholding the image's median intensity and computing the Sobel operator at each pixel. These computations produced our target dataset. For each image of an echocardiography sequence, we refined our model in a four-level hierarchy. First, cardiac chamber center positions were estimated by registering a 4-point set of potential cardiac chamber centers, based on an estimated population average of heart size; the registration target was pixels sampled from blood pools in the image and the algorithm was rigid coherent point drift [2] . Second, the 4-point set registration was nonlinearly refined to the blood pools of the chambers by using a constrained expectation-maximization algorithm. Third, ellipses were Int J CARS (2020) 15 (Suppl 1):S1-S214 nonlinearly registered to the cardiac chamber boundaries by using a coherent point drift algorithm. Finally, a local non-linear refinement to chamber boundaries was computed by using a constrained expectation-maximization algorithm. A substantial speedup was attained by using the temporal coherence of adjacent images in the TEE sequence. The optimized segmentation of an adjacent image was used as the initialization for the segmentation of the current image, with the goal of frame-rate computations that did not penalize the segmentation accuracy. To evaluate the segmentation, for each image of a TEE sequence the root mean square (RMS) distance from each model boundary point to its nearest neighbor in the target dataset was computed. These RMS errors were plotted for visual assessment. Using an ordinary desktop PC, segmentations were computed faster than the echocardiography frame rate of 53 images per second. The four stages of the segmentation hierarchy are shown, for a representative patient echocardiography image, in Fig. 1 . The RMS errors for the three sequences are shown in Fig. 2 . The quasi-periodic fluctuations in the plots had peak errors when the heart valves were open and the assumptions of the segmentation were not entirely applicable; presumably the quasi-periodic nature is due to cardiac arrhythmia in these patients. The minimum RMS error ranged from 0.5 to 3.5 mm. Individual walls of the cardiac chambers were segmented to near-zero error and the larger error arose from imperfect segmentation of the heart valves. Conclusion This work demonstrated that TEE echocardiography sequences can be segmented in a fast, accurate hierarchical algorithm that is suitable for surgical use. The segmentations are of sufficient quality to act as input to a larger system that registered echocardiography to a 3D CT model of the heart [1] . The hierarchical algorithm was successful in eliminating the need for manually outlining the relevant cardiac features during transthoracic minimally invasive beating-heart surgery. The algorithm incidentally provided estimates of cardiac functional phase, particularly when heart valves were open. Knowing the functional phase can be important information in producing a subsequent robust registration method for surgical navigation.  Superficial femoral artery (SFA) calcification segmentation is the critical step and basic for assessing the calcification degree to assist doctors in diagnosis and surgical planning. However, the calcification in computed tomography angiography (CTA) images has small volume, and is mainly scattered along the vessels which are in complicated background and possibly have long occlusion. The uncertainty of the dose and injection time of the contrast medium also cause the overlap of the HU range of vessel and calcification. To address the above problems, this work develops a method that can accurately and robustly segment calcification from SFA-enhanced CTA for calcification assessment. The proposed calcification segmentation method is the main algorithm of CTA superficial femoral artery calcification analysis. As is illustrated in Fig. 1 , the complete analysis process consists of four main steps. In the first step, fixed-size volume patches are generated along the manually extracted vessel centerline. In the second step, data pre-processing is applied on each patch to adjust the range of voxel values. In the third step, a vessel-calcification spatial and shape-aware network segments calcification in each volume patch. The network integrates spatial attention gates [1] and a shape-aware stream [2] into the fully convolutional network U-Net. The attention gate enhances the weaken calcification features due to small size by calculating the spatial distribution probability map of calcification, which is multiplied with the input feature maps subsequently. The shape-aware stream extract vessel and calcification boundaries to analyze the shape and the spatial relationship of the two objects. In the training phase, the network outputs segmentations and boundaries of calcification and vessels and is supervised by accurate calcification annotation and threshold-based coarse vessels annotation with multi-task loss. In the test phase, after the well-trained model outputs segmentation results of all patches, the category of each voxel that is oversampled in different patches is determined by voting strategy based on the majority principle. In the last step, calcification assessment measuring calcification volumes and distribution density is performed to demonstrate the application of obtained segmentation results in calcification analysis. The clinical dataset for retrospective study includes CTA images of 56 patients with 112 SFAs which was collected from the hospital. In this work the dataset was divided into 56 training sets, 24 validation sets, and 32 test sets. The proposed method was evaluated on the test sets. Table 1 lists average performance measurements with the proposed method of baseline methods. The segmentation Dice score of the proposed method is 78.16%, and the detection precision and recall of calcification in transverse plane achieve 86.13% and 96.42% which indicate that few calcifications are missed. Fig. 2 shows the segmentation results of different methods and 3D U-Net on data with different calcification-vessel HU range. Compared with 3D U-Net, the proposed method has better robustness especially in the bright vessel image, in which case Dice score are improved by 11.81% and 12.03% respectively. The correlation coefficient of linear regression between the calcification volume calculated from the truth and the segmentation prediction is R 2 = 0.9720, and the average relative error is 11.5%. Calcification density distribution can effectively point out the density of calcification distribution and provides an intuitive reference for calcification assessment. Compared with baseline, the proposed method can accurately and robustly segment calcification from SFA enhanced CTA. The results demonstrate the feasibility for calcification assessment. Calcification distribution density as proposed parameter can provide an intuitive reference for calcification assessment in clinic. The proposed method can be used to obtain the position, size, and morphology of calcification in the vessel wall to evaluate the degree of calcification and assist doctors in intraluminal planning interventional surgery. It can be also used to build a vessel-calcification model, which can be fused with intraoperative imaging to assist doctors in intraoperative analysis. Purpose Image fusion (IF) of live x-ray (XR) fluoroscopy with pre-interventionally acquired 3D anatomic models has been proven to be useful in transcatheter aortic valve implantation (TAVI) procedures [1] . Due to the static nature of the anatomic models, the superposition is not adapted to the respiratory motion, nor to the motion caused by the heartbeat or the procedure itself. This results in inaccuracies of the IF. Especially, during the implantation of the artificial valve itself, a displacement caused by rapid pacing occurs. These inaccuracies might be reduced by a motion-compensated overlay, which could result in a lower dose of contrast agent, X-ray exposure and interventional time. To achieve full compensation, the complete motion must be considered. By tracking a structure showing a similar motion pattern to the prosthesis and being visible during the implantation, a fully motion-compensated IF can be achieved. It is the objective of this study to identify a suitable target for motion-compensated IF during TAVI. For motion compensation different possible target structures readily visible in the X-ray fluoroscopy data were identified. The investigated target structures include (1) the tip of the rapid pacing catheter (ptip), (2) the edge of the diaphragm (dia), and (3) an obvious point of the artificial valve or its guide wire close to the implantation plane (vref). For motion estimation, the translation of each investigated target structure was identified. The tracking of the catheter tip was performed by template matching using normalized cross-correlation. Since tracking of a point close to the valve and tracking of the diaphragm was not reliably possible with the cross-correlation approach, these two structures were tracked manually. For (1) and (3) x-and y-component of the translation was considered, while doe (2) only the y-component was used for deriving the final displacement vector. Outlier correction was performed applying a 3-point median filter. The feasibility of the investigated targets for motion-compensated IF during valve deployment was tested in 6 randomly chosen cases (4 balloon-expandable, 2 self-expandable). In all cases, an anatomic model of the aorta was automatically derived from pre-interventional CT data. Initial registration was performed manually and the anatomic model superimposed onto the x-ray image. During the deployment of the valve, the location of the model was adjusted according to the identified translation of the different target structures. The quality of the motion compensation was analyzed blinded on a 5-point Likert scale ranging from 1 (very good compensation) to 5 (very poor compensation) by two experienced experts in the field. Non-motion-compensated reference data were added as a reference. Results are presented as median (mdn) and interquartile range (iqr). An example of the investigated motion compensation targets is presented in Fig. 1 . Where without compensation the valve plane (solid line, (a)) does not match the anatomic model (dashed line), after compensation using the vref target perfect alignment of the actual valve plane and anatomic model can be achieved. ptip (d) and dia (c) show clearly inferior performance. The rating frequencies are provided in Fig. 2 . With 67% rated very good the vref target (mdn = 1, iqr = 1) clearly yielded best performance, followed by the ptip (mdn = 2.5, iqr = 1) and dia (mdn = 2.5, iqr = 1) target. Not surprisingly, the non-motion-compensated data ranked worst (mdn = 3, iqr = 2). Where the vref target provided best marks in all cases, in one case dia and rtip and one additional case dia performed inferior to non-motion compensation. Conclusion An obvious improvement of the IF can be achieved by applying a motion-compensated overlay of the anatomic model. The motion of suitable target structures can be transferred to the model overlay to adjust its position and to improve IF during valve deployment (implantation) in TAVI procedures. As expected, the target structure closest to the implantation plane (e.g. the artificial valve) performs best, yielding an almost perfect IF during deployment of the valve. The usage of the diaphragm or the rapid pacing catheter motion on average improved the quality of the model overlay, both perform clearly inferior to the valve approach. Motion compensation by tracking the diaphragm lead to overestimated amplitudes in the y-direction (due to the significant differences between the motion amplitude of the diaphragm and the heart) and lack right-left motion components and any motion due to the heartbeat. Even though the rapid pacing catheter tip is performing inferior to using the valve itself, it is clearly visible in XR and relatively easy to track, thus resulting as a promising candidate for motion compensation during TAVI at stages where the valve is not yet in place. By considering the impact of the C-arm angulation on the extracted coordinates of the catheter tip, a further improvement of the tipmotion-compensated image fusion can likely be achieved.  Identification of ablation target sites in the radiofrequency (RF) catheter ablation of the complex ventricular and atrial arrhythmias relies on electroanatomical substrate mapping, using commercially available systems like CARTO (Biosense Webster) and EnSite NavX (St. Jude Medical). Accurate representation of a catheter tip equipped with a localizing sensor is the key element of the currently used 3D electroanatomical mapping systems. Beside the fact that the mapping is time-consuming, hampered by the intermural location of some scars and can be limited by epicardial fat, the need for the proprietary localization systems significantly increase the costs of the intervention, require additional equipment in the intervention space and are partly only applicable with special catheters, limiting the flexibility of catheter choice during the intervention. Integration of various imaging modalities during RF ablation and imaging guidance have been proven to be advantageous, providing detailed anatomical information, characterizing myocardial scars, and delineating epicardial fat [1] . If the 3D position of the RF ablation catheter tip could be extracted from the real-time biplane x-ray (XR) fluoroscopy only and overlaid on the target anatomy derived from e.g. pre-procedural imaging, a vendor independent solution providing high flexibility in catheter selection appears feasible. The purpose of this work was to apply an artificial neural network (NN) to fully automatically identify the RF ablation catheter tip in biplane 2D XR fluoroscopy for the reconstruction of the 3D position of the catheter tip with subsequent anatomical overlay. Since complex ablation procedures in our institution are performed using CARTO system exclusively, only few XR data are available for training (only XR images of the target anatomy in three different angulations used by the CARTO system for registration). Since NNs are well-known for their generalization capability, the idea was to investigate the feasibility of training the network on similar structures (e.g. the rapid right ventricular pacing catheter tip in the XR fluoroscopy data from transcatheter aortic valve implantation (TAVI) procedures) and application of the trained model for the localization of the RF ablation catheter tip. Methods End-to-end U-net model, consisting of a convolutional part downsampled with the maxpool layer and strided transposed convolutional upsampling part in combination with drop out regularization and residual connection to the output was implemented using Keras deep learning library and trained on Tesla K80 GPU. Training data were generated from TAVI procedures performed at Ulm University Medical Center. The rapid pacing catheter tip was tracked over XR run with template-matching using normalized cross-correlation in Matlab to create the binary mask. 13,164 image pairs of 512 9 512 pixels resolution consisting of lossless grayscale 2D XR images acquired in different orientations and its segmentations as corresponding binary masks were used for training and validation of the NN. Test data comprises 155 XR frames (usually acquired in AP, RAO 30°, and LAO60°orientations) acquired during 6 different ablation procedures. 3D position of ablation catheter tip was reconstructed from two 2D XR projection images acquired at different angulations utilizing the basic principles of epipolar geometry. The reconstructed 3D point corresponding to the tip of the ablation catheter was overlaid on the 3D model of the respective anatomy, obtained out of pre-interventional CT imaging. Results Figure 1 (left) shows an example of training and test datasets. Besides the RF ablation catheter, further catheters (e.g. coronary sinus (CS) and right ventricle (RV) catheters with evenly spaced electrodes, ICD probe with respective electrodes, loop-shaped Lasso or PentaRay mapping catheters, etc.) are present in the XR fluoroscopy data during the ablation procedures. In all test datasets the most prominent structures such as tips or other electrodes of different catheters could be detected by the NN. Ablation catheter tip exclusively ( Fig. 1 , right, AP, RAO30°, LAO60°) or together with few electrodes from other catheters could be localized in 100 frames. In the remaining 51 frames NN put its strong attention on detection of evenly spaced structures of the CS and RV catheters ( Fig. 1 , right, RAO38°). Manual registration of XR fluoroscopy and the CT-derived anatomical models of right (yellow) and left (orange) atria was Int J CARS (2020) 15 (Suppl 1):S1-S214 performed based on the alignment of the lasso catheter in two acquired projections (AP and LAO59°) along the wall of the left atrium (Fig. 2, left) . The locations of the ablation tip automatically identified by NN are highlighted in either XR projection image (AP and LAO59°) on Fig. 2 (right). 3D position of the ablation catheter tip within the 3D model of the right atrium ( Fig. 2 , right, red point) was reconstructed from these two identified locations (Fig. 2 Purpose Screening for coronary artery disease is a major health issue, knowing that the most common cause of death in industrialized countries is cardiovascular pathology (coronary artery disease, stroke, other cardiovascular disease). Computer aided diagnosis systems (CADx) assist cardiologists and play a key role in detecting abnormalities and treating coronary arteries. In practice, the coronary angiography (interventional diagnostic procedure using transcatheter access and injections of radiopaque contrast to detect lesions in coronary arteries) is the image acquisition modality used to detect these abnormalities. We present in this paper a classification method based on transfer learning and fine tuning of three convolutional neural network architectures (CNNs) namely, InceptionV3, VGG16 and MobileNet [1, 2] . The aim of our method is to classify the coronary arteries in two classes (left or right coronary artery, diseased or normal coronary arteries) and selecting the best architecture in the field of coronary angiography image analysis using Deep Learning algorithms. Experimentations have been conducted using an anonymized database from a Belgian hospital (CHR Mons-Hainaut) thanks to a retrospective study. The angiographies acquired were pseudo anonymised with a cryptographic hash algorithm at the hospital. Thanks to a custom application, the images from the videos (angiography X-rays) were tagged by an experienced cardiologist and associated to several classes: Left coronary artery (abbreviated LCA), Right coronary artery (abbreviated RCA), Normal arteries, Grafts and Non-coronary artery (abbreviated NC) as well as fallen into Disease and Non-Disease class based on typical 70% stenosis cutoff value. We created an annotated anonymized database with around 2.250.000 images. An angiogram is usually composed of 60-120 images like in Fig. 1 . The experiments were carried out with 30% of the anonymized database. We used the data augmentation technics to artificially increase the dataset and so significantly improve results. Results Table 1 presents an overview of the results obtained. We divided each dataset into two: training 80% and validation 20%. Each experiment was executed over 10 epochs and with a batch size of 10. We also evaluated the capacity of these CNN architectures with a dataset splitted into three parts: training: 70%, validation: 20% and testing: 10%. The following Fig. 2 presents the results with the LCA/RCA dataset and InceptionV3. The experiments in this study were conducted using a Linux cluster node with 32 CPU cores using a single NVIDIA GeForce GTX 980 with 4 GB memory. Using Keras 2 with Tensorflow 1.8 backend as a deep learning framework. In this paper, we used a big dataset (around 160 GB with 2.250.000 anonymized images from a Belgian hospital) in order to detect coronary artery disease. We achieved a high accuracy score of 92% to detect the type of artery and only 74% to detect diseased artery. Further investigations can be done using LSTM and TCN classifiers with images preprocessing algorithms to improve screening results for diseased arteries. heavily on composite score-based classifications of image-or physiology-based metrics that describe the extent of disease (such as the SYNTAX score: scoring of anatomical complexity of coronary lesions), which are currently -if at all-, calculated by ''hand''. In addition, the interval of guideline publications cannot keep the pace of the accumulating published evidence. In conclusion, implementing evidence-based cardiac care is currently a time consuming, complicated, and not necessarily reproducible task. We hypothesize that computationally augmented decision making in the Heart Team will overcome these obstacles, and thus will improve patient outcomes. To develop a therapy decision support system (TDSS) for heart teams one need to consider the complete decision-making workflow from the source of evidence-based medicine to the clinical decision making. This includes sources for data and domain knowledge, appropriate machine intelligence (MI) methodologies and tools, required expertise, use cases and legal constrains for decision support, and communication interfaces. Various data-driven MI-methodologies exist (e.g., deep neural networks, biophysical models, Arden Syntax, and probabilistic graphical models), each has strength and weaknesses (e.g., self-learning, transparency, readability, and computation complexity) and are therefore more or less suitable to support individual clinical tasks and decisions. A Bayesian network, a probabilistic graphical model, is specifically suitable to support comprehensive interdisciplinary therapy decisions [1] , with a complexity by means of required transparency, human readability and reproducibility, limitations in data access, required modelling by domain expert. MI-based methods usually require large datasets of high quality (i.e., complete, correct, and diverse). Data access is nowadays still limited to institutional sources, and data pre-processing remains a time-consuming manual process. In the next years, new data and communication standards will provide the necessary accesses [2] ; especially promising are HL7's FHIR standard for HER interoperability and the IEEE SDC standard for vendor-independent interoperability of medical devices. The following presents the concept of a Virtual Heart Team (VHT): an MI augmented environment for evidence-based and data-driven therapeutic decision support, see Fig. 1 . Within this concept we consider the general principles of A) Evidence-based Medicine and extend it to a B) Model-based Medical Evidence [2] . In evidencebased medicine the evidence originates from C) Clinical trials that 1) provide the raw data. The raw data include all types of the study participant's records (e.g., personal information, radiological images and genetic analyses) that are gathered routinely in such trials. Further D) Processing and analysis of the data leads to 2) knowledge and publications, which are then used by E) Guideline committee meetings (e.g., the European Society of Cardiology and the American College of Cardiology) to modify existing or develop new 3) guidelines and recommendations. A F) Clinical specialists, representing the G) local expertise of a heart team, apply this knowledge when reviewing the H) patient data to 4) enable appropriate I) therapy decision making and communication at J) heart team meetings, K) patient communication, and in the L) operating room (OR). In the concept of model-based medical evidence, we consider the processes of evidence-based medicine as described but extend them by the M) Development of a Virtual Heart Team; a therapy decision model that integrates and constantly updates the evidence-based knowledge of a heart team. The development begins with the N) Graph modelling that 5) results in a O) VHT graph Model, which is the 6) basis for its P) Probabilistic Modelling. The VHT probabilistic graphical model is semi-automatically developed and maintained through appropriate expert-modelling and machine learning tools. However, a team of Q) Medical knowledge engineers 7) coordinate and support the entire modelling process to ensure a coherent model. They provide Q) repositories of data and tools with engines for expert modelling, [2] ; an information and model communication interface using established data format and communication standards. For the graph model, the guideline committee 9) provides the fundamental knowledge. Ideally, the medical knowledge engineers would provide appropriate modelling tools to support the guideline writing processes at the guideline meetings and at the same time compile the desired graph models. Otherwise, the information can be 10) extracted from the guidelines' underlying analytical results and publications using NLP tools and the support of the knowledge engineers. Once a basic graph model exists, the probabilities can be trained (by means of machine learning) or modelled by experts. Ideally, from the raw data 11) provided by the clinical trials, 12) provided by hospitals through a common or connected MIMMS, and 6) completed by guideline experts when appropriate data is missing. The resulting VHT model is a comprehensive representation of the available knowledge and can 13) serve the guideline committee for a more transparent and comprehensible communication, and 14) serve clinical trial sponsors to target forceful hypotheses. Finally, hospitals will need appropriate tools and repositories to 15) integrate a T) VHT model-based therapy decision support system. With 16) provided patient data, the system 17) infers a U) patient-specific VHT model, which enables a model-based decision support. We conceptualized a VHT development to provide transparent, human-understandable, and reproducible decision support to heart teams. The VHT's strengths are its underlying knowledge base, which is based on medical evidence and enhanced by expert knowledge. To enable the model development will require support from guideline committees and access to raw data, which is the bottle neck the development. Methods and tools for machine learning and expert modelling exist but will need further development. To ensure the quality of a VHT-based therapy decision support system, the model probabilities may require population-related adjustments and validation on site. A quality management concept for probabilistic graphical models exists, but will also need further optimization. Purpose X-ray imaging is a standard diagnostic procedure to detect and classify proximal femur fractures. These types of fractures are mostly common in the elderly population, as the risk of suffering a fracture above the age of 65 increases exponentially. The most common treatment is to proceed with a surgical procedure. Early detection and classification of proximal femur fractures are essential for the indication of surgery and, if so, to choose the adequate surgical implant. In order to determine the optimal treatment procedure or intervention, the vascular anatomy of the femur plays a vital role. In this context, the Arbetisgemeinschaft für Osteosynthesefragen (AO-Foundation) established a generally applicable and valid classification system for fractures of all bones in the skeleton based on radiographs, including the proximal femur. In this standard, fractures of type ''A'' are located in the trochanteric region, whereas fractures of type ''B'' are found in the femoral neck. Further subdivision of classes A and B depends on the specific location of the fracture and its morphology. However, several years of training are needed, and inter-reader agreement ranges between 66 and 71% for trauma surgery residents and experienced trauma surgeons, respectively. Due to the success of Convolutional Neural Networks (CNNs) to automatically extract rich features from natural as well as medical images, we rely on a CNN to predict the classes. Our approach is derived from Curriculum Learning (CL), itself grounded on the starting small concept. Examples are presented gradually increasing the difficulty, i.e. easy examples are presented before ambiguous ones. Following this principle, in [2] the authors proposed to reorder the training images based on medical knowledge to ease the training of the CNN and achieved an improved performance. Instead of reordering, in this work we propose a new deep curriculum strategy to leverage the hierarchy of the labels and improve the baseline classification performance. We design a hierarchical dynamic curriculum [1] that goes from the easier 3-class to the harder 7-class problem. Effectively, this is implemented by dividing the cross-entropy classification loss into two terms as shown in Eq. (1): From evidence-based medicine to model-based medical evidence: Modelling a virtual heart team for therapy decision support Int J CARS (2020) 15 (Suppl 1):S1-S214 S39 The first term aims to differentiate between only 3 classes: fracture type A, type B, and not-fractured. The second term subdivides each fracture class (A and B) into 3 subclasses: A1-A3 and B1-B3, resulting in a total of 7 classes. Since the output of the network is the probability distribution for the 7-class problem, we can aggregate the probabilities to obtain a probability distribution for the 3-class task, as depicted in Fig. 1 . The final loss is a weighted combination of the individual losses, their contribution is regulated by the parameter a. Whereas in multitask learning, typically, their weights would be equal and fixed, our approach updates them dynamically. We define the curriculum strategy relying stronger on the 3-class loss at the beginning and moving towards the fine-grained classification after some epochs. This approach is usually compared against its inverse, so called anticurriculum. In this case, the contributions of the losses is reverted, the 7-class loss is given more importance at the beginning of the training. The value of a is defined as a function of the current epoch as in Eq. (2) (see also Fig. 2 ): where a 0 is a fixed value, e is the current epoch, and E is the total number of epochs. We test the classification performance on the 7-and 3-class problems based on the weighted F1-score. Each model is run 10 times and the mean, median, and standard deviation are reported. We used a ResNet-50 pretrained on the ImageNet dataset. The architecture and curriculums were implemented with TensorFlow and run on an Nvidia Titan XP GPU. Models with curriculum were run for 25 epochs and a 0 was set to 0.6, other models were run for 50 epochs (with early stop if there was no improvement in the last 20 epochs). All models were optimized with stochastic gradient descent and a momentum of 0.9 on mini-batches of 64 samples. We applied the proposed hierarchical curriculum and anti-curriculums approaches to an anonymized dataset composed of 780 patients, with a total of 1347 X-ray images acquired at the Klinikum rechts der Isar, Munich. The dataset consists of 327 type-A, 453 type-B fractures, and 567 non-fracture cases. Subtypes of the fracture classes are highly unbalanced, reflecting their incidence. To address this problem, offline data augmentation techniques such as translation, scaling and rotation were used. The dataset was divided patient-wise into three sets for training (70%), validation (10%) and test (10%). We employ a test distribution that is balanced between fracture type-A, type-B, and non-fracture, and report classification performance based on the weighted F1-score. Clinical experts provided along with the AO classification a square region of interest (ROI) around the femur. ROIs were downsampled to 224 9 224 px to be fit into the model. We compared the performance of our proposed approach against training with only the 7-class loss, for this baseline we found a mean (median) ± standard deviation of 0.5662 (0.5731) ± 0.0423. Applying the anti-curriculum strategy of weighting initially stronger the more difficult problem results into an improvement to 0.6345 (0.6348) ± 0.0184 The addition of the secondary term into the loss helps to reduce errors. Finally, the results for the curriculum strategy (easy-to-difficult) are of 0.6650 (0.6650) ± 0.0174. Both approaches improved over the baseline, and the performance of the proposed curriculum approach is comparable to state-of-the-art results and experienced trauma surgeons [2] . For the 3-class problem, both strategies result in an improvement with respect to the baseline: 0.8063 (0.8171) ± 0.0145, but with similar results between the antiand the curriculum, 0.8566 (0.8623)) ± 0.0145 and 0.8704 (0.8679) ± 0.0118, respectively. We presented a deep curriculum strategy that leverages hierarchical information for the fine-grained classification of proximal femur fractures. Our results indicate that using broader labels helps to reduce errors and improve the overall detailed classification. Pipeline of the proposed method. The input to the network is the radiograph together with the hierarchical classification (e.g. B2 and B). We obtain the 3-class probability distribution by aggregating the probabilities at the output of the network Fig. 2 Value of a at every epoch for the two curriculum and anticurriculum strategies. In the first case, a starts at a high value and decreases smoothly. In the second case, a starts at a small value and increases slowly Int J CARS (2020) 15 (Suppl 1):S1-S214 Keywords 3D printing, patient-specific cast, overnight, distal radius fracture The cast immobilization of the wrist for several weeks is the gold standard in the nonoperative treatment of distal radius fractures. Since several years casts and orthoses based on 3D-printing-technology are of increasing relevance in the nonoperative treatment of different fractures in the upper or lower extremity as well as in the postoperative immobilization after operative treatment. These casts are customized for each patient and printed based on a template that is generated after measuring/filming the affected limb of the patient. The requirements to a new type of cast contain the reproduction of an at least equally good clinical and radiological treatment outcome as well as an at least equally good wearing comfort and safety for the patient compared to conventional casts. Some cadaver-studies, feasibility-and pilot-studies with healthy individuals have been able to highlight those qualities for forearm-casts based on 3D-printingtechnology [1, 2] . So far no prospective randomized controlled trial about distal radius fractures treated with 3D-printed casts has been published. With this study we plan to fill this lack of information. Methods A prospective randomized clinical trial is conducted. Two randomized groups of patients who sustain a non-or little displaced distal radius fracture are compared. Initially, all patients are immobilized in a temporary forearm cast. After randomization, the patients in the intervention group are scanned and receive a customized 3D cast, which is printed in-house. The patients in the control group are immobilized in a conventional plaster cast according to the present treatment standards. Both groups are summoned regularly for control visits. The primary objective is to investigate the patient comfort of the 3D-printed forearm casts in the nonoperative treatment of distal radius fractures compared to conventional plaster casts. The secondary objective is to investigate clinical and radiological outcome parameters (fracture healing, pain, range of motion, grip strength, daily living activities) of the 3D-printed forearm casts in the nonoperative treatment of distal radius fractures compared to conventional plaster casts. Additionally, we hope to gain experience for further clinical trials with 3D printed casts (other fractures etc.) and to set up an efficient system for overnight-printing of customized casts also involving emergency department co-workers and occupational therapists. Preliminary results show that forearm casts based on 3D-printing technology can safely and efficiently be used in the nonoperative treatment of distal radius fractures as a legitimate alternative to conventional plaster casts. Conclusion Patient-specific forearm casts based on 3D-printing technology can safely be used in the nonoperative treatment of distal radius fractures. After a short instruction, this technology can be used by any medical staff, which makes the treatment of simple fractures more efficient. More clinical trials must be conducted to evaluate 3D-printed casts for other pathologies of the musculoskeletal system. We present a biomechanical knee joint model based on position-based dynamics. The model focusses on the ligaments and is built to support planning of ligament reconstructions. Our approach is able to automatically derive the knee joint model from segmented MR images of the knee, and the model is computationally efficient such that it does not delay the clinical workflow or can even be used in interactive settings. The model is built from MR images of the knee joint. Since we focus on the ligament part, we use either GRE or SPACE sequences with fat saturation. We automatically segment femur, tibia and patella as well as the attached femoral, tibial, and patellar cartilage using a U-Net deep learning network; the anterior cruciate ligament, posterior cruciate ligament, medial collateral ligament, lateral collateral ligament, medial and lateral menisci as well as the quadriceps and patellar tendon are manually segmented. From these segmented structures, we build surface meshes from the bones using a marching cubes algorithm as well as hexahedral meshes from the menisci and cartilages. In position-based dynamics, objects are represented by points (with position, velocity, mass) and constraints, which describe the relations between the points [1] . While for surface geometry, typically distance, bending, and area constraints are used, for volumetric structures distance and volume constraints are best suited [2] . Therefore, we generate distance and volume constraints for hexahedral meshes of cartilages and menisci, and distance constraints for the bones surface geometry. In order to ensure rigidity, we implemented a post-processing step in the position-based dynamics constraints solver. The ligaments are implemented as a chain of distance constraints, that are generated from the ligament footprints, i.e. the ligament attachments at the bones. Furthermore, the menisci are attached by distance constraints to the fossa. Motion of the knee joint is defined by setting Dirichlet boundary conditions on the tibia as well as on all tibial or fibular footprints. Motion to the femur is defined by special distance constraints that are attached at the most proximal part of the femur and are parametrized in such a way to achieve a predefined angle between tibia and femur in the motion. The cartilages are attached to the bones by rigid contact conditions, while sliding boundary conditions are implemented between cartilages, bones and menisci. The constraints are determined by collision detection with axis-aligned bounding box hierarchies as well as constraints that are dynamically generated in the areas of intersection to resolve the collisions [2] . Results Figure 1 shows the simulation results at different flexion angles for two healthy subjects. The simulation runs at interactive update rates, where each simulation step takes 40 ms on average and 348 steps are needed to reach the flexion of 90°as shown in Fig. 1 . Figure 2 shows the view on the menisci during flexion at the same intermediate states. We plan a thorough analysis and validation of the knee joint model with 30 subjects, thereby also comparing the model predictions with MR measurements at various flexion angles. We presented the first results of a biomechanical knee joint model using position-based dynamics. The approach is tailored towards automatic model generation from patient-specific MR image data and is computationally efficient to allow for interactive feedback. To find the lymph node is an important process in pathological diagnosis. However, because conventional pathology images only visualize a thin cross section of tissue, an oversight might occur. Micro computed tomography (micro-CT) allows us to fully analyze the three-dimensional (3D) characteristics of a neoplasm non-invasively. We have demonstrated micro-CT can provide useful whole block image (WBI) of formalin fixed paraffin embedded (FFPE) block sample. However, there is a dire need for additional analysis methods. We have been developing analysis applications for WBI with deep neural network (DNN). In this study, we focused on segmenting the lymph node from WBI acquired by micro-CT. Methods Figure 1 illustrates our analytical procedure which uses WBI by micro-CT (the custom-built Nikon Metrology). 17 FFPE colorectal tissue blocks were scanned with the micro-CT and taken through our analysis method. The data were reconstructed into 3D volumetric images for analysis. Because the original WBI was too large for this purpose, all WBIs were down-sampled by a quarter in each direction. 15 WBIs were used for training and two WBIs were used for testing. The Grand truth of the lymph node region was created by the manual annotation. As preprocessing, intensity standardization and blobness measurements were applied to the down-sampled WBI. At first, a histogram was created with intensities ranging from 10 to 90%. A mode intensity I m was detected and a standard deviation of its distribution SD was calculated by Gaussian fitting. All intensities were converted by the following equation, where I in and I out represent input and output intensities. As blobness measurement, binarization and labeling were conducted on the standardized WBI (SWBI). Threshold value was empirically set to 0.3. The blobness was measured on each labeled region by the following equation, where, k i represents i-th eigen value of labeled region. The calculated blobness B was stored to corresponding voxels and the generated image was called blobness image. VNet [1] was used as a basic network structure and customized inception-ResNet block [2] was implemented as a convolution block. A loss between activation result and GT was calculated by binary cross entropy. Some pairs of small patch images with 64 9 64 9 32 voxels were randomly extracted from SWBI and the blobness image for training. During the extraction process, contrast control, rotation and flip processes were conducted. The implement environment was four cores of Intel Xeon E5-2699 v3 2.3 GHz, 40 GB RAM and four GeForce GTX TITAN X with 3584 cores and 12 GB video memory (Nvidia, Santa Clara, CA). Our DNN was implemented by Tensorflow 1.14.0 and Keras 2.2.5 in a Python 3.6 environment. The numbers of network depth and convolution blocks at each time were 3 and 4, respectively. Adam optimizer was used for network training and the initial learning rate was 1.0 9 10 -4 . Results Figure 2 shows two examples of WBI and segmentation results. All lymph nodes were successfully segmented though reactivities of boundary areas were weak. On the other hand, some other tissues Int J CARS (2020) 15 (Suppl 1):S1-S214 were partially segmented as well. However, a simple threshold will remove them because reactivities of mis-segmentation were weaker than those of the lymph node. Constructed DNN could complete the process in a minute for the down-sampled WBI with 350 9 40 9 480 voxels. We have developed a deep neural network to segment lymph nodes from WBI in neoplastic tissue. All segmentation results were promising for future analysis. Since all lymph nodes were segmented in this study, the deep neural network will be improved to identify the metastatic region on the segmented lymph nodes. In the future, our analysis method might easily understand invasion patterns and help to guide the therapeutic decisions. Purpose Breast cancer is one of the most common cancers in women, making improved detection an important field of research. Confirming breast cancer after partial mastectomy, or ''lumpectomy'', usually requires a detailed visual analysis of tissue slices by a pathologist who searches for tissue types such as malignancy, and necrosis. A tissue slice can also be sampled using mass spectrometry, such as desorption electrospray ionization mass spectrometry (DESI-MS), to search for molecular patterns that are associated with these tissue type. Signal processing and clustering, using artificial intelligence techniques, is a promising method for quantitative analyses of DESI-MS images. Our goal was the reproduction of pathology reports as a process improvement for digital pathology. Because there is no ground truth available for metabolic analysis of breast tissue slices, unsupervised clustering techniques were used. Many of the current DESI-MS analysis techniques of cancer tissues investigate the relative abundance of individual molecules for use as biomarkers. For breast cancer these molecules are not yet known and, as a consequence, clustering that uses the entire spectrum may be required. Registration of the corresponding ''gold standard'' histology image to the DESI-MS image allows for determination of the accuracy of the signal processing and allow recognition of molecular cancer patterns on a histology image. Our study was conducted using 10 thin slices of surgically removed breast tumors from 9 patients, for which we had both complete histology and DESI-MS data. The samples had tissue types that included benign tissue, active cancer, and necrotic regions. DESI-MS scanning produced 3D images with approximately 300 9 400 pixels, each pixel being a mass spectrum of 1000 mass-charge ratio prevalences. All 10 DESI-MS images were aligned to have consistent mass spectra before processing. The remaining processing was performed on individual images because of substantial inter-subject variation. Hierarchical segmentation was used to first differentiate tissue from background and pixels, and to then classify only the tissue pixels. A non-negative matrix factorization produced dimensionality reduction to 45 highly relevant mass-charge ratios. For visualization, the rank-3 factors were used to produce a false-color DESI-MS image. Sparse subspace clustering (SSC) [1] was applied to the larger mass spectral tensor. For each stage, random pixels were selected and used with SSC to determine an affine approximation of each tissue subspace. The affine subspaces were used to classify every pixel of each image. The final classifications were visualized using MATLAB. Image processing techniques were applied to the DESI-MS image and histology image in preparation for a three-step registration pipeline. A rigid transformation followed by an affine transformation, both using an expectation-maximization approach, were used to roughly align the DESI-MS and histology tissue edges. Histology pixel grayscale values and DESI-MS false color pixels were registered using a non-rigid ''demons'' deformation [2] , which produced a map from a histology image to a DESI-MS image. The images were visualized and the matches were quantitatively evaluated using a Dice score. Gaussian statistics were used to summarize the results. Graph clustering of DESI-MS data was able to capture most of the tissue in each image and to determine tissue classification similar to the results determined by a pathologist, illustrated in Fig. 1 Registrations for all ten histology images to DESI-MS images were successful. Three breast tissue sample registrations can be seen in Fig. 2 . A paired two tailed t test of Dice score values, as seen in Table 1 , showed that the deformable registration was a statistically significant improvement over the rigid registration (p \ 0.05) and the affine registration (p \ 0.05). This work demonstrated a successful information processing pipeline from mass spectrometry imaging, through automatic tissue classification, to histological image registration. Visually, the pixels that had distinctive metabolite molecular profiles were well matched to histologically distinctive regions. The molecular classification produced automatic size estimates and image locations that were similar to the histological report of the pathologist. These methods were tested on a limited set of surgically excised breast tissue. Further work could include larger sample size and other tissue types. Also of interest is regions in which the histology and molecular profiles are dissimilar; these may indicate regions of altered metabolic activity that have not yet undergone substantial structural changes. Our method could also be used to automate tissue recognition in pathology laboratory applications, and to correct for day-to-day variability associated with non-quantitative profiling techniques. Purpose Atrial fibrillation (AF) is a common heart arrhythmia that can be associated with stroke and heart disease. Postoperative AF (POAF) is associated with poorer patient outcomes and higher hospital costs [1] , and to date there is no risk model that comprehensively predicts this surgical outcome. Most methods for detecting AF make use of realtime monitoring data such as electrocardiograms. This allows for AF to be quickly identified and addressed but lacks predictive capabilities and does not identify underlying physiological features that may be useful in understanding AF. DESI mass spectrometry (DESI-MS) is a mass spectrometry imaging technique which allows for quick, nondestructive tissue analysis in ambient conditions, making it more convenient to use than other forms of mass spectrometry. We propose a stacked learning system that combines DESI with patient demographic data to classify patients who experienced post-operative atrial fibrillation after undergoing heart surgery. Our study was conducted using 10 thin slices of surgically removed left atrial cardiac tissue from 10 patients who underwent cardiac bypass surgery. Each patient sample included a DESI-MS image of the tissue sample and a collection of demographic information that described each patient's gender and relevant health data. The demographic information included entries for age, gender, type 2 diabetes status, and post-operative atrial fibrillation. A pathologist examined the tissue samples and produced a nuclear hypertrophy (NH) label for each patient. The stacked learning system had three main stages after pre-processing: a convolutional neural network (CNN), maximally collapsing metric learning (MCML), and a linear support vector machine (SVM). The stages of the CNN are shown in Fig. 1 . The pre-processing produced images with 45 mass-charge ratios per image pixel and also gave a differentiation of tissue from background. The principal 40% of tissue pixels in each image were assembled into data vectors that were used to train the CNN, using the NH label of that patient as the target. After training, the CNN was used to compute a numerical score of the DESI-MS image of each patient. The CNN score was then appended to the demographic information, creating a new data set for the next stage. MCML [2] was used, taking the POAF labels as target labels, to reduce the new data set to 2 dimensions. The SVM classified the 2-D data set using POAF targets and was tested using leave-one-out cross-validation. Figure 2 provides a visual representation of the stacked learning system. This system was tested in pieces before a full test was performed to evaluate whether smaller parts of the system were sufficient to produce equal results. When the MCML stage was omitted from the system, the SVM was only able to achieve a 70% accuracy. When the CNN was removed so that MCML performed dimensionality reduction on a data set of only demographic information; the SVM was able to achieve a 90% accuracy on this 2-D data set. Finally, when the full system was tested with the DESI-MS score from the CNN and dimensionality reduction from MCML, the SVM classifier was able to achieve a 100% success rate in classifying POAF. These results were consistent over 1000 trials. Table 1 summarizes these results along with the demographic information for each patient sample. Conclusion The stacked learning system presented in our study was able to fully classify POAF in a set of patients by using mass spectrometry of a cardiac tissue sample and demographic information. The full stacked learning system was required to achieve this, because the SVM was not able to fully classify the data set in an incomplete system. The system showed promise for use on larger data sets and also showed that combining multiple different types of data through stacked learning can be an effective strategy.  Surgical workflow management (sWfM) in modern integrated operating rooms (OR) is an essential prerequisite for the straightforward collaboration between the surgical team and sophisticated medical technology, such as medical devices and information systems. Additionally, it is a fundamental technology for process automation, computer-aided surgical assistance, situation-awareness, and intraoperative decision support. For the provision of sWfM, perioperative activities, technical assistance functionalities, and surgical knowledge need to be represented by surgical process models (SPMs). Thereby, personalized and advanced knowledge-based assistance functionalities, as well as needed technical resources, could be provided to the right person at the right moment in the surgical process. Hence, surgical workflow modeling requires a standardized methodological approach for knowledge-based representation, generation, naming, visualization, and execution of process models. The semantic workflow modeling, which combines process modeling with ontologies, has been identified as a solution in the domain of business processes. Ontologies provide a basic vocabulary and domain knowledge to describe and annotate semantic SPMs [1] . For a machine-interpretable representation of SPMs, business process modeling languages, like BPMN 2.0, UML, and EPC, have been utilized. Especially, BPMN and its domain-specific extension BPMN SIX (BPMN Surgical Intervention Extension) [2] , provide essential elements for modeling the behavioral perspective and the process structure of perioperative processes. The objective of this work is to present a process modeling tool for the representation of semantic SPMs and intraoperative context-sensitive process automation. For this purpose, a web-based BPMN SIX process modeling tool (BPMN SIX .io) was developed, which enables the integration of different medical ontologies, such as OntoSPM [1] , SNOMED CT, FMA and LOINC during surgical workflow modeling. In addition, the tool offers modeling functionalities for the description and configuration of networked medical devices on the basis of the IEEE 11073 SDC standard family. For the implementation of BPMN SIX process modeling functionalities and ontology integration, different business process modeling tools were analyzed considering their extension functionalities. Therefore, the well-established web-based modeling tool BPMN.io was selected. BPMN.io is an open-source browser application, which is written in JavaScript. Due to its modular structure, it is highly customizable and implements other notation standards, such as DMN and CMMN. The modeling tool was extended with the BPMN SIX modeling elements for the surgical domain, an ontology representation of the SPM elements as well as functionalities for the IEEE 11073 SDC standard-compliant capability description of medical devices for integrated ORs. In Fig. 1 , the resulting BPMN SIX .io surgical workflow modeling tool is presented. Domain-specific elements of BPMN SIX (e.g. anatomical structures, surgical instruments, and clinical information systems) were added to the symbol palette ( Fig. 1, left) and can be used during the process modeling as additional resources. The newly implemented medical device component was extended with an IEEE 11073 SDC standard-compliant medical device capability description (Fig. 1,  right) . Thereby, personalized automated workflows and specific support functionalities could be modeled and instantiated for surgical workflow management [2] . For the semantic representation of the SPM elements, different ontologies were integrated and can be used during process modeling via the properties panel (Fig. 2) . In the first step, a medical ontology (e.g. OntoSPM, SNOMED CT, LOINC) can be selected for every modeling element. Then the appropriate entity term needs to be selected, which describes the modeled element. In this way, the SPM element can be represented with the vocabulary terms of one or more medical ontologies. The selectable terms are filtered according to the type of the modeled element. For example, an anatomical structure can only be described with the terms of the anatomy part of the  The POAF label was the learning target S46 Int J CARS (2020) 15 (Suppl 1):S1-S214 ontology. In this case, terms that describe medical devices or surgical activities are not available. Consequently, the modeled SPM and the semantic representation were implemented in a BPMN 2.0 compliant XML-representation and could be downloaded as ''.bpmn'' file. BPMN models are executable by BPMN workflow engines. If the process model is enriched with technical parameters (e.g., specific SCO operations, service calls), a surgical workflow management system can transform the modeled abstract functionalities to specific remote-control calls during surgery run-time, which enables personalized and context-aware surgical assistance. In this paper, a web-based BPMN SIX modeling tool was developed, which was extended with domain-specific elements of the surgical process models. Medical devices can be modeled with the IEEE 11073 SDC standard-compliant device representation. In this way, individual surgical assistance functionalities can be developed and executed during surgery. Additionally, the BPMN SIX .io surgical workflow modeling tool integrates different standard medical ontologies, which enable the description of SPM elements with a consistent vocabulary. The resulting semantically-enriched SPMs will enable shareable surgical knowledge, surgical workflow management, and context-aware surgical assistance as well as intraoperative process automation and decision-support. We address the problem of surgical activity classification. We focus on modeling the surgical gestures that are generic and that reoccur in different tasks. For example, the gesture ''Positioning the needle'' might take place in different tasks of ''Suturing'' and ''Needle Passing'' performed in different scenes and settings. We believe that the low-level surgical activities: gestures are better modeled by motion cues which are independent of object and scene features, while the high-level surgical activities:surgical tasks are better modeled by the visual features that are characteristically defined by the objects in the scene. However, we also believe that visual features are complementary to the motion cues and vice versa. So, we propose a novel multi-modal and multi-task architecture for simultaneous low level gesture and surgical task classification in Robot Assisted Surgery (RAS) videos. Many recent studies have shown that exploiting the relationship across different tasks, jointly reasoning multi-tasks and taking advantage of a combination of shared and task-specific representations perform astonishingly better than their single-task counterparts. We propose using recurrent neural networks, specifically long term short memory (LSTM) neural networks, in order to model the complex temporal dynamics and sequences. We aspire to model the sequences of gestures occurring in surgical-task videos with a model that is deep over temporal dimensions. Our architecture is based on the principles of LRCN [1] , a specialized LSTM network that jointly learns temporal dynamics and visual features by convolutional network models. LRCN incorporates a deep hierarchical visual feature extractor and a model that can learn long-term temporal dynamics. In this model, a deep hierarchical visual feature extractor, specifically, a Convolutional Neural Network (CNN), takes each video frame modality and extracts its features. Basically, we first train two branches of CNNs, one for each modality; visual cues (RGB) and motion cues (optical flow). These branch networks are trained to classify surgical gestures based on individual frames. Learned features extracted from these branch networks provide us a fixed length vector representation of the image or motion cues which are then passed into a recurrent learning module as consequent video frame features. This recurrent module is based on a single-layer LSTM with 256 hidden units that learns the consequent video frames'' temporal dynamics. Predictions based on averaging scores across all frames from a fixed vocabulary of multiple tasks of surgical gestures and surgical tasks are made. Our architecture supports multimodal and multi-task learning for surgical gesture and surgical task classification. We experiment on the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) [2] which provides a public benchmark surgical activity dataset. In this video dataset, 8 surgeons with varying expertise perform 3 surgical tasks ''Suturing, Needle Passing and Knot Tying'' that is broken down into 15 surgical gestures on the daVinci Surgical System (dVSS) (Fig. 1) . The gestures are low-level surgical activities where the movement is intentional and is carried on towards achieving a specific goal. The surgical gestures are performed across different surgical tasks under different scene and settings. Our initial experimental results show that our approach is superior compared to an architecture that classifies the gestures and surgical tasks separately on visual cues and motion cues respectively. We train our model on a fixed random set of 1200 gesture video segments and use the rest 422 for testing. This results in around 42.000 gesture frames sampled for training and 14.500 for testing. For a 6 split experimentation, while the conventional approach reaches an Average Precision (AP) of only 29% (29.13%), our architecture reaches an AP of 51% (50.83%) for 3 tasks and 14 possible gesture labels, resulting in an improvement of 22% (21.7%). Our architecture learns temporal dynamics on rich representations of visual and motion features that compliment each other for classification of low-level gestures and surgical tasks. Its multi-task learning nature makes use of learned joint relationships and combinations of shared and task-specific representations. While benchmark studies focus on recognizing gestures that take place under specific tasks, we focus on recognizing common gestures that reoccur across different tasks and settings and significantly perform better compared to conventional architectures. Recent studies in surgical gesture classification highly rely on kinematic data. Kinematic data requires additional recording devices, while a computer vision approach does not. We propose a computer vision only approach by using motion cues (dense optical flow) as input as an alternative to kinematic data. We demonstrate that using motion cues only, we achieve competitive results. While the setting and objects differ during the performance of different surgical tasks, the surgeon's motion and the transitions between these motions remain generic. For example, the common gesture ''Positioning the needle'' might take place in different surgical tasks of ''Suturing'' and ''Needle Passing''. While the scenes and settings differ for these tasks, the motion information of the gesture ''Positioning the needle'' are identical and can be identified by the temporal dynamics. In this way, gestures can be defined with motion cues which are independent of the scene and setting. Similarly, to differentiate between gestures in the same setting, we can use motion as a reliable identifier. We adapt Optical flow ConvNets initially proposed by Simonyan et al. [1] . While Simonyan et al. uses both RGB frames and dense optical flow, we use only dense optical flow representations as input to emphasize the role of motion in surgical gesture recognition and present it as a robust alternative to kinematics and RGB frames. We also overcome one of the limitations of Optical flow ConvNets; Simonyan et al. initializes the weights of their spatial network on RGB frames with a pretrained model on Imagenet, however they do not initialize the weights of their Optical flow ConvNets for the lack of an alternative. We overcome this limitation by initializing our Optical flow ConvNets with the method called cross modality pretraining proposed by Wang et al. [2] . Similar to the architecture proposed by Simonyan et al. [1] (Optical flow ConvNets), the input to our model is formed by stacking dense optical flow displacement fields between several consecutive Fig. 1 Our architecture is a multimodal and multi-task learning architecture that simultaneously classifies surgical gesture and surgical task labels. We first train two branches of CNNs, one for each modality; visual cues (RGB) and motion cues (optical flow). These branch networks are trained to classify surgical gestures based on individual frames. Learned features extracted from consequent frames using these branch networks are then passed into a recurrent learning module (LSTM) Int J CARS (2020) 15 (Suppl 1):S1-S214 frames (we chose the length of consecutive frames as L = 10 in accordance with the proposed model). We compute the dense optical flow and get a 2-channel array with optical flow vectors, (u, v) that represents the horizontal and vertical components of the vector field correspondingly, which results in a total of 20 channels for each input (L = 10 * 2-channels (u, v)). We find their magnitude and direction, and save these two separate images as grayscale image representations rescaled to a [0, 255] range and compressed using JPEG. We adapt a BN-ResNet101, a ResNet with Batch Normalization (BN). Since we work with a small dataset, we also add a dropout layer after pooling of ResNet and perform data augmentation to overcome overfitting (Fig. 1) . We initialize our Optical flow ConvNets with the method called cross modality pretraining proposed by Wang et al. [2] . Using the pretrained Imagenet weights on RGB, we first average the weight value across the RGB channels and replicate this average by the channel number of motion stream input, which in our case is 20. This helps us greatly to overcome overfitting and addresses the limitations of Optical flow ConvNets as initially suggested by Simonyan et al. [1] . Our initial results on JIGSAWS gesture classification task are competitive. We experiment with our model using JIGSAWS's Leaveone-supertrial-out (LOSO) cross-validation scheme. According to this scheme, Supertrial i is defined as the set consisting of the i-th trial from all subjects for a given surgical task. In the LOSO setup for cross-validation, five folds with each fold comprising of data from one of the five supertrials are provided. Table 1 demonstrates the initial results on our experiments. The accuracy for all five folds are averaged to compute average accuracy and also the standard deviation is provided. According to these results, our model's performance is competitive. Moreover, our training takes only about 20 s per minibatch, which we have set at 30, and an epoch for testing takes about 25-30 s for 80-100 Suturing videos which are the longest in the dataset, and about 10-15 s for Knot Tying videos which are shorter. To our knowledge, this is the first paper that addresses surgical gesture recognition using dense motion cues/optical flow information only. We achieve competitive results on JIGSAWS dataset, moreover, our model achieves more robust results with less standard deviation, which suggests optical flow information can be used as an alternative to kinematic data for the recognition of surgical gestures. The current clinical situation is impacted by increasing workload caused by demanding bureaucracy and shortage of personal. Due to legal requirements healthcare professionals are asked to document any task they completed and to write reports which precisely describe anything they have done. In the field of surgery this encompasses beside the general patient report the so called operation report which is a written summary of the undertaken steps during a operative procedure. The operation report serves the exact documentation of any surgical intervention to secure the therapy, but also the preservation of evidence and accountability. In terms of time, the documentation should be made immediately after the treatment. The operation report also plays an important role regarding questions of accounting. We propose a first attempt to relief surgeons from this burden in the future. We present a model, which should guarantee an autonomous creation of the surgical report and thus a relief of the surgeon's workload. Furthermore, the operation report should show the course of the operation in even more detail, so that additional information will also ensure better quality in the future. The operation report should contain a lot of details like the patient's data, the date, the type of anaesthesia, diagnosis and type of operation, the persons involved, indication for the operation, preoperative information and consent, positioning of the patient, perioperative administration of medication and of course the description of the surgical procedure. In order to generate this report, the proposed system is dependent on various data sources. Some data, such as the Fig. 1 Our input to the BN-ResNet101 architecture is a 2-channel array of dense optical flow vectors corresponding to video frames. (u, v) represents the horizontal and vertical components of the vector field. So for a multi-frame length of 10, our input is a total of 20 channels (L = 10 * 2 -channels (u, v)). Our BN-ResNet101 model then classifies videos with surgical gesture labels The accuracy for all five folds are averaged to compute average accuracy and also the standard deviation is provided patient's data, the date of the operation, the type of anaesthesia, as well as the diagnosis and type of operation, can already be found in the hospital documentation. Information about allergies, the patient's medical history or other points can also be derived. More difficult and in the main focus is now a detailed description of the surgical procedure. In order to evaluate how the surgical procedure can be recorded, the laparoscopic cholecystectomy was chosen as a relatively highly standardized procedure. This was divided into 9 phases (1. insufflation, 2. insertion of the trocars, 3. preparation phase, 4. clip phase, 5. dissection phase, 6. packing of the gallbladder, 7. retrieval of the gallbladder, 8. haemostasis, 9. removal of the trocars). For this purpose, a surgical report matrix containing textual building blocks was created. The task is now to complete this matrix automatically and thus create a complete and detailed operation report. In order to achieve this, an integrated operation environment is required, which can guarantee a detailed data acquisition. In our integrated operating room, there are various sensor systems, which should guarantee this. The status of the individual devices can be recorded. Personnel changes are also recorded using personalised tags. The individual instruments in use are also recognized. Especially the data from the laparoscopic video plays a decisive role. Based on the recorded data, the system should be able to recognize the individual phases of the operation and determine whether they are within a defined standard or whether there are deviations from the standard of the operation. In order to ensure exact phase recognition, we trained a CNN model in a fully supervised manner. Training images were sampled from the laparoscopic video recording 1 frame per second. To avoid class imbalance, the training data was balanced so that each class has 3 thousand images for training. As a base model, pre-trained CNN architecture Resnet-50 V2 was used with Adam optimization on Tensorflow. The phase detection was supported by the sensor technology integrated in the operating room. In order to now detect deviations in the operation, everything must be subject to a plausibility check. The individual surgical phases and their median were recorded over 230 operations. This is the ground truth of the system. These times are automatically compared with the detected phases by the software and the operation report matrix is completed for this purpose. The quality of the automated generation was controlled by two independent raters, which manually annotated the data, which were then compared with the automatically registered data. At first the median durations of phases of the n = 230 operations were analysed. The insertion of the trocars took 4:02 min on average. The preparation of the gallbladder and the important vascular and bile duct structures took on average 05:55 min. Finally, 05:18 min. were required to clip them. Releasing the gallbladder from its gall bladder bed took 09:38 min. The gallbladder was finally retrieved in 6:19 min. and then the site was checked once again for bleeding or other abnormalities in a median of about 1 min (01:22 min.). At the end of the procedure, the trocars were removed in 38 s. This data represent the standard phase durations. The average accuracy of the trained network for real time phase detection based purely on video data was about 0.76 on the test set. The proposed system utilized the real time phase detection to compare the durations of individual phases to the standard durations. The resulting deviations influence the textual building blocks, which are used to create the operation report. The operation report was issued without any problems after the operation and deviations from the standard were reliably detected. However, it still left the surgeon the possibility of additional comments and changes. Finally, a detailed operation report was generated, which allowed exact conclusions to be drawn about the course of the operation. The created operation report offered an exact time recording of the individual phases in contrast to the previously usual manual creation. Especially this precise time recording is a novelty in surgery, which offers the surgeon a better assessment of the operation. The accuracy of the phase detection was in a tolerable range. One difficulty, however, is the automatic transfer of the surgical procedure in descriptive text form. This should be subject to a final check by the surgeon, since a 100% understanding of the operation and even individual deviations cannot yet be guaranteed by automated process recognition. We suppose for the future the hand written somehow prosaic report to be replaced by this digital and objective form to reduce personnel workload and to improve accuracy. Evolution of Endoscopic Endonasal Approach brought by the Next-generation Operating Room T. Ogiwara 1 , T. Nakamura 1 , Y. Fujii 1 , R. Hardian 1 , T. Goto 1 , T. Horiuchi 1 , K. Hongo 1 With the rapid development of surgical techniques and improvement of instruments, indications for endoscopic endonasal approach (EEA) for skull base tumors have been expanding. However, some practical limitations with respect to this procedure still exist. Surgical results of EEA depend on the surgical skills of each surgeon or the institutional experiences, because EEA is technically demanding and requires highly skills. Furthermore, some additional devices are necessary in the armamentarium of neurosurgeons, such as neuronavigation, electrophysiological monitoring and intraoperative MRI. On the other hand, it is most important factor to be able to provide the same quality of surgery by many neurosurgeons and institutions regardless of the region for the further development of EEA in the future. We believe that not only the development of surgical devices, but also the environments of operating room can be one of the solutions for this issue. We have three types of operating rooms for EEA; which are conventional operating room with mobile Computed Tomography (CT), hybrid operating room (hOR), and smart cyber operating theater (SCOT). Here, we reported the application of these newly developed advanced types of operating theaters for EEA, along with an evaluation of our experiences. Methods These next-generation operating rooms including the SCOT and hOR have been applied for EEA suitably and properly according to surgical procedure and its pathology. At first, the SCOT is an operating room equipped with leading-edge technologies, including an open MRI, a rapid diagnostic test system, and a 4 K, 80-inch monitor called ''OPeLiNK eye'' (Fig. 1 ). In operation with SCOT, various surgical equipment are connected by middleware called ''OPeLiNK''. With ''OPeLiNK'', various information can be stored in one synchronized timeline, such as location of surgical tools, navigation data, neuromonitoring data, intraoperative histopathological data, etc. Furthermore, intraoperative MRI is available in this room. This system allows efficient real-time intraoperative discussion between the operating room and the ''Surgical Strategy Desk'' which is installed in our office. This application is intended to enable provision of ''uniformly high standards of treatment'' that not depend solely on the skill and experience of the surgeons. Currently, we are developing this unique system, which is applied for EEA, especially for large NFPAs, functional pituitary adenomas and cases requiring complicated skull base procedures to confirm state of tumor removal intraoperatively, and prevent of postoperative bleeding due to remnant tumor, incomplete total resection and surgical complication [1] . Next, the hOR has a high-performance fluoroscope and an operating table specifically suited for endovascular therapy, which is being used S50 Int J CARS (2020) 15 (Suppl 1):S1-S214 increasingly more frequently in various fields, including cardiovascular surgery, orthopedic surgery, and neurosurgery. The hOR in our institution is a surgical theater equipped with an advanced medical imaging device (ARTIS Pheno; Siemens Healthineers, Erlangen, Germany). This robotic device can be moved unobtrusively into and out of the surgical area. It moves from the corner of the room to reach out the patient during the procedure and can be easily moved out of the way. This system can be used to confirm intraoperative real-time imaging findings, such as fluoroscopy, angiography, and cone-beam computed tomography (CBCT) with a flat panel detector angiography module without any trouble by the surgeons and patients. Standard navigation techniques are also available in the hOR. The hOR was applied for EEA in the cases with chordoma, chondrosarcoma and CVJ lesion which the accurate understanding of skull base drilling range and anatomical orientation is important [2] . We have also developed and used a mobile CT system for use in the operating room and applied this CT system to intraoperative monitoring during neurosurgery since 1991. This system includes Toshiba TCT-300 CT system, mobile CT scanner gantry, digitally controlled operating table and head fixation system. The conventional operating room with mobile CT was applied for EEA in the cases of usual pituitary tumor. We successfully completed the surgical procedures in each operating room properly and suitably according to their procedure and pathology. Application of the SCOT system to EEA was feasible. By using the SCOT, integrated information could be obtained and shared easily to help the surgeon in decision making during the surgery. Under this SCOT system, the surgeon can obtain accurate information about the location and volume of residual tumor as well as about the important surrounding deeper structures using intraoperative MRI. On the other hand, the operating and workflow of the SCOT require time-consuming works and high running costs. Therefore, the indications for the SCOT should also considering its cost-benefit for the patient and the hospital. The hOR with a flat panel detector angiography system was helpful to achieve safe and precise EEA, including endoscopic endonasal odontoidectomy (Fig. 2) . EEA in the hOR was feasible and provides accurate intraoperative localization of pathology, thereby increasing the chance of complete skull base drilling, and decreasing surgical risks, such as insufficient surgical removal and delay of suitable treatment for accidental vessel injury during surgery. Intraoperative CT could be also performed without troublesome and timeconsuming works during EEA in the conventional operating room with mobile CT. Thus, the usefulness of these operating rooms for EEA and efficacy of using these operating rooms properly depends of individual cases. We reported our initial experience with EEA in the Next-generation Operating Rooms including the SCOT and hOR. These innovative operating rooms, which are considered a breakthrough in neurosurgery, augment quality and safety of EEA. Sufficient knowledge of the characteristics of each operating room makes it a useful new modality for EEA. It is expected that it these innovations will lead to development and widespread adoption of futuristic endoscopic neurosurgery. Just recently, the field of surgery likewise as the entire healthcare system is profoundly affected by the process of digitalization, artificial intelligence and robotics. While innovators intend to transfer their knowledge and experiences gained in industry and daily life application to the clinic they might miss the needs of patients and healthcare professionals and end up with solutions, which are only adding additional cost without offering a real benefit. Methods Current innovations in the area of medical robotics are explored by a literature research and assessed on a clinical perspective. A differentiation is made between innovations which are already brought to the hospital and those which are still experimental or in development. Additionally, an overview on the current clinical situation is provided and areas are highlighted where robotics could become valuable tools to compensate for shortcomings or limitations in patient care. Nowadays, medical robotics are almost exclusively applied in operative medicine as master slave manipulators and for camera guidance to improve accuracy and precision during operations. In some situations, they help to compensate for shortage of personnel but from an economical point of view the allied costs are unacceptable. The level of automation is low or almost zero and comparative studies so far did fail to show a real clinical benefit. Currents systems are mainly mechatronic reproductions of established interventional methods and do not facilitate innovative approaches or allow for performing interventions which are not with current instruments and systems. Although some researchers were able to develop partly autonomy and intelligent systems, e.g. for camera control or suturing tasks, their application in daily routine is far off or even unrealistic due to legal and ethical aspects. Alternative fields of applications, e.g. for organ exposure during surgery or safety measures are untouched. On the other hand, nursing care and in-clinic transport and the overall workflow could absolutely benefit from robotic support, e.g. to bring patients to the OR table, for rehabilitation purposes or to compensate for functional losses caused by neurologic alterations or amputations. Admittedly, some solutions are already just before clinical use but are not well integrated in the clinical process or are missing an intelligent environment to support full functionality. Conclusion The healthcare system of today is endangered by shortage of personnel, economic restrictions and increasing expectations on treatment results and outcome. Robotic solutions, which in other areas have been proven to improve the quality and precision of manual manipulations, to allow for automation and finally to replace humans for given and repetitive tasks are not yet used in a supportive manner. With this presentation we will give an insight into the current clinical situation and discuss where and in which way robotics could be a highly welcome technique and how hospitals have to be adapted to become an enabling environment. Purpose Building a state-of-the-art, ''Department-Less'', one-of-a-kind acutecare comprehensive integrated neuroscience facility, to eliminate barriers between narrowly focused medical specialties to enable holistic, collaborative, patient-centered care. This facility will house world-leading health innovators, inventing life-saving, neurovascular devices and pioneering transformative treatments. High sustainability goals, on a developer-driven budget presents extraordinary Value Design challenges. A relatively simple massing concept, and rationalized building envelope will keep construction costs low without compromising the goal of design excellence and optimized workflow. Traditional healthcare design has separate buildings intended for Outpatient care, Inpatient care, Diagnostic and Treatment, Rehabilitation and other functions. In current facilities the patient is moved around from department to department, or between buildings, and is literally ''brought to the care''. Traditional acute care hospital design today is the result of a long process started during the 1950's modernist movement. The ''toweron-podium'' offered a neat conceptual and aesthetically acceptable solution, with standardized ward units stacked over a mat of flexible, expandable diagnostic, outpatient and ancillary departments. This design was underwritten by promises of efficiency gains from the centralization of ancillary procedures in the lower block. In practice, the results rarely achieved the formal clarity of the ideal, given the reluctance or inability to centralize functions fully, and the problem of reconciling all the necessary departmental adjacencies with such a design. For many modernists the tower-on podium promised an ideal marriage of function, science, economics and art. It is not surprising, therefore, that by the early 1960s this form had taken its place as orthodox hospital design practice, under the sobriquet ''matchbox on a muffin''. That this was possible depended on two developments, namely the acceptance of wards stacked in a tower, and a rethinking of the way in which people, goods and services were to be moved around the hospital, ushering in a range of mechanized systems of communication. This design was underwritten by promises of efficiency gains from the centralization of ancillary procedures in the lower block [1] . This approach, which still persists today, was not the result of designing a facility that focuses on enhancing the patient experience. At this new model of healthcare facility, the design is organized around ''bringing care to the patient'', with the addition of other functions such as research and industry partnership, in a single ''department-less'' building. Pre-operative, surgery, imaging, recovery, inpatient beds and rehabilitation are collocated within a 90,000 square foot ''patient hub'' on a single floor-and the patient moves minimally during the course of procedure and recovery, see also Figs. 1 and 2. A patient going into surgery is prepared and recovered in the same universal room where intensive care or medical surgical care is delivered after procedures. This novel concept enables the patient to move less and increases patient/family satisfaction, and it also enable clinicians and researchers to collaborate more seamlessly. Clinicians, industry, patients, education are all brought together under one roofa radical change from conventional practice. The entire neurosciences building design solution centers around improving the patient/family experience and putting patients first, not just addressing technical and functional challenges. Extensive research was performed to understand what elements defined and influenced the ideal patient experience, in particular those brands that have disrupted their respective industries. Our research was conducted over the course of 12 months and included neurosurgeons, clinical staff representing over 10 different departments, scientists, hospital operation business leaders and builders. Cost reduction and schedule: The environmental approval process is the critical path for the project. Our team has developed a streamlined project schedule that will enable the early bid packages for excavation, utilities and foundations to be released within days of obtaining entitlement approval. The architect and our consultant team, with the support of the Owner, is negotiating with the Authorities Having Jurisdiction (AHJs) to commence early review of these packages while the EIS process is being finalized. The Global Neurosciences Institute (GNI) located at the Crozer-Chester Medical Center campus in Upland, Pennsylvania, designed by the Yazdani Studio of CannonDesign, will house 145 patient beds, comprising 400,000 gross square feet at a current estimated construction cost of $307 M. The project includes six ORs, a radiology/ imaging suite, a 32 bed ICU, a rehabilitation center, emergency department, outpatient/ambulatory care center with 15 exam rooms, and a wing that houses industry partners and vendors to conduct research in collaboration with the institute. The project is being developed by a private developer in collaboration with an internationally esteemed neurosurgeon who has worked with CannonDesign to develop the unique concept for the institute. This project is the first Int J CARS (2020) 15 (Suppl 1):S1-S214 in the United States to bring ''all things neuro'' under a single roofoutpatient, inpatient, rehabilitation, research, and industry partnership. Included in this facility are several innovation functions and programs, such as functional neurology, neuro-pharmacy, neuroplastics, biomedical engineering/industry collaboration research and development maker spaces. This building represents a radical departure from the traditional ''tower on podium'' hospital design seen today. Unlike its predecessor model, which resulted from a long and slow evolution driven mostly by operational efficiency, advances in technology and department-by-department mentality, every design decision was made thru the lens of what was best for the patient. This project that has completed Programming and Concept Design is aimed to demonstrate the new concept of patient-centric workflow high efficiency in resource utilization highly specialized acute-care facility. The preliminary simulations and functional evaluation of the new design allowed us to predict a significant improvement in patient satisfaction, reduced average length of stay, better clinical care outcomes, and reduced translational research bench-to-bedside implementation time. This pilot innovative project will bring together clinicians, industry, patients, which are an essential part of the process, and the education piece, and it is all together under one roof. It demonstrates a unique design that challenges traditional designs of healthcare facilities and provides a proof of concept for a new generation of setups for multidisciplinary and highly specialized care providers. Most importantly it combines major improvement in patient comfort and satisfaction while optimizing resource utilization and workflow.  The Knowledge Model is a framework for thinking, Fig. 1 . It is focused on increasing the interoperability driven enterprise to achieve increased patient outcomes. Knowledge, comprised of cognitive and intuitive factors requires balance as noted in the quote by Albert Einstein, ''''The intuitive mind is a sacred gift and the cognitive mind is a faithful servant. We have created a society that honors the servant and has forgotten the gift.'' With appropriate balance, significant gains in patient outcomes have been demonstrated. Clinical evidence to understand the inherent obstacles to patient outcomes improvement has been established. Examples include challenges within radiology, visualization between radiology and surgery, and other interoperability gaps. They all contribute to preventing enterprise from reaching fruition. A demonstrated solution to the above areas is interactive mixed reality (IMR). IMR will serve as a catalyst for enhancing intuition, thereby allowing the method to be most effective. In radiology, meaningful data and insights that are embedded within medical images, are often undetectable via routine visual analysis using 2D/2.5D views. (2.5D is used to represent 2D views of 3D volume data sets) Therefore, valuable information often is not considered. There is also a growing complexity and volume of imaging radiology data that is exceeding the individual radiologist's ability to organize, such that fully informed decisions can be made. It has been found that diagnosis of pathology requires consideration of 3D visualization. This represents an approach that is presently being offered by limited types of diagnostic methods. IMR provides a direct path to 3D visualization and increased intuition. Whereas viewing of 2.5D images often creates an impediment to intuition. Radiologists often waste time and effort processing and converting 2.5D to 3D; structures have to be thought out; the spontaneity of thought that allows for intuition is impacted; thus, creating roadblocks to Intuition. IMR presents the 3D world directly to the brain and the mind is immediately able to respond freely with intuition and insights-a direct path to Intuition. learning and compiling information into frameworks that can be easily evoked when needed. A strong case can be made for needing intuition due to often starting with a ''fairly weak aggregate of medical data.'' The sources for input may be flawed, requiring ''human intervention'' to score input quality and interpret output results. A team of doctors can add perspective/intuition to the process of searching for the truth. With IMR, radiology and surgery can ''speak'' in a common language, radiology can overcome many of the issues addressed and surgery can incorporate 3D workstations in their practice; they never open up patients and see 2D views. Bridging the gap between radiology and surgery will have a major impact on improving interoperability and hence enterprise. Purpose A GPU based radio-frequency ablation (RFA) solver for image-guided interventions of liver tumours with the patient-and devicespecific parameters was developed in the earlier work [1] to assist interventional radiologists (IRs). Microwave ablation (MWA) is another treatment option used by radiologists, which also requires a fast simulation solver to predict the ablated lesion within the limited time available during the clinical procedure. Also, radiologists can benefit from online tools to visualize the treatment planning and to improve collaboration between IRs across the globe. Methods A fast MWA solver, similar to a fast RFA solver [1] , is proposed with the help of the graphics processing unit (GPU) and the central processing unit (CPU). With axisymmetric assumptions on the Maxwell equation in a two-dimensional finite element geometry, the specific absorption rate (SAR) is obtained at each finite element node and then projected onto a spherical finite element geometry centred around the tumour with radius of 6 cm. Heat source terms of the Penne's bioheat equations for RFA and MWA are obtained from the point source model and the SAR respectively. GPUs are employed on all finite element computations of the RFA and MWA solvers wherever possible. Both the fast RFA and MWA solvers are integrated into a webframework (see Fig. 1 ) which is capable of generating different organ (i.e. liver, kidney, lung), vessel and tumour geometries from computed tomography (CT) or magnetic resonance images (MRI), to accept patient-specific parameters such as blood perfusion, body temperature, tissue and tumour density and device-specific parameters such as needle geometry, needle locations and power inputs for different ablation protocols. When the web-framework requests the simulation of the ablation protocol, a GPU instance is created to launch the respective solver and then the ablated lesion, which is extracted at the end of the simulation, is returned to the web-frontend for visualisation. The fast RFA solver takes less than 2 min for a treatment duration of 40 min and the fast MWA solver takes less than 3 min for a treatment duration of 60 min. Figure 1 shows the simulated ablated lesion zone of a patient with a 4 cm RFA ablation, Fig. 2 shows the lesion resulting from a 20 min heating/20 W power input protocol for the MWA solver. The retrospective study of the fast RFA solver is already available in [2] . However, the fast MWA solver presented here has only been validated against synthetic data and for a single retrospective case. The web-framework and fast solvers are integrated together and can be used by interventional radiologists across the globe to cooperate together to improve patient treatment. Int J CARS (2020) 15 (Suppl 1):S1-S214 Purpose A peripheral nerve block (PNB) is a type of anesthesia that involves the injection of an anesthetic around the peripheral nerve to block pain transmission. Ultrasound-guided PNB is a method in which the needle position and an image of the anesthetic spreading can be visualized on the sonogram, using an ultrasound imaging probe. Ultrasound-guided PNB is becoming a common procedure in administering regional anesthesia. However, high expertise is required for an anesthesiologist to guide the needle to the target nerve correctly, avoid a vascular puncture, and maintain the needle tip within the sonogram image. Previous studies have proposed mechanical guidance for needle insertion. However, these methods require frequent eye movements between the surgical field and the ultrasound monitor [1] . In this research, two small USB cameras are mounted on the probe, which provide top and side views that are shown on the same monitor with the ultrasound view. Moreover, the stereo cameras capture the marks on the needle; measure the three-dimensional positions; and calculate the insertion position, angle, and depth of the needle. When the target is specified, the navigation line for guiding the needle path is superimposed onto the ultrasound and camera images. Additionally, a needle position alert function has been developed that alerts any deviation in the needle tip position with a mark on the image or an audible beep. The purpose of this research is to evaluate the effectiveness of the developed needle position alert function. The proposed system consists of an ultrasonic diagnostic imaging system, a PC, and two small USB stereo cameras mounted on the probe, as shown in Fig. 1 . Intrinsic and extrinsic camera parameters were obtained in advance from several views of the calibration board, and the distortions of the image were corrected using these parameters. The image of the marks at intervals of 10 and 20 mm on the needle could be recognized automatically by a binarization and labeling process, and the three-dimensional coordinates were calculated using the stereo camera method. Subsequently, the insertion position, angle, and depth of the needle were calculated. When the target position is specified, the navigation line that guides the needle toward the target was displayed on the top and side camera images, and the ultrasound view. The feature of the system is that the diagonally set side camera can provide a direct side view. The doctor can perform the PNB precisely and safely using the superimposed predicted needle path, tip position, and the navigation line. Additionally, the newly developed needle position alert function ensures that the tip of the needle is in the correct position that is shown by the navigation line. Using this function, doctors can concentrate on the ultrasound image without looking at the camera images. Three functions were developed in this study. The first function is to distinguish the side of the needle in the ultrasonic imaging plane. When the needle is on the near side, the predicted line is drawn as a dotted line, and when the needle is on the far side, it is indicated by a solid line. The second function is to draw a diamond mark on the predicted needle line 0.4 mm from the ultrasonic imaging plane. In addition to this, the third function is to beep when the needle is aligned with the navigation line in the top view. The needle insertion task was performed by subjects to evaluate the effectiveness of the developed functions. The experimental setup is shown in Fig. 2 . Agar was used for the object of insertion and a crab stick that could be displayed on the ultrasound image was embedded in the agar as the target. Five subjects performed three types of needle insertion tasks five times. The needle position alert function was not used for the first insertion task type. The alert function without the beep was used for the second type. For the third type, the alert function with the beep was used. The number of successful insertions completed on the first attempt, number of overall tasks completed successfully, operation time, and distance to the target for each insertion task type were compared to each other [2] . The results are shown in Table 1 . The conventional PNB had top and side view cameras; however, only ultrasound images were displayed on the monitor. The insertion task types with the alert function and those without it showed considerable difference in the number of tasks that were successfully completed on the first attempt; this was confirmed using the Wilcoxon signed-rank test (p \ 0.05). However, there were no significant differences between the other factors such as the number of overall tasks completed successfully, operation time, and distance to the target. By using the needle position alert function, the number of re-insertions and the operation time could be reduced because this function made it easier to recognize the position of the needle and reduce the number of times the monitor needs to be looked at. A system that can support a PNB operation was developed. The features of the system are that the three-dimensional coordinates of the needle can be measured using two cameras on the probe, and the needle position prediction and navigation lines are displayed on the camera image and the ultrasound view. A needle position alert function was also added so that when the needle is not aligned properly, the system notifies the doctor with a mark on the image of the ultrasound view or an audible beep. Insertion tasks with and without the notification function were performed to verify its effectiveness. By using the needle position alert function developed in this study, the number of required re-insertions could be reduced. In future studies, anesthesiologists will perform the insertions to evaluate the effectiveness of the developed system. Purpose Percutaneous cryoablation has been used to treat tumors in various sites, such as kidney, prostate, liver, and bone. Percutaneous cryoablation is achieved by introducing one or more cryo-probes into the target lesion and decompressing a gas inside the probe to cool the tip down to temperatures below -100°C by the Joule-Thomson effect. This extremely cold temperature creates an area of frozen tissue around the probe tip so-called ''iceball''. The boundary between frozen and unfrozen tissues can be observed on most intraprocedural imaging modalities, including ultrasound, CT, and MRI, allowing real-time confirmation of tumor coverage by the iceball. However, predicting the final shape of the tissue-kill-zone is often challenging, due to the heat sink effect from adjacent heat sources (e.g., large blood vessels and urethral warmer catheter) and the synergistic effect between the multiple probes. Furthermore, it has been shown that tissue necrosis is induced at a lower temperature (-20°to -40°C depending on the tissue types) suggesting that the iceball might not predict the ablation volume accuracy. The lack of accurate prediction can lead to insufficient or excessive ablation, which impairs the clinical outcomes. Several groups have proposed computational approaches to predicting the lethal isotherm for cryo-probe placement planning. These approaches fall into two categories including geometric and thermal simulation approaches. The geometric approach does not take into account the aforementioned thermal effects providing faster yet practical solutions with compromised accuracy. On the other hand, thermal simulations could potentially offer a more precise estimation, though they are often computationally expensive and require patient-specific biothermal parameters, which are difficult to determine. In this study, we propose a new data-driven approach to predicting an isotherm for lethal temperature. While this preliminary study uses the iceball boundary, which is not at the lethal temperature, as a pseudo lethal isotherm, the model can be trained with the true lethal temperature isotherm, which could be estimated by intra-procedural MR thermometry or post-procedural contrast-enhanced MRI [1] . We implemented and evaluated a logistic regression model to predict the 3D iceball resulted from two and three cryoprobes using intraprocedural images acquired during MRI-guided focal cryoablation of prostate cancer. The model included cryoprobes as cooling sources and a urethral warmer catheter as a heat source. The iceball induced by a cryoprobe typically has an ellipsoidal shape, which can be described by a quadratic equation. Therefore, we used a voxel-wise model based on the square of the distances from each voxel to the thermal sources. Considering the patient coordinate system (RAS) as described in Fig. 1 , u as the shortest distance from the voxel to the urethral warmer catheter, d i as the shortest distance between the voxel and probe, and z i as the distance between the voxel and the center of the probe's cooling area along the S-axis, the probability (p) of a given voxel (v) being within the visible isotherm induced by N cryoprobes is defined as: p v ð Þ ¼ e f ðd;z;uÞ = 1 þ e f ðd;z;uÞ Fig. 1 Distances used in the logistic regression model: z is the distance between the voxel and the center of the probe's cooling area along the S-axis; u is the shortest distance from the voxel to the urethral warmer catheter; and d i is the shortest distance between the voxel and the probes where C, a, b and c are the estimated model parameters and Note that in MRI-guided prostate focal cryoablation the needles are inserted perpendicular to the RA-plane, and therefore d i is the distance between the voxel and probe on the RA-plane. In this study, we used retrospective data of 15 patients who underwent MRI-guided in-bore focal cryoablation of prostate cancer. In 11 cases, the physician used two cryoprobes, while three cryoprobes were used in the other four cases. In all cases, the physician inserted 17-gauge cryoprobes (IceSeed, Galil Medical Inc., Yokeneam, Israel) using a needle-guiding template and custom-made guidance software implemented in 3D Slicer. The probe location was determined on the confirmation images acquired using a T2-weighted turbo spin-echo sequence (20 slices, 3 mm slice thickness, FOV of 157 9 180 mm, and 168 9 192 matrix) after the probe placement. Monitoring images were acquired during the procedure at different time stamps using the same sequence. We manually segmented the visible iceball on monitoring MR images acquired between 9 and 11 min of the first freezing cycle. Misalignment between confirmation and monitoring images was eventually corrected using an intensitybased rigid image registration algorithm. We evaluated the accuracy of the prediction by calculating the Dice Similarity Coefficient (DSC) in a cross-validation analysis using the leave-one-out approach. The average DSC was 0.76, ranging from 0.67 to 0.85. A representative case with two cryoprobes is presented in Fig. 2 . The average DSC of procedures employing two cryoprobes was higher (0.78) than using three (0.73). The results with three cryoprobes were affected by the variability of the probe configurations. In one case, the three probes were distributed along a straight line, while in others, they had a triangular disposition. However, the worst prediction occurred in a situation with two probes, where the distance between the probes was larger than 20 mm. It is worth mentioning that the analysis has some limitations. First, the timestamp of the monitoring images has a limited accuracy as it was manually set during the procedure. Another source of noise is the fact that the center of the image artifact was considered as the probe location, but the artifact is usually wider than the actual probe. The use of statistical machine learning techniques to estimate the isotherm in focal cryoablation was feasible. The average DSC in our cross-validation analysis was consistent with other studies using biothermal simulations (DSC = 0.82) [2] . Our data-driven approach was able to predict the isotherm without the need for patient-specific parameters. We are currently studying the implementation of a similar strategy to predict the lethal isotherm in liver and kidney cryoablations. Furthermore, we plan to test the improvement of using this approach during the planning phase of clinical cases.  Timely, accurate and reliable assessment and follow-up of fetal development with respect to gestational age, are essential to reduce short/longterm risks to the fetus and mother. Ultrasound imaging is currently the primary method for prenatal evaluation. Due to superior capabilities, MRI is increasingly used in cases of unclear US findings, to confirm/ reject suspected abnormalities, and to detect additional abnormalities. Most fetal MRI interpretation is qualitative and subjective, with only a few quantitative measures. Most MRI scans performed to assess fetal brain development include three common quantitative linear measures: Cerebral Bi-parietal Diameter (CBD), Bone Bi-parietal Diameter (BBD), and Trans Cerebellum Diameter (TCD). These three linear measures are performed by the radiologist on a specific scan in two steps: (1) selecting the most appropriate slice for each measure (CBD and BBD are defined on the same slice), and (2) marking the linear measure on the chosen slice. These measurements are performed manually, thus are time consuming, inaccurate and suffer from inter-and intra-observer variability. While several studies were published on automatic assessment of several measures based on fetal Ultrasound, to the best of our knowledge, there is no previous work on fetal MRI. The aim of this study was to develop a method for automatic measurement of these three linear biometrics: CBD, BBD and TCD. The method developed in this study consists of five steps ( Fig. 1 ): Fig. 2 The 3D view of a representative algorithm output is presented in the left, where the green volume is the segmented iceball and the predicted one is represented in blue. The threshold for the predicted iceball was p = 0.5 (1) An axis-aligned bounding box (3D Region of Interest, ROI) was computed containing the fetal brain with an anisotropic 2D U-Net network trained to segment the fetal brain parenchyma; (2) Selection of the slice in which the measurements will be performed. We used transfer learning on a pretrained Resnet34 on ImageNet for two classes: the target slice and all other slices (an average of 25 slices, range 15-35 slices). To compensate for the data imbalance (1 versus 25), we randomly sampled only two slices from all other slices in each epoch. We performed image augmentations that preserve context including random centered-cropping and random affine transformations. We trained the entire network for 10 epochs and then separately trained the classifier for 20 additional epochs; (3) Segmentation of the two brain hemispheres, the cerebellum, and the background with a 2D multi-class U-net network. The network was pretrained Resnet34 encoder with a U-net-like decoder, and trained with Lovasz loss; (4) Computation of the midsagittal line (MSL) by finding the minimal-margin 2D plane with SVM on two hemispheres as two separate classes. Since the cerebellum is inferior to the brain hemispheres, we computed the brain orientation by taking the cross product of the MSL normal vector with the vector defined by the mid MSL and a sampled point on cerebellum; (5) Computation of the three linear measurements. The CBD measurement was computed by finding the maximal width in respect to MSL in the upper part of the segmented brain. The BBD was computed by continuing the CBD line in the two outer directions until each reaches the first local minima of intensity derivative. The TCD measurement was computed similarly to the CBD on the segmented cerebellum by computing the maximal width with respect to the MSL. We evaluated our methods on 200 MRI scans of 135 fetuses at 22-38 weeks' gestational age acquired at the Tel-Aviv Sourasky Medical Center with a 1.5T General Electric Discovery MR450 scanner. Measurements were performed on T2 weighted images with in-plane resolution of 0.60-0.87 9 0.60-0.87 mm 2 and slice spacing of 3-8 mm. Automatic measurements were compared to ground truth performed by an expert radiologist who manually selected the measurement slices (step 2) and performed the linear measures on them (step 5). We compared the slice selection accuracy with two metrics: (1) slice shift, defined as the number of slices away from the ground truth slice Shift = |(argmax P i ) -K|, where P i is the probability predicted by the network for slice S i and K is the slice selected by the radiologist, and; (2) selection accuracy = 1 -(Shift/N), where N is the number of slices in a scan. To evaluate the slice selection for CBD and BBD, we used train/validation/test sets of 49/17/43 scans. The mean selection accuracy was 0.979/0.981/0.968 and the mean shift was 0.454/0.411/ 0.795, respectively. For the slice selection for TCD, we used train/validation/test sets with 134/35/43 scans. The mean selection accuracy was 0.981/0.978/0.975 and the mean shift was 0.477/0.457/ 0.604, respectively. For the measurement step, we randomly selected 40 scans and computed for each measure the Bland-Altman bias and limits (minimum and maximum) of agreement (95% confidence interval). The results are shown in Table 1 . Note that the first row shows the comparison of the algorithm results to the radiologist annotation, while the second compares two radiologist annotations. Conclusion This is the first work to automatically assess linear measurements on fetal MRI. The accuracy of slice selection was found to be within 1 slice shift from the slice selected manually by the radiologist. The computed CBD, BBD and TCD measurements are accurate and the agreement limits are lower than the inter-observer variability as reported in [1] . Purpose Two lines of evidence indicate it is essential to expand surgical resources worldwide, particularly in low-and middle-income countries (LMICs): (1) One-third of all deaths worldwide result from lack of surgery-10 times the deaths due to malaria, tuberculosis, or HIV/AIDS. By 2030, the cost of lack of surgery in LMICs-in terms of GDP lost-will approach US $1.5 trillion annually ( Fig. 1 ) [1] . (2) Over the past 2 decades, natural disasters have cost over 1.3 million lives (mostly in LMICs) and at least US $2 trillion in damage. The annual number of deaths from earthquakes alone is greater than the number of people killed in traffic accidents in North America and the European Union combined. Both a lack of surgical resources for daily care and a lack of resiliency (e.g., power outages) are frequent contributors to unnecessary morbidity/mortality. Deaths due to lack of surgery for acute abdomen alone in India number 50,000 yearly; avoidable deaths from trauma, stroke, difficult childbirth, etc., no doubt number in the hundreds of thousands-if not more-in India annually [1] . The need for more (and more resilient) surgical resources can be addressed by technological advances in radiology and surgery.  Methods Trauma and stroke centers (TSCs) evolved in high-income countries (HICs) with evidence that immediate ''24/7/365'' treatment dramatically improved morbidity/mortality. TSCs are part of the ongoing healthcare system-not a separate entity. Fortunately, the universal humanitarian response to MCDs suspends political, cultural, and socioeconomic barriers that hinder a coordinated response to other global crises: groups frequently at odds with each other unite during an MCD. We propose that MCD response-like TSCs-be integrated into ongoing healthcare systems with Mass Casualty Centers (MCCs) [1, 2] . Each MCC, like a TSC, is staffed by specialists from all aspects of emergency response-available 24/7/365. Integrating the civilian and military medical resources improves efficiency and minimizes duplication. MCCs can be staffed by local physicians and nurses working side-by-side with HIC physicians and nurses (on a rotating basis), following the ''twinning'' model for partnering medical centers in LMICs and HICs [1, 2] . Computer-assisted technology has enhanced both daily healthcare and MCD response-to both natural disasters (earthquakes, typhoons, tsunamis) and un-natural or man-made disasters (infrastructure failures, transport accidents, terrorism) [2] . A common electronic health record and data collection platform across MCCs enhances standardization of best practices and educational techniques. Telemedicine not only mitigate morbidity/mortality in disasters but also provides immediate 24/7/365 telesurgical guidance (and soon, remotely-guided robotic trauma surgery). Battery-powered CT scanners provide resilient infrastructure during both routine power outages (common in LMICs) and MCDs (Fig. 2) . Mobile operating roomsportable by helicopter-enable surgery anywhere world-wide within hours. Drones and robots improve both daily healthcare (e.g., transport blood products and lab specimens over difficult terrain) and MCD response (e.g., identify the living buried in rubble, triage medical resources to maximize benefit). Resilient, full-service, 24/7/365 MCCs augment the healthcare resources of the region served during non-mass casualty times (as do TSCs in HICs): they provide radiology, blood bank, laboratory services, critical care, etc. Groups with resources to optimize costeffective and immediate daily and MCD care include [2]: 1. The International Virtual eHospital (IVeH) has developed telemedicine programs for Albania, Cabo Verde, the Philippines, and the North Atlantic Treaty Organization (NATO). India, provides daily telemedicine consultation services to over 30 countries (mostly in sub-Saharan Africa). 3. The Center for Robot-Assisted Search and Rescue (CRASAR), based at Texas A&M University, provides immediate free MCD response assistance with robots and drones. The initial MCC sites are Iquique (northern Chile) and Peshawar (northwest Pakistan) [1, 2] . In Iquique, a joint meeting was held in 2018 with the local health authorities, the military (Chilean Air Force Base in Iquique), and the Chilean Ministry for Emergency Response (ONEMI). In 2019, additional meetings were held with the Ministry of Health and the Director of the Chilean Naval Hospital. In Peshawar, over the past decade a local neurosurgeon (and member of the MCC project) has opened two general hospitals, a medical school (100 students/year), a nursing school (50 students/ year), and a ground ambulance service. Community-based trauma prevention and rehabilitation programs have been implemented; the Peshawar Chapter of the ThinkFirst Trauma Injury Prevention Program (begun in the USA in 1986) received the Program's 2019 International Achievement Award. The Pakistani Military Commander for the Northern Region supports the MCC project, with several meetings being held in 2019, most recently in November. Meetings with the Surgeon General of Pakistan will be held early in 2020, and the MCC Project is coordinated with the new Pakistani National Vision for Healthcare 2025 Plan. A major academic hospital organization in the USA is adapting their cutting-edge electronic medical data system for the MCC project. Universal training and certification standards, and research protocols across many medical disciplines, will be developed for MCCs by international medical/surgical societies and similar organizations in emergency medicine, critical care, rehabilitation, etc. The MCC team of international experts continues to expand, and presently is composed of trauma surgeons, neurosurgeons, telemedicine and e-health experts, emergency response nurses, former Ministers of Health from Peru and Pakistan, and a former CEO of the National Health Service in the United Kingdom. Conclusion Technical advances in resilient and mobile imaging and surgery, electronic data collection and analysis, telemedicine/telesurgery, robots/drones make not only feasible but economically and morally essential the improvement in both daily healthcare and MCD response that MCCs provide. MCCs implement universal standards for medical/surgical training, and provide an unmatched global platform for research. They foster camaraderie between physicians and staff from various LMICs and HICs, and development of cost-effective medical/surgical techniques. MCCs advance healthcare and economic progress in LMICs, and can be key to realizing many of the UN Sustainable Development Goals (SDGs) for 2030 [1, 2] . There are substantial political and socioeconomic benefits-beyond the healthcare benefits-of integrated MCCs as part of global healthcare. Keywords 5G, Remote ultrasound, Emergency care, Telemedicine With the expansion of the digital functionalities in the sense of ''surgery 4.0'' facilitating important applications such as tele-operational medical assistance, the question of improved data transmission quality becomes more and more urgent [1] . As the currently available technologies for data medical transmission (Ethernet/LAN, WiFi, Bluetooth) are limited in terms of transmission rate, latency and signal stability, the fifth generation of mobile data transmission technology (5G) seem to offer a high potential [2] . Especially for sophisticated medical applications such as remote interventional support for emergency care applications, 5G based data transfer between moving point of care and hospital is of high value to increase both ergonomics and patient safety. In the presented study, we examine bi-directional data transmission for ultrasound, audio and video source in an emergency setting. The ambulance car functions as the first room of hospital, which may be far away of the medical expert in the hospital. Real-time diagnosis by remote ultrasound diagnosis prior to arrival effectively increases diagnostic accuracy and thereby increases patient's safety and comfort. Possible benefits of 5G communication are: • Low latency, ultra-reliable (URLLC). • High bandwidth (eMBB). • Network slicing (guaranteed service). • Higher resolution for video communication. • Update of the product to support higher resolutions and realtime control feedback. In the present study, a mobile ultrasound unit (mUS) is connected to a local 5G facility, facilitating data transmission to a medical expert in a nearby hospital. The data transfer from the mUS to the 5G facility (antenna) is pure 5G, the data transfer to the hospital is cable bound. In addition, real-time video and audio conference between ambulance and hospital is evaluated. The study addresses RAN, Core Network, Orchestration/Slicing and UE aspects. The study experiments with different application KPI settings targeting excellent Quality of Experiences (QoE): • Video: resolution, frame rate, color depth, latency (ambulance to hospital). • Ultrasound: resolution, latency. • Assess importance of coverage/reliability. • Delay of control feedback loops. The study derives necessary vs. available Network KPI's and gap analysis and necessary application KPI's (''video quality'') from a clinical perspective (Fig. 1 ). Video conference is 5-10Mbps in both directions. Sharing of sensor application is \ 5 Mbps in UL. Remote control of the sensor application is \ 1 Mbps in DL. The study proposes the following traffic slicing: • Slice a: video/audio conference (bi-directional) • Slice b: sensor application Different slicing methods are going to be tested e.g. discriminatory packet processing Two scenarios are anticipated: 1. Static scenario: ultrasound examination in stationary ambulance car 2. Dynamic scenario: ambulance car driving around 5G coverage area of the local 5G facility Conclusion 5G data transmission volume, rate and latency will meet the requirements for real-time telemedicine applications. Especially for remote emergency care applications such as mobile ultrasound, 5G data transmission offers a high potential and further research should be carried out. 24th Annual Conference of the International Society for Computer Aided Surgery Fast collision detection approach for elastic embedded objects using dual graph K. Tagawa 1 , N. Tani 2 , H. T. Tanaka 2   1 Aichi University of Technology, Department of Media Informatics, Gamagori-shi, Japan 2 Ritsumeikan University, Kusatsu-shi, Shiga, Japan Keywords Collision detection, Multiple organs, Surgery simulation, Adaptive mesh Minimally invasive surgery (MIS) such as laparoscopic surgery (LS) was introduced because this type of surgery has many advantages: these include increased safety, decreased scarring and faster recovery. However, this surgery requires highly skilled surgical techniques due to limited surgical view and less tactile sensation. Skills training using virtual reality (VR) surgery simulator is effective for the following reasons: (1) simulator can evaluate the performance of the trainees at any time, (2) all simulator sessions can be repeated at any time in a defined identical setting, and (3) simulators allow for the complexity and difficulty of tasks to be scaled to the skills and experience of the trainee. The VR surgery simulator is composed of two heavy computation processes: (1) physics simulation (rigid body simulation, deformable object simulation and collision detection), (2) visual and haptic rendering (sensing of users'' operation and displaying the scene of the surgical field and reaction force). To manipulate the organs interactively, efficient approaches for these processes are required. Especially, collision detection between deformable objects (e.g. collision detection between multiple organs) requires long computation time. In this paper, to realize interactive VR surgery simulation, we propose a fast collision detection approach for elastic embedded objects using dual graph tetrahedral adaptive mesh. In order to realize fast deformation simulation, we proposed an online re-mesh (multi-resolution) approach using a tetrahedral adaptive mesh ( Fig. 1 ) [1] . In tetrahedral adaptive mesh generation, the 3D region (elastic object) is initially enclosed with six root tetrahedra. Then, according to local field properties (e.g. material properties or curvatures of isosurfaces) observed at the nodes, the root tetrahedra are recursively bisected (subdivided at the middle point of the longest edge of the tetrahedron) independently in the region of rapid field variation. This subdivision process is repeated until the entire volume is approximated with the given accuracy criterion. At the same time, triangular patches are constructed by the marching tetrahedra method [2] , and these patches are embedded in the tetrahedra. (2) Fast collision detection approach using dual graph In this paper, we propose a novel collision detection approach using the tetrahedral adaptive mesh. Collision detection between complex objects is often performed by using hierarchical bounding volume for fast computation. James et al. proposed a computationally efficient collision detection approach for deformable objects using ''bounding spheres hierarchy''. However, they used a particular deformation model (they called it ''reduced deformable model'') which is difficult to represent topological change such as cutting of the deformable objects. We propose another approach which uses a dual graph (binary tree and neighbor graph) of the tetrahedral adaptive mesh. An overview of our approach is shown in Fig. 2 . At first, we search a colliding leaf tetrahedron by using its binary tree. We start from six root tetrahedra and compute distances between their split planes and the colliding object's node to determine which child tetrahedron (left or right tetrahedron) includes the node. This process is performed recursively. However, because the split plane of the ancestor tetrahedron roughly approximates split planes of descendant tetrahedra, incorrect colliding leaf tetrahedron may be detected. As a result of correct colliding leaf tetrahedron existing in neighboring searched tetrahedron, we perform additional searching process to detect correct colliding leaf tetrahedron by using its neighboring graph. We implemented this approach into our surgery simulator. Collision detection between multiple soft tissues (liver and hand) was performed. The numbers of nodes of the liver and hand were 4913 and 32471, respectively. Maximum computation time of the collision detection process was 2 ms. This is sufficient for presenting haptic sensation. In this paper, we proposed a novel collision detection approach which uses a dual graph (binary tree and neighbor graph) of the tetrahedral adaptive mesh. This approach is able to represent cutting of deformable objects. Through evaluation experiments, we showed the effectiveness of our approach. Aichi Cancer Center Hospital, Nagoya, Japan 5 Nagoya University, Information Technology Center, Nagoya, Japan 6 Research Center for Medical Bigdata, National Institute of Informatics, Tokyo, Japan Keywords Blood vessel segmentation, Convolutional LSTM, U-Net, Laparoscopic video Purpose This paper proposes a blood vessel segmentation method from laparoscopic video based on deep learning technique which utilizes time-series information. Higher surgical skill is required to surgeons in laparoscopic surgery. Therefore, surgical assistances by computers such as surgical navigation and workflow analysis are being developed to reduce surgical errors. To perform computer aided surgery, automated understanding of operative field is necessary. Understanding of operative field includes anatomical structure and surgical tool segmentation or detection. Among anatomical structures, the blood vessels are important because they relate to almost all organs and accidental damage to them causes severe complications. Surgical navigation systems that shows blood vessel information will be useful. We propose an automated segmentation method of the inferior mesenteric artery (IMA), which is commonly operated during surgeries for the colon and rectum, from laparoscopic video. We use a fully convolutional network (FCN) to segment IMA regions from video. Time-series information is important to process video. Therefore, we introduce a convolutional long short-term memory (LSTM) layer in the FCN to perform segmentation utilizing timeseries information. We perform segmentation of IMA region from laparoscopic video using an FCN. The FCN has a structure based on U-Net with some modifications. We remove some layers in U-Net to reduce the number of trainable parameters and to reduce overfitting. Also, we replace concatenate operations with summantions as proposed in [1] . The image sizes of the input and output are 256 9 256 pixels. In video processing, utilization of time-series information is important. We introduce a convolutional LSTM layer to our FCN. We inserted a convolutional LSTM layer before the last (close to output layer) activation funciton in our FCN. We call this FCN the ConvLSTM U-Net. The input of the ConvLSTM U-Net is a set of time-series images and the output is a segmentaion result corresponds to the final frame of the input image. The length of time-series of the input is L frames. (2) Training and inference of ConvLSTM U-Net We train the ConvLSTM U-Net using images in laparoscopic videos for training and manually segmented IMA regions of them. The minibatch size in the training is 5. In the inference using trained ConvLSTM U-Net, we input images in laparoscopic videos for inference and obtain segmentation results. We applied the proposed method to segment IMAs from 37 laparoscopic videos of the sigmoid colon or the rectum resection surgery. Images for training and inference weer obtained from the videos. Ground truth IMA regions were manually given under supervision of surgeons. The numbers of images used in training and inference were 2108 (from 30 videos) and 458 (from 8 videos), respectively. Segmentation accuracy have relation to the length of time-series used by the proposed method. Therefore, we performed experiments using three lengths including L = 4, 8, and 16 frames. Examples of segmentation results are shown in Fig. 1 . From the results, IMA regions were correctly segmented by the proposed method. Because the proposed ConvLSTM U-Net utilizes time-series information, the segmented regions smoothly changed along the time series. The ConvLSTM U-Net enables stable segmentation. Quantitative results of the segmentation results are shown in Table 1 . The precision rate was highest when we use L = 4 frames. It slightly decreased when we used longer time-series. In contrast, the recall rate was high when we used longer time-series. It means that false negative regions of segmentation results were decreased by use of longer time-series information as shown in Fig. 2 . Although false positive regions increases, reduction of false negative regions is important in surgical assistance to reduce overlooking of blood vessels. It means results obtained by using longer time-series information are more suitable for surgical assistance. The dice scores also showed  Keywords Tissue-mimicking phantoms, Ultrasound imaging, Imageguided interventions, Multimodal imaging Purpose Phantoms are increasingly used for optimising image-guided percutaneous interventions, clinical training and preoperative surgical planning [1] . With an increasing number of procedures that use both ultrasound and X-ray computed tomography (CT) for guidance, there is a need for anatomically-realistic phantoms that are compatible with both modalities. These phantoms require suitable tissue-mimicking materials that can simulate a variety of structural components. Previously, we demonstrated that gel wax, a recently-introduced tissuemimicking material, is well suited to create anatomically-realistic ultrasound imaging phantoms [2] . Here, we propose a new method to incorporate radio-opaque contrast into gel wax and thereby, to create phantoms for multimodal guidance. As a demonstration of the method, a series of phantoms that included wall-less vessel-like structures were fabricated. Creating vessel structures involved two steps. First, metal rods were inserted in a custom-made container and melted gel wax was poured in layers corresponding to different anatomical structures, with cooling of each layer before pouring the next one. Subsequently, the rods were carefully removed to create wall-less (hollow) vessel structures. Each layer was fabricated from gel wax as a base material, with different additives for imaging contrast including solid glass spheres to provide acoustic backscattering and a radio-opaque contrast agent for X-ray contrast. An ultrasound system (Ultrasonix Sonix SP) with a linear array (L14-5) and an intraoperative X-ray system (Medtronic O-Arm) in Cone Beam CT mode were used; a water-based radio-opaque contrast agent within certain vessels provided X-ray contrast. A cross-sectional CT image obtained from a representative phantom is shown in Fig. 1 . The left vessel filled with a radio-opaque contrast agent had the highest X-ray contrast, as compared to the middle and right vessels. The right-most vessel, which contained air, appeared to have lower density. Increased contrast was observed in the third layer, as compared to those from the first two. With B-mode ultrasound imaging, the cross-sections of the vessel structures were clearly visible (Fig. 2) . The left and middle vessels were hypoechoic so that they could be readily distinguished from the background; the right-most vessel was seen a bright reflective surface with posterior shadowing artefact in the image. The three layers could readily be differentiated; the first layer appears anechoic; the second and third layers had a speckled appearance similar to that of biological soft tissue imaging. We demonstrated for the first time that gel wax can be used to develop tissue-mimicking phantoms for multimodal ultrasound and CT imaging. This study will lead directly to clinical phantoms for multimodal image-guided interventions. We discuss how the method presented here can be used to fabricate patient-specific complex anatomical structures extracted from preoperative imaging data. Keywords TaTME, Urethral injury, prostate segmentation, Convolutional neural network Purpose Transanal total mesorectal excision (TaTME) has received considerable attention in literature following the report on the first clinical case in 2010. TaTME aims to address the limitations of TME surgery, particularly for low rectal cancers, and to improve oncological outcomes. TaTME can be a technically challenging procedure, and urethral injuries (UIs) are significant complications pertaining to TaTME. It is important for surgeons to recognize the prostate during TaTME to prevent UI occurrence, and intraoperative image navigation could be considered useful in this regard. Most extant studies regarding prostate segmentation are based on use of medical imaging technologies, such as MRI and CT [1] . However, application of MRI-and CT-based abdominal organ segmentation data into laparoscopic surgical field requires image registration. This process is yet to achieve acceptable accuracy to facilitate the practical utilization [2] . Therefore, segmentation based on intraoperative video can be considered highly useful owing to its omission of image registration. This study aims at developing a deep-learning model for real-time automatic prostate segmentation based on intraoperative video during TaTME. Evaluation of the proposed model's performance has also been undertaken. Methods Five hundred images of the prostate were extracted randomly from the video dataset and divided into two subsets, i.e., 80% of the images were used to train algorithms and 20% were used to test algorithms. The data were divided at a per-case level rather than at a per-frame level. Thus, the frames from a video in the training dataset did not appear in the test dataset. As preprocessing, the videos were converted to the MP4 codec with a display resolution of 1280 720 pixels and a frame rate of 30 fps. Annotation was manually performed by a board-certified colorectal surgeon. Unlike medical images, such as CT and MRI, the recognition of organ area becomes difficult to realize when using intraoperative video owing to the ambiguity associated with organarea boundaries. Therefore, the proposed model is designed to perform segmentation using two colors. That is, prostate areas that are completely exposed are assigned Color A (first color), whereas prostate areas with confirmed presence but incomplete exposure are assigned Color B (second color). All modeling procedures were performed using a script written in Python 3.6, and a computer equipped with the NVIDIA Quadro GP 100 graphics-processing unit (NVIDIA; Santa Clara CA) and Intel (R) Xeon (R) central processing unit E5-1620 v4 @ 3.50 GHz were utilized for model training. A 32-GB random access memory (RAM) was used. DeepLab v3 plus was utilized for the semantic segmentation task. Pretraining was performed on the ImageNet 2012 classification database, and data augmentation was performed using LR flip. The Dice coefficient (DC) was obtained for each image to evaluate the accuracy of prostate segmentation. Five-fold crossvalidation tests were performed (i.e., 400:100 split data were validated 5 times using different combinations to calculate average DC values for evaluating semantic segmentation accuracy). Areas marked with colors A and B were considered different to recognize their relative positional relationship. During testing, however, both areas were considered the same, and DC values were calculated accordingly. The average DC calculated based on the two-color annotation equaled 0.71 ± 0.04 with corresponding minimum and maximum DC values being 0.66 and 0.77, respectively. In contrast, the average DC calculated based on the conventional single-color annotation equaled 0.68 ± 0.02 with minimum and maximum DC being 0.66 and 0.70, respectively. Thus, albeit by a slight margin, the two-color annotation was found to be statistically significant, and its use improved the accuracy of prostate segmentation during TaTME (p =0.0458), Fig. 1 . In addition, the model operated at 11-fps speed, Fig. 2 . Given that not all frames need be processed, the above-mentioned speed was considered adequate and acceptable with regard to real-time performance. The first layer appears anechoic; the second and third layers show a speckle pattern similar to soft tissue Conclusion A fairly high DC was obtained by utilizing our deep-learning model for the semantic segmentation of the prostate in the TaTME video. Moreover, the model showed acceptable performance for application to real-time prostate segmentation during TaTME. To the best of our knowledge, this is the first effort towards realization of computer-assisted TaTME without image registration, and results obtained in this study suggest that the proposed deeplearning model can be utilized for real-time automatic prostate segmentation. As a future endeavor, it is it is intended to improve the accuracy and performance of the said model, thereby facilitating its use in practical applications and verify it capability to reduce UI risks during TaTME. Purpose Improving the training process of surgeons as well as the safety and efficiency of surgical procedures is of outmost importance. With recent developments in the fields of deep learning and computer vision there is a growing interest in using these tools to study and analyze surgical procedures by means of video data. While the use of video is an integral part of minimal invasive surgery (MIS), it is not well established in open surgery. Due to different nature of open surgery and MIS they will require unique algorithms. Tool usage is one example. In MIS the video image typically includes 1-2 tool tips. In contrast, during open surgery we see the hands, the tools being used and in many cases the tool try will be in the cameras field of view. Thus, tool detection will require detecting multiple tools in the image and tool usage will require identifying the tools being currently used by the surgeon. In this study, we developed an algorithm for identifying tool usage in open surgery using video data collected on our variable tissue simulator. Data were collected using a variable tissue simulator [1] . Two different simulated tissue types were presented: tissue paper (simulating friable tissue) and rubber balloons (simulating an artery). The suturing task was to place 3 interrupted instrument-tied sutures on two opposing pieces of material. Surgical tools provided were a needle driver, surgical forceps and suture scissors. Eleven medical students, one resident and 13 attending surgeons participated in the study. Each participant performed twice on the friable tissue simulator and twice on the artery simulator. Thus, there were a total of 100 videos, each approximately 2-6 min long. Video data were captured using two cameras, providing top and side views. In this study only the top view data were used. The main goal was to recognize in each frame hand activity during the suturing procedure. We defined activities according to the relevant surgical tools held: needle drive, forceps and scissors, or using the hand with no tool. The recognition process was performed in three steps: 1. Detection: YOLOv3 [2] pre-trained on the COCO dataset was used to detect the hands and the different tools. In the first stage, 930 frames, taken from 10 videos, were manually labeled. The labeling included a bounding box of each tool and for each hand, as well as the identification of which tool was held in each hand. 2. Tool usage: geometric information was used to infer which tool was used by each hand. After filtering out very small tool bounding boxes, the overlap between the tools bounding box and the bottom medial quarter of the hand's bonding box was calculated. The overlap was measured using Szymkiewicz-Simpson coefficient: Tools that had an overlap smaller than the overlap threshold were excluded from the list. The tool which had the highest overlap coefficient was selected ( Fig. 1 ). 3. Data filtering: For each frame tool usage was based on the majority of the previous 15 frames. In addition, empirical data revealed that when the hands were moving fast, tool detection was significantly reduced due to image blurriness while hand detection remained. Therefore, hand speed was calculated based on hand detection data. Decision regarding tool usage was not changed during hand fast movement. A sample image of the outputs of the tool detection and tool usage algorithms is depicted in Fig. 1 . Tool usage in all 100 videos were manually labeled. This only required labeling the frames in which tool usage was switched, which was significantly faster than the labeling required for developing the detection algorithm. The precision and recall rates of each tool as well total accuracy were calculated (Table 1 and Fig. 2 ). In addition, the total detection rate of the left hand and right hand were measured. The system detected the hands extremely well, approximately 98% of the time. Initial activity Purpose Augmenting x-ray (XR) imaging with 3D anatomic overlays is an essential technique to improve the guidance of the cardiovascular interventions. This method significantly benefits from automatic identification of the instrument position in 2D and 3D, as well as automatic extraction of the respiratory and cardiac motion out of the 2D fluoroscopic images to continuously update the model overlay position or to compensate motion during tracking of the instrument within the static roadmap. The dataset inlcuded 100 videos. From 10 videos we randomly selected 930 frames which were used for training the detection algorithm. Therefore, when analyzing the tool usage algorithm (for the full videos) we analyzed the 10 videos and the remaining 90 videos separately Int J CARS (2020) 15 (Suppl 1):S1-S214 S67 In the transcatheter aortic valve implantation (TAVI) procedures rapid right ventricular pacing is used to ensure balloon stability during deployment of the aortic valve. The required pacing catheter is introduced in an early phase of the intervention, located close to the target anatomy, and fixed relative to the target anatomy. Thus, it is almost synchronously moving with the target structure and can be used as a reference. The purpose of this work is to fully automatic follow the rapid pacing catheter tip in the 2D fluoroscopic images. The method utilizes deep convolutional neural network (NN) based on U-net model to segment the rapid pacing catheter tip in XR images. 151 fluoroscopy runs (14,243 frames of 512 9 512 pixels resolution in total) in which the rapid pacing catheter was present in the frame were preprocessed and labeled. For each frame in the XR run an image pair (sample) consisting of a lossless grayscale 2D XR image and corresponding binary image (where the catheter tip pixels have a value of 1 and the background pixels 0) ( Fig. 1, top) was created semi-automatically. 15 9 15 pixels region of interest (ROI) was manually set in the first frame of each run and the rapid pacing catheter tip was segmented by thresholding the signal intensity values within the ROI. In the consecutive frames the catheter tip was automatically tracked with template-matching using normalized crosscorrelation in Matlab, allowing dynamic shifting of the ROI and automatic segmentation of the catheter tip by thresholding over all frames. To ensure data containing all directions, images were randomly rotated by 90, 180, or 270°. Samples were split into training-, validation-and test-sets with a ratio 84%/8%/8%. End-to-end U-net model similar to those proposed in [1] , consisting of a convolutional part downsampled with the maxpool layer and strided transposed convolutional upsampling part in combination with drop out regularization and residual connection to the output ( Fig. 1 , middle) was implemented using Keras deep learning library and trained on Tesla K80 GPU for 50 epochs with the batch size of 8 samples per pass with the adaptive moment estimation algorithm. To quantify the localization accuracy of the proposed approach, the predicted segmented images of the test dataset were analyzed for all detected contours. The center of the most prominent contour (corresponding to the detected catheter tip) was calculated using image moments in OpenCV. If more than one contour was detected in some images the tracking over fluoroscopic run was kept continuous by minimizing the distance between the centroids for all detected contours in the current frame and the centroid position corresponding to the catheter tip in the previous frame. The ground truth images of the respective test dataset were post-processed in the same way, allowing direct comparison of the target values-horizontal and vertical coordinates of the calculated centroid for contour segmented by thresholding using cross-correlation over one fluoroscopy run with the coordinates of the centroid predicted by NN. Additional test cases (5 fluoroscopy runs) where the cross-correlation did not provide reliable segmentation (e.g. crossing catheters, pacing catheter not visible in all frames) were selected to evaluate the proposed approach in challenging situations. The rapid pacing catheter tip could be segmented and localized in all test datasets. Predicted images overlayed on the grayscale XR images randomly taken from the test datasets can be appreciated on the bottom part of Fig. 1 . Figure 2 visualizes the superposition of target values and prediction for a randomly chosen fluoroscopy run from the test dataset. Despite a mean average error of 0.75 pixels (resp. 0.44 pixels) in horizontal (resp. vertical) direction, an excellent overlap could be achieved over all test dataset. The average time to predict the whole segmented image of 512 9 512 pixels resolution was 26 ms allowing application of the proposed approach in real-time [at standard fluoroscopy operation of 15 fps (* 67 ms)]. Even though catheter tip segmentation was not flawless in challenging cases, in direct comparison with the cross-correlation approach, the NN approach performed superior. In particular, the NN approach allows intrinsic stabilization after the critical situations (crossing catheter, out-of-image catheter). Training data are semi-automatically generated based on signal intensity thresholding within the region of interest dynamically adapted using cross-correlation (top), fed into the U-net model (middle), and the segmented images are predicted and overlayed on the XR images from test datasets (bottom) Fig. 2 Overlay of horizontal (left) and vertical (right) coordinates of the centroid of the contour indicating rapid pacing catheter tip predicted by the neural network (blue) and segmented by thresholding using cross-correlation (green) for an exemplarily chosen test dataset The proposed method yields accurate real-time segmentation and localization of the rapid pacing catheter tip in the 2D fluoroscopic frames. As expected, the robustness of the method depends on the visibility of the catheter tip in the image and interfering features, like e.g. crossing catheters, may limit the accuracy. However, the NN approach has the potential to intrinsically stabilize after the critical situation is solved. Further improvement may be possible by e.g. incorporating a Kalman motion predictor. Acknowledgement This work was supported partly by the project ''Multimodal 3D Navigation System for Transvascular Interventions (TRANSNAV)'' granted by German Federal Ministry of Education and Research (Contact No. BMBF-13GW0372C). Purpose Fetal surgery has now become a clinical reality, for conditions such as twin-to-twin transfusion syndrome (TTTS) and spina bifida [1] , partly thanks to advances in fetal ultrasound and MRI technologies. In TTTS the placental vasculature presents small fenestrations, called anastomoses, where the circulatory systems of the two fetuses are connected causing ischemia and cardiac failures in the twins. The elective therapy for TTTS is selective photocoagulation of the anastomoses where a fetoscope is inserted in the uterus and all of the fenestrations are closed. As in any surgery, the best outcomes are likely when surgeons are prepared preoperatively with an in-depth understanding of the anatomy that they will confront. Specifically, for TTTS, the position of the placenta changes in every pregnancy, making the choice of the fetoscope insertion point one of the most determinant factors for the success of the intervention. In our previous work [2] , we presented a surgical planning application that uses patient-specific virtual 3D models of maternal and fetal anatomy to guide the choice of the best insertion point. In this abstract we present a virtual reality interface that makes the exploration of the fetal surgical planning easier also for junior surgeons and allows to experience first-hand the main difficulties of the intervention before entering in the OR. Image segmentation: Our existing surgical planner [2] was used to perform segmentation of MRI data, creating the mesh of the uterus, placenta and vessels. We focus on those three parts because they are the most important ones as: (1) the uterus is used to define the pivoting point for the fetoscope (2) the placenta is used for planning an entry point which is appropriate to explore the vessels (3) the vessels are the surgical target of the entire procedure. Virtual reality headset: For this work we used the HTC Vive headset. Several previous works showed the benefits of manipulating 3D representation of the organs for instance in liver and breast resection. Programming environment: To develop the application the main program used was Unity, which is one of the most used frameworks to program user interaction experiences. The application is coded in C#. Metric: In order to evaluate the results, we run a survey where both clinical and non-expert users tested the platform and rated their overall impressions. We used the concurrent think-aloud protocol, where users are encouraged to stream their thoughts during the task so that the examiner can better understand the usability issues. The application is able to import real 3D objects directly interfacing with Kinect, as it was the case for the mannequin in Fig. 1 , and the patient-specific planification meshes provided by the planning software. The fetoscope model was directly created using Unity. A camera was included on the tip of the fetoscope as well as a screen to display the images, mimicking the setting of the OR. The camera parameters were configured to resemble the most the real fetoscope view. The patient-specific meshes are generally located inside of the mannequin, although for visualization purposes they are located above it in Fig. 1b . The interaction with the fetoscope (insertion, rotation and coagulation) is mediated by the HTC controllers. As the fetoscope moves, the images from the fetoscope camera are displayed in the monitor above the surgical bed. The final stage consisted on the evaluation of the created application. For that purpose, a usability questionnaire was created. The test contains questions about the user background regarding VR, surgical planning environments and TTTS. In order to assess the application, questions are organized in three main blocks: user interface, visualization scene and training scene. The test contains Table 1 ). The user interface was rated the highest (9.48 points) and the training scene the lowest (8.83 points). Most of the time the users asked to improve the physics of the fetoscope manipulation which seemed not very realistic and to incorporate some haptic feedback that could mimic the resistance of the tissues to fetoscope displacement. In this work we presented an application for the exploration of fetal surgery in virtual reality. We obtained very positive feedback from the users, showing that virtual reality interaction on fetal surgical planning is well received. There are still some unsolved problems such as the need for a haptic feedback to improve the fidelity of the simulation. Following the comments of the clinical users, we will also improve the physics of the fetoscope manipulation. Acknowledgments This work was supported by the European Commission under the ATTRACT MIIFI Project. We thank NVIDIA for the Titan X hardware grant that allowed us to process the images in a faster way. Keywords Image-guided surgery, Hyperspectral imaging, Multispectral image processing, Tissue visualization Multispectral imaging as a contact-free and fast imaging method allows to acquire the remission spectra in the visual and near-IR spectral range as well as the spectral reflectance curve of biological tissue. These spectra contain richer information about the tissue types and the structural features than normal three channel RGB-imaging techniques, allowing to differentiate between neighboring tissue types that are visually similar to the human eye. We have built up different multispectral imaging systems to analyze and differentiate the relevant spectral tissue characteristics not visible to human eyes in order to support the surgeon's tissue differentiation process. For example, the nerve preparation is the most challenging and time-consuming task during parotidectomy and an improved nerve visualization would help the surgeon to accelerate the surgical process and lower the risk of nerve injuries. In this work, we present a first approach to visualize multispectral tissue information without losing relevant RGB-information in the images. We built the proposed visualization approach upon our previously presented imaging setup including two multispectral snapshot camera featuring a 4 9 4 (16-band) mosaic (visual range) and a 5 9 5 (25band) mosaic pattern (visual and near-IR range) as recording devices [1] . Using this setup, we are able to reconstruct the remission spectra of each captured pixel, i.e. of the different tissue types in the situs. A regular RGB image can be calculated from the 16 ? 25 scanned wavelength bands with the CIE color matching functions. Such calculated RGB image holds no additional spectral information. In order to visualize additional relevant tissue information in the image, we propose the following basic concept for partial color enhancement without destroying the color balance. This color enhancement is applied to the high dimensional multispectral image and uses statistical orthogonal transformation. The enhanced multispectral data f e is calculated by where u and v represent the pixel positions, e i is the ith basis vector of the transformation of the measured multispectral image f and a i are the corresponding statistical transformation components, fis the average vector of f and s holds the parts of the multispectral data retaining as much of the variance in the dataset as possible. The weighting matrix W holds the magnification factor m for color enhancement. The magnification factor m is placed into W at the position of the wavelength bands that are meant to be amplified. To identify the best wavelength bands for magnification, e.g. for visual nerve enhancement during parotidectomy, we use spectrophotometer measurements of tissue samples of the major present soft tissue types in order to analyze spectral differences between the single tissue types, e.g. for parotidectomy the tissue types nerve, parotid gland and fibrous connective tissue, and select appropriate spectral bands [2] . This a priori knowledge allows to highlight the tissue of interest (e.g. nerve) in the captured image. To preserve the overall RGB-color balance of the reconstructed RGB image, the enhanced image information is added to only one channel of the calculated RGB image. We investigated different tissue types present in parotidectomy. The most prominent tissue is parotid gland, while nerve (tissue of risk) is the most interesting tissue type besides the tumor and has to be preserved. Figures 1 and 2 depict examples for nerve visualization. Figure 1 shows the RGB-image reconstructed from the calibrated multispectral dataset is shown. This image of the situs corresponds to a typical RGB image. Figure 2 shows the same view with enhanced nerve visualization. In this case, the spectral bands, corresponding to Table 1 Usability study with 10 participants. Answers are grouped in user background, user interface, visualization and interaction Int J CARS (2020) 15 (Suppl 1):S1-S214 higher reflectance intensity for nerve tissue compared to the reflectance intensities of the other surrounding tissues, are intensified by adding this color enhancement data to the blue channel of the reconstructed RGB image. Thus, the overall color impression in the RGB image is preserved and only a selected tissue type receives an adapted color appearance, in this specific case: the nerve is colored bluish. In this work, we present first visualization options to fuse additional multispectral information into the image without concealing relevant anatomical structures and features. In addition, the overall image and color impression is preserved, which allows the surgeon to intuitively understand the visualized added value. These first visualizations show very promising results to bring multispectral tissue analysis systems (using snapshot cameras) into the operation room to open new opportunities for intraoperative assistance and image-guided interventions. Acknowledgments This work is supported by the German Federal Ministry of Education and Research (BMBF -Grant Number 16SV8061). Purpose Ultrasound-guided needle procedures for vascular and nonvascular applications are a mainstay of modern medicine, and their number continues to grow. Safe and efficient execution of these invasive procedures requires a high degree of hand-eye coordination and clear visualization of the target anatomy and the needle in ultrasound imaging. In the current practice, hand-eye coordination is made difficult with the physician needing to divide attention between observing the patient and the needle and the ultrasound image. Innovative image visualization solutions that keep the physician's attention focused on the patient are therefore highly desirable and could help performing these procedures easier and faster while reducing physician fatigue and enhancing patient outcomes. We propose an integrated system to improve the placement and visualization of the ultrasound images with the Microsoft HoloLens (Microsoft, Richmond, VA) head-mounted display (HMD). This mode of visualization should lessen the high hand-eye coordination demands currently needed. The HoloLens is an optical see-through and untethered HMD that can display stereoscopic projections in the user's surrounding environment. The ultrasound images viewed through the HoloLens would appear in situ with the ultrasound transducer, eliminating the physician head-turning and attentionswitching. Nearly all works proposed thus far that use HoloLens, manually place the holograms to align with the desired real-world objects. Our system provides real-time tracking of the ultrasound transducer and, additionally, setting of image size and location parameters through voice commands. We designed a user study to evaluate the HoloLens as an alternative display for ultrasound-guided needle procedures. Using a previously reported and validated training model [1] , we measured the time to complete a number of targeting exercises performed conventionally (ultrasound images shown on ultrasound machine's monitor) and when using the HoloLens. The results suggested that the HoloLens was a viable alternative to conventional ultrasound monitor. Methods We adopted the task trainer phantom designed by [1] as a training platform for ultrasound-guided needle procedures. The phantom was made of Lego blocks as support bases and wooden dowels as vessel targets embedded in gelatin. Both novices (engineers) and experts (residents, fellows, and practicing physicians) were recruited for the study. Each user was trained on the basic functionality and use of the HoloLens. The novices received additional training on how to view the dowels in in-plane ultrasound and how to orient the needle to reach the dowel inside the phantom. During the actual evaluation, which included the user touching the various dowels six times in a random order under in-plane ultrasound image guidance, the net time to advance the needle until it hit the dowels was recorded. Figure 1 shows a user wearing the HoloLens and becoming familiar with using it for needle-based targeting tasks. Figure 2 shows the same user's view as per the HoloLens camera feed. The ultrasound images were streamed via Wi-Fi from the Terason uSmart 3200T touch-screen tablet (TeraTech Corporation, Boston, MA) to the HoloLens for display. A custom app, called HoloUS, with two main components was developed. First, a custom C ?? software was written on the Terason machine to query and package the ultrasound frame to send over Wi-Fi to the HoloLens. Second, an app to display the US images was written in C# using Unity 3D and Visual Studio 2017. The augmented ultrasound image was registered to the ultrasound transducer using ARToolKit [2] , and it tracked the pattern on the purpose-designed 3D-printed holder for the transducer (see Fig. 2 ). Two visualization modes were developed to optimize the visualization of the ultrasound images: tracking mode and floating mode. In tracking mode, the ultrasound image was placed directly under the transducer face and followed the transducer movement. In floating mode (Fig. 2) , the image was statically floated in front of the transducer to provide an unobstructed view of the phantom and the needle path. The image in floating mode relocates if there is a significant movement of the transducer (by default, 50 cm). Voice commands on the HoloLens was implemented to adjust ultrasound image parameters: scale the image up or down and move the image in six Cartesian directions (backward, forward, left, right, up, down). The initial version of the HoloUS app with the described features was developed. It was capable of 20 frames per second refresh rate and was used to conduct the user study. Table 1 lists the task completion time in seconds for novice, expert, and combined groups between using the HoloLens and viewing the image on the Terason machine. The novices improved significantly with HoloUS, most likely because of more intuitive visualization it provided. Experts were slightly slower with the HoloLens. A likely explanation could be that they are already competent with conventional visualization, whereas the use of a new technology (i.e., HoloUS) slowed them down despite its advantages. For each of the user groups, p values did not show significant difference. We attribute this to small number of subjects and high variance, most likely due to different needle handling styles. The initial results show that visualizing ultrasound images through the HoloUS app running on a HoloLens is a viable alternative to viewing ultrasound images on a traditional ultrasound monitor. The user study we conducted further showed the potential of improved efficiency through this mode of visualization. With additional technical development and usability enhancements, it is possible that AR visualization would allow one to learn and perform ultrasound-guided needle procedures faster. The visualization solution presented here apply to any AR glasses; however, HoloLens is currently one of the most advanced AR glasses, and the only one that provides the hardware and software interfaces needed to develop HoloUS. The user feedback was mostly positive. The main limitations described by the users were HoloLens'' narrow field of view (340), weight, and image quality. The newly released second-generation HoloLens 2 has a field of view twice as wide, which should address the field-of-view limitation. HoloLens 2 also redistributed the weight of the device so that it can be worn more comfortably and longer. In the current HoloUS implementation, we down-sampled the ultrasound image to transmit over UDP which affected image quality. We plan to address the image quality by switching to TCP/IP and maintaining the original image resolution. Fig. 1 The user is being trained on the needle targeting task with the HoloLens Fig. 2 The view through the HoloLens camera feed  In this paper, we introduce a software and hardware system for intraoperative video-based detection of the stapedius reflex during CI (cochlear implantation) adaption procedures. The aim of the system is to correlate the visual assessment of the reflexes performed by the surgeon with the detections provided by the proposed algorithm. In current clinical practice, the determination of the stapedius reflex thresholds (SRT) is mostly done via intraoperative visual assessment by the operator. The use of a computer-based system that performs this task automatically would help in the objectivation of the procedure: the possibility to correlate the electrical impulse (triggered by using the Max Programming Interface, MED-EL GmbH), the answer of the operator (reflex/no reflex) and the output of the detection software would add a level of confidence and allow for a documented reproducible evaluation of the electrically elicited stapedius reflex thresholds (eSRT). Furthermore, besides the qualitative information, a computer-based system can yield additional quantitative information and would be the basis for a client-loop system. From an algorithmic point of view, the main difficulty resides in separating the actual reflex motion from other sources of motion in the image (heartbeats, microscope to patient displacements, vibrations …). Motion estimation methods like optical flow are suitable to accomplish such tasks but the post-processing analysis necessary to perform detection in real time impedes a real-time implementation. The main purpose of this contribution is to propose a setup for automatic reflex detection which achieves the state-of -the-art detection performance, while being able to work in a live, intraoperative context. The proposed algorithm is based on the Phase Only Correlation method (see the seminal paper [1] for more details), which relies on a fundamental property of the Fourier transform: namely, a translation in spatial domain corresponds to a phase modulation in the Fourier domain, i.e. where Tx0, y0 denotes the translation by a vector (x0, y09 and F(f) is the Fourier transform of the function f. Thus, for a scene where the motion is dominated by a uniform translation, the motion vector can be directly derived from the peak location of the normalised cross-correlation which is defined as the inverse Fourier transform of the normalised cross power spectrum relative to the images at two different times. In principle, the resulting motion estimation can be applied to each rectangular patch of the image (called region of interest, ROI in the remainder of the paper) taken from two frames at different time points. In practice, the continuous Fourier transform is replaced by its discrete, periodic version, the fast Fourier transform (FFT). The periodicity issue is managed by using classical techniques of smooth windowing of the patches: this is necessary to avoid large spectral artefacts due to the discontinuity of the periodicised image. In order to yield a valid estimate, the selected ROI should contain enough sharp features and the underlying motions must be a homogeneous linear translation. In first approximation (short time interval) the linear translation assumption is straightforward as long as several objects moving in different directions are not in the scene. In order to compute the motion of the stapes relatively to the scene background, the resulting motion estimation is applied to two carefully selected ROIs: The first ROI in the background of the scene (in order to evaluate the global scene motion). The second ROI is selected where the stapes reflex is expected to be visible. The relative motion stapes/background is the vector given by the difference between the motion vectors evaluated for each of the ROIs. In the context of stapes reflex, the motion homogeneity condition means that the selected ROIs should contain contents of the background or contents or the stapes, respectively. While selecting the ROIs, the user should avoid the boundaries between stapes and background, which could lead to ambiguous motion estimate. A further potential issue is related to the presence of noise in the images: this results in some noise in the ROI spectrum and thus in ambiguous peak locations. In order to deal with this problem, an additional smoothing filter is used, before the peak search is performed. Finally the resulting vectors are still highly noisy. A uniform (temporal) averaging of the resulting vectors is done. Stapes reflex detection is then done via a thresholding of the norms of the resulting (averaged) motion vectors. A moving average of size 13 turned out to yield the best results in real-life (clinical) situations. The actual implementation of the algorithm requires the computation of a large amount of FFTs. The use of small ROIs of optimal sizes (avoiding for instance sizes which are multiple of too large prime numbers) allows for very fast computation of these FFTs. Furthermore, in order to obtain real-time, the peak search is done by a suitable marching approach which turns out to be computationally less demanding while maintaining the precision as high as conventional, gradient methods. These optimisations allowed for a significant acceleration of the algorithm: when using ROIs of size 80 9 80 pixels, the complete motion estimation requires less than 15 ms per frame on a CPU implementation. In order to be applied in the clinical context, the analyzing system must be connected to the several components related to the eSRT determination procedure, as depicted in Fig. 1 . The Research Workstation captures the video stream from the microscope and proceeds to the stapedius reflex analysis in real time. In order to correlate the software detections with the stapedius reflexes detected by the surgeon, a foot pedal is provided to this latter: at each detected reflex the surgeon presses on the pedal, which in turn send a signal to the Research workstation and which is synchronised with the other signals. Similarly, the MED-EL Max Processor is connected to the Research Workstation in order to track the triggered electrical impulses. One of the principal strengths of the system is the design of a GUI compatible with the daily clinical routine. Since during the operation abrupt displacement between microscope and patient might occur, a dynamic manual reposition of the ROIS is available. The GUI permits a live visualization of the operation video stream, graphics simultaneously plotting the different signals (electric impulses, operator detections, software detections). The GUI also permits a visualisation of the used ROIs and an interactive (possibly intra-operative) reposition of the ROIs. Furthermore, the synchronised signals are stored in order to ensure reproducibility of the results and to allow for off-line research. A screenshot of the GUI is shown in Fig. 2 . The system has been tested intra-operatively at the University of Rostock in December 2019. During the operation 36 electrical impulses were triggered, leading to 26 eSR. The video stream provided by the microscope at a rate of 60 fps in full-HD (1920 9 1080 pixels) was processed in real time by the analysing software. Using a suitable threshold (fine-tuned after the operation), the software was able to correctly detect 23 of the 26 reflexes, without false positive detections. Only one reposition of the ROIs during the operation was necessary. Detection of all of the 26 reflexes is possible by decreasing the detection threshold, but at the cost of obtaining a significant number of false positive detections. This detection result is comparable to that attained by state of the art algorithms. The remaining 3 reflexes that were not detected are the reflexes with smallest motion amplitude. We introduced a semi-automatized test setup for the detection of stapes reflexes that is able to work intra-operatively in real time. The use of the fully digital microscope (ARRISCOPE) allows to apply a detection algorithm on the video stream in real time. The potential of such a system has been proved: the algorithm allows for a live visualization of the software detection output, together with the conventional visual assessment. This represents an important step towards a system able to provide documentation of an eSRT determination procedure with objectivation via a cross-correlated validation of operator and software. Future work will concentrate on the robustification of the detection for reflexes of very low amplitude. Keywords laparoscopic surgery, flexible surgical tool, grasping and detaching force, curved shape In laparoscopic surgery, the linear shape of the surgical instruments makes some operations difficult. To handle this problem, usage of flexible instruments was proposed. An operation system for flexible forceps is required for the method. Okawa et al. developed a forceps device system for use in flexible endoscopes [1] . In this study, the stroke of the forceps was limited to be the linear actuator stroke which related to the actuator size. To secure enough stroke length without enlarging the actuator size, we have developed a cassette type flexible surgical tool system [2] . In our system, the flexible forceps wound around a drum is fed by a friction wheel with the rotation of the drum using rotational actuators. This means that the stroke of the forceps can be secured long compared to the size of the actuator. The cassette type flexible surgical tool system consists of a forceps and an actuating device. Feeding, winding, and end effector driving operations of the forceps can be performed by connecting the forceps to the actuating device. Cassette-type flexible surgical tools with various functions can be designed as the forceps of the system. They are operated commonly by the actuating device. For a forceps of the system, we have developed an end effector that can generate two directional forces, which are the grasping force and the detaching force, by pulling two wires individually while a general flexible forceps can only generate one directional force because the driving wire can transmit only extensional force. The developed end effector is expected to expand the application of the system. A cassette type flexible forceps with the end effector has also been developed (Fig. 1 ). The prototype size was 120 9 90 9 26.5 mm. The end effector diameter was 8 mm. In this paper, we measured the grasping and detaching force depending on the radius of curvature of the tool shape to evaluate the prototype of the cassette type flexible forceps. The prototype of the cassette type flexible forceps consists of a drum and a flexible forceps with the developed end effector wound around the drum. The drum has the antagonistic two-axis wire drive mechanism which has two wires connected antagonistically to each other through the end effector, a wire puller and two slack compensation mechanisms using a compression spring for each wire. When the wire puller is push down in the direction of the drum's rotation axis, one of the wires is pulled. Once actuating force is reduced, the wire puller is pushed back by the force of the coil spring and another wire is pulled. The slack of the wires due to the difference of the wire path during the operation is compensated by the deformation of the compression spring in the slack compensation mechanism. When a flexible forceps is curved, the space around the driving wire and the friction on the wire reduce the wire tension. This means when the path for the forceps is curved, the force generated by the end effector is reduced. In the prototype, a flexible tube (inner tube) was attached around the wire to adjust the gap and to reduce the friction between the wire and the body tube. We measured the grasping force and the detaching force depending on the radius of curvature of the tool shape to evaluate the force reduction of the prototype (Fig. 2) . The radiuses of curvature (R) of the flexible forceps were ? (straight), 300, 200 and 100 mm. In each condition, the length of the forceps from the cassette to the end effector was 300 mm. The forces were measured using a MEMS (Micro Electro Mechanical Systems) triaxial force sensor (Sho-kacChip; Touchence inc.). The sensor size is 5.5 mm in diameter and 2.1 mm in thickness. The MEMS sensor was attached to the center of one blade of the end effector. The angle between the blades during the measurement was 11°because of the thickness of the MEMS sensor. The grasping force and the detaching force were defined as the maximum absolute force generated during the each actuation by the operating device. The forces were measured 5 times under each condition. The grasping forces for R = ?, 300, 200, and 100 mm were 0.5, 0.4, 0.4, and 0.3 N, respectively. The detaching forces were 0.9, 0.8, 0.7 and 0.5 N, respectively. As the preliminary experiment, we also measured the forces without the inner tube. The grasping forces were 0.4, 0.2, 0.1, and 0 N, respectively. The detaching forces were 0.6, 0.3, 0.2 and 0.1 N, respectively. Though the forces with the inner tube decreased with the decrease of the radius of the curvature, the reduction ratio was decreased compared to that without inner tube. The grasping force was smaller than the detaching force. The main reason may be the usage of the compression spring for pushing up the wire puller. The wire tension for the grasping operation depends on the elastic property of the compression spring though that for the detaching force can be controlled by the actuating device. This problem can be solved by increasing the spring constant of the compression spring. The case R = 100 mm is for example that the forceps is folded along with an organ. Though the force reduction should be improved, it is also important to plan the path to the affected area to be as straight as possible. In this paper, we measured the grasping force and the detaching force of the prototype of the cassette type flexible forceps depending on the radius of curvature of the tool shape. The grasping forces for R = ?, 300, 200, and 100 mm were 0.5, 0.4, 0.3, and 0.2 N, respectively. The detaching forces were 0.9, 0.8, 0.7 and 0.5 N, respectively. The measured values were not large enough but on the same order even when the flexible forceps were bent. These results shows the feasibility of the cassette type flexible forceps that can generate two directional forces. Klinikum rechts der Isar, Forschungsgruppe MITI, Munich, Germany 2 Technische Universität München, MIMED, Garching, Germany Keywords Endoskopy, SPOT, Magnetic clutch, Single-use product One of the central issues when it comes to the use of assistance systems during operations is the achievement of sterility. Dynamic systems such as the robot-assisted surgical system da Vinci (Intuitive Surgical, USA) are usually packaged in sterile films for this purpose. A further possibility is the development as a sterilizable component that can be used again. In practice, however, repeated packaging prior to each procedure leads to increased preparation times, whereas sterilizable designs, especially for mechatronic components, result in increased material costs because specific types have to be used. Another approach is the fenestration of sterile lidding films, which allows equipment to be used across the sterile barrier. However, perforation of the foil leads to an increased risk of contamination of the cost-intensive components [1] . The focus of this paper is therefore on the design of a drive concept for contactless transmission of rotational energy across the sterile barrier. The mechanism will be used to drive flexible endoscopic instruments within snake-like manipulators such as the SPOT system [2] . The proposed drive concept consists of two main components: a magnetic coupling and a single-use cassette. With its function of contactless power transmission, the magnetic coupling has the possibility of using a sterile film without perforating it. The magnetic coupling consists of three parts: an inner rotor (1), a containment shroud (2) and an outer rotor (3), which are arranged coaxially to each other (Fig. 1) . The inner and outer rotor each have ten recesses evenly distributed over the circumference, in which neodymium square magnets (5) (35 9 4 9 2 mm, 1.44 T) are inserted. The sterile containment shroud of the coupling integrated in the cover foil (4) acts as the sterile barrier, thus ensuring contactless rotation of the outer rotor to the non-sterile area. The sterility boundary of the entire system thus runs inside the magnetic coupling, so that in addition to the motor-gearbox combination, all other sensitive mechatronic components are located outside the sterile area and can thus be excluded from sterilization. The single-use cassette represents the second main component of the drive concept. The aim of the disposable unit is to minimize the proportion of components to be stylized and, by eliminating time-consuming assembly steps, to reduce the amount of changing between or during interventions. This is achieved by encapsulating the mechanical drive components, which are necessary for conveying the instruments, in a disposable product. The cassette is located on the side of the drive unit and combines two drive rollers as well as other mechanical elements necessary for conveying the flexible instruments (Fig. 2) . The cassette (1) is connected to the magnetic clutch (2) through the outer rotor by means of the drive rollers, thus establishing the power transmission between the motor and the instruments. For a first experimental setup, the components of the magnetic coupling were manufactured by stereolithography (SLA) from the plastic BLACK FLGPL04 using the 3D printer Form 2 (Formlabs, USA). The single-use cassette was manufactured from biocompatible polyamide (PA12, EOS, Germany) using selective laser sintering in Formiga P100 (EOS, Germany). In upstream tests, a torque of 4.0 mNm was determined, which is necessary for conveying the instruments. In order to determine the torque to be transmitted by the magnetic coupling, it was integrated into the existing test setup. The measurements showed that the coupling can transmit up to 5.0 mNm without slippage, thus meeting the requirements determined. In addition, no significant abnormalities were observed during operation, which means that the running behavior of the magnetic clutch can be described as positive. In addition, the proportion of components to be sterilized was limited on the outer rotor of the magnetic coupling including its magnets. All other components on the sterile side are simply replaced. The presented concept allows the contactless transmission of the torque of a drive unit to flexible endoscopic instruments across the sterile barrier while minimizing the amount of sterile components. This allows the use of less expensive designs, especially with regard Fig. 1 The magnetic coupling consists of three parts: an inner rotor (1), a containment shroud (2) and an outer rotor (3), which are arranged coaxially to each other. The sterile containment shroud of the coupling integrated in the cover foil (4) acts as the sterile barrier, ten magnets (5) ensuring the contactless rotation of the outer roto (2) is positively coupled to the output roller of the single-use cassette (1), allowing torque to be transmitted from the drive via the magnetic coupling to the drive roller to mechatronics. In addition, no perforation of the sterile cover foil is necessary, which reduces the risk of contamination and thus also the cleaning and maintenance effort of the assistance system. In addition, the encapsulation of all other drive components in a single-use cassette eliminates time-consuming assembly steps, thus saving further process time and costs, while at the same time ensuring efficient handling during changeover procedures. In initial preliminary tests, the requirements for both the magnetic clutch, which is manufactured using rapid prototyping, and the single-use cassette were confirmed. Experiments under real conditions are currently still pending. Purpose Although thoracoscopic surgery, in which surgical instruments and a thoracoscope are inserted into multiple incisions, is more cosmetic compared to conventional open chest surgery, there is an anatomical limitation that the incisions have to be made between costal bones, unlike laparoscopic surgery in which the port position can be arbitrarily opened excluding the cases of conglutination. The port positions and positional relationship have an effect on surgical outcome due to working space and operability. To overcome the difficulty of thoracoscopic surgery, we have developed a costae-independent surgical device using a concept of virtual-incision. In previous study, we have proposed a new surgical approach ''virtual incision'' which increases the number of ports virtually, and the usefulness of this approach was confirmed by applying to a surgical robot system for single-incision laparoscopic surgery [1] . Our proposed surgical device consists of a proximal part, a semicircle shape part, a distal part and a dedicated flexing holding arm as figured in Fig. 1a . For a setup, the surgical device is inserted into a real incision from the distal part, and the semicircle part is attached to the distal portion of the dedicated holding arm, and then the surgical device is fixed to a surgical table using the holding arm. The surgical device itself has 3 DOFs including a grip/tip opening and closing, extraction and contraction along the longitudinal axis, and axial rotation, and 2 DOFs of spherical motion around a virtual incision, that is located at central point of the semicircle shape part, can be provided with a combination of the flexing holding arm; thus surgeons can manipulate our proposed surgical device with 5 DOFs same as a conventional surgical instrument. The location of the virtual incision can be changed 360 degrees around the real incision during surgery by adjusting the fixing direction of the surgical device. The flexibility of our proposed surgical device is considered to enhance efficacy of treatment more. To confirm the feasibility of the proposed surgical device, we have developed a prototype device and evaluated it from clinical perspective. Our developed surgical device, which is composed of a commercial surgical straight instrument, a custom-designed bending stainless pipe, a torque coil, and a wire, has an external diameter of 8 mm, a radius of curvature of the semicircle shape part of 50 mm, and a longitudinal length of 450 mm. The commercial surgical instrument was divided into a proximal part and a distal part, and these outer frame and inter shaft were welded to the torque coil and the wire respectively through the flexing pipe. As for the flexing holding arm, the distal portion was designed to fit the semicircle shape part of the surgical device, and the device can be easily clipped on to the holding arm. As shown in Fig. 1b , the tilt of the surgical device could be changed by sliding the semicircle shape part across the distal portion of the dedicated holding arm. From the result of subjective evaluation from an expert respiratory surgeon, it was pointed out that the load during instrument manipulation was a little heavy, but the concept of the surgical device was highly evaluated. It was also indicated the possibility that could be applied not only to normal thoracoscopic surgery but also to single-port endoscopic surgery. We have proposed and developed a new surgical device based on a concept of virtual incision for thoracoscopic surgery which enables to overcome its anatomical limitation. Subjective evaluation from clinical perspective indicated the feasibility of our proposed surgical device and the applicability to other surgeries. We are planning to evaluate the operability and to compare the working area to conventional surgical instrument. Novel design to improve the lifespan and range of motion of 3.5 mm surgical manipulator comprises of elastic elements, through stress management [1] . The 4 DOF comprise of bending motions in 2 perpendicular axes, grasping and rotation. Owing to the simple structure of its design, the diameter of the tip of the manipulator was 3 mm and outside the structure was 3.5 mm. The manipulator was made of NiTi alloy and was designed to be used in multiple surgeries which demands a higher life span. As presented in [1] , the instrument can bend 8000 times in a bending (beta bending) direction. In this paper, modifications to the design are proposed to improve the life span of the manipulator keeping its inherited features intact. An overview of the design of the forceps is shown in Fig. 1a . It comprises of 2 sets of different springs, spring As and spring Bs connected with a pin joint at the tip. Both sets of springs can be pushed and pulled distally to generate bending and grasping motions at the tip. When a spring is pushed and the other of the same is pulled, they generate a single bending motion. In the same manner, the other set generates a bending motion in the perpendicular direction to the first one when they are pushed and pulled (Spring As-alpha bending, Spring Bs-beta bending, see Fig. 1a ). When both spring Bs are pushed or pulled together, they generate opening and closing motion (grasping, see Fig. 1a ). The whole tip can be rotated to generate the fourth DOF. More details of the design and mechanism can be found in [1] . As illustrated in Fig. 1b , after 8000 of bending cycles, spring B breaks closer to the tip. The FEA results also demonstrated a higher stress concentration around the same area where the breakage happens in the prototype. Thus, the design of the spring B was changed iteratively to allow the stress to be distributed evenly along the deforming part of the spring B. Proposed modified design of the spring B is shown in Fig. 1c . As shown, the right-angled step in the original design in [1] , was removed and shape was made smooth to accommodate the deformation along the spring B. In addition, the radius of rotation for spring B motions were increased from 2.4 to 2.58 mm. Further, to improve the lateral rigidity the thickness of the spring B was increased to 0.28 mm from 0.25 mm. Stress distribution of the springs was evaluated using a series of finite element analyses (FEA) using a mechanism analysis software (DAFUL 2019 R1, Virtual Motion Inc., Korea) together with meshing software (FEMAP v11.2.2, Siemens Inc., Germany) for modeling. A range of motion of ± 60°for the 2-DOF bending and 1-DOF grasping motion were assumed during the study. In addition, the grasping force and the repeat accuracy of the forceps were evaluated using the developed protype of the modified forceps. Results of the FEA study are shown in Fig. 2 . According to the results, the resulting maximum strain on spring B during a 60°of beta bending motion was found to be 4.22%. During 120°of grasping, a maximum strain of 3.38% was generated on the spring B. Further, at 60°of alpha bending motion, a maximum strain of 4.31% was generated on the spring A and a maximum strain of 4.52% was generated on the spring B. All the strain values are below the maximum allowable strain limit of NiTi, 7%. Further as reported in [2] , design parameters of Spring B also affect the range of motion of the motions of Spring B. Accordingly, the spring B can move in a range of 90°, constrained by the geometry as explained in [2] . Experiments with the protype was carried out to measure the grasping force by grasping a sheet type force sensor (FlexiForce A201-1, Tekscan Inc., US). It revealed the forceps can generate a maximum grasping force of 6.35 N. In addition, the repeat accuracy was calculated for spring B motions by measuring the motion by a vision motion tracking device (VW-6000, Keyence, Japan). Repeated generation of 30°of beta bending motion showed a deviation of ± 0.03°, while the repeated generation of 60°of grasping showed a deviation of ± 0.09°. Conclusion This paper introduced an improved design of a 4 DOF surgical manipulator comprises of elastic elements to be used for endonasal procedures. Improvements were focused on to improve the life span of the forceps considering the stress distribution on the spring B. In the previous version of the forceps, during 60°of beta bending motion the maximum strain on the spring was revealed to be 5.43%. Similarly, during 120°of grasping, spring B had a maximum strain of 4.2%. In the current study, after the introduced modifications the maximum strain on spring B was reduced to 4.22% and 3.38%, respectively for beta bending and grasping motions. Hence, during the continuous motion of the spring B, the modified version is expected to last more than 8000 cycles of the bending motion, which will improve the overall lifespan of the proposed surgical manipulator. In addition, the grasping force and the repeat accuracies are also on par with the previously proposed design. Further, the improvements also improved the working range of the spring B motions from 72°to 90°. Int J CARS (2020) 15 (Suppl 1):S1-S214 In future, further evaluations are expected to carry out using the developed prototype by performing a fatigue bending test, rigidity test and output force measurement test. Minimal invasive surgical procedures such as laparoscopy are preferred over open surgery due to faster postoperative recovery, less trauma and inflammatory response, and less scarring [1] . Laparoscopic repairs of hiatal hernias require pre-procedure planning to ensure a good exposure and positioning of the surgical ports for triangulation, optimal ergonomics, and optimal instrument's length and angles to avoid the fulcrum effect of the long and rigid instruments. In the present study, we developed a proprietary computer-assisted surgical planning module based on our existing image-guidance platform iMTECH [2] to determine the skin incision location for optimal surgical instrument placement and improved ergonomics in laparoscopic demanding surgery (i.e. knot tying, suturing, tissue manipulation). We tested the software on three cases of human hiatal hernia to assess the feasibility of the stereotactic reconstruction of anatomy and surgical planning. We chose hiatal hernia surgery as a starting point as the intraoperative position of the hiatus consistently matches that of the pre-operative computed tomography (CT), with negligible error for the diaphragmatic excursions of breathing and pneumoperitoneum. Methods Three patients (one male and two females), aged between 46 and 65 years, were included in this study. Patient body mass index was 36.5 kg/m 2 , 24.5 kg/m 2 and 37.6 kg/m 2 . Informed consent was obtained for all three cases. The protocol of this study was approved by the Research and Ethics Committee of the University of Medicine and Pharmacy of Craiova. The study was carried out in accordance to the Code of Ethics of the World Medical Association (Declaration of Helsinki) for experiments involving humans. CT images were acquired using a 20-slice Siemens Biograph mCT equipment (Siemens Healthcare, Germany), with a 512 9 512 reconstruction matrix. A whole-body CT investigation was performed for each patient, including a non-contrast examination followed by a post intravenous iodine contrast (Iohexol -Omnipaque TM 350 mg l/ml) examination. Abdominal virtual models were reconstructed from 526 slices at 1 mm interval, 337 slices at 1.3 mm interval and 442 slices of CT scan at 1.5 mm intervals. We developed a laparoscopy planning module allows for 3D volumetric reconstruction of computed tomography DICOM files. The 3D model displays different tomographic density and color codes, such as air, bone, vascular supply, skin, fatty tissue, etc. The laparoscopy module works through the use of the following elements: forceps, which is a 3D rendered representation of a laparoscopic forceps; camera, offering similar graphic features to the forceps; basic, which is a configurable cylindrical 3d representation; references, rendered as small spheres; and distances, which allow the calculation of the distances between the aforementioned elements. The iMTECH planning software provides calculations of the distances and angles between the abdominal access points and a given anatomical landmark (e.g. pubis, xiphoid) or an optical tracker placed on the surface of the abdomen to optimally position the trocars (Fig. 1) . All the coordinates for the access points were selected by the operators based on the existing evidence concerning the optimum laparoscopic ergonomics. The optical trocar access point was placed on the xipho-umbilical line. The distance on the skin between the insertion point of the optical trocar and the xiphoid was 159.6 mm, 155.7 mm, and 143.1 mm respectively, for an elevation angle of 40° (  Fig. 2) . The position of trocars could also be adjusted manually so they could vary according to the intraoperative requirements or the preference of the surgeons, making it a highly flexible planning platform. Following the pre-procedure planning, all patients successfully underwent the surgical laparoscopic procedures. The surgery included three hiatal hernia repairs using the Nissen-Rossetti fundoplication performed by an experienced team of laparoscopic surgeons. Five trocars were used for each intervention: one 10 mm optical trocar, two 10 mm working trocars, and two 5 mm working trocars. The mean operative time was 3 h 23 min. There were no cases of bowel or vascular injury. Three clinician members of our team independently tested and evaluated the preoperative planning software. Postoperatively, a qualitative assay tested the feed-back of the surgical team concerning the ergonomics, complexity, and utility of the planning system. The general perception was that iMTECH significantly improved the ergonomics, was easy to use, and was particularly useful in obese patients or in case of large hiatal defects. The iMTECH planning software allowed successful tracking of the trajectory of the laparoscopic instruments in the limited space of the Fig. 1 The iMTECH planning software interface showing three views of the patient's CT scan, the 3D reconstruction of the abdomen and the initial instrument positioning before ergonomic optimization and doctor's input Int J CARS (2020) 15 (Suppl 1):S1-S214 S79 subdiaphragmatic region. The iMTECH software integrates the patient's CT characteristics into a unique tridimensional anatomical model which acts as a virtual space for precise geometric positioning of trocars and laparoscopic instruments, as well as validation of different ergonomic approaches to a given pathological condition. The planning procedure allowed the operative team to find the optimal solution for virtual surgical rehearsal, as well as for obtaining adequate ergonomics. It also proved to be an effective and user-friendly preoperative simulation software for trocar placement, showing its feasibility and safety during the early-stage clinical implementation. Future software improvements will incorporate variable length and diameter of the instrument. A full randomized control human study will be performed to assess the advantages and benefits of the iMTECH virtual planning system over the conventional, empirical trocar positioning method. Minimally invasive renal access and treatment via partial nephrectomy is the preferred method for managing small renal masses. This procedure has significant advantages over traditionally invasive, radical nephrectomy. However, such interventions are typically under-utilized, due to challenges arising from often insufficient navigation accuracy achieved via image-guided visualization. Maximizing nephron preservation is the main determining factor of functional recovery. Effective nephron preservation requires precise targeting of the renal masses to be excised or ablated. Moreover, such minimally invasive procedures performed on kidneys with atypical shape (i.e., horseshoe kidney) or vasculature pose additional challenges during pre-operative planning and targeting. There have been technological advancements to improve the accuracy of partial nephrectomy. Patient-specific 3D oriented kidney models were generated and made available to the surgeon to help plan the procedure and improve spatial understanding, toward reducing intervention time. In addition to their pre-operating planning value, such models pose a significant intra-operative guidance advantage. We are currently working on the development of a multi-modality image guidance platform for renal access to facilitate minimally invasive visualization and navigation for renal interventions. Diagnostic, high-resolution CT, or MRI images depicting the kidney are segmented and used to generate faithful patient-specific anatomical model of the kidney that can then be registered to the patient during the intervention, in the effort to merge the pre-procedural plan with the intra-operative guidance. These pre-operative patient-specific models will be further augmented with real-time endoscopic (video) imaging, intra-operate ultrasound (US) imaging, and real-time surgical instrument tracking using either optical or electro-magnetic surgical localization systems, which, together will provide sufficiently accurate and intuitive visualization to access and manipulate the target tissues. The overall targeting accuracy of the image guidance system depends primarily on the surgical tracking and registration accuracy. Most image-guided navigation technologies often face challenges related to registration, as the quality of registration is limited by the intra-operative sampling of the anatomy. Therefore, assessing the baseline tracking and registration accuracy is critical to achieving a sufficiently accurate pre-to intra-operative registration for subsequent image-guided navigation. In the effort to mitigate the tracking and registration uncertainty challenges, here we focus focused on characterizing and studying the accuracy of the surgical tracking technology in the context of an image-guided renal access intervention. A diagnostic MR image of a patient's abdomen was segmented semi-automatically using 3D Slicer and a virtual model was generated. The model was instrumented with 16 intrinsic registration fiducial markers represented by 3 mm diameter hemispherical divots on the surface of the kidney. The virtual model featuring all fiducial markers was converted to a STL file and a 3D printed life size replica of the kidney phantom was used to emulate in vitro image-guided intra-operative navigation experiments. In the effort to characterize baseline surgical tracking accuracy and registration accuracy independent of the kidney application, in addition to the kidney phantom, we also constructed a tracking and calibration phantom/tile of known geometry designed using Auto-Desk Fusion and manufactured in the university machine shop. The calibration phantom features 30 3 mm diameter hemispherical divots of known absolute and relative spatial locations. An NDI Polaris stylus was instrumented with a custom-developed 3 mm spherical tip (meshing perfectly with the 3 mm hemispherical divots on both the calibration and 3D printed patient-specific kidney phantom). We conducted several sets of experiments dedicated to assessing the tracker's fiducial localization error (FLE), fiducial registration error (FRE), and target registration error (TRE) using both the calibration and 3D printed kidney phantom. All image-guided navigation experiment were conducted using the 3D Slicer IGT module (Fig. 1) . A stylus tip and spin calibration was first performed and yielded a 0.05 mm RMS error. The FLE was assessed based on 5 s quasi-static measurements at 20 Hz (i.e., 100 samples) recorded across all 30 Int J CARS (2020) 15 (Suppl 1):S1-S214 fiducial markers of the calibration phantom. The overall FLE RMS error was 0.07 mm. Each phantom was registered to its virtual counterpart using three different sets of four fiducials for the calibration phantom, and three different sets of six fiducials for the kidney phantom. Each registration was performed three times for each set of fiducial registration markers, yielding a total of nine datasets for each of the two phantoms used. The TRE was assessed using the remaining fiducial markers not used for registration (26 fiducials for the calibration phantom and 10 fiducials for the kidney phantom). For the calibration phantom, the overall residual RMS error after registration estimated at the registration fiducials (i.e., FRE) was 0.34 mm ranging from 0.23 mm to 0.46 mm across three different registrations using three different sets of 4 fiducials, while the overall TRE estimated at the remaining 26 fiducial markers was 0.51 mm (ranging from 0.38 mm to 0.70 mm) (Fig. 2) . For the kidney phantom, the overall FRE was 0.70 mm (0.56-0.79 mm) and the overall TRE was 0.78 mm (0.64-0.85 mm) estimated across the three different registrations using three different sets of 6 fiducial markers. The slightly higher FRE and TRE values are associated with potential uncertainties arising when sampling the fiducial markers on the surface of the 3D printed kidney phantom impacted by the inherent uncertainty due to 3D printer resolution and surface roughness. Hence, similar differences would be noted in an in vivo application, where the pre-operative model of the kidney extracted from a pre-procedural scan would not perfectly match the intra-intra-operative kidney anatomy, hence leading to some initial baseline registration uncertainty. Lastly, to further characterize the effect of the FLE on the registration performance, we conducted a 1000 fold cross-validation experiment in which we randomly selected different sample subsets (i.e., 1, 2, 5, 10, 20, 40, 60, 80 and 100 samples) of the registration fiducials sampled over 5 s (100 samples per fiducial) to estimate FRE mean and variance as a function of the number of samples utilized to discretize each registration fiducial. Table 1 shows improvement in FRE mean and lower FRE variance with increasing number of samples, emphasizing the benefit of recording multi-frame averaged measurements that are more accurate and more precise than measurements collected at a single time point (Fig. 2) . As part of our ongoing efforts to develop an image-guided navigation and visualization platform for minimally invasive renal access interventions, here we describe a preliminary accuracy study using both a calibration phantom and a 3D printed patient-specific kidney phantom to characterize tracker accuracy and registration uncertainty both in the context of an image-guided renal application, but also as baseline accuracy metrics. These experiments helped us establish tracking accuracy and precision baseline metrics for the NDI Polaris optical tracking system using a calibration phantom (0.07 mm FLE, 0.34 mm FRE and 0.51 mm TRE), as well as laboratory-based registration performance metrics using a 3D printed kidney phantom (0.70 mm FRE and 0.78 mm TRE). We will also present similar characterization results for the NDI Aurora electromagnetic tracking system, as well as further augmentation of the system with intra-operative imaging capabilities (video and US imaging). Fig. 1 Real-world to virtual registration and image-guided navigation: demonstration and evaluation using calibration phantom and 3D printed patient-specific kidney phantom Purpose Twin-to-twin transfusion syndrome (TTTS) occurs in about 10-15% of monochorionic (shared placenta) twin pregnancies due to an unbalanced chronic blood flow through vascular connections (anastomoses) located on the placenta surface [1] . Fetoscopic laser photocoagulation of the anastomoses is the unique treatment to solve this problem and treat this condition. However, this therapy is complex and risky due to several constraints related to the fetal surgery setting such as the variable placenta location, the limited space to move the fetoscope, and the difficulty in the identification of the vascular equator containing the anastomoses that must be coagulated. The choice of the fetoscope insertion point into the mother's womb is critical, as it directly affects the fetoscope movements and the possibility to reach all target vessels. Preoperative planning and intraoperative navigation can thus be beneficial for severe TTTS [2] . In this study, we implement the first TTTS navigation platform oriented to clinical use. The proposed software loads a 3D intrauterine model generated by our TTTS planner and simulator [2] as surgical reference during the insertion procedure. Afterwards, an electromagnetic tracking system (EMTS) is employed to precisely guide and localize the fetoscope position, thus improving the ablation of placental connected vessels. Methods An EMTS 3D Guidance NDI Aurora System was utilized for tracking purposes. Two sensors were employed, one to track the fetoscope and another for a pointer tool. A friendly and usable MITK-based platform was developed (see Fig. 1 ). The application is composed of five different tabs; namely, load data, tracker SETUP, registration, visualization, and replay navigation. Four different windows are displayed under the load data tab which show the original MRI axial view and the 3D planning model [2] . Several tracking and recording functionalities are provided such as connect and calibrate the fetoscope and pointer, load previously calibrated storage, compute registration transformation matrix, and record the fetoscope trajectory (spatial points). The navigation workflow is as follows (see Fig. 2 ): (1) perform the surgical plan, based on magnetic resonance and (optimally) 3D ultrasound volumes [2] , (2) position the planar or tabletop field generators in the operating room taking into account the placenta orientation, (3) attach an electromagnetic sensor to the fetoscope and the pointer and calibrate them to record the fetoscope movements, (4) place real (on the patient) and virtual (on the 3D model) fiducials on right and left iliac crests, pubis and navel, (5) perform patient-to- Int J CARS (2020) 15 (Suppl 1):S1-S214 world alignment and measure the Fiducial Registration Error (FRE), (6) navigate the fetoscope insertion trajectory to the target anastomoses, and (7) store fetoscope trajectory to replay navigation and analyse specific doctors' movements and decisions. The implemented platform and clinical workflow were assessed by three fetal surgeons, who performed several assisted fetoscope insertions on phantoms and surgical (with a real patient) environments. The FRE and the ratio (%) in which the planned fetoscope insertion point corresponded to the one actually selected during the operation were calculated. In a simulated environment with the tabletop field generator, the FRE error was, unexpectedly, higher for the expert surgeon than for a surgeon with an intermediate expertise. Contrarily, the expert reduced the FRE by 4 mm when a planar EMTS was utilized. Considering the results globally in a phantom scenario, though, FRE were almost identical (9.14 ± 2.60 mm and 9.13 ± 2.45 mm). In a real environment, where only expert surgeons are allowed to work, the FRE using a tabletop field generator was lower than for the planar generator (see Table 1 ). Specifically, although the planar field generator worked fine for posterior placentas, it did not work as well for anterior placentas because the fetoscope sensor was outside the electromagnetic field. In these cases, the tabletop field generator, placed on a tripod, could be more appropriate. The 86% selected fetoscope insertion points were successfully planned through our TTTS planning and simulation framework [2] . Conclusion This work presents a navigation framework for fetoscope guidance in TTTS surgeries. Our system was positively evaluated by three surgeons on phantom and real pregnancies. The mean FRE obtained in the operating room was 10.79 mm. More clinical trials will be conducted on pregnant women to optimize the designed workflow and fully validate its effectiveness into daily practice. We expect to reduce surgical time and allow accurate fetoscope insertion in TTTS minimally invasive treatments. Sensorineural hearing loss is a common cause of disability. According to the World Health Organization, 20% of the population suffering from hearing loss are eligible for cochlear implantation (CI) surgery [1] . However, the outcome of the surgery has a lot of variability because of patient-specific factors and the lack of predictive tools to guide the surgeon. In previous works [2] , we developed an automatic patient-specific surgical planning software that leads to more predictable and controlled CI surgery thanks to the use of image analysis, shape modelling and FEM electrical stimulation of the neural response. A big limitation of our previous model was that it could not simulate precise temporal sequences, but only the stationary state. In this work we extend the previous system so that it can now simulate detailed stimulation pattern in continuous time. We believe that this is a very important step that allows us to improve the clinical outcomes. In order to provide some context, we will briefly describe our previously developed planning software which is used as a starting point for this new contribution (Fig. 1 ). An initial CBCT scan of a patient's head is processed using a statistical shape model (SSM) trained on high-resolution mCT to create a patient-specific computational mesh with very fine details. This mesh is first used to simulate electrode insertion through the reconstructed cochlear chambers, at different depths and positions. Then we run a Finite Element Model of electrical conduction in order to assess how the activation of the cochlear implant would affect the auditory nerve fibers (ANF). This is important because, due to the shape of the cochlea, it is easy to mistakenly stimulate close fibers, which will cause the patient to perceive distorted frequencies. After a CI device is surgically fitted on a patient, various stimulation strategies are tested in order to identify the best patient-specific parameter configuration. In addition, a calibration of parameters such as the electrode stimulation rate at each channel and pulse width is required while the preferred parameters greatly vary among individuals. However, good frequency resolution and pitch discrimination is still an unresolved problem in CI and requires a precise analysis of the transients. Now, our static conduction model was useful to describe the neural response at the microsecond range, but it was inadequate to pre-surgically create metrics for assessing the optimal stimulation strategy that would reduce pitch distortion. To address this issue, we extended our framework to account for time-dependence in order to assess the information loss of a stimulation strategy, electrode configurations, channel interaction and perceived frequency resolution. A transient current conduction model was performed in combination with an information encoding strategy and an optimization module dependent on user feedback. The user inputs short raw audio files of pitches and musical chord structures and a hypothetical Minimum Perceptual Threshold (MPT) of a patient (Fig. 2a) . The signal is then transformed mimicking operations of the device's processor such as amplification, downsampling, neuromorphic filter banking and compression. So far, a n-of-m spectral peak strategy has been implemented in combination with signal envelope and fine structure extraction using the Hilbert Transform. The user then can choose a time range and a characteristic time scale to observe properties of the ANF, such as the phase locking range in frequencies of 2-5 kHz. In addition, as time-dependent FEM conduction models can be very computationally expensive the user can choose the maximum number of timestep intervals. All this information is algorithmically optimized in order to identify the optimal time-step of the simulation and transform the raw audio into the corresponding current densities at the electrode boundaries of the physical model. The time-dependent transient current conduction model simulates the electrical field at each time-step according to a raw audio input. Utilizing the ANF's spiral ganglion, generated for the patient-specific cochlear morphology, a time-dependent representation of the electrical field around the virtual central axons is shown in Fig. 2b . The membrane potential can be calculated for each virtual auditory nerve pathway Fig. 2c . In this work we presented a novel simulator of a patient-specific implanted cochlea that could be used to understand how pitch distortions arises. In future works we will apply this methodology to clinical cases.  A cadaveric wrist was CT scanned and segmented to obtain polygon models of the native Scaphoid and the adjacent carpal bones. Based on the model of the scaphoid, a patient specific scaphoid-prosthesis was produced using additive manufacturing technology. The patented design includes a novel curved channel through the prosthesis from the volar distal to the dorsal proximal pole for suspension. Motion of the native cadaver wrist was assessed using 4D-CT [2] . To this end, the wrist was moved in a hand-shaker and scanned during flexion/ extension and radial-and ulnar abduction. From the 4D-CT data the kinematics of the bones were estimated by registration of the carpal shapes to the individual time frames. Subsequently, the prosthesis was implanted by experienced hand surgeons using a distally based flexorcarpi radialis tendon strip which was passed through the curved channel and fixed with an anchor on the dorsal lunate bone to reconstruct the scapho-trapezoidal-trapezial (STT) ligament as well as the dorsal aspect of the scapholunate intercarpal ligament (SLIL). After implantation of the prosthesis, the 4D-CT scan was repeated using the same handshaker and the same scanning protocol and analysis method. The fixation technique of the prosthesis provided sufficient stability during flexion/extension and radial-/ulnar abduction. Compared with the native wrist movements, the prosthesis moved less, especially during radial abduction with a lack of flexion. The other movements regarding the scapholunate (SL) interval as well as in the STT-joint were less than in the native wrist. The SLinterval remained stable during all movements (Fig. 1) . Conclusion A biomechanical reconstruction of the wrist movements has to be achieved when replacing the scaphoid. Therefore, we used a patient specific design and a well-known technique of ligament reconstruction. Our 4D-CT based analyses of the motion patterns of the carpal bones patters showed a sufficient suspension, a restoration of carpal alignment but a lack of movement of the scaphoid prosthesis compared to the native scaphoid movements. This lack of movement could be explained by a strong reconstruction of the secondary stabilizers of the scaphoid. Our findings provided evidence for a need of a different fixation technique of the prosthesis to allow for more freedom during movement. These findings could also have an impact on the reconstruction technique of the scapholunate ligament.  Keywords 3D printing, Surgical guides, Patient-specific plates, SCFE In adolescent patients with a slipped capital femoral epiphysis (SCFE), the slipped hip head is relocated by an intertrochanteric osteotomy in the best possible anatomically correct position. A clear but also time-consuming pre-operative planning precedes this.The pre-operative planning is made by means of pre-operative physical examinations, a CT scan and/or X-rays to obtain the best possible anatomically correct position during the operation. When performing this osteotomy, it is difficult to maintain the correct orientation of the head relative to the rest of the femur. Since the anatomy of the femurhead is disturbed, the correct anatomical position with respect to the femur is difficult to obtain. This new position (which is approximate) is secured with a standard 90°angle plate (Synthescondylarplate). The use of patient-specific 3D models may improve the planning process, resulting in less OR and fluoroscopy time [1] . In other applications, such as distal femoral osteotomy, the use of 3D surgical guides is described, resulting in increased precision of the OR and less OR and fluoroscopy time [2] . The pre-operative CT scan is imported into specialized software (D2P, 3D systems). Both the affected and the healthy site are segmented using this software. The healthy sited is mirrored and is used as reference for the optimal planning of the osteotomy. The surgery is virtually performed on the affected site to determine the optimal cutting plane and the positioning of the screw holes and plate. A surgical guide is designed based on this planning (Solidworks, Dassault Systèmes). If standard plates don''t fit, a patient-specific plate is designed as well. Post-operative X-rays are made regularly till the end of growth in two different positions (Lauenstein and AP) to assess the performed osteotomy and to asses the other hip for bilateral SCFE. OR and fluoroscopy times will be compared for patients operated in the traditional way and patients operated using a patient-specific guide. Both patientgroups will receive a questionnaire to assess the physical ability after the surgery. Results 14 Patients were included in this study until now. Eight patients with SCFE have been operated in the traditional way without the use of a surgical guide and six patients have been operated using a patient- Fig. 1 Qualitative three-dimensional presentation of flexion and radial abduction of the native wrist and after implantation of the prosthesis specific guide. Four out of these six patients had a patient-specific plate implanted. First results look promising, but quantitative results are not available yet. The design and use of a surgical guide both have a steep learning curve, resulting in shorter design times and shorter OR times after the first two patients. The use of patient-specific plates made it possible to operate complex patients who would otherwise had a suboptimal correction, or had not been operated at all, because of non fitting standard plates. Preoperative planning and use of a surgical guide made it possible to perform the OR faster and more accurate. The use of surigcal guides in southwick osteotomy for patients with slipped capital femoral epiphysis is a promising development, which may result in more accurate intertrochanteric osteotomies and in shorter OR times. Keywords Preoperative 3D planning, Computer-assisted planning, Virtual planning, Imaging Purpose Fracture management may lead to malunion of a bone, and in severe cases to symptomatic complaints and early osteoarthritis. Conventional surgical treatment by an opening or closing wedge osteotomy [1] leads to either a gap in between the bone segments or to bone shortening when a bone wedge is removed. An alternative way to restore bone alignment in the coronal, sagittal and axial plane is to apply an oblique osteotomy [2] and to rotate the distal bone segment about the axis perpendicular to the osteotomy plane, while keeping the bone faces connected. Unfortunately, this procedure is only feasible for cases where a bending deformity coexists with sufficient torsion deformity. For cases where the procedure fails (Fig. 1a) , an oblique double-cut rotation osteotomy may be a good alternative. Planning this approach is not a trivial task since it requires determining the correct location and obliquity of the osteotomies, the rotation angles to apply to the bone segments, and the in-plane translation of the bone segments to achieve optimal alignment. Although the method perfectly restores rotational alignment, a translation error of the bone segments persists if a suboptimal set of osteotomy parameters is chosen (Fig. 1b) . We present a novel method to preoperatively plan an oblique double-cut rotation osteotomy, which iteratively evaluates a set of osteotomy parameters to minimize the residual translation error. Preoperative 3D planning is based on a CT scan of the affected limb and the contralateral limb, which serves as reconstruction target. The affected and target bone are segmented from the CT image and mirror image [3] . Clipped distal and proximal segments of the affected bone are registered to the target and provide the correction matrix Mc to restore alignment of the distal segment as planned. The correction procedure aims at aligning the distal segment by splitting the matrix in two parts, each representing an oblique single-cut rotation osteotomy, together providing the planned alignment. The method further evaluates alignment of the middle segment by comparing its placement with the target bone. Finding the right set of osteotomy parameters, which minimizes the residual translation error is performed by a modified downhill simplex optimizer by Nelder and Mead. The algorithm is guided by a metric function, which quantifies alignment of the distal segment (along the target bone axis and in the perpendicular direction) and alignment of the middle segment with the target bone, and enables the user to choose how to balance between these optimization goals. We evaluate, by simulation, the residual malalignment after preoperatively planning an oblique double-cut rotation osteotomy in a number of patient cases (one case shown below) that were previously treated in our hospital using conventional surgical methods. The initial translation error of the distal segment was 86.3 mm, being 27.3 mm along the bone axis and 81.1 mm in the transversal direction. These translation errors [initial, along, transverse] were reduced after running the optimization procedure but depended on the optimization parameters chosen by the user. When alignment of the distal and middle segments was balanced and no in-plane translation of the bone segments was allowed (Fig. 1c ) the residual translation error parameters were [9.7, 9.7, 0.1] mm. If in-plane translation of the bone segments was enabled, these error parameters [7.1, 6.9, 1.8] mm (Fig. 1d) were smaller but at the cost of a small distal step off. If the procedure was set to optimize only for length, these errors were [15.1, 2.8, 14.8] mm, which indeed reduces the length error at the cost of alignment of the bones segments along the bone axis (Fig. 1e) . The preoperative 3-D method for planning an oblique double-cut rotation osteotomy restores rotational alignment and helps reducing a residual translation error of the bone segments in cases where an oblique single-cut rotation osteotomy fails. The residual translation error depends on the bone deformity and on the set of optimization parameters chosen by the user. The proposed method may be valuable in planning reconstructive surgery of complex bone malunion.  Craniosynostosis is a congenital defect characterized by cranial malformations caused by the premature closure of one or more cranial sutures. When this condition implies elevated intracranial pressure and impaired brain growth, surgical correction is required to reshape the affected bone and avoid functional impairments. Nowadays, surgical reconstruction in craniosynostosis relies mostly on the subjective judgment of the surgeon to best restore normal skull shape. Remodeled bone is manually placed and fixed, and slight variations can compromise the aesthetic result. This methodology is time-consuming and highly dependent on the experience of the surgeon. However, there is still no standard and validated methodology for objective intraoperative quantification of surgical outcomes. Fast and accurate quantitative evaluation inside the operating room will enable surgical corrections to ensure an accurate translation of the preoperative virtual plan and an optimal outcome for the patient. Although intraoperative computed tomography (CT) imaging has been proposed as a tool for reconstruction guidance, this technique requires exposing the children to harmful radiation, increases surgical time and is not available in most centers. In a previous work, we demonstrated the accuracy of 3D photography for cranial vault modeling inside the operating room using a mobile 3D scanner [1] . However, accurate surgical outcome analysis was dependent on manual and time-consuming registration techniques for the alignment of intraoperative scans. In this work, we present and evaluate an automatic workflow for the intraoperative assessment of the surgical outcome during craniosynostosis reconstruction surgery based on the automatic registration of intraoperative 3D photographs. A mobile 3D scanner is used for the acquisition of the reconstructed bone surface. Then, these 3D photographs are automatically aligned and compared with the virtual surgical plan providing intraoperative feedback to the surgeons. The performance of the proposed workflow was evaluated in a 3D printed phantom. Methods A phantom was designed and manufactured to produce a realistic scenario for surgical simulation and performance evaluation. The design of the phantom was based on an infant presenting metopic synostosis and previously operated in our center. Polylactic acid (PLA) and silicone (Smooth-On, PA, USA) materials were used to simulate bone and soft tissue, respectively. A total of 15 reference points were attached to the simulated bone tissue surface for error computation purposes (Fig. 1 ). 3D photographs were acquired using the Artec EVA Ò (Artec Group, Luxembourg) structured light scanner. This hand-held device illuminates the surgical area with stripped-patterns of bright white light and computes a 3D surface mesh from the deformation of these patterns. In addition, the scanner includes an extra camera to obtain color texture information. The automatic alignment of 3D photographs was performed by means of three squared color markers. These markers were attached to the healthy bone surface surrounding the reconstruction area using resorbable pins (Fig. 1) . Automatic identification of the color markers in the 3D photographs was performed applying color filtering over the texture data [2] . Once markers are detected, the center of mass is extracted from each one and subsequent 3D photographs can be aligned using fiducial-based registration. The complete workflow was tested using 5 different spatial configurations of color markers under 3 different illumination conditions (homogeneous, unidirectional and darkness) to assess the performance of our methodology for automatic alignment of scans. Two 3D photographs were acquired by different users for each configuration and illumination condition, acquiring a total of 30 3D photographs. For each marker configuration and illumination condition, we computed the average registration error in the 15 reference points on the phantom. In addition, scanning and processing time were measured for each repetition. The workflow will be included in the clinical setting with the following steps. First, a reference 3D photograph of the exposed cranial surface would be acquired prior to cranial vault remodeling. Then, this 3D photograph would be aligned with the bone surface in the preoperative CT scan using iterative closest point registration. After cranial vault remodeling, new intraoperative 3D photographs could be acquired and automatically aligned with the reference through the proposed color marker identification and fiducial-based registration algorithms. Average registration error of cranial vault 3D photographs was 0.44, 0.53, 0.37, 0.34, and 0.22 mm for the five different marker configurations, respectively (Fig. 2) . A higher error was found in configurations 1 and 2, where color markers were attached close to each other in a specific side of the cranial vault. A lower error was found in configurations 3, 4 and 5, where color markers were distributed separately and surrounding the cranial vault. No significant differences were found between the registration error of 3D photographs acquired under the three different illumination conditions investigated. The average scanning time required for the acquisition of cranial vault 3D photographs was 42.60 ± 8.54 s. In addition, average processing time, to generate the final cranial vault model from the acquired frames, was 97.36 ± 16.89 s. The proposed methodology enables the automatic alignment of 3D photographs based on the attachment of color markers in the scene. Average registration error can be reduced below 0.4 mm when the color markers are distributed around the area of interest. In addition, we demonstrated that the registration error and accuracy of 3D photographs are invariant to changes in the illumination conditions of the environment thanks to the light source included in the Artec EVA Ò scanner. Therefore, this methodology could be successfully applied under illumination conditions found in operating rooms. Furthermore, 3D photography can be included in the current surgical workflow without substantially increasing the surgical time. This novel technique can be used for quantitative evaluation of surgical outcome during craniosynostosis reconstructions surgeries. Multiple intraoperative 3D photographs can be acquired and automatically aligned and compared with the virtual surgical planning to provide feedback to the surgeons. Surgical corrections can be applied according to the intraoperative feedback to ensure optimal surgical outcome, thus improving the reproducibility of the surgery and reducing inter-surgeon variability. A method for estimation of brain function positions based on the standard brain model using retrospective information in awake surgery for glioma Keywords Brain mapping, Decision-making, Normalized brain, Nonrigid registration Purpose Recent developments in awake surgery for glioma necessitates increased assistance to surgeons for surgical decision-making to reduce operation time and patient burden. Intra-operative decisionmaking depends on a surgeon's knowledge and experience. Especially, skilled surgeons choose operative techniques based on their knowledge and expertise. Evidence-based data can be generated by statistically analyzing surgeries performed by skilled surgeons. These data will aid surgical decision-making. Therefore, retrospective intraoperative information of past patients (such as glioma resection area, post-operative complications, and brain function response points) who underwent surgery must be stored safely. To aid intra-operative decision-making, creation of a datasets is warranted to predict the survival rate and post-operative neurological complications, obtained by retrospective analysis of post-operative information. A previous study proposed the creation of digital datasets for statistical analysis and integration of intra-operative brain function positions into the standard brain model using non-rigid registration [1] . Individual differences exist among the intra-operative information of patients who underwent surgery previously; therefore, positional information is unified in the coordinate system of the standard brain model to eliminate individual differences by standardization. Intra-operative information obtained by these studies can be integrated into the standard brain model to generate evidenced- Fig. 2 Distribution of the registration error for the five different spatial configuration of color markers Int J CARS (2020) 15 (Suppl 1):S1-S214 based datasets. However, previous studies have integrated only limited brain function positions into the standard brain model. Therefore, as opposed to previous studies, it is necessary to validate the estimation of brain function positions in an individual brain after integrating a comparatively large number of brain function positions. This paper describes the creation of an evidence-based dataset to aid surgeons with intra-operative decision-making. We propose a estimation method based on patient's brain function positions. Methods Brain function positions were non-linearly transformed into patient's brains using intra-operative magnetic resonance (MR) images to estimate the brain function positions during awake surgery for glioma. This estimation method has four processes such as (1) Selection of brain function positions proximal to the tumor, (2) Non-rigid registration of standard brain model into pre-operative MR images using the finite element method (FEM) [2] , (3) Non-rigid registration of preoperative MR images into intra-operative MR images using FEM and, (4) Estimation of patient's brain function positions based on the standard brain model. Process 1 involves the selection of brain function positions proximal to the tumor. First, we segmented the tumor location from the intra-operative MR images. Next, we selected integrated brain function positions, proximal to the tumor, from the standard brain model. Finally, the tumor-adjacent gyrus was searched, and brain function positions present in the standard brain model was selected as projection candidates. In process 2, we performed non-rigid registration of the standard brain model into the pre-operative MR images using FEM. Briefly, the displacement of gray matter between the pre-operative MR images and the standard brain model was calculated using FEM. In addition, brain function positions of the standard brain model were projected non-rigidly onto the pre-operative MR images. In process 3, we performed non-rigid registration of the pre-operative MR images into the intra-operative MR images using FEM. The displacement of gray matter between the pre-operative and intra-operative MR images was calculated using FEM, and the brain function positions of pre-operative MR images were projected non-rigidly onto the intra-operative MR images. In process 4, we estimated patient's brain function positions. The center of gravity of the deformed brain function position on patient's brains was moved to the nearest brain surface and re-plotted on the intra-operative MR images as a 4-mm diameter sphere. With the above four processes, brain function positions were projected non-rigidly onto the patient's brains and the function brain positions were estimated. Figure 1 shows the projection of integrated brain function positions associated with language and motor areas, estimated from the standard brain model (59 points of 24 patients) onto the patient's brain. Figure 2 shows the result of the estimated brain function positions from the standard brain model onto patient's brain. To evaluate the efficacy of the method for estimating brain function positions, we estimated the error between the estimated brain function positions and the actual brain function positions obtained by brain mapping. Furthermore, as the reference data of the estimated brain function positions, we used data of the electrical stimulation positions obtained from the navigation system during brain mapping. The patient who responded to 1 point of language area and 2 points of motor area during brain mapping was evaluated further. The error between the feature points of the sulcus near the estimated brain function position was considered as the projection error by non-rigid registration. This projection error was 2.5 ± 1.3 [mm]. The estimated brain function position A was related to language area response. In contrast, the estimated brain function positions B and C were the brain function positions related to motor response, and both B and C were included in a group of points similar to actual brain function. Therefore, the estimated brain function positions were more or less close to the positions identified by brain mapping. In this study, we propose a method to estimate and project brain function positions, based on the standard brain model, onto the brains of individual patients intraoperatively to aid surgeons with surgical decision-making. Moreover, the projection error between the estimated brain function positions and the brain function positions recorded by brain mapping was 2.5 ± 1.3 [mm] . Therefore, despite the presence of glioma, the estimated brain function positions were almost similar to the actual brain function positions. Further investigation with a large sample size may enable accurate estimation of brain function positions into the standard brain model. In future studies, we plan to integrate the intra-operative information other than brain function positions into the standard brain model. In addition, by deforming non-rigid intra-operative information integrated into the standard brain and subsequently onto the patient's brains, we aim to estimate the brain function position intraoperatively and predict the post-operative survival rate according to resected area. Purpose SEEG is a minimally invasive method used in the diagnosis of drugresistant epilepsy cases to record electro-encephalic signals via rectilinear electrodes, each one containing several contacts that are implanted deep inside the brain. These patients are monitored for 1-3 weeks after implantation and electrical data is collected from each electric contact (around 120) to define the area responsible for starting the epileptic seizure. This electrical information is visualized as long lists of 2D plots of voltage versus time, one per contact (monopolar) or one for each two consecutive contacts (bipolar). DWI is a magnetic resonance acquisition technique which can be processed by several techniques (for example as Diffusion Tensor Imaging or Spherical Deconvolution) to obtain a whole-brain tractography to estimate white matter connectivity. In the context of SEEG, this information can be of relevance to plan the optimal implantation trajectories or to understand the propagation paths of the epileptic network [1] . Whole-brain tractography yields a vast amount of polylines representing the main axonal pathways, and requires filtering to dissect the most relevant tracts. There exists a variety of software solutions to perform this task (Medtronic StealthDTI TM , Synaptive Modus Plan TM , TrackVis) which provide tools to define regions of interest (ROIs) and logical operations among them. We present an intuitive and effective DWI tractography filtering module for clinical use in the context of SEEG for epilepsy surgery which is integrated into SYLVIUS a 3D stereo multimodal visualization platform [2] . Two tools are presented: a pre-implantation 3D interface to assist in the precise planning of SEEG trajectories with respect to DTI tracts, and a post-implantation module. The latter uses post-implantation CT to bring the electric signal information (from SEEG) into consideration. The pre-implantation planning interaction mode allows the user to modify the trajectories interactively in 3D and visualize which tracts are intersected. It does so by creating, for each trajectory, a cylindrical ROI with the radius set for the security zone (typically 3 mm) similarly as Synaptive Modus Plan TM . When the user selects a trajectory, a cylinder vs line segment collision detection algorithm is used to identify tracts intersecting the cylindrical ROI associated with it. Selecting more than one trajectory performs an ''AND'' operation returning the tracts traversed by all selected electrode trajectories, which allows for modifying one electrode trajectory and seeing which tracts connect with other electrodes (Fig. 1) . The post-implantation filtering tool enables to establish the link between the 3D positions of the SEEG contacts and the electrical signals through white matter tracts (DWI) for further analysis of epileptic propagation networks, and validation of tractography algorithms or axonal speed measurements. It uses all SEEG contacts from all implanted electrodes, segmented from a post-implantation CT and labeled as the electrode name plus a numeric index (e.g., A5 for the fifth contact of electrode A) to generate electrical ROIs of spherical shape. Depending on the type of electrical readings, it is created either between each pair of consecutive contacts (for bipolar electrical measurements), or one centered at each contact (for monopolar measurements). Monopolar ROIs are labeled in the same way as its associated contact whereas in bipolar ROIs the label is constructed by appending the two contact labels together (e.g., A5A6). Later, a recursive procedure call is executed which checks, for every electrical ROI, if the tracts filtered intersect any other one. When it does, it stores the involved ROIs and the tracts traversing them simultaneously as a result. To perform this computation, a line segment versus sphere collision detection algorithm is used. The output of this computation is a list of pairs of electrical ROIs and the tracts connecting them, which can either be visualized in 3D (Fig. 2 ) or used as a text file with connectivity measures. This result, which we call SEEG-electrome, can be interpreted as a graph were ROIs/electrical signals define nodes and DWI tracts define connections. The pre-implantation DTI tractography filtering tool has been used in the clinical environment in combination with the plans created in the ROSA robot platform (Zimmer Biomed Inc.). It was initially designed for the analysis of the SEEG implantation plans in the context of epilepsy surgery, where it has been used to analyze 6 cases in Hospital del Mar, Barcelona. The tool has also been used in 8 anorexia cases for planning the precise positioning of Deep Brain Stimulation which needs to traverse certain tracts (e.g., cingulum) [2] . The post-implantation DTI tractography tool, when used in combination with the electrical signals registered from the patient during observation has been used to provide insights on the relation between the electrical activation from the SEEG electrode contacts and the physical connection provided by the DWI tracts. It has been used to filter tractographies from MRtrix, Dextroscope, and Int J CARS (2020) 15 (Suppl 1):S1-S214 StarTrack. The SEEG-electrome computation could be manually computed, but for 120 contacts in a monopolar setting would require over 7 thousand queries (). To illustrate the results of the computation, for the case shown in Fig. 2 , a whole-brain tractography with over 120 K tracts was used, and 15 SEEG electrodes (163 contacts in total) defined 148 bipolar ROIs. The SEEG-electrome found 106 of these ROIs to be connected via DWI tracts, (32 connected to only one, 2 connected to 9) via 147 pairs/connections -from over 10 K possible combinations-. The dissected tracts (966 out of over 120 K) defined 147 distinct connections with different tract length and density. Tract density ranged from 1 to 61 tracts/connection. Some of the pairs were in consecutive ROIs from the same electrode, but some of them connected two distant electrical ROIs in ways which were hard to anticipate (e.g., frontal to the occipital lobe or crossing hemispheres). As each ROI of the SEEG-electrome has a corresponding electrical signal, further research is to be conducted to investigate the presence of an electrical relationship among detected ROIs. The presented tools can simplify DWI use in clinical practice and help reduce the complexity of the analysis of epileptic propagation networks, and functional brain research in general. Purpose Cerebrovascular surgery is the method of choice to treat arteriovenous malformations (AVMs), arteriovenous fistulae (AVFs) and aneurysms. AVMs and AVFs consist of abnormal collections of blood vessels in the brain, while aneurysms are bulges due to weakness in the wall of a blood vessel. Surgical treatment of AVMs and AVFs consists of clipping the feeding artery or arteries prior to tying off the draining veins and removing the nidus. Clipping the wrong vascular branch can result in severe complications such as haemorrhage, which can lead to neurological deficits. It is thus paramount for neurosurgeons performing neurovascular procedures to be able to distinguish between feeding and draining vessels in and around the pathology. The vessels associated with these vascular malformations often appear as neither true arteries or veins, so practitioners must rely on preoperative imaging to determine blood flow directionality. However, translating this information from preoperative imaging to the surgical scene can be challenging. For this reason, methods such as intraoperative angiography with dye injection, intraoperative Doppler ultrasound acquisition and hemodynamic video analysis have been proposed in the past to provide hemodynamic information. The latter has several advantages, including the fact that it is contactless, contrary to Doppler ultrasound, where external force may damage the already weakened vessel walls. Video analysis can also be performed continuously throughout the surgery, unlike dye injection, which can only be used periodically, since the dye stays in the bloodstream long after injection (more than 15 min). Video-based hemodynamic analysis has only been recently proposed [1] and has yet to be tested intraoperatively. The work presented here shows our initial tests of integrating hemodynamic analysis into MARIN, an open source Mobile Augmented Reality Interactive Neuronavigation system. Methods MARIN consists of an iPad tablet and a desktop computer running the IBIS neuronav platform, Fig. 1 . A video sequence of a few seconds recorded in 1280 9 720 resolution at 30 frames per second is acquired on the iPad. After acquisition, this sequence is sent to the desktop using the wireless local area network (WLAN). A region of interest (ROI) is then selected by the user. This is done by picking contour of the vessel region, on the touchscreen of the tablet. The points are sent to the desktop as well, over the WLAN, using the OpenIGTLink protocol and library. The video sequence is then cropped to keep only the central part of the image. 25% of the original image size was thus removed in order to reduce computation times. This is done since the tablet's camera has a much smaller zoom factor than a neurosurgical microscope. The ROI therefore consists only of a smaller region, which should usually be roughly in the center of the frames. The hemodynamic analysis is then performed on the desktop computer. This analysis consists of image stabilization, motion reduction, intensity magnification in the selected frequency range, as well as filtering, as described in [1] . Once the hemodynamic analysis is completed, a frame showing an augmented reality view of the scene with the direction of blood flow in the selected vessel of interest is sent back to the mobile device for in situ AR visualization. Tests were performed in the lab using a blood vessel phantom and test setup as in [1] . We demonstrated that mobile device's video capture was sufficient for hemodynamic analysis. We believe that the novel ability which allows the surgeon to interact directly with the touchscreen of the iPad to select the vessels of interest, as well as seeing the AR view in situ will both be particularly useful, and allow the system to be easy to use and integrate well into the surgical workflow. Interactive selection on the mobile AR neuronavigation app will enable the surgeon to select a region of interest themselves, without having to rely on a technician. This may potentially streamline procedures, since the tablet can easily be used intraoperatively as we have seen in bringing the system into the OR for AR guidance [1] . Our initial evaluations have shown that, since it is a fairly small and wireless device, it can easily and quickly be brought in when needed and stowed away once guidance isn't necessary. It can also be draped in a sterile bag to allow its use throughout the procedure, giving full control to the operating surgeon. The current work presents our initial feasibility tests on integrating hemodynamic analysis into a mobile AR neuronavigation system. In future work, we will provide a more seamless integration of hemodynamic analysis into MARIN that will involve allowing for real-time augmentation that is updated as the tablet is moved around the scene. The work presented in this abstract shows that the hemodynamic analysis presented in [1] can be integrated in a mobile AR neuronavigation platform. The camera of the tablet device has sufficient frame rate and resolution to estimate correct flow directions in vessels. It also enables easier interaction for the practicing surgeon with the selection of regions of interest for the blood flow computation. The next step of this work will involve bringing the system into the operating room to determine its functionality intraoperatively during neurovascular cases. Keywords Biopsy, Brain tumor, Column specimen, Catheter sheath Definitive diagnosis of brain lesions can be obtained based on histopathological evaluation. A brain biopsy is a surgical procedure to remove a sample of abnormal tissue. There are several biopsy method, which can be roughly divided into the following three categories: open biopsy, endoscopic biopsy, and stereotactic needle biopsy. Open biopsy is the most common form of brain biopsy. Craniotomy is made above the abnormal lesion, and the specimens are taken under a microscope. This technique has an advantage of certainty, but has a disadvantage of invasiveness and disorientation. Endoscopic biopsy is a relatively new method of biopsy using neuroendoscope. Because this method is performed via burr hole, it is less invasive. Moreover the lesion is confirmed directly by endoscope, this method has an advantage of visualization. However, this method requires complicated techniques. Stereotactic needle biopsy is used for the lesion that are deeper in the brain [1] . This method is also less invasiveness, because of burr hole surgery. During this procedure, a stereotactic frame is used to guide the needle into the abnormal lesion. However, this method has disadvantage of uncertainty for diagnosis, because the specimen is small. Of these various biopsy methods, each has both merits and demerits, it is not easy to define the most suitable technique. To solve this problem we developed novel technique ''boring biopsy method'' which is possible for both a certainty and less invasiveness. Boring is the process of making a hole especially with a revolving tool. This study aimed to investigate the usefulness of a new technique of biopsy for brain lesions with currently available medical equipment. Clinical and radiological data from 24 patients who underwent boring biopsy at Shinshu University Hospital and Kobayashi Neurosurgical Hospital were analyzed retrospectively. There are 4 steps for boring biopsy method (Fig. 1 ). First step is planning the trajectory of boring biopsy with neuronavigation system preoperatively [2] . The ideal trajectory is planned to avoid sulcus and vessels. Second step is making the biopsy tool intraoperatively. We modified catheter introducer (Medikit CO., LTD.), which is usually used for angiography. Third step is insertion of the boring tool which was made at previous step. Corticotomy is made and this tool is inserted towards the abnormal lesion via planned trajectory. Fourth step is obtaining the specimen. After reaching the target lesion, the tip of the tool is closed and the abnormal tissue is captured. Then the tool which contains the tissue is pulled out. With this method, vertically long column specimen can be obtained (Fig. 2) . This specimen contains not only abnormal tissue but also normal tissue. Column specimens might provide further pathological information than conventional block or small specimens. Twenty-four patients received boring biopsy at Shinshu University Hospital and Kobayashi Neurosurgical Hospital from January 2014 to introducer. This tool is inserted towards the abnormal lesion via preoperatively planned trajectory. After reaching the target lesion, the tip of the tool is closed and the abnormal tissue is captured. Then the tool which contains the tissue is pulled out. The tissue is pushed out. With this method, continuous column specimen can be obtained Fig. 2 Column specimen of glioblastoma. A twenty-six mm specimen from contrast-enhanced lesion in the left parietal lobe was obtained by boring biopsy. Pathological feature was different in this continuous specimen Int J CARS (2020) 15 (Suppl 1):S1-S214 June 2019. Male was 18 and female was 6. The mean age was 61.4 years (range 18-85 years). Histopathological diagnosis was glioma in 18 patients, primary central nervous system lymphoma in 5 patients, and others in 1 patient. The location of the lesion was frontal lobe in 10 patients, temporal lobe in 7 patients, parietal lobe in 3 patients, basal ganglia in 2 patients, occipital lobe in 1 patient, and other in 1 patient. There were 2 (8.3%) surgical complication: asymptomatic postoperative bleeding in 1 patient, transient aggravation of hemiparesis in 1 patient. Adequate and sufficient specimen was obtained in all cases. A newly-developed technique of biopsy for brain lesions ''boring biopsy'' which needs only currently available medical equipment was introduced. This unique maneuver improves the diagnostic accuracy with less invasiveness. Histopathological diagnosis could be achieved using a boring biopsy method without serious surgical complications in all cases. Although the tool for ''boring biopsy'' could be made simply with catheter sheath, we have to make this tool at each operation. Therefore, we are investing novel boring biopsy device instead of catheter sheath. In future, this method might become the next-generation standard procedure, especially when the histological evaluation is paramount. Overlay of brain structure and estimated brain function position onto microscopy using pre-and intra-operative MRI in awake surgery for glioma Purpose In brain tumor surgery, a surgeon must have precise knowledge of the brain structures surrounding the tumor, which differs among patients, and the brain function positions, for maximal tumor resection and minimal postoperative complication. An expert surgeon estimates them from various pre-and intraoperatively obtained information, and visual information, such as the neural vasculature and relation of brain sulcus, gained from limited craniotomy area. Moreover, awake brain mapping is required to identify higher brain function position. Skilled surgical knowledge and experience are required to precisely understand the brain structure and to accurately and rapidly identify spatial brain functions is needed with less patient burden. Overlaying the brain structure and estimated brain function position onto microscopy images can support surgeons to intuitively grasp them. To overlay these images and information, importantly, the overlay system must be adaptable for use in any environment. This overlaying requires analysis of pre-operative images and their integration onto intra-operative images because it is difficult to extract some feature information from intra-operative images. Thereafter, integration of the estimated spatial brain function onto the surgical field using our proposed method will enable intuitive estimation of brain functions [1] . To support surgeons to accurately grasp the brain structures and spatial brain function, we propose a method which integrates images acquired pre-and intraoperatively with magnetic resonance images (MRI) and microscopy, and overlays the brain sulcus, tumor, and estimated brain function onto the microscopy images. We evaluated the accuracy and efficacy of the proposed method. Methods This method facilitates the overlaying of brain sulcus, tumor, and estimated brain function position onto microscopy image by integration of pre-and intra-operative MRI and microscopy images. Our method performs feature matching based on blood vessel information obtained from both MRI and microscopy images for use in any surgical setting. In addition, non-rigid registration of pre-operative information minimizes the intra-operative extraction and integration process of feature information (Fig. 1) . The proposed method has the following 8 processes: 1. Feature information extraction from pre-operative MRI. 2. Feature information extraction from intra-operative MRI. 3. Non-rigid registration of brain function position of the standard brain to pre-operative MRI. 4. Non-rigid registration of pre-operative MRI to intra-operative MRI. 5. Extraction of the craniotomy area from intra-operative MRI. 6. Extraction of blood vessel information from microscopy images. 7. Feature matching of MRI and microscopy images. 8. Overlay onto microscopy image. In process 1, brain area, blood vessel, and brain sulcus are extracted. Brain area extraction is performed by segmentation using Pix2Pix, a type of GAN (Generative Adversarial Networks), and subsequent manual correction (train data, 1455 images by axial MRI from 11 cases). Segmentation accuracy was 94.84%. In addition, a mesh of the brain area used for non-rigid registration is created by the segmented results. Blood vessel extraction is performed by recognition of the blood vessel using the fast marching method. Brain sulcus extraction is performed by recognition of brain sulcus with the Brain VISA tool, a brain image analysis software. In process 2, tumor is extracted and a mask of the craniotomy area is created. Tumor extraction is performed by segmentation using Pix2Pix with manual correction (train data, 1809 images by axial MRI from 14 cases). Segmentation accuracy is 87.92%. In addition, a craniotomy area mask is created by comparing the skull information with the pre-and intra-operative MRI. In process 3, the estimated brain function position is integrated into the pre-operative MRI by non-rigid registration of the proposed method. In process 4, the pre-operative blood vessel, brain sulcus, and the integrated estimated spatial brain function are integrated onto the intra-operative MRI using non-rigid registration of pre-operative MRI to intra-operative MRI. By this process, intra-operative brain shifts are reflected in the feature information extracted from pre-operative MRI. In process 5, the extracted feature information is determined in the craniotomy area using a mask of craniotomy area. In process 6, blood vessel is extracted from the microscopy images using Pix2Pix (train data, 190 images). This accuracy was 68.99%. In process 7, the blood vessel information extracted from MRI and microscopy images are matched. First, thinning processing of the both blood vessel information is performed. Next, the extracted feature points of blood vessel information are used by SURF; the outliers are removed using RANSAC. Finally, projective transformation matrix is calculated between the extracted feature points. In process 8, the brain sulcus, tumor, and estimated brain function position are overlaid onto the microscopy images using the calculated projective transformation matrix. To evaluate the efficacy of the proposed method, we calculated the accuracy of non-rigid registration of pre-operative MRI to intra-operative MRI; additionally, the integration accuracy of feature information and integration accuracy of MRI and microscope images were evaluated. To evaluate the accuracy of non-rigid registration, the registration error is calculated using 5 sulci landmarks in the proximity of the glioma. To evaluate the integration accuracy of MRI and microscopy images, we used 8 intersectional feature information of MRI as the assessment parameters. The estimated accuracy of nonrigid registration was 2.5 ± 1.3 mm. The integration accuracy of MRI and microscopy image was 6.5 ± 3.4pixel (2.3 ± 1.2 mm estimated). With the proposed method, brain sulcus, tumor, and estimated brain function position were overlaid onto microscopy image. Visualization of the types of brain sulcus enabled intuitive grasp of the brain structure. Visualization of the estimated brain function position facilitated intra-operative identification of brain function position (Fig. 2) . In this study, integrated pre-and intra-operative MRI and microscopy images, and the brain sulcus, tumor, and estimated brain function position were overlaid onto microscopy images intraoperatively to enable surgeons to accurately grasp the brain structure and its function. The estimated integration error of MRI and microscopy image was 2.3 ± 1.2 mm. In future studies, we plan to improve the accuracy of this integration method using 3-dimensional (3D) reconstruction of microscopy images acquired by stereo microscopy, and 3D feature matching of MRI and microscopy image. In addition, we aim to fully automated the feature information extraction process from MRI and microscopy images. Central venous catheterization (CVC) involves inserting a catheter into a major vein such as the internal jugular or subclavian veins. Since major veins lay alongside major arteries this procedure has many risks, and the probability of complications is mainly dependant on the experience of the physician [1] . Not only does CVC pose many risks, but it also has a long and complex workflow that trainees have difficulty remembering. By using video to recognize the tasks in the CVC workflow, our goal is to provide instruction and feedback to trainees without needing an expert observer. The seven tasks that we attempt recognize are: applying the anesthetic, inserting the needle into the vessel, inserting the guidewire, cutting with the scalpel, using the dilator, inserting the catheter and finally removing the guidewire. We have previously attempted to use convolutional neural networks (CNN) alone to recognize the current task based solely on the tool in use from video [2] . While this approach was able to produce modest results, it is prone to error when the tools are obscured by the trainee's hands. Here, we present a method for recognizing CVC workflow tasks from video that combines a CNN and a task identification policy trained using reinforcement learning. To recognize the tasks in the CVC workflow, we first use a convolutional neural network (CNN) to recognize the tool in use. The output from the CNN is then used to determine the current task according to a task identification policy that has been trained using reinforcement learning techniques. For our CNN, we use MobileNet as in our previous study. The network is trained on a collection of 133,135 images of the various tools used in the procedure. The task identification policy is then applied to the CNN output to determine the current task. We use reinforcement learning to create our task identification policy. Reinforcement learning is a form of machine learning that is Fig. 2 Result of overlay brain sulcus, tumor and estimated brain function position Int J CARS (2020) 15 (Suppl 1):S1-S214 concerned with action selection based on a cumulative reward scheme. In our case, the action that we wish to select is how to label the current task that the trainee is performing. We recognize the seven tasks and we also include the option that the user may be doing none of these actions. We model the problem as one of trying to find an optimal path through a grid, where each column in the grid represents a frame in the video, and the rows represent the tasks in the procedure. In this way, our action becomes selecting which row of the grid the agent should be in in the following frame. To train the policy we use Q-Learning, which is used to locate the correct task that will yield the highest reward. To predict the label for a novel frame, we use the policy of the most similar frame in the training set. The most similar frame is one that has the most similar CNN output and occurs at a similar time within the video. To evaluate our approach, we perform 7-fold cross validation. We use 6 videos for training a policy and test on the remaining video. Each video is composed of an average of 2294 frames and we predict the current task in each frame of the video. We measure the average accuracy of the prediction along with the precision and recall for each of the seven tasks. We also compare our results to our previous method of using only a CNN to recognize the tasks based on the tool in use. The approach combining the CNN with Q-Learning achieved an average accuracy of 85%. This can be seen by the strong resemblance between the optimal path and the predicted path, an example of which can be seen in Figs. 1 and 2 respectively. In contrast, the CNN alone had an average accuracy of 61%. The combined approach also achieved higher average precision and recall compared to the CNN alone (Table 1) . The approach combining a CNN and Q-Learning shows promise for recognizing the tasks in the CVC workflow. This approach was able to outperform the CNN alone in average accuracy, precision and recall. The strength of the combined approach lies in the inclusion of temporal information into the task prediction. This allows our approach to still predict the correct task, regardless of whether the CNN produces a correct classification. While 85% accuracy may not seem particularly high, the goal is to use this approach to recognize tasks in real-time. In real-time we are able to classify multiple frames per second. As long as we can recognize the correct task in at least one frame per second, there will be little impact when identifying the transition between tasks.  Bidirectional LSTM Each frame extracted from the video is further processed to extract relevant features. Data augmentation techniques like translation, cropping, resizing and Gaussian blur are applied to frames to generate scenarios wherein the camera is placed in a wrong position or the lighting in the surgical room is not apt. Inception V3 pre-trained on Imagenet is used to extract features. The extracted features are then fed into LSTM as a sequence in forward and reverse direction. Since the data is sequential, we use the concept of sweep instead of epochs. A sweep is a collection of batch size sequences that continue until all sequences in the batch are exhausted. The network is trained for 1200 sweeps. The loss is calculated using softmax crossentropy and exponential moving average. The exponential moving average (EMA) is applied to compute the weighted moving average that gives more importance to recent changes in gestures. Multi-Layer Perceptron ? Bidirectional LSTM To train both kinematic and video data the network architecture discussed in the previous section is slightly modified. A Multi-layer perceptron consisting of two hidden layers is added to process the kinematic data. The processed kinematic data is concatenated with the video data and passed as input to the Bidirectional LSTM. The confidence of the model is estimated using three methods, namely Monte Carlo Dropout, Naïve ensemble and Bootstrap ensembles. Predictive entropy is used to measure the uncertainty of the model, as it is a classification task. The model was trained and evaluated using the JIGSWAS [1] dataset. Accuracy and edit distance are used as key performance indicators. The evaluation is performed for both the leave-one-user-out (LOUO) and leave-one-trial-out (LOTO) cross-validation formats. Our model achieves competitive performance using both the video and kinematic data. For the suturing task, an accuracy of 81.38% and an edit distance of 10.51 was recorded. The first line in Fig. 1 represents the ground truth and second-line the model's prediction for the suturing task performed by a surgeon. Each color depicts a unique gesture performed. The third line provides an estimate of the model's confidence. Darker the red is more uncertain the model is about its prediction. White color indicates that the model is confident. The bootstrap ensemble can capture the model's low confidence during erroneous predictions and near the boundaries of each segment while transitioning from one gesture to another. Recognition of surgical gestures during minimally invasive surgery has to be performed accurately with high confidence. But the classification of such fine-grained actions cannot be performed using only contextual cues. Capturing nuances like the difference in the position of the needle, location of right and left arm, etc. is important for decision making. In this work, we have tried to explore models that can achieve this task with high confidence. Variations in the model's performance based on the type of data is also evaluated along with an estimate of uncertainty. Purpose Surgical Navigation is widely used to support surgical procedures by visualization of blood vessels, tumors, and other tissues. In the navigation, positional tracking is needed to obtain relative positions between surgical tools and a patient. For the tracking, position sensors by optical or electro-magnetic tracking devices are generally used and applied in clinical setting. However, measurement failure often occurs by line-of-sight occlusion or magnetic field disturbance. To address the issue, multiple tracking devices are needed for compensation. However, it is not practical approach due to the costs and the limited space around a patient. Meanwhile, an inertial measurement unit such as an accelerometer, a gyroscope, and a magnetometer are widely used for attitude estimation of aircrafts [1] . The integrated sensor information can provide accurate attitude. Therefore, we propose that attitude compensation by integration of position sensor and gyroscope by applying the IMU sensor integration technique. Methods Figure 1 describes the steps of attitude integration algorithm of a position sensor and a gyroscope by Kalman's filter. The filter estimates angular errors and gyroscope bias. At first, attitude in k-th step is predicted from received angular velocity, and estimated bias and attitude in previous frame as: where q is a quaternion, x is an angular velocity, and b is the gyroscope bias. The Dq is the rotation within a very short time Dt as: As the measurement error, the error vector is defined as the x, y, zaxis difference between the position sensor and the gyroscope as: where x, y, z are the axis directional vector, P and G means the position sensor and the gyroscope, respectively. The difference is related to the attitude error h e,k and the gyroscope bias error b e,k through the measurement matrix C k as: Using the measurement matrix, the Kalman gain and the estimated error vector are given by: where Q W,k , Q v,k are the covariance matrix of the noise process and measurement noise process. Finally, predicted quaternion is corrected by using the estimated attitude error by: The bias is also updated using the estimated bias error for the next iteration. To investigate the performance of the algorithm, measurement experiments were performed using an electromagnetic position sensor (Aurora V3, NDI) and an inertial measurement unit (BMX055, Bosch). The IMU consists of three-axis accelerometer, gyroscope, and magnetometer. In this study, only the gyroscope was used. The IMU has I2C interface, and connected with a PC via a I2C-USB converter (FT232H, FTDI). One of the software for the communication and data collection is implemented using C ?? language, and another for attitude estimation explained in section 2 is implemented using Matlab 2019b. In the experiment, a sensor coil for the electromagnetic tracking and the IMU was fixed, and the assembly was manually rotated around each axis about 10 s. During the motion, the tracking data and the angular velocities with timestamps were collected and stored in files, respectively. Using the collected data, the attitudes with and without the integration were compared. Figure 2 shows the respective attitudes using roll-pitch-yaw. From the comparison, the integrated attitudes denoted the same tendency of the position sensor data. The result demonstrates that the integrated method would compensate the attitude in the measurement failure of the tracking device. In this study, we proposed that the sensor integration for improvement of positional tracking in navigation systems. The proposed algorithm is an attitude integration method among the position sensor and the gyroscope. The results demonstrate the integrated attitude would be improved than non-integrated tracking. In future, absolute accuracy will be validated by using potentiometers as ground truth, and compensation in the situation of tracking failure will be investigated.  Planning surgical procedures is a complex task that needs to take multiple constraints and factors into consideration. A key element in operating room (OR) planning is predicting procedure durations. Predicting procedure duration pre-operatively can only reach a certain degree of accuracy due to various unpredictable factors that could happen during the surgery. Therefore, intra-operative information about the surgery progress is essential to adapt the daily schedule accordingly. Ideally, intra-operative information should be retrieved automatically, instead of adding compulsory registration or interrupting the surgical process to communicate with the OR team. Recordings made during laparoscopic surgeries are a potential source of information about the progress of the procedures. The videos show anatomical structures and the handling of surgical tools. A trained eye is able to recognize the on-going surgical phase from watching the videos. The introduction of deep learning techniques has brought up opportunities to recognize automatically information contained in surgical videos. However, the training of such algorithms still requires extensive manual labour such as the annotation of events in the videos. The aim of this study is to: (1) assess the efforts required to generate datasets that are sufficient to train deep learning algorithms, (2) apply state-ofthe art deep learning techniques on a set of laparoscopic videos, and (3) to assess the performance of the technique in terms of tool and phase recognition, and scalability to other types of surgeries. Methods Two datasets were used in this study. The first dataset contained 36 recordings of a laparoscopic cholecystectomy (Lap Chol), the second For all recordings, one frame per second was manually labelled with the tools that were present in the frame as well as the on-going surgical phase. This annotation process was done using the user interface NOUS (COSMONiO, the Netherlands). For this study, an InceptionV3 neural network was implemented and adapted to fit the needs of this application. In this neural network, all frames were analysed independently and no information was available on patient characteristics, duration of the surgery or frames before or after the current frame. A dedicated server (NOUS Learner, COSMONiO, the Netherlands) was located in the hospital. The Lap Chol dataset was divided into three subsets: a training subset containing 26 recordings, a testing subset containing one recording, and a validation subset containing nine recordings. The TLH dataset was not divided into subsets, it only functioned as a validating set. The performance of the network in terms of tool and phase recognition was assessed in the Lap Chol validation subset by calculating the recall and precision. The scalability of the network was assessed by using the network trained on the Lap Chol training subset to recognize the tools present in the TLH procedures. The tool recognition was assessed only for the four surgical tools that were present in both types of surgery. Acquiring the datasets and being allowed to use them to train a deep learning algorithm was only possible after the agreement of the ethical and legal department of the hospital. Then, the videos were selected and prepared for the annotation process. At last, annotating the frames of the videos took about 3.5 times the total duration of the videos. For the tool recognition in the Lap Chol validation subset, the weighted average recall was 78.2%. The minimum recall was 1.6% for the drain, and the maximum was 83.0% for the grasper. The weighted average precision was 88.0%. The minimum precision was 52.3% for the scissors, and the maximum was 100.0% for the retrieval bag. For the phase recognition in the Lap Chol validation subset, the weighted average recall was 71.1%. The minimum recall was 30.3% for phase 7 (Final check & irrigation) , and the maximum was 82.9% for phase 2 (Preparation & dissection). The weighted average precision was 76.2%. The minimum precision was 19.3% for phase 7, and the maximum was 79.8% for phase 2. For the assessment of the scalability of the network to TLH procedures, the recall and precision of the tool recognition in the TLH dataset were lower than the ones in the Lap Chol validation subset. The average recall was 34.0% for the TLH and 69.9% for the Lap Chol. The average precision was respectively 36.4% and 72.3%. However, the results for the grasper, which was identical and used intensively in both surgeries, were comparable. The results presented at this stage of the study show the potential of deep learning to recognize automatically information contained in surgical videos and provide insight in the investments needed to obtain the necessary datasets. The neural network is able to recognize the tools and the phases in the images of the videos. However, the results show a large variability between the different tools and phases, as well as between the different videos. Factors such as a very zoomed-in video or a different brand of surgical tools highly affect the performance. The next step in this study is to incorporate temporal information about the frames of the recording to the neural network. This will probably improve significantly the phase recognition. Moreover, the scalability of the neural network trained on Lap Chol procedures needs to be assessed on a larger TLH dataset. Purpose Lung cancer is the most common cancer globally with over 2 million new cases diagnosed every year [1] . Fortunately, if caught early, the likelihood of survival is over 61% over 5 years. Early diagnosis requires finding and sampling (biopsy) small (\ 10 mm), peripheral nodules that are located in the parenchyma of the lung outside the small airways. Currently available bronchoscopes are too wide to reach the small peripheral airways. Therefore, the surgeon advances blindly a sharp biopsy needle from the bronchoscope into the lung tissue in the approximate direction of the lesion which comes with a high risk of misdiagnosis. To improve the accuracy of the biopsy procedure, real time X-ray (fluoroscopy) is implemented, exposing the patient and physician to harmful radiation. Electromagnetic Navigation Bronchoscopy (ENB) and medical robotics present viable solutions but are still not optimal at present. Some of limitations are the high acquisition and procedure costs, limited use due to the complexity of the procedures, suboptimal targeting accuracy, large OR footprint and lack of suitable instruments for specific procedures. Moreover, the ENB approach alone is not sufficient. Despite a correct diagnosis, the lesions cannot be precisely identified and removed when the lungs collapse during the thoracoscopic lobectomy procedure. To address these issues, we have developed an improved second generation of a robotic arm with a dual electromagnetic and optical navigation system (Fig. 1) previously described [2] . The current improvements include a more compact and robust design and a smoother control of the flexible tools during translation and rotation. Additionally, the optical navigation for rigid instruments was integrated with the electromagnetic navigation for flexible instruments (used with or without the robot) within the same software platform therefore allowing the control and guidance of multiple surgical instruments by a single operator for diagnosis and treatment of cancer during the same surgical procedure. We built the outer case and connecting parts of the robot using additive manufacturing (Fig. 1) . We used Maxon micro-motors with position encoders and planetary gearheads (Maxon Group Ag., Sachseln, Switzerland) to activate the robot. For electromagnetic tracking, we used the Aurora EM field generator and Mini 6DOF electromagnetic sensor 1.8 mm 9 9 mm (Northern Digital Inc., Ontario, Canada). For optical navigation we used the Polaris (optical) Systems also from Northern Digital Inc. with active and passive Int J CARS (2020) 15 (Suppl 1):S1-S214 infrared trackers in a single tracking system (Fig. 2) . We have developed our own surgical navigation software platform, iMTECH. The iMTECH software allows the operator to: load the pre-operative CT scans, generate the 3-D model of the patient's anatomy, co-register the live location with the CT space, identify and navigate towards the target, and automatically correct the registration during the live procedure based on anatomical landmarks inside the lung airways. We developed a novel collision avoidance algorithm to prevent medical instrument from colliding and penetrating the lung airways during the virtual bronchoscopy procedure. We have developed a functional prototype of robotic electromagnetic and optical navigation bronchoscopy (rEONB) system to reach small lesions at the periphery of the lung using electromagnetic guidance. Subsequently, rEONB system can guide a surgical procedure from outside the lung using optical guidance (Fig. 2) . Initial tests of design concept and limited evaluation of alternative solutions were performed for the iMTECH software, the robot, and the catheter instruments. After design verification of the future iMTECH algorithms, critical component specifications and testing scenarios were developed for hardware and software components of the system. Initial proof-of-concept for first versions of iMTECH software and robot were successfully demonstrated in an artificial lung airway model. Engineers and doctors have tried the system's accuracy using a joystick and the automatic control mode, with and without breathing motion simulation. Registration was performed based on one external market placed on the top of the lung airway model. The doctors reached a target at the periphery of the lung in 8.1 ± 0.5 s in the manual mode vs. 15.5 ± 1 s in the automatic mode. Once the EM sensor has reached the airway target, the optical guidance component was tested by touching the EM sensors with the external optically guided rigid tool. We have developed a prototype of the rEONB system which could be more affordable, smaller and easier to manipulate in the operating room than existing medical robots (i.e. Da Vinci, Monarch, CorePath, etc.) and, as a unique feature, assists both diagnosis and surgical removal of malignant lesions. From a clinical perspective, the new robotic system will allow a more precise guidance of the biopsy catheters to targets located in the peripheral airways, will reduce injury, and improve the clinical outcomes in early cancer diagnosis and treatment. Future studies will include animal testing for a more accurate simulation of human lung tissue. Design of a novel dexterous continuum manipulator for medical application using compliant rolling-contact joint  Increasing progress made in continuum manipulator has improved minimal invasive surgery (MIS) continually and enabled many new surgical procedures, which benefits both the patients and physicians. MIS of numerous disciplines, ranging from abdominal to otolaryngologic surgery make use of continuum manipulator. Yet these surgical applications differ in accessibility, size of surgical cave and tissue property significantly. In deep and narrow regions of human body, the application of continuum manipulator remains challenging. Therefore, it is vital to have manipulator that is scalable to a small size and has a large range of motion so that physicians can perform dexterous manipulations. In our work, we developed a continuum manipulator based on the compliant rolling-contact joint (CRCJ) [1] , which has a substantially higher range of motion compared to conventional continuum manipulators. The structure of the proposed manipulator is illustrated in Fig. 1 . The tendon driven manipulator is comprised of three parts, forceps on the tip, a two-section flexible continuum body and an actuator unit. The Fig. 1 Prototype of the rEONB system for catheter manipulation. All parts of the EndoRo robot are 3D printed from medical grade polyetherimide (ULTEM) using STRATASYS Fortus 400mc, including the robot and the bronchoscopy phantom The CRCJ is composed of two half-cylinder cams which are joined together by four thin flexure straps in the alternating sequence. These straps allow rolling contact between the cams and avoid sliding. Unlike revolute joints, CRCJ has a moving axis of rotation, which is always coincided with the contact point between the cams. Compared to conventional flexure hinge without rolling contact (e.g. leaftype), CRCJ can achieve wider range of motion since the rollingcontact mechanism causes much less stress and strain energy by deformation. More importantly, by applying CRCJ in our tendondriven manipulator, the sum of the length of the agonist and antagonist tendon of each CRCJ remains the same during actuation, as shown in Fig. 1 . Consequently, the entire tendon length remains unchanged and there is no play in the tendon during actuation and the torque generated form the actuation unit can be transmitted to the tip [2] . Besides we also designed a handle to actuate the manipulator. All parts are designed using our inhouse modelling toolbox written in MATLAB called SG Library. Since the CRCJ-manipulator is not limited to one specific application, all structural parameters are parameterized so that the continuum manipulator and its hinge configuration can be scaled and adjusted easily. We performed a non-linear Finite Element Analysis (FEA) to numerically analyse the internal stress distribution. Tendon actuation forces are modelled as a force couple resulting in a torque on the tip of the manipulator. Normal forces acting on the spacing disks are also considered. Face-to-Face penalty contact formulation is used between the lateral surface of the cam and the flexure straps to model the contact-rolling mechanism. The definition of boundary conditions, meshing and FEA calculation are carried out with SG-Library and open-source softwares (CalculiX and Cubit). A prototype with the above-mentioned structure was implemented. Our protype consists of only 4 parts (handle included), which are fabricated using selective laser sintering with Formiga P100 with polyamide PA2200 (EOS, Germany), approved as biocompatible according to EN ISO 10993. Without complicated assembly, using CRCJ flexure hinges simplifies the sterilization process considerably. In order to evaluate the dexterity of our novel manipulator using CRCJ, the maximal range of motion is analysed and compared with a previously developed manipulator with leaf-type hinge. As shown in Fig. 2 , the CRCJ manipulator can easily reach a bending angle greater than 120°whereas the previous model has a maximum bending angle at about 100°. Furthermore, actuation force required to achieve the same bending angle is remarkably lower with the CRCJ-manipulator, which is also reflected in the FEA simulation. A good agreement with respect to deformation can be shown between the FEA simulation and our experiment. From the FEA simulation it can be noticed that mechanical stress is concentrated in the flexure straps of the CRCJ. With the same deformation, the von Mises stress in the CRCJ-manipulator is lower than in the previous model by a factor of 2, which indicates that the CRCJ-manipulator has better fatigue resistance. We proposed a novel continuum manipulator for medical application using CRCJ, which allows better dexterity of the manipulator including wider range of motion, smaller bending radius and better transmission of actuation force. These will benefit physicians with a greater flexibility and accessibility in MIS. By using a parametrized part design with SG-Library and rapid prototyping, the manipulator can be easily scaled, reconfigured and fabricated to fulfil different surgical applications. Through experiments and FEA simulation our prototype showed promising results in mechanical properties. We are currently working on the integration of our CRCJ-manipulator into surgical instruments, e.g. manipulator for single port laparoscopy procedure, to further explore the potential of the proposed mechanism in MIS environment. Our future work also includes fatigue test and fabrication of the manipulator with metallic material. For first-aided surgery, tracheal intubation is the first step to keep patients breathing. In the tracheal intubation process, surgeons need to open patient's mouth and put the laryngoscope inside. Then surgeons use the laryngoscope to depress the tongue and expose patient's glottis so that the catheter can be inserted into the tracheal. The existing robotic surgery [1] systems exist the following problems: (1) It is difficult for equipment like manipulator to be transported to emergency scene; (2) Rigid structure of manipulator is easy to cause secondary injuries. To address these problems, a kind of flexible tracheal intubation equipment in order to carry out first-aided tracheal intubation surgery is designed. The framework of the tracheal intubation instrument is shown in Fig. 1 . The tracheal intubation instrument consists of tongue depressor, steering mechanism and feeding mechanism, which is based on the silica gel pneumatic soft actuators, which is shown in Fig. 2 . In the design and implementation of the tongue depressor, the steering mechanism and the feeding mechanism, the silica gel soft actuators are applied to provide the necessary driving force of these movements in the process of intubation surgery. The tongue depressor is used to depress the tongue and expose the entrance of glottis so that the catheter can be inserted into the tracheal. Two silica gel soft actuators are used in the tongue depressor. The soft actuator will elongate axially to the upper jaw thus the depressor plate will rotate with the spin axis and depress the tongue, causing the glottis exposed. On the tongue depressor there is steering mechanism, which is applied to adjust the direction of the catheter so that it can be aimed at the glottis accurately. There are two silica gel airbags in steering mechanism. If the air is pumped into the right airbag, the airbag inflates and push the catheter to the left and vice versa. The feeding mechanism is used to insert the catheter into patient's tracheal after the tongue is depressed and the glottis is exposed. And theoretical modelling and finite element simulation are also researched. The theoretical model of the silica gel actuator is established based on the strain energy density function and Yeoh model. The calculation and simulation of silica gel pneumatic soft actuator is conducted by computer using ABAQUS simulation software to verify whether the actuators meet the design targets. According to the surgeons, the driving force required for tongue depression is 15 N and the actuator needs to realize the elongation of 15 mm. The finite element simulation results show that at the pressure of 0.06 MPa, elongation of the actuator is 19.2 mm with the driving force of 18 N, which meets the requirements of the intubation surgery. The maximum stress of the actuator is 3.83 kPa and the maximum elongation ratio of the actuator is 107%, which is less than the limit of the silica gel material (7 MPa and 250%). Therefore, the safety of actuator is verified. The results of finite element simulation are listed in Table 1 . In conclusion, a kind of flexible tracheal intubation instrument including feeding mechanism, tongue depressor and steering mechanism, is proposed to address the problems of traditional surgical robot with rigid structure existing when operating the intubation telesurgery. Silica gel pneumatic soft actuators are applied in the instrument. The theoretical model and the finite element simulation are also researched. The results show that the flexible tracheal intubation instrument meets the requirements of tracheal intubation telesurgery at emergency scenes. The low-cost materials and the onetime use design of the soft silica gel actuator help to reduce costs and avoid the difficulty of instrument disinfection at emergency scenes. Purpose Over the last decades, minimally invasive surgery (MIS) has shown its great advantage in improving the efficiency of surgery and reducing trauma of patients. Since its inception, numerous research efforts have been carried out to control the forceps force of MIS for safe manipulation of the vulnerable tissues and organs. Compared to the conventional rigid-joint-based forceps, compliant forceps with elastic and flexible mechanical structures can provide more haptic feedback during grasping movement, hence they are widely used for controlling and optimizing grasping forces [1] . In our institute, we have developed a compliant MIS forceps with adaptive gripping jaws, which can safely grasp the soft tissues without squeezing them completely. The proposed compliant forceps was automatically synthesized by using topology optimization techniques. The entire automatic synthesis process of the adaptive compliant forceps was implemented in our automated design platform in Matlab, the Solid Geometry (SG) Library [2] . The first step of the topologyoptimization-based synthesis was to define the design problem, consisting of a 2D initial design domain and the corresponding boundary conditions (see the schematic diagram in Fig. 1 ). Due to the symmetry of forceps, only half of the design domain was considered in the synthesis process. As can be seen in Fig. 1 , the left boundary of the design domain was set to be fixed. A dragging force F in was applied in the design problem to actuate the forceps. An elastic spring k tissue on the middle of the gripping jaw was used to imitate the soft tissues. The objective of the optimization-based synthesis process was to maximize the vertical displacement u out of the forceps tip while the spring k tissue was slightly deformed. In the second step, the topology optimization process, the initial design domain was meshed into small triangular elements by using finite element method. The contribution of the density re of each element to u out was determined by qu out /qq e . The re was then iteratively modified according to qu out /qq e to maximize u out . The modification process was illustrated by the 4 evolution plots in Fig. 1 and converged at the 75thiteration. The black and white area in the plots represented the solid (q e = 1) and void (q e = 0) domain, respectively. In the last step of the synthesis, we employed the iso-contour method to extract the boundary of the solid domain from the last evolution plot. The extracted boundary was then symmetrically duplicated and extruded into a 3D forceps by using our SG Library, as is shown in Fig. 1 . The synthesized adaptive compliant forceps was fabricated with selective laser sintering technology by using the material polyamide (PA2200). To evaluate its adaptive grasping functionality in a MIS setup, we have conducted an endoscopic experiment in a training box (see Fig. 2 ). In the experiment, an artificial artery printed with the stereolithography printer Form2 was used to imitate the material properties of the soft tissues. The synthesized forceps was integrated into a commercially available laparoscopic manipulator (Karl Storz 33500 M) to perform grasping movement. To show the effect of stiff grasping for comparison, a conventional MIS forceps based on rigid joints (Karl Storz 33310 MN) was also used, in which the forceps jaw was rigid and not deformable with the soft tissues. The results of the endoscopic experiment are presented in Fig. 2 . As is shown in the figure, the proposed adaptive compliant forceps could successfully grasp the artificial artery with deformed forceps jaws while the conventional forceps squeezed the artificial artery completely. The adaptive grasping functionality of the synthesized forceps was thus demonstrated. In our work, we proposed a novel adaptive compliant MIS forceps which was automatically synthesized by using topology optimization techniques. The proposed forceps was 3D printed with the material polyamide, and could safely grasp the vulnerable soft tissues without squeezing them completely. The developed design framework is very versatile and greatly simplifies the synthesis process of task-specific compliant forceps. In our future work, we plan to use other materials, such as titan and nitinol, to fabricate the proposed forceps and deeply explore the potential of our automatic synthesis method in MIS applications. Purpose In order to avoid inaccuracies and thus repetition of biopsies in minimally invasive interventions, new methods for the targeted assistance of the operating doctors must be researched. The procedure should also be performed more gentle for the patient. By integrating a tissue-differentiating biopsy needle [1] with a navigated KUKA LBR iiwa in combination with a multi-camera tracking and an Artis zeego system, achieving a more precise and efficient treatment of oligometastatic patients and supporting health professionals seems possible and to our best knowledge does not exist yet. Also, the usage of the above-mentioned needle is aimed at the problem of tissue being pushed in front of the needle when it is fed in the direction of feed. In addition, this enables a standardised process for tissue removal. For integrating the various systems used for the robot assisted biopsy, we used a microservice architecture with a Machine-to-Machine (M2M) messaging functionality and a document based database for data storage. A basic overview of the architecture is displayed in Fig. 1 . Different microservices like a coordinate transformation service or a speech recognition service are connected to a Message Broker and can publish or respectively subscribe to relevant topics. This enables a very modular environment where more components and systems can easily be integrated. For low-latency and highbandwidth requirements, the services can request data directly from other services via direct RPC connections. At last, persistent data is stored in a document based database, which can handle big data sets, like DICOM images or for example, information about the planed needles with their location, name or when it was planned. This enables the creation of a digital twin of the intervention, because all relevant actions are protocolled and can be reviewed or analysed afterwards. The process to plan and place a biopsy needle in collaboration with the assistance system is as follows. At first the health professionals plan a needle trajectory on the registered multi modal dataset. This includes data from the intra-operative DynaCT, performed for example with an Artis zeego system, and the pre-operative scans from CT or MRT systems. The planned trajectories are stored simultaneously into the document based database. When all needle trajectories are planned, the physician can perform the needle placement in collaboration with the KUKA LBR iiwa robot. To perform the placement procedure, the light weight robot can be controlled by voice commands. This allows a sterile and interactive handling of the robot. The voice control is activated by a trigger word, followed by a command word. Thus a valid command could be ''Robot start''. We use deeplearning methods for this speech to text analysis and work on a biometric based speaker recognition for a better authentication based on LSTM networks. Due to a multi-camera tracking system, the planned trajectory is updated during the placement procedure in case a patient movement occurs or is necessary. This is possible because of the calibrated image, tracking and robot coordinate systems. The transformations between the coordinate systems are stored inside the document based database as well. When the tissue is punctured by the biopsy needle, a tissue differentiating biopsy needle [1] sends a signal in the event of a detected transition from liver parenchyma to lesions. Then the needle progression is immediately stopped. Additionally, this is displayed by a virtual needle displayed on the cross sectional views as is visualized in Fig. 2 on the top left corner by a magenta needle. Also a signal indicates if the needle has detected a transition. In case no transition is detected the robot stops the feed at the planned needle tip spot. Complete needle placement procedures can be performed on a phantom in collaboration with the assistance system. To control the robot, voice commands are used. Additionally, a prototype of the tissue differentiating needle of [1] was integrated into the robot end effector. Finally, the needle feed is stopped when a tissue transition is detected. A main accomplishment of the presented work is the integration of all components like the Artis zeego, the KUKA LBR iiwa, an app for needle positioning, a multi-camera tracking system and a speech recognition system into a working prototype for robot assisted needle placement. In conjunction with a virtual needle in the cross sectional views and the intelligent biopsy needle the developed prototype has the potential to reduce the repetition rate of biopsies by providing more information about the actual position of the needle inside the tissue. This enables a standardized process for tissue extraction. Due to the developed microservice architecture, the system is easily expandable with new components, algorithms or systems. For example, modern deep-learning systems can plan needle trajectories on pre-segmented datasets in the developed app or the patient surface can be tracked with modern deep-learning methods based on camera images to adjust for patient breathing. Purpose Robotic systems provide high precision and repeatability of nontrivial tasks during surgery. Especially image-guided interventions that rely on accurate tool positioning and often involve sophisticated workflows can draw benefits from robotic approaches. However, high costs and the difficulties of integrating such systems into the operating room (OR) narrow down the number of success stories like that of the DaVinci (Intuitive Surgical, USA). Furthermore, current systems are often limited to a single use case, enhancing these problems even more. The new IEEE 11073 standard for service-oriented device connectivity (SDC) that was developed in the OR.NET project [1] may help to ease the complexity of robotic integration and extend the range of clinical use cases. This work, therefore, presents the current progress of the development of an easily integratable robotic dual-arm system (DAS), utilizing two KUKA robot arms for the application in multiple image-guided intervention scenarios. The combination of two robots shall, at that, provide high flexibility for different tasks and demonstrate the possibility to employ systems with high complexity. Accuracy evaluations and functionality examples are presented to show the feasibility of the system's use in the clinic. The DAS consists of two collaborative KUKA LBR iiwa 7 R800 robots (KUKA AG, Germany) and their respective cabinet PCs, each mounted on a mobile platform respectively (Fig. 1) . A standard Ethernet switch is used to connect both robots via network to a third central PC that maintains the functionality of both arms as a single unit. The internal harmonization of the separate robots was realized by adjusting the iiwa_stack application [2] to incorporate multiple KUKA arms and allow for information exchange via ROS (Robot Operating System) nodes. Thereby, the robots can be simultaneously included in one planning scene using the MoveIt framework. To allow the system's OR-integration, its functionalities and state information (i.e. interaction commands like button presses or touch-gestures, current positions, etc.) were modeled and implemented as a service provider inside the IEEE 11073 SDC standard and can, therefore, be accessed via network by any SDC conform service consumer. This allows the system to be used in different scenarios by implementing use-case-specific consumer applications that can operate the DAS as needed. An overview of the network communication is shown in Fig. 2 . To take safety measures into account and avoid collisions between the robots and other obstacles, a Polaris Vicra Tracking camera (NDI Inc., Canada) is used to observe both robots via optical markers that are rigidly attached to the platforms. An optical reference tool was built to co-register the separate systems to each other via landmark registration. Since reacting to possible collisions is still of high importance, all real-time dependent situations are not handled via SDC but internally on the robot cabinets. Use Case Scenarios To show the system's usability in different scenarios, two use case workflows were implemented as SDC consumer applications. Firstly, an approach for ultrasound-guided biopsies that utilizes a single arm to position an ultrasound (US) device with a needle-guide was implemented to test for elementary functions like hand-guiding and touchgestures. Secondly, a program that utilizes both arms and allows for the application of focused ultrasound (FUS) under ultrasound imageguidance provides the possibility to test both arms in simultaneous action. Both programs are executed with an augmented-reality-based tablet application that is used to plan tool trajectories. Due to content limitations, this work focuses on the second use case in which one robot steers a US image probe to provide the target information from the US image for the focus of the FUS probe that is attached to the second robot. Incorporating collaborative functionality, both robots can be steered with hand-guidance and touch-gestures, however, autonomous movement for pre-positioning and targeting is included as well. Since in the FUS scenario one robot is providing positional information to the other, the accuracy of three different co-registration approaches for the interdependent positioning of the robots was measured. Landmark registrations using 3 and 5 fiducials, as well as an optimized 3-fiducial method that applies a fitness function were examined. By attaching the reference tool the first robot was moved two times to 10 different positions, which were recorded via the NDI camera. The second robot steered the reference tool to the interdependent positions, by processing them with the registration transformation for each approach. These positions were recorded with the tracking camera and the Euclidean distance between the codependent points was measured. Results Table 1 shows the positional accuracies measured for each registration approach. It can be seen that the position error of the 3-fiducial approach came out last with a maximum error of 4.17 mm. The 5-fiducial method produced a maximum error of 3.05 mm and the optimized 3-fiducial method resulted in a maximum error of 2.58 mm. The presented work shows the feasibility of a dual-arm robotic system to be integrated into the clinic via the IEEE 10073 SDC standard. Two different image-guided intervention scenarios were presented, to provide a proof of concept for the system's flexibility to be used in multiple use cases. Examining the conducted accuracy measurements, it was shown that the system can also be utilized in highly complex workflows, such as interdependent tool positioning for the application of FUS. The presented maximum error of 2.58 mm of the optimized registration is sufficient for FUS treatments of targets with a 3 mm size or more. This number may be further optimized since the used NDI camera does not provide the optimal tracking space for the presented reference approaches. This work is a preliminary study and, therefore, cannot make any propositions on user behavior. A study with a variety of users with different expertise is underway. Keywords Lattice Boltzmann, Nasal breathing problem, Nasal valve, Optimization Nasal septum deviation or malformation may be a cause for breathing problems. The nasal valve (NV) contributes most to air flow resistance, being the narrowest portion of the airway passage posterior to the nasal vestibule at the nostrils. Due to mass and momentum conservation laws this reduced air flow cross-section (AFCS) induces a significant pressure drop. Surgical interventions to reduce AFCS are not always successful and so new patient-specific planning tools to Int J CARS (2020) 15 (Suppl 1):S1-S214 S105 support preoperative surgeon's decisions are currently in development. We present laser Doppler anemometry (LDA) validated lattice Boltzmann (LB) simulations [1] to calculate the nasal airflow with a constant flow rate of 0.6 L per second as measured by rhinomanometry. Preoperative CT datasets (0.3 3 mm 3 voxel size) of 5 patients with AFCS reduction were automatically segmented [2] to show the air-tissue boundary used for the CFD simulation. Flow inlet boundary conditions are defined by a sphere around the nasal tip. Outflow boundary conditions are defined by a cuboid. A surgery criterion [2] , developed in a study with five patients with nasal septum deviation and five without breathing problems, allows finding surgically relevant points (SRP). Via a graphical user interface (GUI) the surgeon can confirm, add or remove SRPs. Optimization increases the AFCS on SRPs iteratively, fluid flow and SRPs are recalculated and used for the following optimization step. Iteration terminates when no SRPs are found any more. SRPs and the determined resection volume(s) are shown in the CT dataset to support surgeon's decisions. The presented work is a first explanatory study to investigate whether this optimization process is practical and useful to aid surgeons in improving geometrical problems at the nasal valve. The nasal airflow LB simulations were validated with LDA in a previous study [1] . Therefore, A CT dataset of a patient with nasal septum deviation was air segmented to perform the LB simulation. A phantom was 3D printed to perform the LDA measurements of the same flow. Results showed that the maximum velocity difference between LDA and LB is smaller than 15%. LB simulations of the airflow through the nasal cavity were performed with Sailfish CFD, a free GPU (graphical processing unit) python based framework to solve for the velocity vector and pressure fields, respectively, with the given anatomic fluid boundary conditions. The numerical simulations were initialized with zero velocity. On the outlet Sailfish CFD the Dirichlet velocity boundary condition is used to set the fluid flow to 0.6 l/s (from rhinomanometry). At the voxels of the inlet surface the Sailfish CFD boundary condition was set to ambient pressure. The simulation was stopped after an inhalation time of 0.0255 s, when the flow kinetic energy became stationary. The calculated flow Reynolds number (Re) was * 5000. As Re is ) then the critical Re for pipe flow turbulent flow phenomena will appear. Limited computational resources did not allow LB simulations to not resolve turbulent structures temporally and spatially and so the LES turbulence model with default coefficient cs = 0.14 was used. In order to find surgically relevant points (SRP) a streamline criterion with 1 Pa/mm [2] was used. Along streamlines, the pressure p was extracted. A pressure drop indicated a constriction causing pathologic breathing. Streamline calculations needed the following parameters: the number of streamlines, 532, was determined by a sensitivity analysis [2] . The termination criterion for a maximum streamline length was set to 30 cm, greater than the size of the nasal cavity, which is usually 10-13 cm. Iteratively, points on the streamlines with pressure drop [ 1 Pa/mm were marked as SRPs and used for optimization. In Fig. 1 the calculated resection volume (in magenta) on coronal slices at the nasal valve are depicted. The resection volume in yellow is not of interest for surgery. The resection volume is based on the LB fluid flow simulations. In all five patients the surgery criterion allowed detecting air flow problems at the nasal valve. The optimization takes place mainly on the lower half of the nasal valve. However, there are suggestions of resections which are not of interest (NOI) for surgery: at all five patients there is resection volume determined outside of the nasal cavity. This is a side effect from the optimization method [2] , when the optimization region overlaps the boundaries of the human face. Furthermore, in patient 3, 4 and 5 the septum would be reduced partially, which is not possible anatomically. All results are visualized with 3D Slicer. The suggested corrections of the nasal valve reduce the pressure drop between nostril and nasopharynx at by 63%, 91%, 62%, 79%, and 82% for patients 1-5. Patient 2 had the most profound chronic nasal obstruction. In contrast to rhinomanometry, LB fluid flow simulations based on CT datasets can provide valid information for surgical planning. LB simulations provide local regions with high pressure drop that can be used for optimization. By an increase of AFCS the pressure drop is reduced. Specifically, for all patients with a problem at the nasal valve, LB fluid mechanic simulations could detect the pathological conditions and suggest an optimized AFCS. Keywords Motion control Effectiveness Kinematic and dynamic Robotic-assisted surgeries Purpose Robotic-assisted minimally invasive surgery (RAMIS) systems become more and more popular among both surgeons and patients. The related technologies are attracting great attention from many researchers all over the world. Currently, many prototypes and practical products have been successfully developed and achieved great success in clinical practice, RAMIS systems can be divided into two major groups: specialized and versatile systems. The former applies to specific applications or diseases, such as endoscopic minimally invasive surgery [1] or a single surgical task. On the other hand, the versatile surgical systems assist surgeons in a wide range of applications. However, the safety and efficiency of physical humanrobot interaction need to be further improved because of the lack of features of collaborative robots. Method Fig. 1 Coronal views of the nasal valve of the patient CT dataset showing the calculated resection volumes (magenta). The resection volume in yellow is not of interest for surgery S106 Int J CARS (2020) 15 (Suppl 1):S1-S214 A prototype was developed using integrated joints with force perception, as shown in Fig. 1 , similar to the most surgical system, the MIS system is a master-slave robot, it contains four manipulators as slave, two omega7 devices as master. An unified model-based control framework was proposed. The kinematics and dynamics were modeled for the MIS robot which has a series parallelogram mechanism with passive joints. The dynamic model was linearly parameterized and the procedure of identification has been applied based on least squares regression method and inverse dynamic model for more accurate dynamic parameters. Several experiments and simulations involving kinematics and dynamics were performed. A control framework combined compliant control and teleoperation is proposed. The relative errors of the measured and predicted torques of the first two joints are less than 8% and 20% for the third joint integrated with steel belt transmission. The results show the promise for the clinical application. In order to improve the safety and efficiency of physical human-robot interaction for surgical operation, A MIS prototype integrated joint torque sensing has been developed. The mechanical design significantly reduced accelerated masses to 8 kg through adding special steel strip transmission structure. The kinematics and dynamics of the MIS robot are analyzed in detailed for the mode-based control algorithm. For more accurate dynamic parameters, the dynamic model was linearly parameterized and the procedure of identification has been applied based on least squares regression method and inverse dynamic model. Finally, A unified model-based control framework is proposed through combination teleoperation and compliant control. Fig. 1 Overview of the surgical robot platform Int J CARS (2020) 15 (Suppl 1):S1-S214 S107 Chairman: Hiroyuki Yoshida, PhD (US) S108 Int J CARS (2020) 15 (Suppl 1):S1-S214 Purpose Autism spectrum disorder (ASD) is a neurodevelopmental disorder beginning in childhood that results in challenges with social skills, communication, and repetitive and restricted patterns of behavior [1] . ASD affects approximately 1 in 66 children in North America, with boys being affected four times more frequently than girls. Currently, diagnosis is made primarily based on clinical features and no robust biomarker for ASD diagnosis has been identified. Potential imagebased biomarkers to aid ASD diagnosis may include structural and functional properties of deep gray matter structures in the brain. For example, the corpus striatum, which is composed of the caudate nucleus, putamen, and globus pallidus, is known to play a major role in cognition and motor and action planning, while other deep gray matter structures such as the amygdala and the accumbens play a major role in cognitive processing and emotions, all of which can be altered in children with ASD. Given the cognitive challenges observed in ASD, we hypothesized that these brain regions will demonstrate structural and functional alterations in children with ASD. Micro-structural tissue properties can be measured using diffusion-weighted MRI (DWI) and often exhibit alterations before macrostructural changes, as assessed by regional volume measurements, become clinically evident. Therefore, the aim of this work was to investigate if children with ASD show micro-and macrostructural alterations in deep gray matter structures compared to neurotypical children and if these features can be used for an automatic classification. Magnetic resonance imaging (MRI) data was obtained from 23 boys with ASD ages 0.8-19.6 years (mean 7.6 years) and 39 neurotypical boys ages 0.3-17.75 years (mean 7.6 years). Among others, DWI with an in-plane resolution of 0.94 9 0.94 mm 2 and slice thickness of 4.0 mm and two diffusion weightings of b = 0 s/mm 2 and b = 1000 s/mm 2 was acquired for all study participants. The DWI datasets with and without diffusion weighting were used to calculate the quantitative apparent diffusion coefficient (ADC) map for each child employing the Stejskal-Tanner equation. After this, non-linear registration was used to register the MNI brain atlas to each DWI dataset, which was subsequently used to align the Harvard-Oxford subcortical atlas brain regions defined in MNI space to each DWI dataset. The registered atlas brain regions were then used for volumetric analysis as well as extraction of median ADC values for each subject within the whole cerebral cortex (gray matter), as well as for the cerebral hippocampus, thalamus, caudate, putamen, globus pallidus, amygdala, and nucleus accumbens. The atlas-based analysis method used in this work is described in more detail in [2] . The extracted quantitative regional volumetric and median ADC values were then used for the development and evaluation of an automatic classification method using an artificial neural network. The neural network used in this work consisted of six layers: an input layer integrating all regional volumetric and ADC measurements as well as age, two pairs of a hidden layer with 20 neurons followed by a dropout layer with a 50% chance of setting inputs to zero, and one output layer. The dropout layers were used to avoid overfitting. The network weights were computed with the ReLU activation function. The Adam optimizer and the binary cross-entropy as loss function were used for parametrization of the model. The training was computed in 10 epochs. The classification model was evaluated using a 10-fold cross validation resulting in an overall accuracy of 76%, which is considerably better than chance level (62%). More precisely, 33 neurotypical boys were correctly classified, whereas 6 neurotypical boys were falsely classified as ASD. For the ASD group, 14 boys were correctly classified by the proposed method while 9 boys were incorrectly classified as neurotypical. This translates to a precision of 70% for the children with ASD and 79% for neurotypical boys. The corresponding average f-score for this classification problem was 0.754 with a ROC AUC of 0.703. Univariate statistical tests corrected for multiple testing revealed significant differences in ADC of the cerebral cortex, putamen, pallidum, amygdala, and accumbens. No significant volumetric differences were found in any structure except for the brainstem. To the best of our knowledge, this is the first method to classify children with ASD using micro-and macrostructural tissue properties of deep gray matter structures in the brain determined using commonly available DWI datasets. The first results are promising and considerably better than chance level. The finding that ADC is significantly altered within the cerebral cortex, putamen, pallidum, amygdala, and accumbens correlates well with the clinically significant differences in cognitive and emotional differences noted in children with ASD. The sensitivity of the proposed method to identify children with ASD is not yet suitable for a clinical application. However, there are multiple interesting avenues to improve the classification accuracy and sensitivity. First, more datasets of children with ASD should be added. Second, additional image-based biomarkers such as the cerebral blood flow could be integrated into the classification model. Third, convolutional neural networks might lead to better results but also require a much larger database. In conclusion, the first results of the proposed machine learning method to identify children with ASD using image-based biomarkers are promising but need to be improved for a realistic clinical application. strategy. However, genetic testing is invasive, expensive, and timeconsuming. Therefore, it is difficult to repeat a genetic test. On the other hand, diagnostic imaging is noninvasive and can be repeated several times. Hence, the estimation of the gene pattern of low-grade glioma from diagnostic imaging has great value or significance. In this study, we proposed a method for classifying brain tumors using gene expression level (mRNA) obtained using biopsy and another method for classifying brain tumors using radiomic features obtained from magnetic resonance imaging (MRI). We collected 114 mRNA and 114 fluid-attenuated inversion recovery (FLAIR) MRI images from the TCGA-LGG dataset of public The Cancer Imaging Archive database. First, hierarchical clustering with mRNA was performed for classifying the gene pattern of brain tumors. The brain tumors were classified into 3 gene patterns, and they were defined as ''truth'' when we estimated the gene patterns of low-grade gliomas using FLAIR images. Second, we selected a slice with the largest tumor diameter on the FLAIR image, and manually segmented the tumor region. We determined 38 radiomic features such as size, shape, contrast, etc. from the segmented tumor region. We estimated three gene patterns of the tumors using the radiomic features. We used quadratic discriminant analysis (QDA) or classification and regression tree (CART) algorism as the classifier. Results Figure 1 shows the result of clustering with mRNA. Multi-dimensional scaling (MDS) was used for the reducing the dimensions from 114 to 2. As shown in the figure, tumors with pattern A demonstrated a different gene expression from that of the other tumors. The overall accuracy of estimating three genetic patterns was 70.1% (80/114) when QDA was used as a classifier. Pattern A was accurately detected in 10/10 cases, pattern B was detected in 42/54 cases, and pattern C was detected in 28/50 cases. The overall accuracy was 72.8% (83/ 114) for the CART algorithm. Patterns A, B, and C were detected in 9/10, 31/54, and 43/50 cases. It should be noted that almost all pattern A tumors, which have different gene expression from others, were accurately detected using imaging features. The CART algorithm yielded better performance. It is important for radiologists to know the relation between the tumor's phenotype and genotype. The CART algorithm can automatically generate a diagnostic tree of imaging features in order to estimate the gene pattern of the tumor. Therefore, we can visually understand the relationship between the imaging features and the rules for classifying the gene pattern of the tumor. Thus, we concluded that the CART algorithm was better than QDA from this perspective. We proposed a method for estimating gene patterns of tumors using noninvasive imaging. Surgery reservations at university hospitals are commonly booked full for a few months ahead. Therefore, our method could be useful for selecting patients with high-grade cancer at the preoperative stage. The purpose of this study is to validate the lesion classification performances of a deep convolutional neural network (CNN) pre-trained by local anatomical structures. The deep transfer learning has recently received increasing attention from medical image analysis researchers and has been successfully applied to many domains. Generally, a deep neural network trained by a general image dataset, such as CIFERdataset, is used as the pre-trained model. However, a significant difference between a pre-training domain and a target domain often causes an insufficient transfer. It is thought that the difference between general images and medical images is worth considering. In this study, a deep CNN pre-trained by a medical image dataset is transferred to a computer-aided detection task. The source domain is a patch dataset of anatomical landmark (LM) appearances on CT. The LM is a unique local structure having anatomical meanings and plays a crucial role in analyzing medical images [1] . The LM appearance data can be collected from both malignant case data and normal case data obtained from such source as group examination. In this pilot study, the pre-trained deep CNN is not fine-tuned by the lesion dataset (target domain) but is used as an image feature extractor for lesion classification. Two kinds of pre-trained deep CNNs are applied to classify aneurysm patches extracted from head MRA volumes. The deep CNN used in this study has VGG like structure, including thirteen convolutions, five max-pooling, and two full-connections, shown in Table 1 . The input is three channelled image patch that size is 32 9 32 pixels. The batch normalization and the dropout are also inserted in the CNN layers appropriately. Pre-trained CNN is not finetuned to the aneurysms classification. Instead, the output of the first full-connection layer, which consists of 512 units, is extracted as the image feature set. Since there is a possibility that the feature set includes unnecessary features for the aneurysms classification, the classifier is trained by the AdaBoost algorithm, which has the feature selection function. The classifier ensemble consists of several decision stumps. Furthermore, its classification performance is evaluated while changing the number of decision stumps within the classifier ensemble from five to fourty. Fig. 1 The result of clustering with mRNA and the result of estimating three genetic patterns by using MR image S110 Int J CARS (2020) 15 (Suppl 1):S1-S214 The two kinds of pre-trained deep CNN are as follows: (1) LM-CNN: It is a small revision of the previous CNN proposed in [1] and is trained by the 2.5-dimensional (2.5D) LM patch dataset that includes twenty kinds of LM data and not-LM class data (21 classes in total). The dataset is extracted from 80 CT cases and consists of about 1600 LM related patches and about 24,000 not-LM patches. The isotropic resolution of every patch data is 2 mm. (2) CIFER-CNN: It is trained by the CIFER-10 dataset, including ten classed RGB color images. In the pre-training, 5000 images per class are used. In the experiments, these classier ensembles learn and classify the 2.5D vessel patches extracted from 150 cases of head MRA data scanned at the University of Tokyo Hospital. Every MRA data includes one or more cerebral aneurysms with the lesion area annotations by experienced radiologists. Before extracting vessel patches, the scaling process of MRA data to 0.469 mm isotropic resolution and the vessel region extraction [2] from the scaled MRA are performed sequentially. An original 3-dimensional (3D) vessel patch is 32 9 32 9 32 voxel cube and is sampled from the vessel regions at sixteen voxel intervals. The 2.5D vessel patch is obtained as the axial, coronal, and sagittal maximum intensity projection images of the original patch. One hundred cases of MRA data and the rest are used to train the classifier ensemble and to evaluate the classification performance, respectively. Two classifier ensembles, one of that used LM-CNN features and the other used CIFER-CNN features, were evaluated by ROC analysis. The relationship between the area under the ROC curve (AUC) and the number of weak classifiers (decision stumps) is shown in Fig. 1 . While the number of weak classifiers is small, the ensembles using the LM-CNN feature set had better accuracy than those using the CIFER-CNN feature set. It suggests that the LM-CNN features included a few features that are particularly useful in the aneurysm classification. The result says that the LM classification task and the aneurysm classification task seems to be close to each other. Also, the AUC maximums of these ensembles were approximately equal. Since the CIFER-CNN was pre-trained by the big dataset, CIFER-CNN features also contributed to the accurate aneurysm classification. To a simple comparison of two CNN features, the shapes of LM-CNN input was set as the same shape of CIFER data; three-channelled 32 9 32 images. However, the original LM data and the original aneurysm data are 3D image data. The data converted into three channelled 2.5D data results in a loss of the 3D appearance information. The addition of oblique sections to the input 2.5D data will bring an improvement of the accuracy. The fine-tuning of the Int J CARS (2020) 15 (Suppl 1):S1-S214 S111 whole or partial CNN to the lesion classification is also the future work. As a pilot study, the lesion classification performance of the deep CNN pre-trained by the LM patch dataset was verified. The LM-CNN was used as the image feature extractor for the aneurysm classification. By experimental evaluation, it was confirmed that the LM-CNN provided useful features in the aneurysm classification. Breast cancer is the leading cause of cancer death for women. Early detection and diagnosis is the best way for reducing the mortality. In breast cancer detection, ultrasound (US) is widely used for breast examination. The advantages of US are the low-price relative to other scanning tests, the painless, and the availability. The conventional way is using a handheld probe (HHUS) for US scanning. But, the HHUS has some limitations such as difficulty to reproduce, depending on the operator, and hard to capture the entire breast images. To overcome those issues, the automated breast ultrasound (ABUS) had been introduced in whole breast examination. The ABUS is capable of providing a three-dimensional (3-D) reconstructed volume constructed by two-dimensional (2-D) slices with less operatordependence. Generally, the ABUS image is reviewed by the radiologist for further diagnosis. To advance the time efficiency and interpretation performance, the computer-aided detection (CADe) system is proposed to assist the diagnosis. Recently, convolutional neural network (CNN) has widely been used as the basic architecture for developing CADe system. Hence, the CADe system using onestage architecture is proposed in this study. In this study, the proposed breast tumor CADe system is composed of a one-stage 3-D deep learning model for tumor detection and a post-processing method to detect and locate the breast tumor in the ABUS image. The one-stage 3-D deep learning model, 3-D YOLO (You Only Look Once), is applied on the ABUS image to obtain the tumor VOIs and probabilities. For all obtained VOIs, they are regarded as tumor candidates if the tumor probabilities are higher than user-defined threshold T. However, the tumor candidates probably overlap each other and may cause problems of the false positive (FP) and inaccurate tumor position. Therefore, a postprocessing method, non-maximum suppression (NMS), is performed for solving the problems. In this study, the materials are 3-D ABUS images obtained by using the scanning system (Invenia ABUS, GE Healthcare) with the automated 6-14 MHz linear broadband transducer, covering areas of 15.4 9 17 9 5 cm. The dataset is made up of 258 patients, where each patient has 1-4 passes and at least one pathologyproven tumor for evaluating the performance of proposed CADe system. In total, there are 523 pathology-proven tumors including 287 malignant and 236 benign tumors. In system evaluation, the 5-fold cross validation is used to validate the CADe system. In our experiments, the proposed CADe system achieves the sensitivities of 98%, 95%, and 90% with the FPs per pass 3.8, 2.0, and 1.0, respectively. Moreover, the execution time of tumor detection is less than 1 s per pass. In conclusion, the proposed CADe system using one-stage 3-D CNN is much more time-efficient and has better performance than previous works. In this study, a one-stage CADe system based on the 3-D YOLO architecture is established and proposed for ABUS tumor detection. For time efficiency, the whole ABUS image is detected directly by executing our system. The proposed one-stage CADe system is much more time-efficient and possess higher performance than previous works. In future, the overall performance may be further enhanced by introducing other FP reduction method into our system or modifying our detection model architecture. Keywords automated breast ultrasound, convolutional neural network, 3D capsule neural network, 3D U-Net In recent years, breast cancer had become one of the most common cancers diagnosed in female. With the early detection, diagnosis, and treatment, the survival rate of the patient can be increased significantly. There were many ways for breast tumor detection, including magnetic resonance imaging, mammography, and ultrasound imaging. Among all the modalities for breast tumor scan, the automated breast ultrasound (ABUS), a scan that provided the three-dimensional (3-D) volume of the breast which was reconstructed by a series of two-dimensional (2-D) slices, had been widely used for the physician reviewing since it was a low-cost and painless scan for breast tumor. However, even for a well-experienced physician, it was sometimes hard to diagnose the tumor as benign or malignant by simply reviewing the ABUS images. Therefore, it would be helpful to develop a computer-aided diagnosis (CADx) system to provide tumor information for the physician to do a preliminary diagnosis. The S112 Int J CARS (2020) 15 (Suppl 1):S1-S214 CADx system using 3-D convolutional neural network (CNN) is proposed in this study. In this study, the proposed CADx system for tumor diagnosis consists of the VOI extraction, a 3-D CNN tumor segmentation model, and a 3-D vector-based deep learning model tumor classification model to determine tumors as malignant or benign. In the VOI extraction, the volumes of interest (VOIs) are defined by the experienced physicians. Then, the defined VOIs are fed into a 3-D CNN tumor segmentation, 3-D U-net, to obtain tumor masks. After the tumor segmentation, the VOIs and tumor masks are delivered to a 3-D capsule neural network (CapsNet) model expanded from 2-D CapsNet to obtain two representative vectors in each tumor. Finally, the length of the representative vector would be regarded as the probability for determining tumors as malignant or benign. In this study, the materials were collected by InveniaTM automated breast ultrasound system (Invenia ABUS, GE Healthcare, Sunnyvale, CA, USA). Each breast was scanned in the supine position by an automated 6-14 MHz linear broadband transducer, covering areas of 15.4 9 17 9 5 cm. The dataset consists of 396 patients and 446 tumors with the pathology-proven report are used in total. In pathology-proven tumors, there are 229 malignant and 217 benign tumors. In addition, for system evaluation, three performance indices including accuracy, sensitivity, and specificity and the 5-fold cross validation are used to validate our CADx system. In experiments, the accuracy, sensitivity, and specificity of proposed system are 85.20%, 87.34%, 82.95%. In conclusion, the results indicate that the proposed system has potential for discriminating malignant tumors from benign ones. In this study, a CADx system made of the 3-D U-net and the 3-D CapsNet models is proposed for tumor classification in ABUS images. First, the VOIs are defined by the experienced physicians. Then, the masks of VOIs are generated by 3-D U-net model. Finally, both VOIs and masks are fed into the 3-D CapsNet for determining tumor types. The proposed CADx system takes advantage of CNN for medical images. Moreover, the end-to-end CNN has been proven its efficiency and power in natural image classification tasks. The overall performance of this study may be further improved by investigating the application of end-to-end CNN in 3-D ABUS images. Logistic regression to predict malignancy of breast tumors using IVIM parameters One of the most advanced techniques is DW-MRI (diffusionweighted MRI), a particular MRI sequence that does not require any type of contrast agent, thus resulting completely non-invasive and without contraindications. DW-MRI is based on the observation of the spin of water's molecules, after and before the application of a magnetic gradient, depending basically on the molecules'' diffusion. The water molecules, naturally present in organs, act as a contrast agent. The goal of this work is to predict the malignancy of a lesion from the analysis of DW-MRI in a retrospective study. The DW-MRI sequence is used to compute the intravoxel incoherent motion (IVIM) parameters that allow to divide the water movement into diffusion (due to the water present in the tissues) and perfusion (due to the water present in blood flowing in the capillaries). This second movement is not random, but oriented in the direction of the capillaries, but if we recall that capillaries are very short, randomly oriented and with a high density per volume, we can consider the perfusion as a ''pseudo-diffusion''. Knowing that benign and malign breast tumour have different perfusion characteristics, if we could identify and quantify this feature, we might be able to determine the type of the tumour. In this work, we use state of the art algorithms to compute the IVIM parameters which are then plugged into a learning algorithm, based on retrospective data, that infer the malignancy of the lesion. The actual DW-MRI protocol allows to measure only ADC, i.e. Apparent Diffusion Coefficient, which contains both diffusion and perfusion terms. Following the work of Le Bihan [2] , we compute the perfusion coefficient (called D*) separating it from the diffusion phenomenon (here the coefficient is called D). The different gradient intensities in the DW-MRI data is given by a so-called b-value. Taking data with different b-values allows also a higher accuracy in estimating ADC. Anyway, the main concept is that with high b-values the principal phenomenon involved is diffusion, while with low b-values it is clearly appreciable also the perfusion event, related to microcirculation. With this consideration we make use of the bi-exponential model of IVIM [2] : Here Then, we calculate f IVIM with the relation where b boundary is the b-value from where perfusion phenomenon is no longer evident and the diffusion gets involved, which is 200 in our case The final step is to compute D* by using the formula (1). The computation of these parameters is done by using a ROI (region of interest) containing the full DW-MRI sequence of the suspected lesion. On each ROI we then calculate the mean of the IVIM parameters: D, D* and f IVIM . The analysis is performed on a cohort of 25 patients of which 11 with benign tumours and 14 with malignant tumours. All patients underwent biopsy after the MRI scanning and prior to the study. A logistic regression algorithm based on non-linear decision boundaries was employed to compute a prediction model based on these training data. The model was then tested on 5 patients that were not included in the training data. All the software was implemented in Matlab (MathWorks, Inc., Natick, Massachusetts, United States). Results Figure 1 shows the plots of the two classes of lesions divided by the decision boundary computed through the logistic regression algorithm. We have a few outliers, due to some errors and/or noise that we have not considered. The overall accuracy of the prediction is 85% for now. We are introducing more patient data in the study so that the model will improve and we expect to reach an accuracy of the prediction of more than 90%. We have also calculated a colormap for each parameter, which can be useful for a medical direct observation. An example is shown in Fig. 2 . Conclusion A method to predict malignancy of a breast tumour based on DW-MRI data was presented. The method uses the IVIM analysis to compute the perfusion and diffusion coefficients in a ROI representing the suspicious area. The coefficients are then used to compute the boundary of a logistic regression model that allows to predict the malignancy and keep learning from new results.  Since 1981, cancer has been the leading cause of all deaths in Japan. As for cancer mortality in 2017, the leading site was lung (about 20%), and the number of lung cancer has been increasing every year [1] . It is true that the task of the radiologists is increasing, with the increase of the number of the lung cancer screening by chest computed tomography (CT). To assist the task of the radiologists, many studies have been made on the computer-assisted diagnosis for CT images. Although the computer-assisted diagnosis system by artificial intelligence has been performed in recent years, it takes long time and many training data to use effectively. The purpose of this study was to investigate the discrimination of the pulmonary nodules by homology-based image analysis in chest CT images. Homology is a mathematical concept that can be used to quantify degree of contact [2] . We deal here with two indices; ''b0'' and ''b1'' values. b0 value represents the number of part of connection. b1 value represents the number of regions surrounded by around, that is the number of holes. We developed an original software for homology analysis using Visual C ?? (Microsoft Corp.), and performed homology-based image analysis using above indices (b0 and b1). In this study, a total of 31 CT images were classified into solid nodules and ground glass nodules (GGN) and used in this study. This study was approved by the Research Ethics Committee of Osaka University (No. 17489). First, the nodule images cut from each type CT images along the shape as accurately as possible and measured the area. Second, these nodule images were converted to binary images. Third, b0 and b1 values in each image of binary threshold values were calculated by the homology method. Finally, we calculated 3 features (b1/b0, b0/area, b1/area). In addition, the features of blood vessels and lung fields were calculated in the same way. About the average of b1/b0, the peak value of solid nodules was near the binary threshold value 140, the peak value of GGN was near 130, the peak value of blood vessels was near 125-150, and the peak value of blood vessels was near 15 (Fig. 1) . The maximum value of b1/b0 exhibited a similar tendency (Fig. 2) . About the average of b0/area, the peak value of solid nodules was near the binary threshold value 40 and 120, the peak value of GGN was near 15, the peak value of blood vessels was near 35 and 120, and the peak value of blood vessels was near 5. The maximum value of b0/area exhibited a similar tendency. About the average of b1/area, the peak value of solid nodules was near the binary threshold value 135, the peak value of GGN was near 50, the peak value of blood vessels was near 135, and the peak value of blood vessels was near 15. The maximum value of b1/area exhibited a similar tendency. Fig. 1 The average value of b1/b0 in solid nodules ground glass nodules, blood vessels and lung fields. Horizontal axis and vertical axis represent binary threshold value of CT image and b1/b0 value Fig. 2 The maximum value of b1/b0 in solid nodules ground glass nodules, blood vessels and lung fields. Horizontal axis and vertical axis represent binary threshold value of CT image and b1/b0 value It was considered that the pulmonary nodules is accurately discriminated by homology-based image analysis in chest CT images. Towards convolutional neural network on primary lung tumors to predict histophatological type, distant and lymph node metastasis Purpose Between all cancer-related deaths in the world, lung cancer is the leading cause of deaths worldwide [1] . Among the complex tasks such as interpretation of medical images and detection of pulmonary nodules, the treatment decision of lung cancer is a great challenge as well, as it varies according to several factors, such as tumor staging at diagnosis [1] . So, computer-aided diagnosis (CAD) systems have been developed to help specialists improve diagnostic accuracy by acting as a second opinion through a computer-supplied suggestion. One machine learning technique that has emerged to improve CAD systems is the convolutional neural network (CNN) that is based on deep learning [2] . CNN extracts the image's features, selects the most important features and classifies the input images. Nevertheless, modeling the best CNN architecture for any specific problem by hand can be an exhausting, time-consuming and expensive task. Therefore, the use of algorithms for hyperparameter optimization in CNNs may be an alternative to achieve good results. In this work, we performed a convolutional neural network analysis with a hyperparameter algorithm on primary lung tumors to predict distant metastasis, lymph node metastasis staging, and histopathological subtype. This was a retrospective study approved by our institutional review board with a waiver of patients' informed consent. A cohort of 85 patients with lung cancer diagnosed and treated at our hospitals during 2010-2017 with definitive surgical or pathological diagnosis and clinical staging was used. First, the tumor images were cropped from thin slice contrast-enhanced computed tomography (CT) scans, Fig. 1 . The images were used with 3 channels (RGB), instead of 1 channel (grayscale), to augment the database and improve CNN's performance in the experiments. Three associations were analyzed: A) distant metastasis, classified as M0 (absent) or M1 (present); B) lymph node metastasis, classified as N0 (absent) or N1-3 (present); and C) histopathological subtype, classified as adenocarcinoma (ADN) or squamous cell carcinoma (SCC). In association A we used 68 M0 and 17 M1 cases, in association B we used 46 N0 and 39 N1-3 cases, and in association C we used 51 ADN and 21 SCC cases. Cases in each association were separated for training (balanced) and testing (unbalanced) phases, where all slices of each tumor were used. For training, 10-fold cross-validation was employed using the CNNs thought the Keras v.2.0.9 with Tensorflow v.1.3.0 backend frameworks and the Hyperopt tool using the Tree-of-Parzen-Estimators (TPE) algorithm to tune the hyperparameters in the CNNs'' architectures. In association A, 14 cases of M0 (555 slices) and of M1 (549 slices) were separated for training, and 3 cases of M1 and 51 cases of M0 for testing. In association B, 28 cases of N0 (880 slices) and of N1-3 (1,076 slices) were separated for training, and 11 cases of N0 and 12 cases of N1-3 for testing. In association C, 17 cases of ADN (602 slices) and of SCC (613 slices) were separated for training, and 4 cases of ADN and 32 cases of SCC for testing. In our experiments, each convolutional layer was followed by a max-pooling layer. Because of the size of our images, 32 9 32, we limited the exploration of the CNNs with up to 3 convolutional followed by max-pooling layers, and with up to 3 fully connected layers, counting with the last layer to do the binary classification. In Table 1 is summarized the hyperparameters explored and their range values. In convolutional layers, we explored the number of filters (Filter) from 1 until 256, the kernel size was fixed in 3 9 3. In max-pooling layers, the kernel size was fixed in 2 9 2. In fully connected layers we explored the number of units (Units) from 2 until 256. We also explored the optimizers'' functions, and the optimizers explored were: RMSprop, stochastic gradient descent (SGD), adagrad, nadam, adam, adamax, and adadelta. The activation function used in each layer was ReLU, with exception to the last one that we used the sigmoid function. We also evaluated the best value of batch size (BS) from 1 until 50. In association A it was used 100 epochs, and in association B and C, it was used 200 epochs. The number of iterations for the Hyperopt tool find the best size of CNN's architecture and hyperparameters'' values in association A and B was 100, and in association C was 30. As it was used CNN 2D, the testing cases were analyzed by three different strategies considering the volume of the tumors: (1) majoritory label from all slices, (2) maximum probability from all slices, and (3) average probability from all slices. To evaluate the performance of each association classification, we used the following statistical metrics: area under the receiver operating characteristic curve (AUC), sensitivity, specificity and accuracy. The confidence interval (CI) used was 95%.  This study performed a CNN modeling integrated with an algorithm to optimize its hyperparameters for prediction of distant metastasis, lymph node metastasis staging, and histopathological subtype on CT scans of primary lung tumors. Integration of the TPE algorithm through the hyperopt tool in a CNN architecture in the presented work showed great results to help specialists in these difficult tasks. The proposed model does not require preprocessing steps on images like data augmentation, segmentation, and extraction of handcrafted features, or search manually the best CNN architecture, improving the relevance of the model proposed. The volumes of interest (VOIs) which of size is 30 9 30 9 30 [pixels] applied to our proposed 3D-CNNs are set up on the lung region of current images. The center of each VOI is located on the point of the current image, which corresponds to the gravity point of an initial nodule candidate region. We designed the networks with five patterns of shortcut skips considering linear multi-step method, which are based on VGG16 and extended to 3D. We regard the part between one max pooling and the next max pooling as a sub-block. Shortcuts going across feature maps of two sizes like ResNet are not used. Our architecture has 5 sub-blocks which have 2 convolutions in the first and the second sub-block and 3 convolutions in the other subblocks. The description of each sub-block is following: (a) The third, the fourth and the fifth sub-blocks has 1 shortcut skipping 2 convolutions. Each output is following: x n-1 = f n-2 (x n-2 ), x n = f n-1 (x n-1 ), x n?1 = x n-1 ?f n (x n ). (b) The third, the fourth and the fifth sub-blocks have 2 shortcuts skipping 2 convolutions. These are crossed. Each output is following: x n-1 = f n-1 (x n-2 ), x n = x n-2 ? f n-1 (x n-1 ), x n?1 = x n-1 ? f n (x n ). (c) The third, the fourth and the fifth sub-blocks have 3 shortcuts skipping 1 convolution. Each output is following: x n-1 = x n-2 ? f n-2 (x n-2 ), x n = x n-1 ? f n-1 (x n-1 ), x n?1 = x n ? f n (x n ). Figure 1 shows the sub-block. The second sub-blocks have 2 shortcuts skipping 1 convolution. (d) The third, the fourth and the fifth sub-blocks have 3 shortcuts skipping 1 convolution and 2 shortcuts skipping 2 convolutions. These shortcuts have trainable coefficients (parameters) respectively except the first shortcut skipping 1 convolution. Each output is following: x n-1 = x n-2 ? f n-2 (x n-2 ), x n = (1k n-1 )x n-1 ? k n-1 x n-2 ? f n-1 (x n-1 ), x n?1 = (1 -k n )x n ? k n x n-1 ? f n (x n ) where k n-1 , k n are trainable parameters. The second sub-blocks have 2 shortcuts skipping 1 convolution, 1 shortcuts skipping 2 convolutions and one trainable coefficient. (e) In the network (e), each term of the equations as expressed in (d) has a trainable coefficient. Each output is following: x n-1 = a n-2 x n-2 ? c n-2 f n-2 (x n-2 ), x n = a n-1 x n-1 ? b n-2 x n-2 ? c n-1 f n-1 (x n-1 ), x n?1 = a n x n ? b n-1 x n-1 ? c n f n (x n ) where a m , c m (m = n -2, n -1, n), b l (l = n -1, n) are trainable parameters. The first sub-blocs have 1 shortcut skipping 1 convolution and 2 trainable coefficients. The second sub-blocks have 2 shortcuts skipping 1 convolution, 1 shortcut skipping 2 convolutions and 5 trainable coefficients. In all patterns (a)-(d), the first sub-block has no shortcuts. The second sub-block has 1 shortcut skipping 2 convolutions in (a) and (b). We used 31 cases in training but evaluate 28 cases including 28 nodules. The nodule size in 28 cases range 5-20 [mm] and the counterpart in 2 cases range less than 5 [mm]. The slice thickness is 2 [mm] in all cases. The pixel spacing of a slice is among 0.586-0.792 [mm]. We conducted data augmentations (translation, rotation and inversion) for the dataset. The positive data are much smaller than the negative data, so that mini-batch was created using a bootstrapping method to compensate the imbalance. We applied free-response receiver operating character analysis (FROC) and the average of each sensitivity at seven predefined false positives per scan, which are 0.125, 0.25, 0.5, 1, 2, 4 and 8 false positives per scan. We conducted leave-one-out cross validation. We compared the result of each proposed network (a)-(e) with the counterparts of 3D-AlexNet-like network, 3D-VGG-like networks (11, 13, 16, 19 layers) and 3D-ResNet-like (8, 20, 32 layers) networks. The FROC curves of the networks (a)-(e) are closer to the left top compared with the counterparts of 3D-VGG-like networks and 3D-ResNet-like networks as shown in Fig. 2 . In Table 1 , each average of the networks (a)-(e) is higher than the counterparts of 3D-VGG-like networks and 3D-ResNet-like networks, which shows each of the networks (a)-(e) has higher discrimination capabilities than 3D-VGG-like networks and 3D-ResNet-like networks. Each average in the network (b) and (c) outperforms the counterpart of 3D-Alex-Net-like network as shown in Table 1 , which shows networks (b) and (c) have higher discrimination capabilities than the other networks. In the network (d) and (e), we assume that trainable parameters of shortcuts have led an excessive approximation to test data (overfitting to test data) and each generalization capability of them has got worse. We proposed using 3D-CNNs having residual sub-blocks with linear multi-shortcuts for detection of lung nodules from TS images. The proposed method outperformed 3D-VGG-like networks and 3D-ResNet-like networks. For the future work, we should implement networks considering both mathematical analysis and regularization, and evaluate on more image datasets. Acknowledgements This work was supported by Grants-in-Aid for Scientific Research of JSPS (17H02110). Purpose Idiopathic pulmonary fibrosis (IPF) is a chronic interstitial lung disease (ILD) that is characterized by progressive fibrosis of the lung parenchyma without any known causes. The prognosis for patients with IPF is very poor, with a median survival of 2-5 years from the time of diagnosis. Although a clinical biomarker called the gender, age, and physiology (GAP) index is becoming the de facto standard for prognostic biomarkers of IPF, currently there are no established imaging biomarkers that would predict the overall survival of patients with IPF. The purpose of this study was to evaluate the effect of U-Net derived radiomic features, called U-radiomics [1] , and their combination with CT radiomic features, called hyper-curvatures [2] , on the prediction of the overall survival of patients with IPF in comparison with the GAP index. For the training and test datasets, we retrospectively identified, from the medical records of our institution, high-resolution lung CT images of 72 patients with rheumatoid associated arthritis (RA-ILD) and those of 75 patients with IPF. The CT scans were performed with full inspiration from the lung apices to their bases. The slice thickness was 0.6-1.5 mm and the reconstruction interval was 0.6-5.0 mm. The tube voltage was 120 kVp or 140 kVp with automatic tube-current modulation. For the training dataset, an experienced observer (an internist with 15 years of experience in pulmonary disease diagnosis and treatment) delineated approximately 4,500 regions of interest (ROIs) on the axial CT images of all RA-ILD patients and labeled them as having one of the following five lung tissue patterns: normal, consolidated, groundglass opacity, reticular, or honeycombing. The labeled axial CT images were used for the training of the U-Net shown in Fig. 1 for semantic segmentation of the CT images into the five lung tissue patterns. Then, the trained U-Net was applied to the axial CT images of the test datasets. A U-radiomics vector for an axial CT image of a test patient was identified as the bottleneck layer of the U-Net. As the number of channels of the output layers in our U-Net was 512 and the image size was 2 9 2, a 2048-dimensional U-radiomics vector was obtained for each image. The U-radiomics vector of the patient was obtained by averaging of the U-radiomics vectors across all the axial CT images of the patient. We also computed, from the test dataset, hyper-curvature features [2] that included 4D principal curvatures, curvedness, blight-sheet, blight-cylinder, blight-blobs, dark-cylinder, dark-line, dark-blob, and the scale thereof on the lungs [2] . The mean values of each of these features were calculated on the bronchi and aerated lung regions separately, and they were used as independent 363 hyper-curvature features. The above U-radiomics vector and hyper-curvature features were concatenated into a single vector and subjected to a Cox proportional hazards model with elastic-net penalty (hereafter called elastic-net Cox model) for selecting and combining the elements of the concatenated vector to build a Cox model that optimally predicts the survival of the patient. The performance of the combination of the U-radiomics and hyper-curvatures was evaluated by means of bootstrapping with the use of the concordance index (C-index) as a metric of the prediction performance. Bootstrapping with 500 replications was used to obtain an unbiased estimate of the C-index values and their confidence intervals. We evaluated the comparative performance of the U-radiomics with the GAP index. The difference between the estimated C-index values obtained from a pair of features was tested by the use of the two-sided t-test. A p value of less than 0.05 was considered to indicate a statistically significant difference. A pair-wise two-sided t-test showed that the difference between these features was statistically significant. These results indicate that both the hyper-curvatures and U-radiomics outperform the GAP index statistically significantly in predicting patient survival. Moreover, the combination of the U-radiomics and hypercurvatures outperformed either one of these feature sets (p \ 0.001), indicating that the U-radiomics and hyper-curvatures are mutually complementary in their prediction performance. In this study, we demonstrated that the combination of the U-radiomics and hyper-curvatures outperforms the established clinical biomarker of GAP index in predicting the overall survival of patients with IPF. This combination of features also significantly outperforms the individual U-radiomics and hyper-curvature sets of features. Thus, our preliminary results suggest that the combination of the U-radiomics and hyper-curvatures could be used as an effective prognostic biomarker for the survival of patients with IPF.  Keywords Radiomics, Idiopathic Pulmonary Fibrosis, Interstitial Lung Disease, High Resolution CT Idiopathic Pulmonary Fibrosis (IPF) is a rare disease of unknown cause that leads to progressive and irreversible decline in lung function. As pulmonary fibrosis gets worse over time, breathing efficiency is reduced and lungs cannot take in enough oxygen. Complications may include pulmonary hypertension, heart failure, pneumonia or pulmonary embolism. IPF is the most severe between other 180 types of Interstitial Lung Disease (ILD), as life expectancy following diagnosis is about 2-3 years. It has no curative treatment. In some cases, lung function decline can be slowed by anti-fibrotic medications or, in others, there is a recommendation for lung transplant. IPF diagnosis can be difficult since many diseases can cause shortness of breath and cough, with similar symptoms and radiographic results. High Resolution Computer Tomography (HRCT) is essential for diagnosis and may abbreviate the need for lung surgical biopsy when it shows a typical Usual Interstitial Pneumonia (UIP) pattern [1] . Even if not representing the disease IPF, UIP is the worst pattern of ILD, even when representing other diseases on the lungs. Recently, radiomics have demonstrated an unprecedented potential as quantitative imaging biomarker tool to assist diagnostic and prognostic in precision medicine. The purpose of this preliminary study was to develop and evaluate an artificial intelligence-based prediction model which support radiomics analysis to associate quantitative HRCT features with the diagnosis of UIP/IPF. Methods This work was based on a retrospective study approved by our institutional review board with a waiver of patients informed consent. The cohort consisted of 643 patients with ILD diagnosed and treated in our hospitals from 2012 to 2018. Differential diagnosis for ILDs provided by the specialists were based on clinical evaluation, laboratory exams, HRCT images and histopathologic analysis (surgical lung biopsies) when necessary. The pipeline for building the pulmonary radiomics-based predictive model of IPF was divided in four steps: (1) Image's preprocessing; (2) radiomics (radiologic phenotyping) features extraction; (3) radiomics features integration within a predictive machine learning model; and (4) validation. First, 3D HRCT DICOM images (* 60 GB) were pre-processed through normalization (Houndsfeld units) and spline-based resampling (3D voxel resolution: 1 mm 9 1 mm 9 1 mm and 344 9 456 9 456 slices). Then, k-means-based automatic algorithm was used to segment the lung interstitium while excluding pleural cavity. Second, for sake of quantification of radiologic phenotyping, a pool of lung radiomic features-i.e., first order statistics, shape-based, texture-based, size-based and intensity-based features, including log and wavelet of the first order statistics features [2]-were extracted using the open-source PyRadiomics Python Package. By default, PyRadiomics calculates first order features using bins equally spaced of 25 pixels while for second order features, image input is normalized to twelve grays level (i.e., 1-13). Thus, a total of 412 features were included in the resulting radiomics features dataset. Third, supervised machine-learning model was implemented using Among other models, we chose SVM as its design has shown good performance to avoid overfitting. Our machine-learning framework was implemented using Scikit-Learn Python Library (sklearn). Regarding to dataset definition, after reviewing medical records, ILD patients who had conclusive diagnosis (i.e., either idiopathic or non-idiopathic subjects) were include in this study, while patients with inconclusive diagnosis for UIP/IPF (i.e., under investigation) were excluded. Eighteen subjects had positive diagnosis of IPF in the dataset. Therefore, we selected further set of 18 (non-idiopathic) ILD subjects at random to balance the data set. As a result, data set totalized 18 images positive for UIP/IPF and 18 images negative for UIP/IPF. Last, sklearn permutation-based test was used to evaluate the competence and significance of our classifier through the cross-validated accuracy score. In addition, the performance on associating quantitative HRCT features with radiological patterns was assessed using the area under the receiver operating characteristic curve (AUC). Using a 100-fold cross-validated analysis, each of 412 features were individually tested. Thus, we could identify 3 statistically relevant quantitative HRCT features to discriminate between patterns: (1) correlation (Gray Level Co-occurrence Matrix-GLCM); (2) sum entropy (GLCM); and (3) high gray level run emphasis (Gray Level Run Length Matrix-GLRLM). Evaluation of 1000 permutations of 20-fold cross-validation set of HRCT using the selected features demonstrated average AUC of 0.866 and standard deviation of 0.02 (p value \ 0.001 and significance level of 0.1), as showed in Fig. 1 . This study performed an AI-radiomics analysis on ILD patients for associating quantitative HRCT features with specific characteristics of UIP/IPF. Assemble of quantitative features showed potential results in assisting differentiation of idiopathic and non-idiopathic patterns. Such a tool may help the radiologist in the HRCT classification of ILD and aid in the diagnosis of UIP/IPF, which are, respectively, the most important pattern and disease to be recognized. Medical treatment involves detection of the lesion, followed by differential diagnosis, and subsequently, treatment. Computer-aided diagnosis (CAD) studies have developed techniques for detecting lesions and for distinguishing between benign and malignant. On the other hand, radiomic studies aim to estimate the genotype of the lesion from the phenotype, and thus support medical treatment after the lesion is detected. Therefore, CAD can be classified as an artificial intelligence (AI) system that supports the first half of medical treatment, and radiomics can be considered as an AI system that supports the second half of treatment [1] . In this study, we develop a method of radiomic investigations for predicting the effect of neoadjuvant pharmacotherapy from the diagnostic image. Neoadjuvant pharmacotherapy is essential for patients with breast cancer who wish to preserve the breast, because breast-conserving surgery may be possible due to the shrinkage of the cancer [2] . Neoadjuvant pharmacotherapy may eliminate the cancer cells completely, which is known as pathologic complete response (pCR). Patients with pCR are known to have a lower risk of recurrence. The ability to predict the effect of pharmacotherapy is an important factor in determining treatment strategies. Therefore, several studies have been actively performed to predict the efficacy of cancer treatment. However, biopsy is stressful for the patient. The purpose of this study was to develop a simple method for predicting patients who achieve pCR by neoadjuvant pharmacotherapy using radiomic features obtained from non-invasive magnetic resonance imaging (MRI). Fat suppression T2-weighted MRI images of 125 cases were identified from the ISPY1 dataset of the The Cancer Imaging Archive public database. There were 35 cases with pCR and 90 non-pCR cases. The slice with the largest tumor diameter was selected from the MRI and the tumor regions were manually segmented. A total of 371 radiomic features including size, shape, pixel value, etc. were calculated from the tumor region. Since the number of radiomic features was higher than the number of patients included in this study, it was necessary to limit the former by selecting radiomic features that were useful for the distinction of pCR. We selected 7 radiomic features using lasso in this study. Linear discriminant analysis (LDA) and support vector machine (SVM) with 7 radiomic features were used for predicting patients with pCR. The Gaussian kernel was used in the SVM. The leave-one-out method was employed for training and testing the classifiers. Receiver operating curve (ROC) analysis was used to evaluate the accuracy of differentiation. Int J CARS (2020) 15 (Suppl 1):S1-S214 S121 The result of ROC analysis showed that the area under the curve (AUC) of LDA was 0.696 and that for SVM was 0.871 for distinguishing between pCR and non-pCR. Since there was no strong correlation between a single radiomic feature and pCR prediction, it was difficult to use a specific radiomic feature as an image biomarker for pCR prediction. However, in the high-dimensional space composed of multiple radiomic features, 2 classes showed a relation with pCR and non-pCR cases tended to be distributed at different locations. Therefore, it was suggested that MRI contains information for predicting the treatment effect of neoadjuvant therapy. Non-invasive MRI has the potential to predict the efficacy of neoadjuvant pharmacotherapy. It is necessary to improve the prediction accuracy in the future, in order to create an inexpensive and low burden prediction system. Purpose Urinary incontinence remains one of the most bothersome postoperative complications even after robot-assisted radical prostatectomy (RARP). We aimed to make a novel prediction system that can be used preoperatively to inform patients of the accuracy of early recovery of urinary continence after RARP using a deep learning (DL) model from magnetic resonance imaging (MRI) information and preoperative clinicopathological parameters. Methods A retrospective cohort study was conducted on prostate cancer (PC) patients who had undergone RARP at Fujita Health University Hospital between August 2015 and July 2019. Patients using no pads/ no leakage of urine or the use of a safety pad within 3 months after RARP is categorized into ''good'' continence and others into ''no good'' continence. MRI DICOM data from axial, coronal and sagittal imaging as well as preoperative clinicopathological covariates (age, BMI, prostate volume, serum PSA level, Gleason score, clinical stage) and intraoperative covariates (main operator, operation time, console time, with or without nerve sparing, bleeding) were assessed. Supervised DL algorithms, which included AdaBoost, Naive Bayes, Neural Network, Random Forest, and SVM were trained and tested as binary classifiers (good or no good). To evaluate the DL models from the testing data set, their sensitivity and specificity, as well as their accuracy, receiver operating characteristic curve, and area under the receiver operating characteristic curve (AUC) were analyzed. Results Data were available for 400 patients in the study period. Continence rates at 1, 3, 6, and 12 months after RARP were 47%, 66%, 87%, and 94%, respectively. The Naive Bayes DL algorithm using MRI information in addition to clinicopathological parameters had the highest performance with sensitivity at 71.1%, specificity at 68.8%, and AUC at 77.1% for predicting early recovery of urinary continence, while that using clinicopathological parameters only had the performance with sensitivity at 69.9%, specificity at 43.5%, and AUC at 60.3%. Furthermore, no increase in AUC was observed in the DL algorithms using intraoperative parameters in addition to preoperative information from MRI imaging and clinicopathological parameters. Our results suggest that the DL algorithms using MRI information are highlighted as an accurate method for strongly predicting early recovery of urinary continence after RARP. Our results might also highlight the importance of various anatomical factors rather than intraoperative parameters to determine the severity of postoperative urinary incontinence. Thus, DL predictions may help allocation of treatment strategies for PC patients who dislike prolonged urinary incontinence after RARP. Generative flow for data augmentation in computer-aided detection for CT colonography Tonami General Hospital, Toyama, Japan Keywords Generative models, Deep learning, Data augmentation, CT colonography One of the limitations of deep learning is that the training of highperforming convolutional neural networks (CNNs) requires large amounts of training data. In CT colonography, the available clinical datasets are limited, and they contain relatively small numbers of polyps. Although generative adversarial networks (GANs) have been used recently for augmenting datasets in deep learning applications, flow-based generative models have several theoretical advantages over GANs [1] . The purpose of this study was to evaluate the potential of a flow-based generative model in data augmentation for deep learning in computer-aided detection (CADe) of colorectal polyps in CT colonography. We trained a 3D flow-based generative model (3D Glow) to generate synthetic 3D image patches of colorectal polyps along with their surrounding normal colorectal anatomy in CT colonography. Such patches can be included in the training datasets of 3D CNNs to increase the number and variety of true-positive (TP) samples (polyps). The 3D Glow learns the distribution of the observed data in terms of a normalizing flow, which is a sequence of bijective transforms between the observed (often complicated) target distribution and a simpler distribution that is computationally easy to handle. The bijective transforms were implemented by the use of reversible CNNs. Unlike GANs, the 3D Glow provides an explicit model of the observed distribution, thereby providing a meaningful latent space for manipulating the data. After the training, the 3D Glow can be used to generate new samples with visual characteristics similar to those of the training dataset (Fig. 1) . To train the 3D Glow, we extracted 608 volumes of interest (VOIs) including polyps [ 5 mm in size from the CT colonography scans of 203 patients. The scans had been acquired over a wide variety of protocols, including cathartic and non-cathartic bowel preparations and multiple CT scanners (Siemens, Philips, Toshiba, and GE Medical Systems). The VOIs of polyps were augmented by flipping and rotation to yield a total of 38,912 polyp VOIs for the training of the 3D Glow. For a preliminary evaluation of the effect of 3D Glow-generated synthetic polyps on deep learning, we trained our previously developed 3D DenseNet CNN to classify VOIs of polyp candidates into real polyps (TPs) and false positives (FPs). First, we used our previously developed CADe system to detect polyp candidates from the CT colonography scans of the 203 patients at a 100% polyp detection sensitivity. Using the detected polyp candidates, we randomly sampled 50 VOIs of real polyps and 50 VOIs of FPs to construct two distinct datasets for the training of the 3D DenseNet. For the training dataset A, we included the VOIs of the 50 real polyps and 50 FPs that were flipped in three orientations to yield a total of 600 training VOIs. For the training dataset B, we included the VOIs of the 50 real polyps and 50 FPs, and also 50 synthetic polyps generated by 3D Glow, that were flipped in three orientations to yield a total of 900 training VOIs. The independent test dataset included 36 patients prepared for a CT colonography examination with a reduced cathartic bowel preparation by use of magnesium citrate and iodine or with a PEG-C protocol [2] . The CT acquisitions were performed with SOMATOM Definition (Siemens Healthcare, Erlangen, Germany) or Acquilion (Canon Medical Systems, Tochigi, Japan) CT scanners at 120 or 140 kVp energy and 0.5-1.0 mm slice thickness. Polyp candidates were detected from the CT colonography datasets at a 100% sensitivity by the use of our previously developed CADe methods. There were 92 TP and 6519 FP detections. To balance the numbers of TPs and FPs for a meaningful receiver operating characteristic (ROC) analysis, the TPs (polyp VOIs) were augmented to yield a total of 12,407 TP and FP VOIs that were used for the testing of the classification performance of the 3D DenseNet. Also, to evaluate if the 3D Glow-generated synthetic polyps are similar to real polyps, we performed a comparative analysis of the radiomic features of the polyp regions of the VOIs of the 50 real polyps and 50 synthetic polyps. The polyp regions were delineated from the VOIs by the use of our previously developed CADe methods. We calculated four volumetric features that we have previously identified as effective in differentiating true polyps from FPs. The differences in the feature values between the real and synthetic polyps were tested by the use of the Kolmogorov-Smirnov test. Results Figure 2 shows the classification performance of the 3D DenseNet in terms of ROC curves. The performance was higher when the CNN was trained including synthetic polyps (Az = 87.4%) than when the CNN was trained including real polyps only (Az = 85.5%). The difference was statistically significant (p \ 0.01). In the radiomic analysis of feature values between real and synthetic polyps, the differences were not statistically significant for any of the features, including variance of CT value (p = 0.64), mean of volumetric shape index (p = 0.29), and the mean (p = 0.15) and variance (p = 0.25) of the magnitude of the 3D gradient of CT value. Our preliminary results indicate that the 3D Glow can be used to generate realistic synthetic polyps for data augmentation of deep learning in CADe of colorectal polyps for CT colonography. Radiomic analysis indicated that the synthetic polyps are indistinguishable from real polyps. Augmentation of the training dataset by 3D Glowgenerated synthetic polyps significantly improved the classification performance of a 3D DenseNet. (Fig. 1) . On the contrary, perpendicular vascular changes (PVC) develop perpendicularly towards the mucosa and are associated with papillomatosis and malignant lesions (Fig. 2) . The evaluation of LVC and PVC therefore has diagnostic relevance. Contact Endoscopy (CE) is an optical technique that allows a detailed examination of vascular patterns in laryngeal mucosa. CE can be combined with other methods such as Narrow Band Imaging (NBI) to provide enhanced visualization of vessels. However, the evaluation of vascular patterns in CE ? NBI images is a subjective process causing difficulty in differentiation between benign and malignant lesions. The main objective of this work is to present the preliminary comparison of multi-observer manual classification versus automatic classification of CE ? NBI images that were proposed in our previous work [2] to provide a confident way for clinicians to optimize therapeutic decisions. For the manual classification, a series of four to five CE ? NBI images were randomly selected for each patient from the previously extracted images. Three otolaryngology specialists (experienced observers) and three otolaryngology residents (less-experienced observers), blinded to the histologic diagnosis, independently visually evaluated the series of images to detect benign and malignant lesions based on vascular patterns. PVC-positive lesions, with the malignant histological diagnosis, were considered true positives for the calculation of sensitivity, specificity. The calculation was conducted with IBM SPSS Statistics software package (V-26), and the average value for each group of observers was presented. In the previous study [2] , we proposed a method for the automatic classification of the CE ? NBI image by characterizing the level of the vessel's disorder. Each image went through three different steps of preprocessing. In the present study, the Jermann filter was used as a new enhancement technique to overcome the observed deficiencies of the already used Frangi. Then, five indicators were computed based on the evaluation of the consistency of gradient direction and the curvature analysis of vascular patterns in preprocessed images. 24 features were extracted based on the qualitative properties of the indicators and fed into four supervised classifiers, including support vector machine (SVM) with polykernel and radial basis function (RBF), k-nearest neighbor (kNN), and random forest (RF). For the classification scenarios, randomly selected CE ? NBI images for the manual classification were used as the testing set (336 images, * 20% of the dataset), and the rest of the images were used for training (1296 image, * 80% of the dataset). We used this training and testing set strategy to be able to compare the results of manual and automatic classifications. The sensitivity and specificity were calculated from a confusion matrix for each classification scenario. Results Table 1 presents the results of manual classification performed by six experienced and less-experienced observers and automatic classification using four supervised classifiers. The mean sensitivity value of manual classification showed a difference between experienced and less-experienced groups. Showing that the interpretation of vascular patterns in CE ? NBI images is subjective and highly depends on the experiences of the clinicians. The mean specificity of 0.630 and 0.609 values in manual classification can be explained by the difficulty of visually distinguish between benign and malignant lesions based on the vascular pattern. Papillomatosis is a benign histopathology that has similar vascular patterns to malignant lesions leading to misclassification of papillomatosis as malignant lesions. The automatic classification showed a sensitivity of 0.959 by SVM with RBF and specificity of 0.932 by kNN. These results emphasise the ability of the automatic approach to solve the problem of misclassification of laryngeal lesions such as papillomatosis and with that is valuable for experienced and less-experienced observers. The results confirm the relevance of the vascular patterns to benign or malignant lesions. The automatic approach has the potential to operate as an assisting system to provide a confident way for clinicians to make the final decision about the stage of laryngeal cancer in the routine surgical procedures. University, Biomedical engineering, Saitama, Japan 2 University, Medical, Shimono, Japan Keywords Image processing, AlexNet, Malaria, Blood cell Malaria is one of the three major infections in the world, along with tuberculosis and Acquired immune deficiency syndrome. Plasmodium falciparum and Plasmodium knowlesi are malaria species that can infect humans and even result in death; thus, early diagnosis and identification are vital [1] . Currently, majority of the diagnostic methods utilize microscopy; however, this approach is dependent on the skill level of the diagnostician, is time consuming, and cannot determine the type of malaria perfectly. Devices that can automatically determine the presence and rate of infection in approximately 60 s have been developed in previous studies. However, these devices are large in scale, expensive, and unsuitable for use in infected areas [2] . In this study, a system combining microscopy and smartphone technology that is also applicable spread in infected areas was developed. Our study aimed to develop a system that can screen for infections in a rapid and simple manner by utilizing a smartphone application that records and uses microscopic images, and deep learning methods with the learned AlexNet to perform the diagnosis. Methods First, a microscope eyepiece was directly attached to a smartphone and images were captured. Next, the images were transferred to a server, and image processing and diagnosis were performed. Finally, the result was returned from the server to the smartphone. Cultivated specimens of Plasmodium falciparum and blood were used as the sample images. The presence or absence of infection was determined by using deep learning methods with the learned AlexNet and a single class SVM. The captured image was segmented into each blood cell, trimmed, and normalized into a 10 9 10 pixel image. Using this method, the value of abnormality of each images can be calculated and the value of infected red blood cell (IRBC) was higher than that of red blood cell (RBC). However, when white blood cells (WBC) were included in an image, the extent of observed abnormality was also high because the color of WBC nuclei was similar to that of the area stained by the IRBCs. To distinguish the WBCs from IRBCs, we focused on images whose original WBC sizes were larger than that of RBCs, including IRBCs. Three programs were developed. The first program A determined the presence of IRBCs when the original image size was smaller than 1.5 times the mean RBC image size. In the second program B, images larger than 1.2 times the mean RBC image size were extracted. Next, images were sorted in ascending order of abnormality. The line that pixel number difference between before and after images firstly exceed the setting threshold is border between IRBC and RBC was discriminated. The third program C is a combination of programs A and B. When an image was determined to contain IRBCs by either program A or B, the image was determined as containing IRBCs. An example of an abnormality calculation is illustrated in Fig. 1 ; the processing time was 5.61 s. The images are displayed in descending order: images surrounded by thick lines are those of the IRBCs or WBCs; RBCs without a thick line are perfectly distinguishable. However, the IRBCs with a thick red line and WBCs with a thick cyan line cannot be distinguished. The results of the attempts at separation using programs A, B, and C, by inspecting 15 sets of images that include 100 blood cell images, are depicted in Fig. 2 . A maximum discrimination rate of 99.6% was obtained for the WBC Fig. 1 Results of the image abnormality calculation by AlexNet, displayed in descending order Fig. 2 Results of distinguishing white blood cell (WBC) and infected red blood cell (IRBC) by using programs A, B, and C Int J CARS (2020) 15 (Suppl 1):S1-S214 S125 images using program A; a rate of 98.5% was obtained for the IRBC images using program C. The specificity of program B was lower than that of program A. This is because the difference in image size between the IRBC and WBC images was in some cases not sufficiently large. As for the IRBCs, a higher discrimination rate could be obtained by utilizing program C, a combination of programs A and B. In this study, a system that can automatically diagnose malaria infection using the learned AlexNet was developed. The main feature of this system is its ability to identify infected blood cells by a rapid and simple screening method. Three programs were developed to distinguish images of WBCs from those of IRBCs based on the result that the WBC pixel size is larger than that of IRBC. More than 95% specificity is required to prevent misdiagnosis of severely ill patients; the results of the method could fulfil this requirement. However, 100% sensitivity is necessary to prevent an incorrect diagnosis. As this method could not fulfill this requirement, the rate of discrimination needs to be improved with a new processing program, focusing on the shape of the cytoplasm and nucleus, to distinguish between the blood cell types. Furthermore, we intend to develop a function to derive the total rate of infection from all blood in our future study. The final diagnosis of cancer is determined by tissue classification based on pathological examination. However, molecular classification based on the genotype of the tumor has been used in recent years, as the mechanism of tumorigenesis has been elucidated by the progress in genetic analysis technology. Against this background, the World Health Organization (WHO) Brain Tumor Classification published in 2016 classified brain tumors according to the histological and molecular genetic information. The principal reason for the shift from conventional tissue classification to molecular genetic classification is that objective genetic information can ensure a biologically clear classification system and it is possible to create patient groups for a given set of diagnosis and suitable treatment. However, gene analysis is expensive and time-consuming and the limited facilities are available for performing gene analysis. Therefore, radiomic studies are conducted to simply estimate the genotype from the tumor's phenotype observed on medical imaging [1, 2] . The genetic parameters included in the WHO Brain Tumor Classification are isocitrate dehydrogenase mutation, 1p/19q codeletion, ATRX deletion, and TP53 mutation. In this study, we propose a method for distinguishing between the presence and absence of 1p/19q codeletion using noninvasive magnetic resonance imaging (MRI). We identified 52 cases of grade II brain tumors from the LGG-1p19q deletion dataset in The Cancer Imaging Archive public database. The lesions targeted included 2 astrocytomas, 35 oligodendroastrocytomas, and 15 oligodendrogliomas. There were 33 cases with 1p/19q codeletion and 19 cases without 1p/19q codeletion. Images contrastenhanced T1-weighted and T2-weighted MRI were used in this study. The matrix size was 256 9 256, which was converted using linear interpolation. The slice with the largest tumor diameter was selected from among the MRI images. The tumor regions were determined using the region map attached the database. A total of 371 radiomic features including size, shape, pixel value, etc. were calculated from the tumor region. Since the number of radiomic features was higher than the number of patients included in this study, it was necessary to limit the former by selecting useful radiomic features. We used lasso for selecting 10 radiomic features for this study. Linear discriminant analysis (LDA) was performed with 10 radiomic features for distinguishing between the presence and absence of 1p/19q codeletion. The leave-one-out method was employed for training and testing the classifiers. Receiver operating characteristic (ROC) analysis was used to evaluate the accuracy for differentiation. ROC analysis showed that the area under the curve (AUC) for contrast-enhanced T1-weighted imaging was 0.73 and 0.87 for T2weighted imaging for distinguishing between the presence and absence of 1p/19p codeletion. Thus, we found that the information contained in T2-weighted imaging estimated gene codeletion with greater accuracy. Figure 1 shows the weighting factors in the LDA with 10 radiomic features obtained from the lesions on the T2weighted image. Since the radiomic features were normalized, weighting factors with large absolute values represented the radiomic features that mainly contributed to the differentiation. The presence or absence of gene codeletion can be roughly distinguished by only two radiomic features, as seen on the scatter plot. Therefore, we think that physicians can use these two radiomic features as image biomarkers for estimating gene codeletion. Our findings showed that the presence of 1p/19q codeletion could be estimated using radiomic features obtained from MRI. Therefore, we think that the tumor's phenotype contains information for estimating its genetic characteristics. Our method is useful for cases where it is difficult to obtain cancer cells for examination using biopsy, as the genetic characteristics of brain tumor can simply be estimated with noninvasive MRI. The aim of this study is to isolate which features of radiography-based radiomic analysis could be considered the most significant predictors of metastasis in oncological patients with osteosarcoma [1, 2] . The Institutional Review Board (IRB) of our institution approved this retrospective study.Fifty pretreatment radiographies imaging osteosarcoma confirmed by histopathology and chest computer tomography (CT) at presentation and 2 years following-up were analyzed, which include 25 pulmonary metastatic (MT) and 25 negative pulmonary metastatic patients (non-MT).These lesions were manually segmented via 3D-slicer software by two radiologists with 20 and 10 years'' experience in orthopedic oncology, and 93 radiomics features were extracted by Pyradiomics software, Figs. 1 and 2 . The radiomics features differences for MT and non-MT groups were compared. The receiver operating characteristic curve (ROC) was used to determine the optimal cutoff value of each radiomics feature parameter for differentiating TNBC from non-TNBC, and the corresponding area under the curve (AUC), sensitivity and specificity were obtained. p \ 0.05 indicated that the difference was statistically significant. There were statistically significant differences for 6 radiomics features between MT and non-MT datasets (p \ 0.05). These features were dependence non uniformity (572 vs 1134 p \ 0.038); gray level non-uniformity GLDM (771 vs 1448, p \ 0.039); energy (39,640,000 vs 74,080,000, p \ 0.013); gray level non uniformity GLRLM (92.96 vs 140.5, p \ 0.027); size zone non-uniformity (5.14 vs 12.1, p \ 0.019) and strength (0.09 vs 0.07, p \ 0.04). The results suggest that radiography-based radiomic analysis in patients with osteosarcoma may be helpful to distinguish the pulmonary metastatic and non-metastatic groups. Purpose Spondyloarthritis (SpA) refers to a set of immune-mediated diseases characterized by chronic inflammation in the axial skeleton, peripheral joints, and enthesis. Identifying the sacroiliac joints active inflammation (sacroiliitis) is essential to start treatment as soon as possible. The primary perform for early diagnosis of sacroiliitis is made by Magnetic Resonance Imaging (MRI) evaluation. Besides that, predicting disease progression is also a challenge due to the severity of SpA that varies widely. Some evaluation indices suggest that several biomarkers may predict progression in SpA, for example, BASDAI (Bath Ankylosing Spondylitis Activity Index), BASFI (Bath Ankylosing Spondylitis Functional Index) and MASES (Maastricht Ankylosing Spondylitis Enthesis Score) [1] . Therefore, the identification of novel biomarkers for disease progression is a priority to improve patient management but can be used several approaches can. A current approach so-called radiomics consists of the massive extraction of quantitative features from medical images and their association with clinical outcomes. The purpose of this study was to discover potential biomarkers to identify sacroiliitis and predict disease progression. S -) . Was calculated the Spearman correlation coefficient (q) to measure the intensity of the association between radiomic biomarkers and standard clinical variables (BASDAI, BASFI, and MASES). Diagnoses and clinical data were obtained from patient's records. Association was assessed by the area under the receiver operating characteristic curve (AUC). The univariate analysis showed that the biomarker of skewness from the gray-level histogram yielded the highest performance to identify the presence of inflammation in the sacroiliac joints with AUC of 0.86 (p \ 0.0001, Fig. 1 ). The skewness of gray-level histogram also yielded values statistically significant Spearman's correlation with the clinical variables ( Table 1 ). All q values were directly proportional to clinical variables, which indicates that high values of this biomarker more likely to have a poor prognosis. Identifying biomarkers for diagnosis of sacroiliitis as well as disease progression of SpA is a priority to improve patient management due to the severity of SpA. Skewness of gray-level histogram presented high association with sacroiliitis presence. Furthermore, the biomarker in question exhibited a moderate correlation with BASDAI and BASFI and a weak correlation with MASES. In conclusion, it was possible to show that the application of the radiomic approach constitutes a potential noninvasive tool to aid the diagnosis of sacroiliitis and to evaluate disease progression based on MRI images of sacroiliac joints. This technique can identify critical findings for SpA, like subchondral bone marrow oedema, which corresponds to an early stage of the disease. The sequences short tau inversion recovery (STIR) and spectral attenuated inversion recovery (SPAIR) are the current MRI standard for SpA evaluation [1] . In an attempt to aid the early diagnosis and subtyping of SpA, in this work, we performed a radiomic study to associate quantitative MRI features extracted from both fat saturation sequences with SpA and its subtypes. To do that, we assessed the modeling of artificial neural networks trained with the radiomic MRI features to predict the diseases. We performed a retrospective study approved by our institutional review board with a waiver of patient's informed consent, which was selected 47 patients imaged with SPAIR and 45 patients imaged with STIR with suspected SpA who underwent MRI examinations. A diagnosis of SpA was previously established after clinical and laboratory follow-up in the rheumatology division of our hospital using additional radiography or MRI exams. After anonymization, six consecutive images from each MRI exam were selected. Each image was manually segmented by a musculoskeletal radiologist and processed by the warp geometric transform. A total of 230 features from Histogram, Haralick, Tamura, Fourier, Gabor, Wavelet, and Fractal were extracted from each MRI image being characterized by the mean and standard deviation [2] . A machine-learning model based on the ReliefF feature selection method and an Artificial Neural Network (ANN) used the quantitative image features as input to predict the diseases. This model finds the highest performance for the combination of x selected features and y ANN neurons, which x varied from 1 to all 230 features, and y ranged from 1 to the number of samples of the majority outcome group (Fig. 1) The statistically significant difference between them was found (p \ 0.05). All interactive features selected by both models developed using SPAIR and STIR sequences are listed in Table 1 . The early diagnosis of SpA is vital to initiate treatment and minimize the mobility limitations of the patients. Due to the complexity of clinical manifestations to diagnose SpA, radiomics could be a tool to aid physicians in clinical decisions because it can identify fine details of the medical image. MRI-based radiomics showed the potential to identify SpA patterns besides active inflammation on sacroiliac joints and to distinguish the subtypes of the disease using a single imaging exam of the pelvic girdle region. SPAIR and STIR fat suppression sequences were essential to machine learning and SpA association, corroborating with the current clinical protocol, which uses both sequences for inflammation evaluation. For future directions, we propose to extend the analysis for patient prognostic assessment and therapy response prediction. . Therefore, the CNR in the MDCT images acquired with the reference and the above two ALADA protocols was measured. The CNR of the masseter muscle and tongue were measured separately in relation to air. All datasets were viewed using a soft tissue window (window width/window level: 400/40). For both the masseter and tongue measurements, 5 axial sections were selected from each of the three datasets at identical positions in relation to the head. In each section, four regions of interest (ROIs) were selected, each one measuring 0.5 9 0.5 mm in size. For both the masseter and tongue study, two ROIs were placed within the air space lateral to the jaws, one on each side of the head. For the remaining two ROIs, in the masseter study one ROI was placed within the boundaries of each masseter muscle bilaterally, and for the tongue study, the ROIs were placed in the midline of the tongue anteriorly, and posteriorly. The mean of the Hounsfield units (HUs) and their standard deviation (SD) were recorded within each ROI. The recorded values were used to calculate the CNR using the following equation: CNR ¼ jl muscle À l air j= p ðr 2 muscle þ r 2 air Þ Where l is mean HU, and r is SD. Ten CNR values were obtained for each dataset. The average of the ten CNR values was then calculated. A descriptive analysis of the results was performed. For the masseter muscles, the CNR from the three examination protocols were as follows: Reference dose examination: 26.5; ASIR 100 ALADA dose: 25.6; FBP ALADA dose: 7.7. For the tongue, the CNR from the three examination protocols were as follows: Reference dose examination: 26.5; ASIR 100 ALADA dose: 27.5; FBP ALADA dose: 8.3. The CNR in the images using the ALADA protocols for FBP and ASIR 100 were largely different. The CNR from the ASIR 100 ALADA dose was similar to the CNR from the FBP reference protocol, which imparted ten times the FBP ALADA dose. The CNR of the masseter and tongue muscles in relation to air is not similar in the MDCT ALADA protocols for identification of the IAC using different reconstruction techniques. Therefore, CNR cannot be used to predict the ALADA dose for localization of the IAC.  According to this study, the narrowest CSA and volume of upper airway should be considered as important parameters that need to be evaluated independently and they don''t correlate to the severity of the disease which still evaluated by AHI only. However, the retropalatal airway morphology may give an indication of the severity of the disease. Keywords Orthognathic surgery, Facial change prediction accur, Lip shape analysis, Qualitative evaluation Purpose Accurate facial change prediction is one of the most important factors for the improvement of surgical outcome following orthognathic surgery. Prediction accuracy in the clinically critical regions, i.e., the lips, is crucial because the shape of the lips significantly affects facial aesthetics. However, traditional facial prediction accuracy evaluation method shows inaccurate result compared with the clinician's qualitative evaluation, especially in the lips. In this study, we developed a new quantitative method to accurately evaluate the facial change prediction accuracy in the lips. The developed method is based on the geometrical analysis of the lip shape and reflects clinician's qualitative evaluation. We develop a two-step optimization approach to find the optimal lipfitting circles that describe the shape of the lips. An optimization approach is applied to overcome the limitation of general fitting circles. A general fitting circle is not suitable for lip shape analysis because the lip profiles are not exactly circular. In the first step, local optimal lip-fitting circles are calculated. In the second step, global optimal lip-fitting circles that best describe the lip shape are acquired among the local optimal lip-fitting circles. Finally, the facial prediction accuracy is evaluated based on the parameters of the global optimal lip-fitting circles. In the first step, the local optimal lip-fitting circles are calculated by finding the circles that fit the lip profiles on the facial midline. For separate evaluation of the upper and lower lips, the upper and lower lip segments are extracted from the facial midline profile (Fig. 1a) . Given one point on the lip segment, every possible combination of sequential points sets including the given point is acquired. Then, general fitting circles are calculated based on each set of the sequential points. Among the acquired circles, the largest circle, whose geometrical error for any point is larger than -0.3 mm, is selected. The threshold value is empirically determined to find the largest fitting circle that describes the lip shape locally while staying inside of the lip profiles. This process is repeated for every point on the segment. As a result, a set of local optimal lip-fitting circles are acquired (Fig. 1b) . In the second step, the global optimal lip-fitting circle that best describes the shape of each lip is found among the local optimal fitting circles. Convex part of the lip segment is assumed to best represent the shape of the lip (Fig. 1a) . Therefore, the convex part of the lip segment is acquired using 2nd order differential. Then, the geometrical errors of all the local optimal lip-fitting circles are Int J CARS (2020) 15 (Suppl 1):S1-S214 S133 calculated over the convex segment. Finally, the fitting circle with the smallest geometrical error is selected and designated as the global optimal lip-fitting circle (Fig. 1c) . The global optimal lip-fitting circle is calculated for the upper and lower lips, separately. The prediction accuracy in the lips is calculated by comparing the parameters of the global optimal lip-fitting circles of the predicted face with that of the postoperative face. Specifically, the lip-shape prediction error is a square root of the sum of the squared difference between the centers of the circles and squared difference between the radius of the circles of the predicted and postoperative face (Fig. 1d) . To evaluate our method, the qualitative evaluation was performed by CMF surgeon. The surgeon visually compared the predicted face with postoperative face and rated the predicted result using a Likert scale (4 grades; 4: acceptable, 3: likely acceptable, 2: likely unacceptable, 1: unacceptable) for the upper, lower and entire lips. Traditional quantitative error evaluation method, i.e., surface deviation error, was also performed for comparison. Our new prediction accuracy evaluation method for the lips was evaluated using 10 datasets of the patients who underwent orthognathic surgery. Three different simulation methods were used to generate the predicted facial soft-tissue surface [1, 2] . Each of the predicted results was considered as an individual result, thus resulting in a total of 30 predicted faces for evaluation. Linear regression analysis was performed to evaluate the correlation between our new quantitative evaluation method and clinician's qualitative evaluation. The result showed a statistically significant linear relationship between two evaluations (p \ 0.05) with R-squared value of 0.76 for the upper lip and 0.73 for the lower lip. The surface deviation also showed a linear relationship with the qualitative evaluation, but with reduced R-squared value of 0.30 for the upper lip and 0.51 for the lower lip. Our new quantitative evaluation method was significantly correlated with the clinician's qualitative evaluation result. Our new method also outperformed the traditional surface deviation error. Our new quantitative method successfully quantified the subjective qualitative evaluation based on the geometrical analysis of the lip shape, leading to scientific and objective evaluation of the facial change prediction error in the lips. In the future, the performance of our new evaluation will be evaluated with a more extended number of data set to prove the significant relationship with the qualitative evaluation.  In orthognathic surgery, surgical planning is defined based on the patients'' facial proportion, skeleton deformation, and the occlusion condition. It is very important to make proper surgical planning of orthognathic surgery because it has a direct influence on the patients'' esthetic outcome and the occlusion function [1] . Usually, the surgeon needs to perform cephalometric measurements and clinical measurements. In total there are more than 100 parameters to be measured. Based on the parameters proper surgical planning can be defined. Due to the lack of surgical experience, it is not easy for the junior assistant to define proper surgical planning. The purpose of this study is to develop a tool to train the assistant to make proper surgical planning using machine learning. This clinical study includes the results of clinical measurements, cephalometric measurements, and corresponding surgical plans. A total of 260 patients are included, 208 patients are used to train a machine learning model and the rest 52 patients are used to evaluate the accuracy of the algorithm. The trained machine learning model is able to predict the surgical type: double jaw surgery, Le fort I surgery or mandible surgery. In the meanwhile, it should report whether genioplasty is necessary. Within each surgical type, the machine learning model should predict the movement of the maxilla in the anterior-posterior, left-right and intrusion-extrusion direction. The clinical data includes 39 features, and the image data includes 81 features in the experiment, and the first challenge of this study is that each patient includes multiple characteristic parameters, which have different value types. Each feature contributes differently to the final result. In our study, we first normalize different features. Normalization is to make the features between different dimensions have the same value range, which can greatly improve the accuracy of the classifier [2] . Secondly, in order to highlight the contribution of different features, we use the multi-head self-attention mechanism to extract highdimensional features. The multi-head self-attention mechanism copies three copies of the input as query, key, and value, and uses multiple scaling dot products, that is, multi-head attention, to analyze the contribution of features to the final result. Parameters are not shared between headers. The multi-head self-attention mechanism compares different features and assigns a weight to each feature. This weight is used to indicate the degree of influence of this feature on the final result. In the experiments, we first copied the feature vector of clinical data of each patient into 3 groups and generated more abstract feature vectors through a linear transformation. The 3 groups of feature vectors are used as the query, key, and value of the multi-head self- Fig. 1 Example of facial change prediction accuracy evaluation using an optimal lip fitting circle for the upper lip. a upper lip segment (green) and convex part (magenta) of the upper lip, b local optimal lip-fitting circles and their centres represented with dots, c global optimal lip-fitting circle, d global optimal upper lip-fitting circles for the predicted face (blue) and postoperative face (red) attention mechanism. Our multi-headed attention model contains 8 attention modules, each module has a dropout function to alleviate the overfitting problem. The 8 attention modules synthesize their final output by outputting a linear function layer. Finally, the high-dimensional features are input to the neural network of two hidden layers. The first hidden layer contains 80 neurons, and the second hidden layer contains 40 neurons. The activation function we use is the ReLU function, which is widely used in neural network models. ReLU is more efficient for gradient descent and backpropagation: it avoids the problem of gradient explosion and disappearance. In addition, ReLU simplifies the calculation process: there is no effect of other complex activation functions such as the exponential function; meanwhile, the dispersion of activity makes the overall computational cost of the neural network has decreased. The last layer of the neural network first uses the softmax function to normalize the output of the second hidden layer and then uses inverse normalization to obtain the final decision value. We perform experiments using only clinical data. The accuracy rate of the classification of the surgical type can reach 73%. Although the data set has an imbalance problem, that is, the amount of data in some classes is much larger than the amount of data in other classes, but the F1 score of our model can still reach 50% at 60,000 epochs. We developed an network to classify the surgical type of orthognathic surgery with an accuracy of 73%. In the future work, we plan to use sampling technology to deal with the imbalance problem to further improve the F1 score, and based on this, realize surgical planning for orthognathic surgery [6]. In this type of intervention, where complex anatomy is present and high accuracy is needed, surgical navigation becomes a useful tool to improve guidance and clinical outcomes. The accuracy and safety provided by this technology has already been tested for this particular clinical case (ACC) [1] . However, the reference frame used in [1] to track the head movements was attached to the patients' forehead, ensuring a precise tracking but entailing a highly invasive approach. In contrast, patient registration was done using anatomical landmarks such as tooth cusps or bone structures, which are less precise and can reduce navigation accuracy significantly. In this work, we propose an alternative setup with a non-invasive approach for reference frame placement, by means of a patientspecific silicone mold fixed to the teeth, with a 3D-printed reference attached. Instead of anatomical landmarks, we suggest the use of screws placed across the gum to improve patient registration. The procedure has been tested in a real clinical case of a patient suffering ACC. Resection limits were recorded using navigation and then compared to the real margins obtained from the postoperative CT to evaluate the precision of the system. Methods A preoperative computed tomography (CT) scan of the patient's head was acquired in order to study the lesion and extract the 3D anatomical models of the skull and tumor for navigation. Prior to image acquisition, 5 screws were placed in the patient covering the gum above the upper teeth. These screws were also segmented to generate 3D models and establish fixed intraoperative registration markers. A silicone template was fabricated to fit on the patient's upper left teeth. A reference frame was 3D printed in polylactic acid (PLA) using the Ultimaker 3 Extended (Ultimaker B.V., Netherlands) 3D printer. This reference was designed to include 3 passive optical markers and to be attached to the silicone template, acting as reference of the patient's position during navigation (Fig. 1) [2] . Polaris Spectra (NDI, Waterloo, Canada) optical tracking system was used for navigation. Two different instruments were tracked during surgery: the pointer required to perform the registration and to collect points (NDI passive probe) and a piezoelectric handpiece from Mectron to resect the tumor. This handpiece is composed of a body and an interchangeable saw in its end. In order to track it, a rigid body was designed and printed in PLA to be placed at the top of the instrument (Fig. 1) . A second tool, including 6 small holes for registration, was designed and 3D printed to fit at the end of the instrument. The interchangeable saw has a non-fixed rotation around the longitudinal axis of the instrument. This results in a different positioning each time it is placed. To represent the correct orientation of the saw in the navigation scene it was necessary to add an extra registration step. This step consists in recording the position of the saw tip and finding (automatically) the rotation of the virtual saw model which minimizes the distance to the corresponding virtual tip point. All 3D printed tools were sterilized before the surgical procedure. A specific module was implemented on 3DSlicer open-source software. This module was used during surgery for real-time visualization of the tracked instruments with respect to the models and CT slices, for patient registration (repeated 3 times to ensure accurate alignment throughout the intervention) and for the handpiece registration (repeated twice). It also enabled to record the movements of the handpiece while cutting and to collect points along the final resection with the pointer (repeated 3 times). The collected points were then used to evaluate the navigation accuracy of the system by comparing them with the margins found in the postoperative CT, after registration with the preoperative CT. The registration errors in each repetition were 0.77, 0.93 and 0.81 mm for patient registration and 1.12 and 1.6 mm for the handpiece registration. Figure 2 shows the measured distances between the collected points along the resection margins and the margins obtained from the postoperative CT, considering both bone and tissue. Distances are divided into repetitions and regions in order to analyze possible differences. We propose and evaluate a navigation setup for ACC resection based on 3D printing and open-source software, including a non-invasive placement of the reference frame as well as an accurate registration procedure. The comparison between the resection margins recorded intraoperatively with the navigation software and the real margins from the postoperative CT show errors around 1 mm, where posterior areas present a higher error. This is expected as they are further away from the reference frame and less accessible. Distances in the left area are higher as the silicone mold was still placed when recording points but not at the time of the CT acquisition. Taking into account all these limitations, we consider that 90% of the points presenting errors below 2 mm is an accurate result. Overall feedback from the surgeons was very positive in a surgery where, as they stated, without navigation they would not have felt confident enough to keep these margins and would have followed an alternative approach including the complete removal of the maxilla and upper teeth. Computer-assisted dental implantation is considered to be state of the art. Based on imaging data acquisition (CT, CBCT), the number, position and geometric dimensions of the dental implants are digitally planned [1] . For the use of dental implants a suitable local bone support is necessary. If this is not available due to long years of edentulism, trauma or tumor, an augmentation (bone replacement) is necessary. For this purpose, manually cut replacement bone from other body areas or bone replacement materials are used. The aim is to achieve a form-fit bone replacement that reflects the defect geometry for aesthetic, medical and economic reasons. This allows the restoration of the bone shape, makes the implantation easier and has a positive influence on the ingrowth behaviour of the bone. In addition, the ingrowth behaviour can be improved by inserting a specific individual porosity. For this reason, activities for the production and medical approval of bio-scaffolds (lattice structures) made of bone cement, which are produced by means of additive manufacturing (3D printing), are being carried out [2] . It has been proven that these scaffolds are suitable for augmentation in the maxillofacial area. A major obstacle to the use of these scaffolds is the lack of a continuous data workflow. Int J CARS (2020) 15 (Suppl 1):S1-S214 The process steps to be carried out for the production of individual bone replacement structures are integrated into the existing process of dental treatment, since the design of the bone replacement structure depends on the indication of the dental implants (Fig. 1 ). This means that the specific disciplines must be closely interlinked and clear interfaces for the transfer of data and models between the parties involved (physicians, dental technicians and designers) must be established. (1) Planning of the bone replacement structure The planning of the bone replacement by the surgeon before the design is a decisive advantage within the process chain. Planning parameters and planning constraints must be defined for different cases. The cases to be distinguished are the posterior tooth region in the upper jaw (sinus lift), the posterior tooth region in the lower jaw and the anterior tooth region. The geometric designs of the bone replacement structures and the necessary planning data are different for these three cases. In addition, the following required planning steps were identified: • Setting up planning parameters that describe the bone replacement structure (e.g. size) • Set up planning parameters which identify the necessary surgical tools (e.g. cutting position) • Assessment of the dental situation and description in planning data (boundary conditions, definition of metadata) • Setting up planning parameters for the fixation (e.g. axial position of screws) As a result, for each category exist unique planning parameters (e.g. characteristics describing geometry) and planning constraints (e.g. metadata, descriptions). (2) Local data processing (segmentation) For an almost contour-identical positioning of the bone replacement structure, it is necessary to identify the local residual bone surface. For this purpose, two essential methods have been developed which will be evaluated in future investigations: • 2D segmentation of the bone area in a certain amount of image data (e.g. by means of superpixel methods), which represent the area of interest, and subsequent contour derivation • 3D segmentation of the bone area using an Active Shape Model, which has been trained for the three categories mentioned above Furthermore, the generated surface data must be transferred to the applied modelling tool. Here either curves (polylines) or a discrete surface (e.g. STL data) are transferred. (3) Digital design of the bone replacement structure The replacement structure is designed in a modelling tool (e.g. SolidWorks, Geomagic Freeform). For this purpose, a method is currently being developed in order to derive generally valid geometric description forms for the three categories mentioned above for quick adaptation to the respective patient case (recurring geometric elements, configurations) as well as to integrate design elements for necessary functional features (e.g. holes for fixation, holes for handling). The bone replacement structure is characterised by a defined graded porosity, functional integrations and an optional closed membrane (protection against contamination). (4) Design and manufacture of tools for production For the additive manufacturing of bone replacement structures, aids may be required which are digitally generated in advance and then also additive manufactured (e.g. by FDM process). This includes, for example, the production of a support structure (shell) [2] . (5) Manufacture of the bone replacement structure The production of bone replacement structures is carried out by the company INNOTERE (Radebeul, Germany) using a special paste 3D printer [2] . The material is a certified, resorbable bone cement (calcium phosphate), which is also manufactured and distributed by INNOTERE. (6) Design and manufacture of surgery templates For the surgery, the surgeon is provided with a range of individual surgical tools (e.g. templates), which are adapted to the respective patient case. The following tools will be considered: • Cutting templates (display of the cutting area on the bone) • Positioning templates (display the position of the bone replacement structure) • Scanning and drilling templates for dental implantation [1] • Possibly necessary individual surgical instruments Combinations with existing tools from the dental treatment process are also possible (tool combination). After each completed section within the workflow, a review of the results by the parties involved (usually the surgeon) must be performed in accordance with the new Medical Device Regulation (MDR). The process is currently being tested on up to 30 retrospective case studies up to the stage where physical models are created. First results show the general applicability and functionality of the process chain. Figure 2 demonstrates a case study with planning (top left), review (top right) and production results (bottom). Furthermore, it is planned Fig. 1 Complete workflow for the treatment of a patient with an individual bone replacement structure Int J CARS (2020) 15 (Suppl 1):S1-S214 S137 to apply the process to several real patient cases. One patient has already received such clinical care. The presented integrated interdisciplinary process chain enables the fast and precise production of individual bone replacement structures in the dental area as a basis for dental restoration. The methods and materials used are predestined for fast, targeted and reliable patient care. This has already been successfully tested on a specific patient case. In the future, the goal is to further establish and improve the process chain and to apply it to additional retrospective and real patient cases. Intraoral radiography is the most important diagnostic imaging technique in dentistry, and intraoral imaging provides high-resolution images of teeth and jawbones. The ISO standard determined that the size of the # 2 intraoral image is 30 9 40 mm. Because the intraoral image contains up to three or four teeth, multiple intraoral radiographs must be taken to see the wider area of the patient's dental arch. The whole oral series contains 10 or 14 radiographs and is most often done to diagnose periodontitis. In the field of dentistry, it is known that appropriate anatomical knowledge and experience are required to correctly determine which tooth the acquired intraoral image corresponds to. Therefore, the purpose of this study was to examine the feasibility of a convolutional neural network (CNN) for image recognition of such intraoral radiographs. Methods 500 sets of full-mouth intraoral series (5000 radiographs) were prepared for deep learning training and evaluation. The full-mouth intraoral image set consisted of 10 images, which were upper right molar, upper right premolar and canine, upper incisor, upper left canine and premolar, upper left molar, lower left molar, lower left premolar and canine, lower incisor, lower right canine and premolar, and lower right premolar region. Intraoral images of deciduous and edentulous jaws were excluded from the study, Fig. 1 . The matrix size of the original # 2 intraoral image was 649 9 490 pixels, but the size of the input image for deep learning was changed and converted to a 115 9 115 pixels square format. The study was performed using webbased deep learning software (Neural Network Console, Sony Corporation, Japan). Training, validation, and test data ratios were set at 7:1: 2. The accuracy of image recognition was evaluated under the following conditions.  All CNN methods used provided over 90% accuracy. Testing intraoral images that did not upside down resulted in greater accuracy. When testing upside down intraoral images, we observed a high dependence on data volume. Although the overall accuracy was very high, some misrecognitions occurred in the canine and incisor area images. Using the deep learning available to the GPU, to complete the training under the condition that the maximum epoch = 100 and the batch size = 50, VGGNet (9 convolution layers) or ResNet (18 convolution layers) required 30 min of learning time. AlexNet (5 convolutional layers) spent about 20 min to complete the training, while LeNet (2 convolutional layers) took 3 min. Conclusion Neural Network Console software provides a simple operational GUI (Graphical User Interface) for developing and testing deep learning programs. Therefore, various CNN technologies can be applied to image recognition of intraoral radiographs without the expertise of artificial intelligence. For dentists and dental hygienists, properly positioning the oral radiographs throughout the mouth was a time-consuming task. The results of this study suggested that image recognition of intraoral radiographs was not a difficult task for CNN. The development of deep learning image recognition and placement software for intraoral radiography should be meaningful for dental practice. demonstrate the findings on diseases of the chief complaint, but also detect incidental findings. However, the radiopaque structures, including the palate floor, nasal cavity floor and inferior turbinate, overlap to the maxillary sinuses, and therefore, it may be difficult to diagnose maxillary sinus lesions for the inexperienced dentists. Incidental significant findings may be overlooked by busy dentists. The first purpose of the present study was to clarify the detection performance of healthy maxillary sinuses, inflammatory maxillary sinuses, and cysts in maxillary sinus regions on panoramic radiographs using deep learning object detection technique [1] . The second purpose was to clarify the diagnostic performance of maxillary sinus lesions in comparison with healthy maxillary sinuses. Methods Subjects were selected from imaging database our dental hospital. Healthy maxillary sinuses (587 sinuses as Class0), inflammatory maxillary sinuses (416 sinuses as Class1), and cysts of maxillary sinus regions (171 sinuses as Class2) were enrolled in this study. The panoramic radiographs of maxillary sinuses were downloaded in BMP format and cropped in square of 900 9 900 pixels. The labels including class names and the coordinates of the upper left and lower right corners of the unilateral healthy/lesional maxillary sinuses were created in text format per each image. The imaging data and labels were assigned to training, testing 1, and testing 2 datasets shown in Table 1 . Testing 1 was also used for validation. Using deep learning network 'DetectNet' for object detection, the training process for 1000 epochs were performed and a training model was created. The testing 1 and testing 2 datasets were applied to the model, and the detection sensitivities and the false-positive rates per image were calculated. The sensitivities for identifying the inflammation group (Class1) and cyst group (Class2) were calculated as compared with the healthy group (Class0). Results are shown in Table 1 . Detection sensitivities of healthy (Class0) and inflammatory (Class1) maxillary sinuses were 100% for both testing 1 and testing 2 datasets. Those for cysts of the maxillary sinus regions (Class2) were 98% and 89%. False-positive rates per image were 0.00 for healthy (Class0) and inflammatory (Class1) maxillary sinuses. Those for cysts (Class2) were 0.00 and 0.18. Sensitivities for identifying maxillary sinusitis were 88% and 85% for testing 1 and testing 2 datasets, respectively. Those of cysts of the maxillary sinus regions were 80% and 100%. Examples with successful detection and classification are shown in Figs. 1 and 2 . Healthy maxillary sinuses were presented in red, inflammatory maxillary sinuses in light blue, and cysts in green. This study clarified performance of detecting maxillary sinuses and performance of identifying maxillary sinus lesions using deep leaning object detection technique. Detection sensitivities of maxillary sinuses were high and false-positive rates per image were almost 0. Performance of identifying maxillary sinus lesions were over 80%, and especially, those of sinusitis were over 90%. Deep learning could detect and identify maxillary sinusitis and cysts of the maxillary sinus regions.  When performing dental treatment, it is required to write the medical record about the dental treatment content, but it is necessary to take sufficient time for that, and since the medical record is often written after dental treatment, some contents may be forgotten to write down. In order to overcome these problems, it is necessary for someone to grasp the dental treatment contents and point out any omissions [1] . Since this method is not practical in terms of securing personnel, we thought it was practical to do this work by machine. Our current study aimed to examine the possibility of developing the machine which could recognize the contents of dental treatment. Movies capturing the dental treatments are required to develop the machine recognizing the contents of dental treatment. Considering patients' privacy, taking movie of only instruments table for dental treatment as a method of recording dental treatment. Dentists use a variety of instruments depending on the purpose of the procedure. Therefore, we thought that it was possible to estimate what treatment was performed by recording the instruments used. An IoT device consisting of a Raspberry Pi 3 model B and a camera module was prepared to record the movie of the table on which dental instruments were placed (Fig. 1) . Using this device, the table was recorded at full HD 30fps during dental treatment was recorded. Image recognition was performed using YOLOv3 [2] trained with 22 kinds of dental instruments and hand images using stored images. This makes it possible to obtain information on the presence of 22 kinds of dental instruments and hands present on the table where the instrument is placed at a certain point in time, thereby recording dental care as time series data of 23-dimensional vectors. With this method, a record of 49 dental treatments consisting of four types of dental treatment contents (caries treatment, periodontal treatment, root canal treatment, root canal filling) was created. We considered two methods to learn and estimate the dental treatment contents from this record. One is a method of learning and estimating based on the number of times each instruments is used during one dental treatment, and the other is a method of learning and estimating based on the use order of each instruments. Keras (Ver. 2.1.1) and Tensorflow (Ver. 1.1.0) were used as libraries for machine learning. A three-layer neural network was used for learning and estimation based on the number of use of the instruments, and a one-layer LSTM was used for learning and estimation based on the order of using the instruments. Accuracy evaluation was performed by leave-one-out cross validation using 80% as learning data and 20% as verification data. The overall accuracy of dental care content estimation based on the number of instrument use was 61.2%, but the recall rate for caries treatment was 0%. This is considered to be due to variations in the method of using the instrument in caries treatment. The overall accuracy of dental care content estimation based on the instrument use order was 42.9%, but the recall and precision of caries treatment, periodontal treatment, and root canal filling were 0%. After the number of dental treatments was increased to 65 (Table 1) , the recall and precision was improved except for root canal filling, so it was considered that the low rate of accuracy was probably due to the small number of samples. It was suggested that there is possibility of the machine estimating the contents of dental treatment performed by taking movie of the table on which dental instruments are placed.  Dental panoramic radiographs are frequently obtained for dental checkups and diagnosis in dental clinics. Automated analysis of dental conditions on panoramic images can be useful as prescreening, longitudinal comparison and efficient recording of dental charts. In Fig. 1 Schematic diagram of recording method during dental treatment Table 1 Learning and accuracy evaluation of dental instruments time series data (N = 65) S140 Int J CARS (2020) 15 (Suppl 1):S1-S214 addition, there are large numbers of unstructured dental records in the clinics. They should be electronically filed for standardizing the nationwide health record and organizing the information for various purposes such as forensic radiology. In order to process a large volume of data, an automated system is desired. As a first step to the automatic analysis of dental panoramic images, we investigated an individual tooth detection and numbering scheme using a multiclass object detection network. In our previous study, we proposed a cascade method, in which all the teeth were detected as one class using the first network, and the detected teeth were classified as 4 tooth types, including incisors, canines, premolars and molars using the second network. However, some neighboring teeth, such as incisors and canines, are very difficult to distinguish in bounding boxes detected by the first network. It is natural to use the location information and interrelationships between the teeth for tooth type classification. Therefore, in this study, we investigated the use of one network to simultaneously detect and classify teeth. The target classes were 14 types, including central incisors, lateral incisors, canines, first premolars, second premolars, first molars and second molars in upper and lower mandibles separately. The third molars were not included because of the small number of samples. Our proposed detection network is based on the single shot detector (SSD) [1] , which extract features from multiresolution feature maps for detecting multi-sized objects. For improving the multiclass detection, we included side layers, which determines one dimensional vector corresponding to the presence or absence of each tooth as shown in Fig. 1 . This layer is learned in a supervised way and the result is combined back to the network for multiclass tooth detection. The network is trained to minimize the combined losses from the SSD and the side layer. The databased used in this study consists of 100 panoramic radiographs. Because of the small dataset, the proposed method was evaluated by 4-fold cross validation. Using the proposed network, the average correct-class detection accuracy was 92.2%, compared with 90.6% using the original SSD network. Figure 2 shows the result of successful detection. The classification accuracy alone for the detected teeth was 95.5%. The results of the proposed method were superior to the previous cascade method, even with the increased number of class types from 4 to 14 classes. Further improvement can be achieved by optimizing the network and including the post processing method. This study has several limitations in the small database used, images from the single institution, and exclusion of the third molars. For complete dental chart filing, 32-class classification including the third molars is necessary. The proposed method can be useful as the initial step for the automatic recording of the dental charts. Further investigation is needed with the larger dataset from multi-institutions.  Liver cancer is the fourth most common cause of cancer-related death worldwide. Treatment selection depends on tumour characteristics, the severity of underlying liver dysfunction and other comorbidities, but the therapeutic arsenal is relatively wide. If surgery has played a central role until now, alternative strategies can now be employed. Systemic and local chemotherapy, techniques of thermo ablation and, more recently, stereotactic radiotherapy have shown encouraging results and are more and more used currently in combination with surgery. For the sake of intervention planning, liver organs need to be segmented based on pre-operative routinely acquired images. The endovascular procedure planning EndoSize software plans to propose a specific module dedicated to interventional liver procedures, and will integrate our segmentation works. In this context, one of the steps is the pre-operative segmentation of the venous tree based on CT images, in particular the adequate delineation of the hepatic and portal veins. However, the complexity of the vasculature and the low contrast between vessels and surrounding tissues due to low quality acquisitions hinder this task. The method presented in this paper has the advantage to provide a fast, fully automatic and robust venous tree segmentation method. The proposed method follows the pipeline depicted in Fig. 1 . The input data is a 3D computed tomography angiography (CTA) volume, routinely acquired for patients with liver cancer. The portal venous phase CTA was preferred for segmenting the venous tree. In a previous work, we implemented a fully automatic, fast and robust liver segmentation method based on adaptive gray-scaled and active contour approach [1] . Requiring the liver segmentation as input data, the following pipeline consists of 3 main steps. Step 1: The first step consists in obtaining a preprocessed image. A tight volume of interest is cropped around the liver area using the liver segmentation to reduce the computation time. The liver mean value is then computed using the histogram highest peak in order to fill the background after liver mask applying. The volume is then smoothed with a curvature flow filter to highly reduce image noise while preserving vessels edges. The preprocessing last step consists in resampling the volume with 1 mm isotropic voxels. Step 2: Vessel enhancement is a crucial step in this work. This step is based on Hessian-based vesselness filter introduced by Frangi et al. [2] . Considering the computation time and the low variation of hepatic vessels, we choose the single-scale method. A vesselness measure is finally deduced from all eigenvalues of the Hessian. Step 3: The last step of this method consists in defining an adaptive thresholding. The threshold is calculated according to a linear regression model learned from 30 patients based on vessel enhancing result intensity variance: Topt = 23.4 ? 0.11*Variance. The proposed method has been validated on a database of 28 venous portal phase CTA images recovered from 2018 Medical Segmentation Decathlon challenge. The accuracy of the segmentation was assessed by comparing the segmentation results with manual delineations using the Dice coefficient, the MICCAI score and the maximum surface distance. Compared to manual delineations, the proposed segmentation algorithm has demonstrated an average Dice coefficient of 0.47 ± 0.12, a MICCAI score of 45.27 ± 6.32, a maximum surface distance of 6.56 ± 1.25 mm. The low results are partially due to the approximation of the manual delineation which might not be always exact. Besides, the segmentation highly depends on CT injection and liver segmentation accuracy. In terms of computation burden, the liver segmentation pipeline is performed with an average processing time of 4.84 ± 1.21 s and hepatic veins with an average processing time of 4.55 ± 1.26 per CT scan on a standard machine. Even if other approaches based on deep learning recently showed higher Dice and MICCAI scores, computation time was not taken into account in the majority of the cases. Our approach is a good compromise between time and accuracy, and can be introduced into clinical routine. The multi-scale vesselness filter was sensitive in detecting vascular structures at different contrast levels. Considering the balance between performance and computational burden, the single-scale vesselness filter has been used providing a result slightly lower, but reducing by twice the computation time. The optimal parameter settings for the single-scale vesselness filter were obtained with a sigma value of 2.5 mm. Fig. 1 Workflow of hepatic venous tree segmentation algorithm Int J CARS (2020) 15 (Suppl 1):S1-S214 S143 This paper presented a full automatic approach for hepatic venous tree segmentation from portal phase CT images. In combination with liver, arterial tree and tumours segmentation, our overall segmentation workflow allows an accurate visualization of all organs of interest involved in interventional oncology procedures dedicated to liver cancer, Fig. 2 . The next step will include the separation of the intertwined portal and hepatic venous systems in the liver, which is critical for liver intervention planning. Static and moving phantom experiments were conducted to validate the impact of respiratory gated PET/CT scan for texture analysis [1] . Three sphere phantoms of 22, 28, and 37 mm in diameter were placed in a body phantom, and the phantoms were filled with 18 FDG solution. A body phantom (background) was filled with 1028 Bq/ml of 18 FDG, and spheres were filled with homogeneous 18 FDG activity to achieve source-to-background ratios of 10. The body phantom with sphere phantoms was installed into a moving unit and scanned for 10 min by GE Discovery 710 PET/CT equipped with a Varian RPM respiratory gating system. The total translation distances were 0, 10, 20, and 30 mm and a period of oscillation was 4 s. Moving data during CT and PET scan were recorded to generate 10 breathing phases. The CT acquisition were performed using a tube voltage of 120 kVp, automated tube current with a noise index of 35, and rotation time of 0.5 s. Non-gated PET images and respiratory gated PET images were created from list-mode respiratory-gated PET/CT data. Volumes of interest were placed on spheres to measure maximum SUV (SUVmax) and average SUV (SUVavg) of FDG. Texture analyses were applied to FDG voxel distribution in tumors to calculate the heterogeneity indices of skewness and area under curve of cumulative SUV-volume histograms (AUC-CSH), that is, a parameter defined as % of total tumor volume above % threshold of SUVmax. Here, a larger skewness, and smaller AUC-CSH were considered to correspond to a higher heterogeneity. The parameters were compared between static and moving phantom experiments by Wilcoxon's rank sum test. Results Figure 1 shows the relationship between the total translation distances and SUV or the heterogeneity indices. On non-gated PET images, the SUVmax and SUVavg of sphere phantom decreased significantly dependent on the translation distance. In addition, the index indicating heterogeneity changed significantly as the translation distance increased. In contrast, on respiratory-gated PET image, SUV values were maintained regardless of movement. And no significant difference was seen on skewness or AUC-CSH of translation distance was 10 mm. Int J CARS (2020) 15 (Suppl 1):S1-S214 This preclinical study indicates respiratory-gated PET/CT images have the potential to reduce the uncertainty caused by respiratory movement for texture analysis. On the other hand, there is a limit of correction. The respiratory-gated PET/CT can be used for texture analysis when total translation within 10 mm. Purpose Digital Subtraction Angiography (DSA) is image subtraction processing where subtraction of mask image(s) from live images. In DSA processing, relatively large motion artifacts will occur when organ and/or body moved during X-ray exposure. Motion artifacts in DSA image deteriorate blood vessel visibility in DSA images. There are many reports for reduction of motion artifacts of DSA images. ''Semiautomatic pixel shift technique'' has been used in clinical situation. However, this method cannot process in real time when images have large motion artifacts. The radiological technologists will take several hours for image processing per case to processing ''semi-automatic pixel shift''. Therefore, DSA was not applied for coronal artery because of its frequently motion due to heart beating and breathing. However, DSA image itself will be useful for also coronary artery. Because, coronary arteries will clearly visible in DSA images, whereas background structures such as bone is not visible. We have developed DSA technique for coronary artery using deep learning technique, where generate mask images from live images [1] . Our method is able to apply DSA even if large motion of organ is present. To improve image quality of our DSA method, semantic segmentation model will be useful. In addition, a method to generate DSA images directly from live images will lead to generation of highquality DSA images, because it not includes subtraction process. For training the semantic segmentation model, live image and DSA image pairs are required. However, high quality DSA images of coronary artery cannot be obtain by conventional DSA, because coronary arteries are always moving with heart beating during X-ray exposure. Therefore, we employed ''transfer learning'', where using the model trained by another dataset. The purpose of this study is to develop new DSA technique for coronary artery where DSA images were generated directly from live images without subtraction using transfer learning of deep learning technique. U-Net based model was developed and used in this study. This study has been approved by the Ethics Review Board. In this study, we used PC with GPU (NVIDIA TITAN V) and Chainer 6.4 (Preferred Networks, Tokyo) as a deep learning framework. We made deep learning model by modifying U-Net [2] for generating DSA images directly from angiographic live images as shown in Fig. 1 . For training the model, we employed three types of dataset, which was consisted of ten abdomen angiograph series, ten head angiograph series, and mixed dataset involving both ten abdomen series and ten head series. Each image in dataset was divided into image patches of 64 9 64 pixels for efficient calculation. The number of patches of abdomen angiograph series, head angiograph series and mixed data set were 2250, 1845 and 4095. We trained U-Net based model using angiographic images as input data and the DSA image as teaching data. Each training was performed until epoch was 5000. As results, we obtained three types of trained model. Using trained models mentioned above, we generated coronary DSA image by inputting coronary angiographic image data. Consequently, three coronary DSA series were obtained. Image evaluation by comparing to conventional DSA of coronary artery was carried out subjectively by four radiological technologists. The experimental results demonstrated that coronary arteries were clearly visualized in DSA images generated by inferencing using each model with extremely low-level motion artifact compared to conventional DSA. As results of subjective image evaluation, coronary DSA images generated from modified U-Net models trained by the mixed abdomen and head angiograph series showed better evaluation results than that trained by abdomen angiograph series as shown in Fig. 2 . We developed advanced DSA method for coronary arteries with low motion artifacts (Fig. 1 ) using transfer learning of deep learning technique. These results indicate that angiographic images including both soft tissue area of body such as abdomen angiographs and bone area such as head angiographs will be better than that of only abdomen angiographs and head angiographs as the training dataset for coronary DSA based on transfer learning. Purpose During liver surgery, successful delivery of treatment depends on a comprehensive understanding of the spatial relationships between interventional targets, hepatic vessels, and surgical instruments. Image guidance aims to localize these components by registering information from preoperative imaging with the intraoperative anatomy of the patient. However, accurate registration of subsurface structures remains challenging due to organ deformation that compromises the fidelity of image-to-physical registration. To compensate, many liver registration techniques rely on the shape of the organ surface to predict the underlying state of deformation. However, the visible extent of surface coverage is often limited and can be insufficient for achieving accurate registrations throughout the depth of the liver. Tracked intraoperative ultrasound (iUS) can extend the coverage of intraoperative data available for registration; however, limitations in lesion detection, interpretability, and workflow encumber iUS as the principal means of intraoperative guidance. In this work, we evaluate how information from very sparse iUS data can be applied to best improve the performance of deformable liver registration by examining and comparing iUS feature constraints visible within individual iUS planes. Methods A simulation study was performed to compare registrations to subsurface features from 16 orientations of 2D tracked iUS imaging data. Models of the liver parenchyma and portal and hepatic veins were created from a contrast-enhanced preoperative CT of a deidentified human patient. A linear elastic finite element model was then used to deform the liver to a known intraoperative organ presentation using the data generative method from [1] . Intraoperative features from tracked iUS were simulated by intersecting 16 potential image plane orientations with the ground truth deformed model. In each iUS image plane, these features described the posterior surface of the liver typically visible in iUS and intrahepatic vessel contours with associated centerline positions approximated by feature centroids. Additionally, sparse anterior surface data were derived from a clinical pattern of digitization on the ground truth model for registration purposes. Figure 1 shows one of the 16 configurations of data. The linearized iterative boundary reconstruction method [1] was used for deformable registration of the original model to simulated data. While [1] aimed to understand how data from multiple iUS planes could be combined to improve registration accuracy, the focus of this work is to characterize how incorporating distinct feature constraints from a single iUS plane can impact registration accuracy. For each of the 16 iUS views, target registration error (TRE) was evaluated to determine: (I) whether informational redundancy might exist between vessel and posterior surface features when they are derived from the same image plane, (II) the potential need for manually defining correspondence of iUS vessel features to the correct branch of the 3D vessel model, and (III) the tradeoffs between centerline and contour representations of vessel features. TRE was computed as the average nodal distance between the ground truth and registered mesh, and differences in average TRE were statistically Int J CARS (2020) 15 (Suppl 1):S1-S214 tested against zero mean using a two-tailed, one-sample t-test at significance a = 0.05. Results (I): Close spatial proximity of features within an iUS image plane may suggest that registering to only a subset of these features might be necessary for accurate alignment. To test this possibility, average TRE was computed for the 16 iUS registration scenarios in three conditions: using only vessel contour features (8.6 ± 1.8 mm), using only posterior surface contours (8.9 ± 1.7 mm), and using both posterior and vessel contours (7.6 ± 1.9 mm). Significant improvements in average TRE were observed when using both features, with average improvement of 1.0 ± 1.4 mm (p = 0.01) compared to using only vessels and 1.3 ± 1.5 mm (p = 0.005) compared to using only posterior, suggesting that vessel and posterior surface features visible in iUS offer partially independent constraints best leveraged together to improve registration accuracy. Overall distributions of the differences in TRE are shown in the first two columns of Fig. 2 . The largest improvement in average TRE across the mesh was 5.0 mm when using both features instead of a single feature in one of the 16 configurations of data, decreasing TRE from 11.4 ± 7.1 mm to 6.4 ± 4.5 mm. Reciprocally, it is possible that posterior surface data may help direct vessel features towards the correct branch of the vessel model, and vessel data may assist with regularizing the posterior constraint that can traverse across the rear surface of the liver. (II): During registration, correspondence between iUS vessel features and the full 3D vessel model can be estimated by closest point distance. However, these correspondences can become incorrect when features are located near bifurcations or in regions with large deformation compared to the inter-branch distances of the vessel tree. In these cases, directing a skilled sonographer to manually label the vessel branch of correspondence during data collection can ensure correct alignment between the iUS feature and the model. With manual designation of centerline branches, average TRE improved by 0.4 ± 1.2 mm over unconstrained closest point correspondences (Fig. 2 third column) , from 8.0 ± 2.2 mm to 7.6 ± 1.9 mm. While this difference was not statistically significant (p = 0.17), in one case the manual constraint improved TRE from 12.5 ± 6.1 mm to 7.8 ± 4.4 mm when a vessel feature had initially been misregistered to an incorrect branch. Steps to address vascular feature correspondence between 2D iUS planes and 3D models therefore may improve robustness of registration algorithms. (III): Whereas vessel centerline representations have reduced dimensionality that may smooth the optimization landscape and obscure physiological changes to vasculature during registration, centerline approximations from iUS image planes may not be accurate when vessels are imaged obliquely or near bifurcations. Additionally, information encoding the orientation of the vessel is lost. Although registration using vessel contours may avoid these shortcomings, the apparent diameter of the intrahepatic blood vessels may change based on segmentation, pulsatility, and vasoregulation. To compare, differences in average TRE were computed for registrations to vessel contour and centerline approximations of the portal and hepatic vein features in the simulated iUS planes. No significant difference in average TRE was found between using the unlabeled contour or the correspondence-labeled centerline representations of vessels (p = 0.99), with an average difference of 0.0 ± 0.7 mm (Fig. 2 fourth column) . While this comparison was performed using simulated data free from noise, it is possible that combining contour and centerline feature constraints in more realistic registration scenarios may complement the shortcomings of each and improve overall registration robustness. Conclusion Feature constraints from tracked iUS were investigated for application in image-to-physical liver registration. Significant improvement in average TRE was found when registering to a combination of hepatic vessel features and posterior surface features detectable within single iUS images. Non-significant improvement was found when model-data correspondences of vessel features were manually constrained, and no difference in average TRE was found between registrations to unconstrained contour and manually constrained centerline representations of vessel features.  Medical images stored in hospitals'' picture archiving and communication systems (PACS) are unsorted and lack semantic annotations such as types of patients, scanned body parts, and field-of-view (FOV), thus are hard to use directly in machine learning studies. Our group has been constructing a data cloud that exhaustively collects all CT scans obtained in routine practice every day from PACS of multiple hospitals in Japan. The cloud contains more than 300 K examinations (as of January 2020) each of which has multiple series of CT scans and a radiology report. The purpose of this study was to develop a pipeline using deep neural networks (DNNs) to organize the significantly heterogeneous (i.e., including multiple scanners, diverse patient cohorts, target body parts, scan protocols, etc.) CT database and extract knowledge about human anatomy. This study specifically targeted understanding of musculoskeletal anatomy in the pelvis region that is important, for example, in the analysis of body posture changes as a natural consequence of aging. Fig. 2 Quartile distributions of the change in average TRE comparing iUS feature constraints across 16 registrations to simulated data (*p = 0.01; **p = 0.005) Figure 1 shows the proposed pipeline. An unsupervised clustering using t-SNE and k-means was applied to digitally reconstructed radiograph (DRR) of each CT to understand the overall distribution of the scanned anatomy. The clustering on 2D projection images instead of the original 3D volumes helped reduce the amount of data while keeping information sufficient for detecting the FOV, which also helped the human observer to look through a large number of 3D volumes quickly. We selected 1,000 CT volumes that contain the pelvis region and manually drew bounding boxes of the pelvis and proximal femur on the DRR, which were used to train an object detection network, YOLOv3 [1] . The trained pelvis and proximal femur detector was applied to all the CT volumes in the database and the pelvis regions were extracted. Then, a separately trained landmark detection and semantic segmentation networks were applied to detect seven landmarks in the pelvis and segment three bones and nine-teen muscle structures. A previously proposed Bayesian U-net [2] trained by 20 manually annotated CTs was used for the segmentation. From the landmarks and segmentation labels, four parameters determining the pelvis anatomy, namely anterior pelvic plane (APP) angle, pelvic incidence (PI), sacral slope (SS), pelvic tilt (PT), and the volume of each muscle and average bone density of the pelvis and sacrum were automatically calculated on all the pelvis CTs. Out of more than 300 K examinations, we queried CT volumes with slice thickness less than 3 mm and the total number of slices is more than 100, which resulted in 85,963 CTs. The pelvis and proximal femur detector selected 12,523 CTs containing the entire pelvis region without having the hip implant (about 1-2% had the implant). Computation time for the landmark detection and segmentation was approximately 3 min for each CT on NVIDIA DGX-1 server with Tesla P100. The process was parallelized on three servers with 24 GPUs in total, which amount to about 30 h for the processing of all 12,523 CTs. Figure 2 shows the result of the analysis of APP angle as a function of patients'' age and sex, which revealed a trend of the posterior inclination of pelvis after 50's in both male and female and that the inclination angle of male and female switch around at the age of 80's. Conclusion A pipeline of DNN models was proposed for organizing an unsorted heterogeneous large-scale data set to mine knowledge about the musculoskeletal anatomy. We showed that the pipeline resulted in a dataset of 12,523 well-annotated pelvis CTs which is valuable in analysis of the change of the musculoskeletal anatomy with aging and sex difference. A straightforward future work includes application to other anatomies such as the spine for the analysis of aging in whole body posture. A more detailed shape analysis using the pelvis CTs and a non-rigid registration based atlas creation is also underway. In a clinical environment, physicians conduct further medical examinations when identifying disease is difficult from medical images; thus, reliable clinical decision support systems with medical bigdata, such as via medical image/report analysis and medical image report generation, could prevent unnecessary examinations. In this regard, a big challenge lies in evaluating physicians' reports despite inter-observer variability. Unfortunately, especially in Japan, overcoming each hospital's different ethical codes and report-writing styles is an urgent issue to share multi-institutional medical images and reports. Therefore, we firstly propose to adopt few-shot learning with Bidirectional Encoder Representations from Transformers (BERT) [1] for automatic binary anomaly classification from Japanese multi-institutional CT scan reports. Contributions. Our main contributions are as follows: -Few-Shot Learning We firstly propose to adopt BERT-based fewshot learning for automatic medical record evaluation. -Multi-Institutional Medical Datasets We firstly show that pretraining on large-scale sentences can perform accurate classification with a limited number of multi-institutional medical records for training. We use a CT scan report dataset written in Japanese, collected by the authors from six Japanese hospitals; those reports are written from various CT scans, including chest/abdomen/head CT scans, with various anomalies, such as lymphadenopathy, hepatocellular carcinoma, and chronic pancreatitis. For few-shot learning, our dataset is divided into: (i) a training set (400 reports); (ii) a validation set (20 Fig. 1 Proposed pipeline using an unsupervised clustering (t-SNE and k-means), YOLO, and landmark detection and segmentation networks Fig. 2 Histogram of the patients with the pelvis CTs extracted by the proposed pipeline and the plots of the anterior pelvic plane angle as a function of patients'' age and sex. Note that the age was binned at 10-year interval and each box plot contains different number of patients S148 Int J CARS (2020) 15 (Suppl 1):S1-S214 reports); (iii) a test set (100 reports). During training, reports with/ without anomalies are labeled 1/0, respectively. A pathological-tohealthy (i.e., label 1-to-label 0) ratio is 1 : 1. For automatic binary anomaly classification, we adopt BERT-Japanese, pre-trained bidirectional/unsupervised language representation model for Japanese (https://github.com/cl-tohoku/bert-japanese); thanks to its pre-training on millions of sentences from Japanese Wikipedia, it can correctly classify anomalies under limited training data. During training, we use a batch size of 32 and 5.0 9 10 -4 learning rate for Adam optimizer. We train the model for 400 steps while adopting parameters at the lowest validation loss for testing. As a baseline, we Support Vector Machine (SVM) with Bag-of-Words (BoW) features. For SVM's kernel, we adopt RBF kernel. We omit frequent words appearing over 50% in the training set for the BoW and apply Latent Semantic Indexing (LSI) [2] for dimension reduction; thus we obtain a 30-dimensional feature vector. Results Table 1 shows the anomaly classification results. Although our CT scan reports are written by multiple physicians from six Japanese hospitals, few-shot learning with BERT achieves 84.0% accuracy (sensitivity: 90.3% and specificity: 75.0%) thanks to the representation learned by pre-training. Meanwhile, BoW-SVM produces high sensitivity in return for poor specificity. The BERT's such high sensitivity with moderate specificity, achieved by focusing on anomaly-related words, can alleviate the risk of overlooking the diagnosis. Figure 1 shows its example attention map, where words with higher attention weights are more highlighted in red. It implies that the BERT pays more attention to specific words for identifying anomalies similarly to physicians: (i.e., present in Japanese) has the highest attention weights under label 1; meanwhile, (i.e., absent in Japanese) draws the highest attention under label 0. Conclusion BERT-Japanese achieves 84.0% accuracy with 90.3% sensitivity in automatic anomaly classification from Japanese CT scan reports with various report-writing styles despite only 400 reports for training; overcoming the inter-observer variability, it can shed light both on reliable medical report evaluation/generation for preventing unnecessary examinations. Obtained attention map implies its physicianlike ability to interpret the medical reports for identifying anomalies. As future work, we plan to investigate the attention map on different training set sizes since the current one mainly focuses on specific words (e.g., and ). We would also significantly increase train-ing/validation/test set size for robust classification-we currently possess over 300,000 unlabeled CT scan reports from six Japanese hospitals. For automatic medical report summarization, we would extend this work to identify disease types written in the reports. Tumor vascularity, an important factor correlated with tumor malignancy, would be used to evaluate the effect of the neo-adjuvant chemotherapy prior to surgery. High-definition flow (HDF) Doppler ultrasound was performed to investigate blood flow and solid directional flow information in breast tumors [1] . In this study, 3D brightness mode (B-mode) and HDF power Doppler ultrasound imaging was extracted as early predictors for evaluate chemotherapy effects. Firstly, this study utilized tumor area from the B-mode ultrasound imaging to crop the corresponding vascular volume-ofinterest (VOI) on HDF power Doppler ultrasound imaging. Then a multi-view convolutional neural network (MV-CNN) extracted vascular characteristics and employed as a tumor response predictor to neo-adjuvant chemotherapy for breast cancer. Data acquisition 76 consecutive T2 breast cancer (tumor size [ 2 cm and B 5 cm) patients, who received neo-adjuvant chemotherapy were recruited for this study. The diagnosis of breast cancer was made by core needle biopsy. Pre-operative intravenous chemotherapy was given for six courses in each patient and 3 weeks per cycle. Epirubicin (Pharmorubicin, Pfizer Pharmaceuticals, New York City, NY, USA) 80-90 mg/m 2 , cyclophosphamide 500 mg/m 2 and 5-Fluorouracil 500 mg/m 2 on day one every 3 weeks. Sonographic examinations were done (period N1-N6) by using 3D power Doppler ultrasound with the HDF function (Voluson 730, GE Medical Systems, Zipf, Austria, equipped with RSP 6-12 transducer). The period N0 was the sonographic before the chemotherapy. Vascular feature extraction Figure 1 shows an example of vascularchanging process (stages N0-N3) on HDFDoppler ultrasound imaging.This study cropped the vascular VOI from the tumor area andshell outside thickness 3 mm surrounding the breast lesion. However, the 3D convolutional neural network could make full use of the spatial 3D context information in vascular VOI. The multi-view strategy had been shown to be useful for improving the classification performance of 2D CNN [2] . This study performed the MV-CNN to evaluate the neo-adjuvant chemotherapy response of breast tumor. The proposed method using the 3D MV-CNN with both chain architecture and directed acyclic graph architecture, including 3D Inception and 3D Inception-ResNet. All networks employ the multi-view-one-network strategy, as shown in Fig. 2 . The chemotherapy treatment effect of the 76 patients was evaluated by the clinical tumor response. The clinical tumor response was classified as complete response (CR), partial response (PR), stable disease (SD) or progressive disease (PD). The patients were classified into two groups: the responder, who was classified as CR or PR, and the non-responder, who was classified as SD or PD. Among the patients, 57 were responders to chemotherapy, 19 remained stable in their disease or progressive disease. This study utilized the MV-CNN model to conduct each patient a binary classification (responder and non-responder) on HDF Doppler ultrasound imaging of breast cancer. In this study, the simulation evaluated the vascularity morphology indices from both the responder and non-responder groups. The k-fold cross-validation method was used to estimate the performance of the proposed method (k = 5). For the N0-stage (before the chemotherapy), the accuracy of the proposed response predictor is 65.8%. For the N1-stage (first cycle of chemotherapy), the accuracy of the proposed response predictor is 82.9%. Moreover, with the N2-stage (second cycle of chemotherapy) the proposed response predictor performs that the overall accuracy is 73.7%. This study evaluated vascularity morphology from the cases of the neo-adjuvant chemotherapy using 3D HDF Doppler ultrasound imaging. Experimental results demonstrated that the feasibility of the proposed system for physicians to prognosis the effect of chemotherapy treatments. Early prediction of the effect of chemotherapy treatment could diminish the unnecessary chemotherapy treatments for patients. This is helpful for patients to suffer less pain and spend less money by terminating subsequent useless chemotherapy. Cardiac Magnetic Resonance (MR) Imaging is widely applied for the diagnosis and follow up of cardiovascular diseases. MR images provide relevant information about the structures within the heart and allow evaluating its functioning. Particularly, in patients with Pulmonary Hypertension (PH) MR images aid detecting right ventricle (RV) hypertrophy, which is a specific sign that characterizes the disease. PH related to left heart disease is the form that accounts for most of the cases. Hence, a previous segmentation of the cardiac ventricles is essential to extract imaging biomarkers that help better characterizing PH. Lately, Convolutional Neural Networks (CNNs) based on the U-Net architecture have shown to improve the results of previous approaches for accurate cardiac ventricle segmentation, specially left ventricle (LV) segmentation, where researchers have mainly used 2D approaches. Yet, the performance of automatic RV segmentation techniques is still poor, due to the low contrast between the ventricle and neighbouring structures, and the complex shape and irregular wall of the ventricle. Thus, in this study we aim at comparing different approaches to segment both cardiac ventricles using 3D CNNs. We propose two strategies: (1) train one model for the segmentation of each ventricle separately, and (2) train a model to segment both ventricles at once. The experiments have been carried out using cine MR images provided by CIC BiomaGUNE obtained from 6 different pigs (25-35 kg Sus socrofa domesticus) used for a study related to PH secondary to Int J CARS (2020) 15 (Suppl 1):S1-S214 acute lung injury. For each subject, images were acquired in several conditions: normal condition, after inducing acute respiratory distress syndrome and up to three different positive end-expiratory pressure conditions. Ground truth annotations for both ventricles were semiautomatically obtained using the Segment software developed by Medviso. In total, we have worked with 26 cine images and divided them in 18 volumes for training (4 pigs) and 8 for testing (2 pigs). Cine MR images have been split according to the trigger time, resulting in 1300 volumes of size 320 9 320 9 15 (900 for training and 400 for testing). We have resized all the images to 160 9 160 9 15 to decrease the computational cost and we have applied rotations and translations to increase the number of input images to train the network. Network architecture and training We have designed a modified 3D U-Net architecture based on [1] . The U-Net has the advantage of combining low-level features with high-level features precisely locating the objective structure. The network is composed of a contractive path followed by an expansive path, where features at different levels are combined using concatenation. Our main adaption to the network is the computation of max pooling operations with different kernel sizes. As our images only have 15 slices, we first employ a kernel size of 1 9 2 9 2 and then size 2 9 2 9 2, to avoid an excessive dimension reduction. We propose the use of an active contour-based loss function [2] that considers the geometrical information of the areas of interest. The initial learning rate is set to 0.01, which is reduced by a factor 0.2 when the validation loss stops improving. We have trained the network during 40 epochs using a batch size of 2 images and the Adam optimizer. We train the network three times with the same images but with different ground truth annotations, creating three models: (1) LV segmentation model, (2) RV segmentation model and (3) joint LV and RV segmentation model. We evaluate the performance of each one of the models with 400 volumes coming from 8 cine MR images split according to the trigger time. Despite having only two pigs for testing, the images are different as they were acquired in different pig conditions. Table 1 summarizes the obtained Dice and Jaccard coefficients in each experiment. Although the difference is small, the results suggest that specific models are more accurate than the joint LV and RV segmentation model, which seems to imply that segmenting the ventricles together does not contribute to improve the localization and limits of the structures. The positive aspect of model 3 is the efficiency when needing fast segmentations for both ventricles. These results could be refined with more accurate ground truth annotations including more slices in which the ventricles are not as clearly observed and have not been included in the ground truth. This causes our three models to over-segment as compared to the annotations, since they can detect the shape of the ventricles in more slices. This over-segmentation is illustrated in Fig. 1 . Omitting the over-segmented slices, the dice coefficients for the LV model and RV model improve up to 0.94 and 0.89, respectively. The accuracy for the right ventricle is especially high comparing it to other automatic approaches in the literature. Figure 2 shows a single slice comparing ground truth annotations to the predicted annotations. Conclusion Hereby, we have proposed three 3D CNN models based on the U-Net architecture to automatically segment cardiac ventricles. The study has shown that specific models for each ventricle have a higher accuracy than the joint one. Moreover, the proposed architecture together with the active contour-based loss function seems to outperform previous RV segmentation approaches with a dice score of 0.89, but this should be confirmed with more datasets. As future work, we would like to improve our models by training them with more images and corrected ground truth annotations to Table 1 summarizes the obtained Dice and Jaccard coefficients in each experiment Keywords Segmentation, Deep learning, Liver, 3D print Purpose Computer assisted liver surgery has been performed to enhance understanding of patient specific anatomical structures. Several research groups have developed surgical simulation and navigation systems for computer assisted liver surgery. Surgeon can perform preoperative surgical planning and intraoperative surgical navigation of the liver resections while observing the 3D reconstructed patient specific anatomical structures from CT volumes. A 3D printed liver model is also used for preoperative and intraoperative assistances [1] . In order to perform the preoperative surgical planning and the intraoperative surgical navigation, these assistance systems often requires the patient specific anatomical structures extracted from CT volumes. If anatomical structures in CT volumes can be recognized automatically, the burden of the extraction tasks can be reduced in these simulation and navigation. The purpose of this study is to extract anatomical structures in the liver from CT volumes automatically. In the liver resections, information on the portal and hepatic veins in the liver is very important. Therefore, we focus on the extraction of these blood vessel regions from CT images. Several research groups has proposed the multi-organ segmentation method from CT volumes using fully convolutional networks (FCN) and achieved high extraction accuracy. We have also proposed the multi-organ regions segmentation method using 3D U-Net which is one of the FCNs [2] . This method extracted seven organs in abdominal CT volumes. However, this method could not extract the portal and hepatic veins in the liver. In this paper, we try to extract the blood vessel regions, the portal and hepatic veins, in the liver using 3D U-Net based segmentation method. The proposed method extracts three anatomical structures, liver, portal vein, and hepatic vein, from contrast-enhanced CT volumes. The proposed method consists of two extraction processes. The first process extracts the liver region in CT volume and the second process extracts the portal and hepatic veins in the liver. We use 3D U-Net for both the liver and the blood vessels segmentations. In the previous segmentation method, 3D U-Net is trained by a large number of abdominal CT volumes with labels of multi-organ regions including the liver. Therefore, we extract the liver regions from input CT volume using the previously trained 3D U-Net in the first process. In the second process, we restrict processing area in CT volume to the liver region for reducing false positive regions outside of the liver. This liver region is obtained by enlarging the extracted liver region in the first process using dilation operation. Input volume in the second process is created by clopping input CT volume using bounding box of the dilated liver regions. We extract the portal and hepatic veins using 3D U-Net from this clopped volume. This 3D U-Net is obtained by performing fine-tuning from the previously trained 3D U-Net in the multi-organ regions segmentation method. In the fine-tuning, we use small number of contrast-enhanced CT volumes with labels of liver, portal vein, and hepatic vein. We also perform data augmentation using translation, rotation, and free-from deformation using B-spline in the fine-tuning to increase training data. We extract the portal and hepatic vein regions in the liver using the fine-tuned 3D U-Net. We extracted liver, portal vein, and hepatic vein regions from portal vinous phase contrast-enhanced CT volumes using the proposed method. The proposed 3D U-Net segmentation method was implemented using Keras. The previously proposed 3D U-Net for multiorgan regions segmentation was trained by 340 CT volumes. The proposed 3D U-Net for extracting the portal and hepatic vein regions was fine-tuned by 20 CT volumes. We evaluated segmentation accuracy using 5 CT volumes. A mean dice coefficient of the liver, the portal vein, and the hepatic vein were 0.91, 0.78 and 0.77, simultaneously. Examples of extraction results are shown in Fig. 1 . Right, middle, and left figures indicate volume rendered images of the liver, the portal vein, and the hepatic vein regions extracted by the proposed method, simultaneously. As an example of application to surgical assistance, we fabricated a 3D printed model of the liver using the extraction anatomical structures and a 3D printer (Agilista, Keyence, Osaka, Japan). Figure 2 shows the 3D printed liver model. As shown in these figure, the proposed method could extract the portal and hepatic veins in the liver. Therefore, the proposed method is useful for computer assisted liver surgery. In this paper, we described a method for extracting liver, portal vein, and hepatic vein in the liver from CT volumes. The proposed method extracts these blood vessel regions based on 3D U-Net which is one of the fully convolutional networks. The experimental results showed that the proposed method could extract the portal and hepatic veins in the liver from portal venous contrast-enhanced CT volumes. Future works includes evaluation using a large number of CT volumes and improvement of segmentation accuracy. Purpose Twin-to-Twin Transfusion Syndrome (TTTS) is a serious condition that occurs in about 10-15% of monochorionic (shared placenta) twin pregnancies. In most cases, blood is unevenly distributed between the fetuses through vascular connections (anastomoses) in the surface of the placenta. This leads to neurologic, cardiac, and pulmonary sequelae, which can become fatal if left untreated [1] . Fetoscopic laser photocoagulation consists on closing the anastomoses located on the placenta surface that connect both fetuses to restore blood flow equilibrium. Ultrasound (US) imaging is the standard diagnosis tool for TTTS due to its low cost and rapid acquisition. Power Doppler US can be employed to track down and assess blood flow in the placenta and the umbilical vein and arteries. In fact, most fetal interventions are USguided. In this context, the segmentation of the placenta, its vasculature and the umbilical cord insertion points could significantly improve TTTS laser photocoagulation outcome [1] . Specifically, the localization of these intrauterine anatomies could help clinicians plan the best fetoscope insertion point and reach all connected vessels. In this work, we implement a GPU-accelerated random walker (RW) algorithm to detect the placenta, both umbilical cord insertions and the placental vasculature from power Doppler US volumes. Furthermore, we integrate our methodology into a MITK-based plugin to improve TTTS treatment planning in daily clinical practice. The RW algorithm is a semi-automatic multi-class segmentation method that, given the input image and a prior containing some labeled pixels, propagates the labels by weighting pixel distance and intensity of the input image [2] . In this work, we segment the aforementioned intrauterine tissues using a real-time implementation of the RW algorithm (see Fig. 1 ). The proposed workflow consists of two RW iterations, which aim to reduce user interaction without compromising accuracy. Firstly, a prior is generated through a manual labelling of both placenta and background in 10-20 slices (out of 245), which is combined with an Otsu thresholding of the placental vasculature. Using this prior, the RW algorithm is executed to obtain a rough approximation of the placenta volume in all slices. Secondly, this approximated volume is combined again with the Otsu segmentation of the vessels to generate another prior. Finally, the RW is computed using this new prior to obtain the refined segmentation. The method has been implemented in C ??, ITK and Eigen3. However, the RW algorithm has to solve a large sparse linear system which is the bottleneck of the whole pipeline. Therefore, a GPUaccelerated approach has been developed using CUDA. Since the final objective is to design a clinical tool to help improving TTTS treatment planning, the algorithm has been integrated into a flexible MITK-based graphical user interface (see Fig. 2 ). Our semi-automatic approach was tested on 5 monochorionic and 24 singleton pregnancies. The reliability of the segmentation workflow was analyzed by computing the inter-subject variability between two expert clinicians and one non-clinician. We report the detection time and compare both CPU and GPU implementations of the RW algorithm. According to clinicians, visual inspection revealed that placenta surface was accurately segmented, and 75% and 70% of umbilical cords were detected in singleton and monochorionic pregnancies, respectively. Although satisfactory performance was achieved for vasculature segmentation, some anastomoses were still neglected due to the presence of US reverberation artifacts. Placenta volume was well-defined in all patients, but some boundaries with small texture gradients led to considerable discrepancies among users (Dice C 78.7%). However, a high interuser similarity (Dice C 96.5%) was attained for vasculature segmentation. The labelling of the RW priors took between 3-4 and 5-6 min for both clinical and non-clinical staff, respectively. While CPUs required from 5 to 10 min to obtain a faithful segmentation, our GPUaccelerated code only needed less than a minute (109 faster). The proposed semi-automatic method provides a real-time user experience, and requires short user training without compromising segmentation accuracy. Our GPU-accelerated code is publicly available along with a comfortable graphical user interface to be used among doctors with different levels of expertise. The MITK-based plugin allows for fast (approx. 45 s) and precise (Dice greater than 78.7%) segmentation of the placenta, its vasculature and both umbilical cord insertions. Hence, our RW methodology could be potentially used for TTTS treatment planning and other fetal diseases and pathologies. Int J CARS (2020) 15 (Suppl 1):S1-S214 S153 Purpose Segmentation of small anatomical structures (like airways and pulmonary vessels) is a highly unbalanced problem that poses a main challenge for accurate performance of deep learning methods. Unlike classification problems, segmentation balancing cannot always be approached using a suitable selection of the training samples. An alternative to sample selection is to formulate a loss function mitigating class imbalance [1] . Current approaches relay on a weighted multiclass loss with weights computed according to the frequency of each class in the training set. Losses with constant weights computed from the whole training population might be inappropriate in case of processing 3D volumes by either patches or slices, since it is not guaranteed that they contain all classes in the same proportion as they are in the whole population. In this work, we propose a weighted average of each class loss with weights computed for each sample in the training set in a multiorgan approach. To further alleviate class imbalance, organs are grouped into classes according to their geometrical type: spherical, tubular, cylindrical or unstructured. Geometrical classes are split into organs according to their appearance and texture using convolution with a bank of filters in a post-filtering step. Our approach has been tested in a UNet architecture for the segmentation of lung structures in CT scans. Fig. 2 Our RW graphical user interface (GUI). The developed RW segmentation plugin appears (right) allows users to select the input Doppler US volume, the labels (priors), the beta parameter and the CPU/GPU computation engine Fig. 1 Multi-class segmentation workflow using two RW iterations. The first prior performs a manual labelling of the placenta and background, and utilizes the Otsu thresholding for the vasculature. The second prior combines the first output with the Otsu thresholding The loss function we propose to tackle with multi-organ unbalanced segmentations is a weighted average of the DICE score computed for each organ with weights adapted to each sample in the training set. The DICE score is a gold-standard measure of volume overlap between a binary segmentation mask, namely Seg, and a ground truth mask, namely GT, given by: In a multi-organ segmentation problem with c = 1, …, N organs to be segmented, let Seg i c , GT i c be, respectively, the segmentation and ground truth masks of organ c for the i-th sample in the training set. Then, we define its weighted multi-organ DICE score as: being P i t is the total number of pixels/voxels (of all the classes) in the i-th training sample and P i c is the number of pixels/voxels in the i-th sample that belong to the class c. The loss function that we propose is formulated as: In order to further mitigate imbalance, instead of considering each organ as a class, they are grouped into according to the topological type of its anatomical shape into 4 classes: spherical, cylindrical, tubular and unstructured. Organs belonging to the same topological class can be separated in a post-processing filtering step using the convolution to a bank of filters and morphological operations. Our weighted loss (labelled Uadapt) has been tested to segment pulmonary structures in CT scans using a simplified 2D UNet, illustrated in Fig. 1 . Lung structures were labeled as cylinder (body), sphere (lungs), tube (vessels and bronchi) and an unknown unstructured class for the background. Vessels were separated from bronchi in the tube class using the tubular filtering for vessel detection described in [2] . The CT scans were acquired at Hospital de Bellvitge from 21 patients (with 4 CTs for each patient) [2] , available at http://iam.cvc.uab.es/downloads/. For comparison purposes we also trained the same UNet with multi-organ (body, lung, bronchi, vessels and unknown) loss with constant weights, labelled Uconst, and the weights adaptation described in [1] , labelled USudra. All networks were trained from scratch using 2D slices uniformly sampled from the volumes of 14 patients during 450 epochs. The 4 scans of the remaining 7 patients (28 cases in total) were used for testing. Testing volumes were processed slice by slice to obtain a 3D volume and 3D segmentations were assessed using precision, recall and dice. Ground truth was defined from manual editing of the segmentation of lung structures described in [2] . Figure 2 shows DICE box plots for the 4 anatomical structures (lung, body, bronchi and vessel) and the 3 losses. Table 1 reports the statistical summary (mean ± standard deviation for the test volumes) for each quality score and loss function. The adaptive weights loss described in [1] is the worst performer (with even some missing structures), probably due to a poor convergence arising from its highly non-linearity. Our Uadapt increases recall of both minority structures, especially bronchi. However, for the latter, precision also drops, which results in a dice similar to Uconst. Given that the ranges for precision in the detection of tubular structures are 0.9 ± 0.02 with the same recall, we attribute the drop in bronchi precision to the postprocessing used to split the tubular class, which uses classic filters highly sensitive to vessels. This also explains the substantial increase in vessels's DICE score. Our experiments show that grouping organs into topological types in a multi-shape approach using a weighted loss with weights adapted for each training sample can alleviate class imbalance. Our immediate work will focus on improving the splitting of each topological type into the different organs using a convolutional neural network. Int J CARS (2020) 15 (Suppl 1):S1-S214 S155 Recent digital radiography technology provides sequential chest radiographs during respiration and/or cardiac beating. Changes in lung volume and circulation dynamics are observed as temporal changes in radiographic lung density on dynamic chest radiographs. Therefore, pulmonary function can be evaluated by time-series analysis of radiographic lung density. Previous animal and clinical studies indicated that trapped air, limited air flow, and pulmonary embolism could be detected as reduced changes in pixel value in the lung regions on dynamic chest radiographs [1] . However, quantitative capability of this method remain to be determined. The aim of study was to investigate the diagnostic performance and quantitative ability of this method in pulmonary function evaluation, compared to findings in a lung scintigram. Methods Sequential chest radiographs of 26 patients with pulmonary disease (40-88 years old: mean, 73 years; M:F = 18:8) were obtained using a dynamic flat-panel detector (FPD) system (Prototype, Konica Minolta), consisting of an indirect-conversion FPD (PaxScan, 4343CB, Varex Imaging Corporation, Salt Lake City, UT, USA), X-ray generator/tube capable of pulsed irradiation (DHF-155H II/UH-6QC-07E, Hitachi, Ltd.). Imaging was performed in standing position, in posteroanterior direction, during labored breathing for ventilation analysis and during breath holding for perfusion analysis (100 kV, 0.2 mAs/pulse, 15 frames/s, SID = 2.0 m). The total exposure dose was less than the limit for two projections (PA ? LA) recommended by IAEA. The matrix size was 1024 9 1024 pixels, the pixel size was 417 9 417 lm 2 , and the gray-scale image range was 16 bits. High linearity was confirmed between the detector's input X-ray dose and the output pixel values. High pixel values were related to bright areas in the images, indicating high X-ray absorption in the body. The lung regions were automatically segmented by deep learning techniques [2] , and were equally divided into three regions; upper, middle, and lower lung regions. The maximum changes in average pixel value (= Dpixel values) due to respiration and cardiac pumping were measured, and then the percentage of Dpixel values (= Dpixel values %) in each region to the summation of all the regions were calculated. The percentage of Dpixel values were compared with the percentage of radio isotope count (= RI count %) in each lung region on ventilation or perfusion scans. In addition, the ration of respiratory Dpixel values % to circulatory Dpixel values % was calculated aiming to V/Q study in lung scintigraphy. To facilitate visual evaluation, regional differences in Dpixel value were visualized in the form of a color display, representing increased Dpixel values as higher color intensities (blue, red, and pink), Figs. 1 and 2. Approval for the study was obtained from our Institutional Review Board, and the subjects gave their written informed consent to participation. There was a high correlation between Dpixel values and RI count % in each lung (Ventilation: r = 0.75, Perfusion: r = 0.85). The correlations were decreased when compared in each lung region due to a mismatch of each lung region between chest radiographs and lung scintigrams (Ventilation: r = 0.49, Perfusion: r = 0.51). However, correlation coefficients were recovered up to those calculated in each lung by manually correcting the mismatches. In a visual evaluation based on color-mapping images, we confirmed that the distribution of Vessel 0.92 ± 0.02 0.90 ± 0.02 0.00 ± 0.00 0.98 ± 0.01 0.87 ± 0.06 0.00 ± 0.00 0.96 ± 0.01 0.88 ± 0.02 0.00 ± 0.00 Fig. 1 Color-mapping images for the visualization of temporal changes in radiographic lung density during a respiration and b breath holding, and the corresponding lung scintigrams for comparison, and c color-mapping images based on the ratio of respiratory and circulatory changes in radiographic lung density (79years-old man, Postoperative patient with r-lung cancer) regional Dpixel values was similar to that of RI count %, along with functional abnormalities depicted as color defects in many clinical cases. We confirmed the distribution of respiratory and circulatory changes in radiographic lung density is highly correlated with those of RI count on ventilation or perfusion scans. These results indicated that dynamic chest radiography is capable to ventilation-and perfusionrelated parameters based on temporal changes in radiographic lung density, even without the use of RI or contrast media. In addition, the present method has a possibility to realize a functional diagnosis compatible to V/Q study in lung scintigraphy. Further studies in subdivided lung region are required in a large number of clinical cases. Purpose Visualization of vessels and structures of the brain is important for diagnostics of numerous disorders. However, when it comes to transcranial diagnostics, modern ultrasound devices provide images of disappointing quality. The image quality is dependent on the beamwidth, which determines the spatial resolution, and the presence of side lobes, which determine the contrast resolution. The finest quality is assumed for the medium with a spatially uniform speed of sound and attenuation, whereas in the transcranial imaging waves penetrate through layers with different properties and the speed of sound varies from 1500-1550 cm/s for brain tissue to 2500-2900 cm/s for bones. Due to the substantial difference between the actual speeds of sound in tissue layers and the predefined speed 1540 cm/s, used to calculate focusing delays inside the machine, the focusing procedure fails to achieve the best possible signal-to-noise ratio and resolution. Traditional transcranial visualization provides low-quality images because it is performed at a low-frequency range with poor resolution and does not take into account phase and amplitude aberrations. It uses areas called acoustic windows since the aberrating structures there have more uniform thickness and composition. Improvement in lateral resolution through increasing the size of the aperture is limited by the size of the acoustic windows. The majority of attempts to improve transcranial imaging are focused on estimating phase delays induced by aberrating structures and amending a phase map for correct beamforming. The purpose of our work is to design and test a technology capable of correction phase aberrations induced by the temporal window of an adult human skull. For aberration correction one can use an active source as a beacon according to the method originally proposed by Miller-Jones [1] and further developed in [2] and several other studies. We used a singleelement transducer as a point-source placed at one temporal window and a multi-element transducer at the contralateral acoustic window. The signal emitted by the single-element transducer accumulates phase and amplitude distortions while traveling through the patient's head. The distorted signal is acquired with the multi-element transducer, digitized and saved in a memory unit. Knowing the distance between the transducers and assuming the law of attenuation, one can calculate an ''ideal'' signal in the absence of aberrating structures and then compare the ''ideal'' signal to the received one to determine changes and use them for correction in accordance with a time-reversal mirror approach. In our system, the described method was complemented with a control procedure to test the accuracy of the correction. In this procedure the signals which were acquired with the multi-element probe and stored in the memory are now reversed, i.e., their phases change signs, the weakest amplitude becomes the strongest and vice versa. These signals are fired back one by one, received with the singleelement probe, coherently summed on reception and compared to the signals initially emitted by the single-element probe. If the resulting difference is small enough, then the correction is verdict successful and the estimated parameters can be used for transcranial imaging, otherwise, the procedure is run ones again. We prepared a set of five 5 mm thick silicon plates to induce near field aberrations. The plates have waves on the larger surface with the number of waves for different plates ranging from 1 to 5. The speed of sound in the plates is 1200 cm/s; the waves were expected to induce a phase shift in the order of p. The plates were attached to the transducer with a specially designed and 3D-printed fixator. For better acoustic contact, the head of the probe was submerged in degassed water. A post-mortem temporal bone from a human skull was also used in the experiments. Results Figures 1 and 2 contain sonograms before and after correction along with the aberration profiles and photos of the transducer and aberrators. The silicone aberrator induced a phase shift as high as 120°and led to the appearance of two target lines instead of one. The corrected image received up to 7 dB increase in a signal level compared to the uncorrected state. The phase shift created by the skull bone was almost twice as big as the one observed with the silicone aberrator. The signal level had an 11 dB improvement after correction. We have designed and tested a new aberration correction system created for the transcranial ultrasound imaging. This system design showed a significant improvement in a signal level compared with the conventional ultrasound diagnostic devices. The current system works with 1D array probes, however, it would benefit from 2D arrays. Transperineal targeted ultrasound (US) biopsy through a robotic system requires target identification in ultrasound (US) images by fusion with magnetic resonance (MR). In most of the fusion systems, the segmentation of MR will give hints for the alignment with the US images. Some systems may require the segmentation of the US images to perform the alignment automatically, but the segmentation is manually performed by the physician most of the times. In this work, we present an infrastructure for automatic semantic segmentation of the prostate shape in real-time US image acquisitions, to be integrated in a navigation system for prostate biopsy. Segmentation is performed on a sequence of frames recorded during the operation with a robotic system that allows the reconstruction of the 3D US volume. Segmentation is implemented using a deep neural network (U-Net architecture [1] ). The segmented images will be used by a 3D Slicer ad-hoc created module to create a 3D volume representation of the prostate. By fusing the segmented 3D shape with the segmented shape from MRI, the biopsy target can be identified precisely. US is segmented real-time after a scanning sequence is recorded. Since the transperineal procedure employs a biplanar US probe, we have tested the algorithm on both coronal and sagittal image plane. With a proper ground-truth and training, this method can be applied to any anatomy and even for different types of images (e.g. US and MRI). The precision we have achieved is around 80% (expressed as Dice Coefficient) and the computation time for a sequence of 300 frame is 10 s. A single 2D frame may be segmented in 30 ms, therefore is compatible with the framerate of the US machine, making our approach feasible in real-time. The training and the test set images were acquired using an Ultrasonix US machine from a standard synthetic phantom cube (CIRS 070, Computerized Imaging Reference Systems, Inc. (CIRS), Norfolk, Virginia, US) containing the prostate with urethra and seminal vesicles, the bladder and the rectum. Ground truth was created using a semi-automatic procedure. This procedure, implemented in python, uses spatial filtering and Self Organized Maps clustering [2] . The automatic generation of the ground truth is then validated through a graphic user interface written in Gtk#. With this interface (Fig. 1) is possible to classify every segmented image in 3 classes: valid, fixable and discarded. Valid images will be directly inserted in the dataset, Int J CARS (2020) 15 (Suppl 1):S1-S214 discarded ones will be eliminated. Fixable images are manually adjusted before being added to the dataset. The interface will visualize 3 images: the original, the segmented and the overlapping of the two. By clicking on one of the 3 buttons (each corresponding to a class) it's possible to classify each image. Dataset is composed by 2347 images and was split with a ratio of 0.8 in training set and validation set. The 3D surface reconstruction is performed after the dataset is segmented and using a common reference frame which is given by the navigation system. Neural network architecture used was U-Net. The U-Net architecture was implemented in Keras based on Tensorflow. This implementation supports CUDA GPU acceleration for both training and prediction. Training was performed on a GTX 1080 video card. Optimization algorithm used is Adaptive Momentum Estimation (Adam). Loss function used is Dice Coefficient. Test with Binary Cross Entropy has also been made (lesser results). Testing of segmented predictions has been made implementing a script in MATLAB which uses Dice Coefficient (DC) built-in function to test pixel-similarity between ground truth images and segmented results. Average precision is computed between all samples. X is the set of ground-truth pixel belonging to the mask. Y is the predicted set of pixels of the mask. An average precision score of 80% was obtained on the 469 test images. DC similarity has been computed on every couple of samples (ground-truth image and image segmented by U-Net). Final precision score has been calculated by taking the average result between all precision scores computed on every sample. A scriptable module, called Prostate Segmenter and available online as an extension of the software 3D Slicer version 4.11, has been realized to allow testing on real-time data. Along with IGT Plus Remote module it's possible to acquire a sequence of US images and segment it via Prostate Segmenter module, then 3D Slicer will be able to perform 3D Volume rendering (Fig. 2) . Conclusion A method for the segmentation of the prostate in US images was implemented to be used in conjunction with a navigation system for prostate biopsy. Since this is a preliminary work, we have now the infrastructure to start testing the workflow on human data. The next step will be to acquire US data from around 100 patient and to create a ground truth for the neural network with the support of the physicians. We envision to acquire for each patient more than 100 different US frames, putting the probe in different positions with respect to the prostate, so that the training data can have a large variability. The 3D Pelvic Inclination Correction System (PICS) was proposed as a universally applicable coordinate system for isovolumetric imaging measurements on MRI images [1] . The original approach allows the quantitative assessment of pelvic organ positions over time and independent of patient movement relative to the scanner. For this purpose, a coordinate system is constructed utilizing four bony landmark points, that are easily identifiable for radiologists. The inferior pubic point as the origin, the sacrococcygeal joint and two points marking the ischial spines. Any image points or measurements can subsequently be transformed from the original scanner coordinate system to the newly established pelvic coordinate system. The bony landmarks points, as well as the image points and measurements currently need to be We enhanced the original approach by using machine learning for the identification of the bony landmark points and integrating the coordinate system construction directly into the web-based medical imaging viewer. This enables the radiologist to do instantaneously direct comparable measurements of organ positions inside the pelvis across different patients or over time. No further interaction is required. We used 68 female MRI-scans of the pelvis (sagittal and axial slices, T2 weighted-imaging) to explore the feasibility of the automatic landmark detection. None of the MRI-scans included tumors that might distort the pelvic anatomy. The inferior pubic point, the sacrococcygeal joint and the ischial spines where annotated by a radiological expert. To compare the suitability of different convolutional networks [2] we limited the initial problem to the detection of the ischial spines inside the axial slices of the provided MRI-scans. On this we trained various convolutional networks (2D and 3D): • 2D and 3D adapted AlexNet • 2D and 3D adapted VGG16 The 2D convolutional networks have been trained and limited to the appropriate 2D-slice of the MRI-scan containing the ischial spines. However, the results show that they are inappropriate for the intended task despite the reduced problem space. Adapting these 2D convolutional networks to 3D was restricted by the available graphics card memory (8 GB) allowing training only on downscaled MRI-scans. Therefore, we created our own 3D convolutional network (Pics-Net) that can exploit the full resolution of MRI-scans, keeping all details. As 68 MRI-scans are a quite small sample size for machine learning, we employed data augmentation. 20 MRI-scans where excluded before data augmentation (10 MRI-scans for validation and 10 MRI-scans for testing, Fig. 1 ). The remaining 48 MRI-scans where used to generate a training dataset of 480 MRI-scans using the Python image augmentation library (https://imgaug.readthedocs.io/) and the following settings: • Linear Translation: -5% to ? 5% on x-and y-axis, 50% probability • Linear Scaling: 80% to 120% of original size, 50% probability • Linear Rotation: -15°to ? 15°on xy-plane, 50% probability. • Elastic transformation alpha = 12, sigma = 5, 30% probability. PicsNet was trained with this data for the detection of each ischial spine (left and right) independently. Finally, the accuracy of the bony landmark detection was assessed by comparing the results achieved from machine learning with the interrater reliability. This interrater reliability was calculated based on 10 MRI-scans that have been randomly chosen and annotated by 2 radiologic experts (Fig. 2) . Results Table 1 shows the mean average error (MAE) in voxels over both ischial spines. Our 3D convolutional network PicsNet trained with the augmented dataset achieves the smallest voxel error in comparison to all other evaluated convolutional networks. Furthermore, the significant improvement of the mean average error obtained from the 3D PicsNet with data augmentation compared to 3D PicsNet without data augmentation shows the importance of a large training dataset size, Table 1 . The interrater reliability for the same task is 3.26 voxels. As the 3D PicsNet convolutional network achieves already a mean average error of 2.95 voxels, the automated detection of both ischial spines can be considered sufficiently precise with machine learning. We showed that the automated detection of bony landmark points with machine learning is feasible on MRI-scans and that the achieved results are indistinguishable from those obtained by radiologic experts. The next steps in this ongoing work will focus on the detection of the inferior pubic point and the sacrococcygeal joint on sagital slices. We will be increasing the training dataset drastically to 850 MRIscans that will be annotated by 2 radiologic experts, expecting to improve the accuracy of our PicsNet even further. Finally, we will be enabling our web-based medical image viewer with direct measurements inside the 3D Pelvic Inclination Correction System (PICS) with no user interaction needed. Bony landmarks will be detected based on the pre-trained 3D PicsNet convolutional network. Thus, we have the opportunity to explore the capability of introducing machine learning to clinical practice.  In the last three decades, several advanced radiotherapy techniques, such as image-guided radiation therapy and tomotherapy, have been proposed and applied in clinical. These techniques provide accurate dose delivery to improve the dose coverage of target lesions and reduce the dose to normal organs, thereby optimizing the therapeutic ratio of radiotherapy. To employ these techniques effectively, treatment planning is essential and important. In treatment planning, computed tomography (CT) images are usually used to define the contour or boundary of the target and normal organs. However, the spatial resolution of the CT images is limited by several factors, such as filter kernel and the size of detector elements. Consequently, the visibility of the organs degraded, which lower the accuracy of the definition of the organs using manually or automatic segmentation tools and further reduce the quality of radiotherapy. To improve the spatial resolution of images, a very deep super-resolution (VDSR) network has presented the high performance in single-image SR by reconstructing high-frequency features from low-resolution (LR) images [1] . However, the performance of the VDSR network on the SR of the CT images remains unknown. In this study, we evaluated the performance of the VDSR network on the single-image SR of the CT images using clinical patient images. In this study, 17,111 CT images acquired from the head, chest, and abdomen of 169 patients for treatment planning of radiotherapy using a Toshiba Aquilion PRIME CT scanner (Aquilion PRIME, Toshiba Medical Systems, Tokyo, Japan) with a tube voltage of 120 kVp and automatic modulated tube current were included as ground-truth images. Each ground-truth image was down-sampled by a factor of 0.1 to generated a LR image. Residual images, that represented the high-resolution ground-truth features to be reconstructed by the network, were calculated by subtracting the original images with the down-sampled images. The down-sampled and residual images obtained from the same CT image were paired as a set of training and testing data and were used as the input and output of the network, respectively. For training and testing, 80% (13,689 images) and 20% (3420 images) of the images were used for the training and testing, respectively. The applied VDSR network architecture consisted of 20 convolution layers with an input layer and a regression layer [1] . After the input layer, a 2-D convolution layer, which contains 64 filters with a size of 3 9 3 was followed. Eighteen 3-D convolution layers included 64 filters with a size of 3 9 3 9 64 were then followed. The 2-D and 3-D convolution layers were all followed by a rectified linear unit (ReLU) layer. The last convolution layer has a single filter with a size of 3 9 3 9 64 for reconstructing residuals with an output size of 512 9 512. The last convolution layer was followed by a regression layer, which was used to computes the mean-squared error between the original and reconstructed residual images. The VDSR network was trained by updating the weight factor for each layer using the error back-propagation with the stochastic gradient descent with momentum (SGDM) optimizer. The learning rate was initially 0.001 and lower by half per 2 epochs. The number of epochs was 20. After training, SR images were generated by adding the inputted LR image with the residual image reconstructed using the trained VFSR network. Image indices, including normalized root mean squared error (NRMSE), peak signal-to-noise ratio (PSNR), and mean structure similarity (MSSIM), were calculated to evaluate the performance of the VDSR network. A five-fold cross-validation was performed to assess the performance consistency. Results Figures 1 and 2 show an example of the LR images and the corresponding SR images. The visibility of the edges and fine features of tissues and bones are improved, indicates the VDSR network could effectively reconstruct high-resolution features from LR CT images. Table 1 lists the mean PSNR and MSSIM values with their standard deviations calculated by comparing with the ground-truth images from the five-fold cross-validation. In average, the mean PSNR and MSSIM values were improved from 30.963 ± 0.295 to 42.020 ± 0.409 and 0.910 ± 0.004 to 0.980 ± 0.002 after the SR process, respectively. All indices indicate that the VDSR restores the high-resolution image features without introducing additional image artifacts. Also, the noise in the SR images was lower than that in the original images, indicates that the VDSR network could avoid the reconstruction of high-frequency noise features and further improved image uniformity. Int J CARS (2020) 15 (Suppl 1):S1-S214 S161 In this study, the performance of the VDSR network on the SR of the CT images was evaluated. The results show that the VDSR network could improve the spatial resolution of the CT images with lesser image noise. We concluded that the VDSR network could be applied in the CT images before the manual or automatic image segmentation for high accuracy boundary definition of target and normal organs in advanced radiotherapy. Purpose Atherosclerotic plaque in coronary arteries can lead to myocardial infarction and is one of the leading causes of death. Intravascular optical coherence tomography (IVOCT) can be used to image the affected blood vessels for assessment and treatment. However, catheter bending often causes changes in the rotation frequency of the optical probe during acquisition. The resulting non-uniform rotation distortion (NURD) artefacts complicate the image interpretation and may affect the diagnosis. Deep learning methods have been proposed to analyze IVOCT image data, including plaque detection [1] and feature extraction [2] . We present a novel approach to directly estimate the rotation frequency of the optical probe from a sequence of IVOCT images. We illustrate that this allows a proper correction of NURD artefacts. Our approach is based on a large amount of experimentally acquired IVOCT data with a known rotation frequency. The setup to generate the data consists of a modified, torsionally rigid IVOCT imaging probe performing continuous rotations without pullback. Simultaneously, the encoder position of the electric motor was assigned to the allocated OCT data. A spectral-domain OCT system (Telesto I, Thorlabs) with an A-scan rate of 91 kHz was utilized. Firstly, we obtained data in 3D printed vessel phantoms at constant rotation frequencies between 3.75 and 22.5 Hz at intervals of 0.375 Hz. Then, the data was partitioned into segments of 512 consecutive A-scans, each 512 pixels in length. In total, we obtained a data set with 13770 labeled segments. This data set was divided into 70% training, 15% validation, and 15% test set, respectively. Secondly, we acquired an additional test set containing simulated NURD artefacts by manually inducing abrupt changes in the rotation frequency. We used four state-of-the-art convolutional neural networks (CNN) for the estimation of the rotation frequency. We compared the architectures Resnet18, Densenet121, SE-ResNeXt50, and Effi-cientNet-B0 for both training from scratch and transfer learning with models pre-trained on the ImageNet dataset. The final fully connected layer was adapted to a single output. We trained our models using the Adam optimizer with a learning rate of 1 9 10-4. For the pre-trained models, we used early stopping based on validation performance. The results were compared with the mean absolute error between predictions and targets. Afterwards, we used our trained models to track the changes in rotation frequency in the additional NURD data set. Finally, we applied artefact correction based on the estimated rotation frequency. Instead of assuming equidistant line scans, the acquired A-scans were interpolated to match their actual orientation during acquisition. For visualization the results, three consecutive frames were reconstructed and mapped to the three colour channels of one image. Thus, well aligned frames resulted in black and white images while NURD artefacts were visible as colourful shape distortions. Our key results are summarized in Table 1 . The SE-ResNext50 architecture resulted in the lowest error of 0.53 Hz (6.3%). The other models performed slightly worse. Overall, our results demonstrate that an estimation of the rotation frequency is feasible from just a short sequence of A-scans. The additional test set then showed that the trained models can be used for the correction of NURD. Figures 1  and 2 show artificial artefacts before and after compensation using the estimated rotation frequency. In Fig. 1 , the NURD artefact results in a serious distortion and unrealistic vessel shapes. Furthermore, Fig. 2 illustrates that considering the estimated rotation frequency when reconstructing the frames effectively mitigates the artefact. We consider a novel approach for the correction of NURD artefacts in IVOCT by directly estimating the probe's rotation frequency using CNNs. Our quantitative results show that the employed architectures estimated the probe's frequency. Abrupt relative changes within the considered frequency range could be tracked. Future work could investigate the transferability of our approach to IVOCT data acquired with a clinical catheter. References  The majority of radiomics is conducted using a sub-section of an entire scan. The region of interest (ROI) is typically the tumor isolated from surrounding tissue and can be created either by drawing by hand, using a seed-point region growing algorithm, or automatically generated using deep-learning techniques. The final-say as to whether the ROI is appropriate/accurate, however, ultimately comes down to a radiologist's interpretation. The work presented here aims to investigate variability in radiomics via the differences a radiologist makes in the radiomic analysis of breast tissue lesions. We aim to look at both the statistical differences caused by the ROIs, and, the effect on the ability to classify patients as either benign or malignant. Two cohorts of 100 patients each were selected for this study where the two cohorts were defined as mass and non-mass lesions respectively. These were chosen to assess whether differences in radiomic parameters would be present in both ''easy'' cases (mass) as well as (or instead of) ''hard'' cases (non-mass). The patients in each cohort were classified as either benign or malignant based on the histology report following biopsy. Four trained radiologists segmented the identified lesions independently from each other, to the standard and accuracy which they deemed appropriate. Creating the ROIs on postcontrast contrast T1 images, all four readers used ITK-snap as their segmentation tool, with all four using a method of seed-growing/ thresholding and/or manual segmentation for all 200 lesions, Fig. 1 . Radiomic data was calculated using CERR using a modified script in MatLab (2017b) to allow for batch processing of the images. 22 first-order and 78 higher-order statistics were calculated for each patient. Statistical analysis was conducted in SPSS (v.25) utilizing interclass correlation tests to test for consistency between the readers [1] , as well as ROC curves, and Mann-Whitney U-tests to test for significant parameters useful in classification. MatLab was used for LASSO regression to identify the most significant contributors to diagnostic status, and these radiomic features were used to create models for classification [2] . Results ICC values were found to be higher in the mass cohort than the nonmass cohort in the overwhelming majority of features tested. In the mass cohort 73% of all features had excellent agreement (ICC [ 0.9) with 89% having at least good (ICC [ 0.8) agreement between all 4 readers. In the non-mass cohort only 45% of features had excellent agreement with 76% having at least good agreement. The number of significant (p \ 0.05), and highly significant (p \ 0.001), U-tests varied between readers, as did the number of significant (AUC [ 0.700) ROC curves, Fig. 2 . The difference between number of significant U-tests was greater for the non-mass cohort (range: 41-57) than in the mass cohort (range: 34-36), and the number of significant ROC curves also had a greater range (range: 13-27) for the non-mass cohort when compared the mass cohort (range: 24-28). The number of parameters which were highly significant for all readers varied between the mass and non-mass cohorts (25 and 13 features respectively), as too did the number of features which were found to be significant for at least one reader but not all four (9 and 26 respectively). No significant differences were found in the performance of the models created across all readers when comparing AUC value, overall accuracy, PPV and NPV, Table 1 . The high agreement across the majority of radiomic features tested and high proportion of significant (and highly significant) univariate tests for all readers would suggest that the differences in feature extraction are negligible when taken as a whole. The aim of radiomics is to provide a discreet value to a patient so they can be measured against others, and to make classification possible, and a large variation in patient ranking would be evident by lower consistency across patients, parameters, and readers. In the setting of machine learning, where multiple parameters are used to classify patients automatically, techniques such as LASSO or MRMR are used to select the most significant parameters and increase performance of models without overfitting. In this context, the work presented here would suggest that the effect of the ability to identify patients as either benign or malignant is negligibly affected by the choice of radiologist, and subsequent segmentation. Classifying benign and malignant tumors in breast MRI is a relatively simple task for radiomics, and so the work presented here would need to be tested in other more challenging classification tasks to provide a more robust conclusion on the role of reader dependence. Bronchoscopy is the endoscopic procedure for visualizing the inside of the airways. It is one of the most common lung procedures, for both diagnostic and therapeutic use. Video imaging is displayed on a monitor near the operator. Since its introduction in 1987, a number of other image sources have been added, like fluoroscopy, endobronchial ultrasound for lymph node sampling, and image guidance systems (navigated bronchoscopy). Today's diagnostic work-up for sampling of peripheral lung nodules often includes bronchoscopy, fluoroscopy, virtual or navigated bronchoscopy, as well as endobronchial ultrasound-guidance, most requiring a separate monitor. In addition, complex bronchial or mediastinal anatomy may require a second look at the preoperative CT and PET-CT-scans, which often disrupts the workflow of the examination entirely. Microsoft HoloLens is a head-mounted display for mixed reality visualization, which allows for augmented reality (AR) together with off shelf components. HoloLens has been studied as a navigation tool in visceral, spinal and neurosurgery and as a planning tool before congenital heart surgery. HoloLens has been utilized in pain management and for learning purposes. As far as we know mixed reality visualization with HoloLens has not been explored in pulmonary medicine. The aim of this work was to evaluate the use of HoloLens as a visualization tool during bronchoscopy performed by pulmonologists in a preclinical setting with a lung phantom. We wanted to study how the HoloLens experience was perceived by the operator and whether the workflow was better preserved by displaying relevant image sources on the HoloLens. Methods Nine pulmonologists, at different levels of expertise, tested the HoloLens mixed reality display during bronchoscopy examination on a lung phantom. Two different clinical scenarios were evaluated. The first scenario as depicted in Fig. 1 was bronchoscopy with bronchial inspection with endobronchial forceps biopsy combined with electromagnetic navigation bronchoscopy (ENB). The procedure set-up was bronchoscopy video displayed side-by-side with ENB in the HoloLens display (see Fig. 2 ). ENB in this study was provided from the open source research navigation platform CustusX [1, 2] . The electromagnetic (EM) sensor was attached at the tip of the bronchoscope. ENB/virtual bronchoscopy images were segmented from CT data of the lung phantom. The second scenario was bronchoscopy with endobronchial ultrasound (EBUS). Bronchoscopic video, endobronchial ultrasound images and CT scans in axial planes were displayed together on the HoloLens. An EM sensor attached to the EBUS bronchoscope enabled real-time and hands-free scrolling of CT-slices. Seven of the nine pulmonologists took part in the second scenario. The streaming software, which is responsible for sending data to the HoloLens, was implemented as a CustusX plugin. A video stream from CustusX is obtained via a custom screen video provided by the framework. Support for capturing additional video streams is available via Blackmagic capturing devices (Blackmagic Design Intensity Pro 4 K) and was used to connect to the bronchoscope device. Each source is captured in an independent thread. Additional image processing can be defined for each source. Since the data is transferred via a WiFi connection, frame rate throttling, frame dropping, and stream prioritizing were added to the server software. The HoloLens application was implemented in Unity (Unity Technologies, California, USA). A receiver written in C#, that is able to receive the video streams, was developed. Each stream of images was converted to a Unity compatible texture that was applied to mesh objects in the Unity scene. All pulmonologists evaluated the two scenarios on a questionnaire immediately after the procedures. The pulmonologists graded five different statements from 1 to 4 (low to high agreement). Of the nine pulmonologists only one found the HoloLens to some extent heavy to wear in the experimental setting. Two of the participants reported discomfort using HoloLens due to wearing own glasses underneath the HoloLens. The color rendering was found to be adequate so that important pathology would not be missed. All except two considered the room not too bright during the procedure. During the procedure the lights was turned off in the bronchoscopy suite, but some light could enter from the windows. The ergonomics of endobronchial needle sampling while not having to look for the video screen monitor was deemed beneficial. The above statements graded by the participants are summarized in Table 1 . The ultrasound images from the EBUS-scenario were perceived as clear as the traditional monitor screen set-up. Two of the pulmonologists considered the CT data displayed on HoloLens to be too dark or unclear. All pulmonologists except one perceived the technology to be time-saving and beneficial for the workflow for both scenarios and the overall quality of the images seemed sufficient. Conclusion Endobronchial procedures using the Microsoft HoloLens was perceived feasible in a pre-clinical setting both for conventional bronchoscopy and EBUS bronchoscopy. Under the right conditions bronchoscopy with HoloLens could be favorable to conventional bronchoscopy, especially in more complex situations where more imaging modalities are in use, but more studies are needed. The next step is an ongoing clinical proof-of-concept study with patients. The aging of the population has been accompanied by an increase in the number of home patients. This has resulted in the requirement for a home care support system to provide adequate care for home patients. The Ministry of Health, Labor, and Welfare in Japan developed a system for the implementation of palliative care and emphasized the need to increase the quality of life (QOL) of home care patients [1] . To improve their QOL, it is necessary for home patients to be able to live in the community with peace of mind. Home patients must be able to share medical information with medical staff at the hospitals responsible for their treatment. The present study was performed to develop a system that is capable of sharing medical information, as shown in Fig. 1 . The system for sharing of medical information was developed using the Apache web server, PHP programming language, MySQL database, and the OpenPNE social networking service (SNS) Engine on a personal computer running Microsoft Windows. The system was evaluated by 12 mock patients consisting of one woman and four men in their 20 s, two men in their 30 s, one woman and one man in their 40 s, and one woman and two men in their 50 s. The methods used for evaluation of the system developed here were task achievement level, task achievement time, and Web Usability evaluation Scale (WUS). The features of each age group could be analyzed by the WUS. Furthermore, the medical information in this system is shared using SNS with access only granted by full invitation. In addition, the system has an opinion exchange function and treatment schedule function that can also be shared. The color rendering would not miss important pathology 1.4 (0.5) 1 9 Too bright in the room for optimal functionality 1.7 (0.9) 1 9 I consider HoloLens to be an advantage during sampling 3.1 (1.1) 3 9 Eye focusing/accomodation evoked discomfort 1.6 (1.0) 1 9 S166 Int J CARS (2020) 15 (Suppl 1):S1-S214 The information exchange system allows the opinion of the patient to be shown to the medical staff. In addition, the patient and medical staff can share not only the opinion but also the patient's vital data. Although one subject required the manual to operate the system, the task completion percentage was 100%. The system developed in the present study can be used for patients in age groups that are unfamiliar with personal computers. With the continuing adoption of Internet usage, this system built using a web application framework will be well received by patients. Subjects in their 20 s showed the greatest increase of 4.0 in overall judgment on a 5-point scale at WUS, as shown in Fig. 2 . Items showing differences between age groups included ''Display.'' Standard deviations of ''Useful'' for all age groups were small, and unrelated to age group, with values of about 3.5 in all age groups. Using the system the second time was easier for users than the first time. In addition, the clinical assessment indicated that using the system allowed home patients to have the same consideration as those receiving hospital treatment. Conclusion In using the system developed in this study, there were no relations between age group, task achievement time, and task achievement level. The system developed here can be recommended for use by home patients. The patient can receive the same treatment as at the hospital while at home, leading to improved QOL. Therefore, the system developed here improves the QOL of these patients.  Chest computed tomography (CT) images of 512 9 512 9 16 bits (67 slices) were used in this study. The EPR data, the hash values of the regions of interest (ROI), name of the institute, and the data of the patient support system were hidden in the regions of non-interest (RONIs) in a chest CT image series in the Digital and Communication in Medicine (DICOM) format, see Fig. 1a . Medical information (CT images, etc.) was hidden in a scene photograph. We call the cover photograph containing medical information a stego-image. A body CT image series of 512 9 512 9 16 bits (100 slices) was used for steganography. These CT images were stored in a folder after compression using 7-Zip. This folder was then embedded in the cover picture of a scene photograph. This stegoimage was then transmitted to another medical institution, see Fig. 1b . In the present study, we examined the hash value, peak signal to noise ratio (PSNR), and structural similarity (SSIM) of the images, as shown in Fig. 2 . When part of the ROIs was altered during transmission, the hash value decoded from the received cover picture was different from that before transmission. For a watermarked image in which 4000 words were embedded or for the stego-image with a hidden CT image, the structural similarity (SSIM) and PSNR were 0.99 and 65.3 dB, respectively. In addition, when the medical information was embedded in the low-bit plane, such as the first-and second-bit planes, the radiologist was unable to identify the embedded information. Application of our technique resulted in no alterations in the image capacity of CT images or cover pictures before and after embedding. Using a combination of digital watermarking technology and steganography technology, we have developed a medical information hiding system that can be used both to ensure the copyright of the images and to protect privacy and safety transmission of the EPR and Purpose Currently, the 3D representation of medical images of several modalities, such as computed tomography (CT) and magnetic resonance imaging, is indispensable for morphological and anatomical diagnoses as well as surgical treatments. In interventional radiology (IVR), a 3D representation is also useful; for instance, a hepatoma feeding arteriogram [1] is created from CT during aortography obtained with an IVR-CT for the navigation of a transcatheter arterial chemoembolization of hepatocellular carcinoma. However, organs that have a complicated structure, such as vasculature, are hard to observe even when their 3D representations are displayed on an ordinal two-dimensional (2D) monitor. In our previous study, we proposed a vascular virtual handling system where physicians can observe 3D representations of the vasculature using a head-mounted display (HMD), which offered realistic display of blood vessels; it also enabled the handling of blood vessels using a 3D pointing device for endovascular intervention assistance. In the study, it was indicated that while finding a lesion, the visibility of the vasculature in the proposed system is superior to the one in the conventional system, which uses a 2D monitor [2] . Although we expect that the system makes it easy to find the lesion, plan the treatment, and insert a catheter to the lesion for the endovascular intervention, the effect of the vascular handling, especially the vascular behavior on the visibility is not adequately assessed. In this study, the effect of vascular behavior on the visibility in this vascular virtual handling system was evaluated; this evaluation was performed by using the system to find a pseudo lesion embedded into cerebral blood vessel. The vascular virtual handling system consists of an HMD, a 3D pointing device to handle the blood vessel and a personal computer. The HMD used in this system has a sensor to track the head movement so that the viewpoint of the user can change according to the head position and attitude. Furthermore, this system enables the user to virtually handle the individual blood vessels as well as to translate, rotate, and scale the whole vasculature so as to improve the visibility of the blood vessel of interest, which are obscured by other blood vessels. For example, the blood vessel handled by the pointing device behaves like a rubber string. In order to assess the effect of the vascular behavior on the visibility in the proposed system, two types of vascular behaviors such as the rubber string behavior and the erasure behavior, in which a vascular portion handled by the pointing device is erased, are evaluated; this is performed via an experiment to find a pseudo lesion embedded randomly into a cerebral artery as shown in Fig. 1 . The shape of the pseudo lesion is a cube whose edge length is 0.9 times larger than the diameter of the blood vessel at the position. The average time taken to find the pseudo lesions in five trials is considered as a measure of the visibility. This experiment was carried out on 24 males and females in their twenties. As a comparison of the effect of the vascular behaviors, the same experiment was carried out using this system without the vascular virtual handling, where the examinee can only translate, rotate, and scale the whole vasculature. Three sets of five random positions of the pseudo lesions are prepared considering habituation effect. That is, the examinee finds the pseudo lesion at a different potion in each trial during this experiment. The use order of each system for each examinee is also randomly determined considering order effect. The average time taken by the vascular virtual handling system for each vascular behavior is shown in Table 1 . There is no significant difference in the average time taken by the system with vascular handling of the rubber string behavior compared to the one without vascular handling; however, the average time using the former system tends to be longer than that using the latter system. This indicates that the effect of the vascular behavior like rubber string does not contribute to improving the visibility of the vasculature compared to the effect of 3D representation using the HMD. Also, there is no significant difference in the average time taken by the system with vascular handling of the erasure behavior compared to the one without Fig. 2 Relation between the PSNR a and the SSIM b of the CT image series and embedded medical information capacity in the digital watermarking technology vascular handling; however, the average time using the former system tends to be shorter than that using the latter system. The average time using the system with the vascular handling of the erasure behavior is significantly shorter than the one using the system with the vascular handling of the rubber string behavior (p \ 0.05). Since the purpose of this experiment is to find the pseudo lesion, it seems to be effective to erase the exterior blood vessel. The results suggest that the vascular handling can be effective if the vascular behavior is designed according to the purpose of use. Note that there is no significant difference in the average time among the three sets of five random positions of the pseudo lesions regardless of the type of the vascular handling. In this study, the effect of the vascular behavior on the visibility in this system was evaluated by an experiment to find a pseudo lesion embedded into cerebral blood vessel. The average time taken to find the pseudo lesions using the proposed system is not significantly different from the one using the system without the vascular handling. Although the results indicate that the effect of the vascular handling does not contribute to improving the visibility for finding lesions compared to the effect of the 3D representation using the HMD, the experimental results suggest that the vascular handling can be more effective if the vascular behavior is appropriately designed according to the purpose of use. In a future study, we will evaluate the effect of the vascular virtual handling on the visibility for other purpose such as planning a treatment and inserting a catheter. Our goal is to automatically segment Auerbach's plexus and detect GCs inside it. Nevertheless fully convolutional networks (FCNs) can detect and segment those regions, there are also similar structures to Auerbach's plexus or ganglion cells in it (e.g. Meissner's plexus, goblet cells) outside the Auerbach's plexus. In this paper, to remove false positives produced in the outside the Auerbach's plexus, we introduce masking process based on superpixels to focus only on the muscularis, which contain Auerbach's plexus. The proposed method detects GCs on an input VSI of intestine tissues. Our method consists of two parts including: (1) masking of processing targets and (2) GC detection with partly traced training data. (1) requires more macroscopic analysis across VSIs. Since tissues inside the intestine have different colors, we introduce macroscopic segmentation based on superpixels [1] and HSV color space. Although (2) is performed by U-Net, it is difficult to produce training data. This is because manual tracing completely on multiple VSIs is very tough since each VSI contains hundreds of GCs. (1) Mask image generation VSIs contain regions of (a) mucosa, (b) submucosa, (c) muscularis, (d) serosa and (e) background. We detect GCs only in the muscularis region. Therefore, we perform muscularis region segmentation to obtain an mask image ro be processed VSIs are firstly downsampled for d-times. SLIC superpixels [1] are used to separate the downsampled VSIs. As partly inspired by [2] , by converting color representation of those downsampled VSIs to HSV color space representation, mean of H-(hue) and V-(values) channels are calculated for each superpixel. Cascaded-thresholding process are used to choose superpixels belonging to muscularis regions. This process is summarized as follows: Thresholding (1) Select superpixels mean saturation value is higher than s. Thresholding (2) Select superpixels whose mean intensity value is higher than the threshold obtained by Otsu's thresholding. Thresholding 1 is introduced based on the fact that superpixels corresponding tomucosa and muscularis regions have higher saturation than other regions. Thresholding 2 is introduced based on the fact that superpixels corresponding to muscularis regions are brighter than those of mucosa regions. The mask image is obtained by these thresholding process. The superpixels corresponding to the muscularis region have 1 and the other area have 0 in the mask image. Then this mask image is enlarged to the original size of the input VSI by interpolation. (2) Detecting GCs with partly traced training data Ground-truth data preparation Since each VSI is huge, manual tracing of all GCs existing in a VSI is very difficult. Therefore, we separate a VSI into multiple subimages called ''bars''. Each bar siize is denoted as W (pixels) in width (X-axis) and the same size to the VSI in height (Y-axis). W is a manually adjusted. Experts randomly selects several bars for each training data and trace GCs on it. Training The bars having ground-truth are used for training. Patches are cropped from those bars so that have the same size as the network input. We utilize a U-Net with introducing batch-normalization. Inference Inference is conducted using the trained U-Net. It is conducted only in the regions where the mask image is 1. Inference is perfumed not only in bars having ground-truth data but also bars not having ground truth data. The Figure 1 shows segmentation results of the muscularis. From the input VSI image shown in Fig. 1a , superpixels corresponding to the mucosa and the muscularis regions were almost properly chosen, since they are clear purple or pink and have high saturation. Then superpixels only of the muscularis were chosen by values, which were brighter than superpixels of the mucosa. Detection and segmentation results of GCs are shown in Fig. 2 . Auerbach's plexus segmentation results are shown in the same figure. As shown in Fig. 2a , many Auerbach's plexus regions can be observed as blue in the lower part of the muscularis. Figure 2b shows magnified view of Fig. 2b . From this figure, we can see GCs were segmented properly. As shown in these figures, most of GCs were detected and segmented properly, although some false positive and false negative regions exist (as evaluated in Fig. 2c ). Masking allowed reduce false positives outside Auerbach's plexus efficiently. Our cropping scheme allowed detection on entire regions with small amount of manual tracing for each training data. Light microscopy, such as two-photon microscopy, is a key tool to observe cellular properties. Stereological analysis of a two-photon microscopic volume is expected to be useful to unveil brain function. Fig. 1 Mask generation process. a Input WSI. b Segmentation of mucosa and muscularis, which have higher saturation than others. c Segmentation of muscularis, which are brighter than mucosa Cell segmentation from a three-dimensional (3D) image is a crucial process to analyze hundreds or more cells. Several deep-learning-based methods have been proposed to extract cells from a confocal microscopic image. A major problem in the existing approaches is the low performance in segmentation, which might be caused by gradient vanishing and exploding in the training process of deep networks, and/or low contrast of cells. The former problem makes training of a deep network difficult, while the latter leads to lower segmentation accuracy in the boundary of cells. This study proposes a deep-learning-based Purkinje cell somas segmentation process from a two-photon microscopic volume of a mouse brain. The process consists of three sub-networks-a U-Net type 3D fully convolutional network (FCN), a boundary network, and a fusion network. The contributions of this study are as follows. (1) A deep supervision [1] based loss function is introduced to avoid gradient vanishing and exploding during the training of the 3D FCN. (2) A boundary network [2] is integrated into the 3D FCN to enhance segmentation accuracy in the boundary of a low contrast cell. We demonstrate the effectiveness of the proposed process using two-photon microscopic volumes with 864 cells. Methods A moving window of 64 9 64 9 32 [voxel] scans over an input microscopic volume with an interval of 28 9 28 9 12 [voxel] to extract patch volumes, which are inputs of the proposed networks for cell segmentation as shown in Fig. 1 . The channels, convolution, and pooling kernels in layers are three dimensional. A major part of the segmentation process is a U-Net type 3D FCN surrounded by a red frame. The network is boosted by deep supervision and a boundary network is incorporated into the network. The output of the FCN (upper) and that of the boundary network are forwarded to the fusion network, which outputs a patch volume of 28 9 28 9 12 [voxel]. Further, a tiling process of output patch volumes is carried out to reconstruct the segmentation labels for an input whole volume. A deep network might suffer from gradient vanishing or exploding in the training process. Dou et al. proposed a deep supervision technique that calculated the loss functions of the output layer and intermediate layers to prevent this problem [1] . Here, deeply supervised cross-entropy losses are computed between true labels and outputs of three layers marked by stars, as observed in Fig. 1 . The summation of the three losses is minimized in the training step. An unnatural shape in the boundary can be observed in the segmentation of low contrast cells, leading to a decrease in the segmentation performance. This study combines a boundary network [2] with the FCN, both of which share encoding layers, but the outputs are different from each other as shown in Fig. 1 . Further, a fusion network is employed to integrate the output of the FCN (upper) and that of the boundary network. The losses of the boundary network and the fusion network are cross entropy losses. The segmentation results of the proposed networks are discussed in the following section. This study used four microscopic volumes of Purkinje cell somas of a mouse brain-two were used for training the deep learning model, one for validation of the model, and one for testing the model. The size of each volume is 744 9 999 9 122 [voxel]. The total number of Purkinje cell somas in the dataset was 864 (455 for training, 219 for validation and 190 for testing). The number of patch volumes (inputs of networks) for training was 16,000, a part of which was artificially generated by flipping patch volumes. The optimizer of the networks was Adam. The results of the test volume obtained by using the trained network, whose training epoch was optimized by the validation volume, are as follows. Figure 2 shows a slice image of the test volume and the corresponding segmentation result of the proposed networks. Segmented labels C 0.569 of DICE coefficient (inter-observer variation of manual labeling) were colored from yellow to red, which meant that the segmentation accuracy of extracted cells with these colors was greater than or equal to that of manual labeling. When the sensitivity of cell extraction was fixed at 90%, the number of false positives of connected components with 26 connectivity was The sensitivity is defined as a ratio of correctly segmented cells whose DICE coefficient is C 0.569 (inter-observer variation). We proposed a deep-learning-based Purkinje cell somas segmentation process from a two-photon microscopic volume. The experimental results demonstrated the effectiveness of the combination of the FCN with deep supervision and the boundary network followed by that of the fusion network, whose number of false positives was minimum in our experiment. In future, an experiment using a large microscopic image database of Purkinje cell somas will be conducted to prove the high generalization performance of the proposed segmentation networks. Subsequently, a biological study using the segmentation result, or automated stereological analysis and spatial distribution analysis of cells, will be performed. Purpose Interventional radiology involves performing minimally invasive surgeries in which a catheter or guide wire is inserted into a blood vessel to treat lesions such as cancer, cerebrovascular disease, and heart disease. However, there is a risk that the physician will be exposed to radiation because X-ray imaging is performed continuously while guiding the catheter or guide wire to check the position of the catheter [1] . Consequently, remote catheterization is an effective method for preventing radiation exposure. Furthermore, collision force detection is important in catheter guidance systems because there is a possibility of puncturing the blood vessel wall. Currently, a teleoperation system with only position control and a special catheter in which a force sensor is embedded have been developed. However, it is not possible to detect collision with a blood vessel wall or to utilize a usual catheter. Therefore, in this study, a slave device that performs linear and rotational motions of a usual catheter or guide wire and that includes a force detection mechanism has been developed. This study aims to evaluate whether a collision of the guide wire with the vessel wall can be detected and whether the collision force can be estimated using the developed mechanism. During an actual catheter operation, linear and rotational motions exist. Figure 1 shows a configuration of the teleoperation system for catheterization that enables both movements. First, when a doctor manipulates the master device in the manipulation room, the magnitudes of movements are measured using the encoder in the master device. Subsequently, these data are transferred from the master device to the slave device using Arduino microcontrollers connected with a ZigBee wireless interface. Finally, the slave device moves the catheter or guide wire in the operating room according to the position signal of the master device. The collision force applied by the wire is detected using the force sensor attached to the slave device. The data are transferred in the opposite direction, i.e., from the slave device to the master device. Furthermore, the force is fed back to the doctor using an electro attractive material (EAM) brake mounted on the master device. The force detection mechanism attached to the slave device detects the collision force of the guide wire. The required specifications of force detection ranging from 0 to 0.2 [N] with a resolution of 0.02 [N] were derived from the pressure that penetrates the blood vessel wall [2] . The load applied to the guide wire pushes backward the roller that drives the wire, and the frame on which the roller is mounted also pushes the force sensor. As the force sensor, HSFPAR303 (Alps Alpine Co.), whose force detection ranges 0-4 [N] with a resolution of 0.01 [N], was used. The sensor size is 4 9 2.7 9 2.06 [mm], and thus, it is very small and thin. The experimental set-up shown in Fig. 2 is used to evaluate whether the load applied on the blood vessel wall by the guide wire can be estimated from the output of the force sensor mounted on the slave device. The guide wire driven by the slave device is transferred through a polyvinyl tube that imitates the blood vessel and impacts on a load cell. As the load cell, LVS-100GA (Kyowa), whose range is 0-1 [N] with a resolution of 0.005 [N], was used. The outputs of the force sensor and load cell were measured simultaneously and the results were compared. Furthermore, the effect of moving speed was evaluated by changing the driving speed of the guide wire. Int J CARS (2020) 15 (Suppl 1):S1-S214 The outputs of the force sensor and load cell changed simultaneously when the guide wire collided with the load cell. The relation between the outputs of the force sensor and load cell varied when the velocity of the guide wire was changed. This is caused by the elasticity of the guide wire. The relations were classified into three groups for the guide wire speeds of 12-14, 15-21, and 22-70 [mm/s], respectively, and were approximated linearly. When the speed was low (first group), the relation was not stable. When the speed was high (third group), the output of the force sensor became larger than that of the second group. Using the equation, the output values of the force sensor were converted into the corresponding force values of the load sensor. Errors were calculated by subtracting the measured values from the estimated values. Table 1 shows minimum, maximum and average of negative and positive errors of each group. As shown in Table 1 , the minimum negative error was -0.0445 [N] , and the maximum positive error was 0.0626 [N] . Conclusion A mechanism for collision force detection was developed. It could estimate the force from the output of the force sensor. The relation between the force and the output value of the force sensor varied when the velocity of the guide wire was changed. Therefore, to convert the output to force, the speed of the guide wire should be measured. The maximum positive error, which overestimates the force, and the minimum negative error, which underestimates the force, were derived from the experimental result. As it is dangerous to underestimate the force, the negative error becomes a problem. The current minimum negative error was -0.0445 [N] and it did not reach the required resolution of 0.02 [N] . The error should be improved by selecting a better sensor and a different mechanism. Moreover, cases where the tube is bent and is thus similar to a real operation should be tested in a future study. Introduction of an algorithm for landmark segmentation to guide the application of regional anaesthesia with ultrasound imaging Hochschule Furtwangen University, Villingen-Schwenningen, Germany Keywords Ultrasound, Segmentation, Regional anesthesia, Supraclavicular brachial plexus Purpose Supraclavicular brachial plexus block, a form of regional anesthesia for the immobilization of the upper limb, offers many advantages compared to general anesthesia [1] . A remaining inconvenience of regional anesthesia is guidance during induction. Due to its proximity to the subclavian artery and the pulmonal pleura, the block of the supraclavicular brachial plexus imposes several hazards such as pneumothorax induction or intravascular administration of the anesthetic. Although ultrasound guidance aids the induction of the needle, the correct ablation of the anaesthetic remains still a challenging task and requires a high level of experience and training from the anaesthetist. Thus, an assistive system would be beneficial. This abstract introduces a first approach for an automatic segmentation system of the subclavian artery and the clavicle as fundamental landmarks as a guidance. The algorithm introduced here was developed using the software environment MATLAB (The MathWorks, Natwick, MA, USA). Images of 25 patients were recorded using an ultrasound device (Mindray, Shenzen, China) in B-mode. All images were provided by the clinical partner from the Ortenau Hospital Offenburg, Germany. The developed system first improves the image quality in an initial pre-processing step to reduce speckle noise in the input ultrasound image. Second, the algorithm segments the anatomical structures based on their morphology and size compared to the surrounding objects. Here the magnification used by the operator is considered. The image read-in operation offers the possibility to select various files which allows the input of individual frames of a video and potentially enables real-time guidance during treatment. After cropping the input image to extract the region of interest showing the actual anatomical structures and converting it into grayscale, adaptive histogram equalization is applied aiming the elimination of inequalities in illumination produced during image acquisition. In order to reduce speckle noise a median and a Gaussian filter are applied. The pre-processing of the images is fundamental for optimal results during segmentation. The segmentation of the subclavian artery and the visible segments of the clavicle are mainly based on their unique shape and size differentiating them from other objects visible in the respective image. Since these properties strongly depend on the magnification chosen during examination, this setting has to be considered prior to the actual segmentation process. This is accomplished by evaluating the scale bar located at the right-hand side of each image. First, the segmentation of the subclavian artery is explained. The extended-minima transform detects structures surrounded with borders that are lighter than their center, i.e. dark objects. Next, morphological opening is performed which consists in a morphological erosion and a subsequent dilation in order to separate the objects in the respective image which facilitates the following segmentation procedure. For this morphological operation, a disk-shaped structuring element is used since the subclavian artery adopts a circular outline. The total amount of objects located in the respective image is estimated and the perimeter and the circularity of each object is evaluated. In order to segment the subclavian artery from all other structures showing similar morphological features, ranges based on averaged values were defined for each zoom setting individually in an ad-hoc approach. If an object that fulfils the predefined requirements is detected in the image, the respective structure is displayed on an individual binary image which is then used to determine its boundary. Next, the object borders are projected on the original cropped input image marking the detected subclavian artery (Fig. 1 ). If no object was found that fulfils the requirements, a note is given out to notify the user that segmentation was not successful. Second, the clavicle is segmented in a similar principle. The extended-maxima transform instead of extended-minima transform is used since the clavicle is displayed with high pixel values on the ultrasound images. Hence, objects with borders that are darker than their center, i.e. bright objects, are favored in detection. Once this is done, morphological closing is performed which is a combination of morphological dilation and subsequent erosion in order to fuse closeby structures. This aims to support the final segmentation of the whole clavicle since it tends to split up into various parts. The following segmentation process is similar to that used for segmenting the artery (Fig. 1) . Consequently, it is also based on perimeter and circularity ranges which are specific for each zoom setting. The results of this experiment were promising since only five out of 25 images were analysed wrong, meaning neither the clavicle nor the artery were correctly segmented (Table 1 ). In these cases the system gave out the according notification to the user. However, in five images the issue of declaring the failure of detection by the system arose even though the artery and the clavicle were clearly visible on the input image. This was probably caused by a deviation in dimensions of the respective structures or by severe noise which could not be removed satisfactorily by the pre-processing step. Furthermore, it can be observed that there are slight differences in performance regarding the individual magnifications used during image acquisition. While lower zoom settings seem to produce more consistent results, higher magnifications tend to spot both or single landmarks in incorrect locations. This is probably imposed by the commonly increased speckle noise in images acquired with higher zooms that could not be eliminated totally by the pre-processing step. The developed segmentation of the anatomical landmarks during regional anaesthesia induction is a promising first approach. However, further improvement needs to be done prior a clinical application.  Lower limb peripheral artery disease (PAD) is a common and serious disease. The treatment of PAD has evolved considerably, with endovascular revascularization techniques becoming the established first-line treatment for peripheral artery lesions. A typical scenario begins with Doppler ultrasound to establish the diagnosis and provide a morphological and hemodynamic description of the lesions. It is followed by another morphological examination such as CT angiography (CTA) or MR angiography (MRA), and then arteriography with intent to treat is performed in the operating room (OR). The purpose of intra-operative arteriography is to fully identify and map lesions to be able to navigate and treat them under fluoroscopic guidance. However, arteriography and fluoroscopy acquisitions generate high levels of radiation to the patient and medical staff and the arteriography contrast agent limits its use. Alternatively, three-dimensional (3D) ultrasound (US) provides a non-invasive and non-ionizing method for the visualization of arteries. Pre-operatively, 3D US could help visualizing lesions and enhance the diagnosis step. Intra-operatively, 3D US could partially or entirely replace the arteriography and fluoroscopy to limit the radiation and contrast used during these procedures. The main limits of these systems are their expensive price and their limited field of view, preventing the visualization of the whole artery with a single acquisition. Therefore, the purpose of this work is to propose a freehand and sensor-less method to automatically reconstruct a full US volume from a sequence of 2D US B-mode images at the level of the femoral artery. The proposed method of 3D freehand US is image-based solely to avoid adding supplementary material in the OR. A convolutional neural network (CNN) is employed to predict the relative motion between two consecutives 2D US frames in term of translation and rotation (Fig. 1) . The network therefore takes a pair of 2D US images as input and outputs the displacement between them. In addition to the 2D US images, we also added 2 channels to the input corresponding to the optical flow (using Farneback method) between both images to help the network extracting motion features, as also employed in [1] . A dropout layer was inserted before the output layer with a dropout rate of 0.5 for regularization. The final output contains three parameters for the translation (t x , t y , t z ) and three for the Euler angles (H x , H y , H z ). Therefore, the final volume can be reconstructed by positioning each new frame according to the relative motion with the previous one (Fig. 2) . Clavicle (2) Clavicle (5) Clavicle (1) S174 Int J CARS (2020) 15 (Suppl 1):S1-S214 We used an optical tracking device (NDI Polaris Spectra) to create the ground truth data by recording sequences of B-mode images and their corresponding position. By tracking the probe and after achieving spatial and temporal calibration, the image position in the world coordinates system was determined. Spatial calibration was needed to assess the transformation between a pixel of the B-mode image and the tracked sensor fixed on the probe. For this purpose, we used a N-wire phantom. Ten sequences were recorded for this work on the same healthy volunteer. All sequences were acquired from proximal to distal parts. Four sequences were recorded following the femoral arteries (two for the left and two for the right leg). Since the network should consider background motion features, we included sequences that did not follow the femoral artery: four sequences on the thigh (left and right) and two on the thigh with high motion. The two last sequences with high motions were acquired to improve the network robustness. In total, 4470 images with their relative positions were obtained. Consecutive pairs of images were considered in the database but also, to increase the robustness, pairs of non-consecutive images by skipping up to four frames. Finally, 17,780 pairs of B-mode images were obtained. Two sequences following the femoral artery were isolated, one to test our network final accuracy (331 pairs), one for the validation to optimize our learning parameters (417 pairs). The remaining data were employed as training data to feed the network (13,298 pairs). Note that the low number of pairs for the test and validation data is because we only considered consecutive frames for testing. Adam optimizer was used for the optimization with a learning rate of 10 -3 . The network converged after 100 epochs. Table 1 presents the results on the test database. The low error on the three angles is due to the relatively low rotational motion between pairs during the sequences acquisition. All sequences were acquired on the same healthy volunteer and more tests are needed to estimate the reproducibility of this method. During volume reconstruction, the prediction errors will propagate through the volume and the final error will need to be evaluated in future studies. The accuracy of the network can be improved by adding training and testing data acquired on more volunteers to create variations in the base. This study demonstrates the feasibility of freehand 3D US following a deep learning strategy. This method does not require any additional device. The low computing time allows this method to reconstruct a 3D US volume in real-time. This study is very promising and is the first step to integrate 3D US in PAD procedures for both the planning and the interventional steps. Keywords Pre-procedure planning, Safety margin determination, Signed distance function, Liver biopsy Purpose Percutaneous puncture is a crucial step for thermal ablation of hepatic cancer. Preoperative needle path planning is key to successful puncture which aims to find a trajectory with least damage to surrounding tissues, while satisfying multiple clinical constraints. Thus, this is a multiple objective optimization problem, making it very challenging to find an optimized plan that can fulfill all the above conditions. Methods This paper first presented a 3D signed distance field fusion method for puncture feasible region definition, here we combine the signed distance field [1] of feasible structures (liver, tumor and phantom) where the puncture path can go through and unfeasible structures (large vessels and ribs) inside which the needle through is not allowed. For a voxel inside the object, the distance is marked with a sign of minus and for the voxel outside the object vice versa. Besides, the signed distance field can also indicate the distance from a voxel to an enclosed 3D object. Therefore, the unfeasible region is defined with the voxels, which are marked as minus in the SDF of unfeasible structures. The fusion of the SDF takes the value for each voxels from the SDF of the feasible and unfeasible structures respectively and its value of each voxel takes the minimum value among those in the SDF for the same type of structures. In order to get a safety puncture path, the path to the critical structure should no shorter than a threshold, which is a minimum distance for safety puncture. Thus, only those voxels outside the unfeasible structures, whose SDF is no smaller than the threshold, will be marked as the feasible region. Then we proposed three constraints to automatically determine the puncture trajectory. The first one is obstacle constraint, which is critical in thermal ablation to penetrate large vessels or ribs. This constraint can easily computed via our fused signed distance field, which represents safeness of the puncture path. However, if one of the voxel on the candidate path is inside the unfeasible region (has the negative SDF value or is smaller than the safety puncture threshold), the path will be removed from the candidate list. Besides, the path length should be no shorter than a threshold, which is the minimum insertion length for the tissue supporting the needle well. Moreover, we should also guarantee the path distance should be as short as possible, so that patients can suffer the least damage. The path is determined when the first two constraints achieve the largest value and the third one is smaller enough. Here we designed an energy function by summing up above three constraints with different weight. Here we perform our experiment on the triple modality 3D abdominal phantom (Model 057A, Computerized Imaging Reference Systems, Inc.) as the experimental object. It is a plastic model for medical education usage including artificial liver, vessels and tumors. In addition, the anatomical structures of the phantom can be identified by 3D MR images, Fig. 1 . We invite the experienced doctors for path planning, and compare our planned trajectory with doctors'. Experimental results demonstrated that all the generated paths can satisfy the clinical constraints and our optimal solutions are more precise than that sketched by experienced doctors according to the quantitative analysis, see Fig. 2 . Our method has been proved its effectiveness in path planning for single needle biopsy, while for large tumor, multiple electrodes are needed and clinical constraints will become more complex, so that our immediate plan is to extend our method to multiple electrodes path planning. In addition, the ablation region parameters is defined according to the doctors' experience, we'd like to explore the datadriven method for parameters determination tailored for our optimal path planning method. Fig. 1 The 3D signed distance field fusion. Signed distance field: green denotes plus value (point outside the structure), blue denotes the minus value (point inside the structure). In the feasible region determination step: red is obstacles, blue is feasible region for needle path, dark gap between red and blue is the unfeasible region for needle. Yellow denotes the boundary of phantom structures and the tumor Fig. 2 Comparison between our optimal planned path and doctors'. Magenta: tumor. Green: puncture path given by experienced doctor. White: Our optimal puncture path Institute of Advanced Biomedical Engineering and Science, Shinjuku-ku, Japan Keywords Surgical robot, Third arm, Master-slave control, Laparoscopic surgery By integrating locally operated surgical assistant robots in a sterilized area, a surgeon can perform safe and accurate robotically assisted laparoscopic surgery while controlling a laparoscope-holding robot for view stabilization and a forceps robot for grasping and pulling organs. It is important that the manipulation of surgical robots be intuitive and easy-to use. Therefore, it is necessary for the surgeon to handle an operating device for controlling one of robots selectively. Although a master-slave control such as the da Vinci system provide an intuitive manipulation, the combination of the master and slave devices cannot be exchanged with another system because of non-separable connection. At present, there is no master-slave control system with switchable connection for the master device and each of assistant slave robots that can provide the manipulation intuitively and easily. A new locally operated master-slave selectable control system with multiple connection for robot assisted laparoscopic surgery performed by a surgeon in a sterilized area was developed. We previously reported a master-slave control system with a portable operating device and a locally operated detachable end-effector manipulator (LODEM) for laparoscopic surgery [1] . The present study describes the newly proposed master-slave selectable control system with multiple connection in detail. In addition, we report the performance of the proposed control system evaluated in simulated laparoscopic surgery. We propose a new master-slave selectable control system selected to one of master devices and one of assistant slave robots, through each provider and middleware ORiN-based medical robot architecture (MRLink), regulated by a robot integrator (RI) application as shown in Fig. 1 . ORiN is a commercial middleware advocated by the Japan Robot Association, and has already been used in industry. The MRLink that allows to freely select and connect masters and slaves between different telesurgical robots enables rapid evaluation of different master/slave combinations. Also, the MRLink can be easily connected to smart cyber operating theater (SCOT Ò ) [2] which is a fully functional integrated operation system that can be used for practical treatments. These master devices embedded encoders are a forceps insertable operating device with three degree-of-freedoms (DOFs) in the gimbal coordinate system (Master 0) and a forceps handle shaped operating device with five DOFs in the joystick coordinate system (Master 1). These slave devices are a mobile LODEM composed of parallel linkage crank-slider mechanism driven by stepping motors with five DOFs in the gimbal coordinate system (Slave 0), a guiding LODEM composed of circular ring guided rails mechanism driven by stepping motors with three DOFs in the polar coordinates system (Slave 1), and a portable LODEM composed of gimbal-mounted parallel linkage mechanism driven by DC servo motors with three DOFs in the gimbal coordinate system (Slave 2). Providers with device information such as the number of axes and the kinematics model, and with device control commands such as sensing or driving of the rotated angle and velocity, are constructed using CaoVariable and CaoController functions in ORiN2. RI application with unilateral master-slave point-to-point control are simply constructed using the position and the orientation of both of the selected master and slave devices. Each controlling slave device is exchanged under the specified motion of the master device. The master-slave control is performed when a footswitch is pressed. In order to evaluate the new master-slave selectable control in multiple connection system, the system prototype connected to Master 1, Slave 1, and Slave 2 were used. Simulated laparoscopic cholecystectomy was performed on a surgically realistic gall bladder model (50128, Limbs and Things, Bristol, UK) in a training box (Endowork-pro II, KARL STORZ) by a specialist. A laparoscope attached to Slave1 fixed at the foot side of the surgical table, and a forceps attached to Slave 2 positioned on the left-hand side of the table. The operator, an endoscope specialist, stood at the foot side of the table and used scissors in his right hand and forceps in his left hand. The selected slave devices were controlled by the Master 1 positioned to the right of the operator. The specified motion of the mater device to exchange the slave device is set to open and close of the handle at three times within three seconds when the foot switch is not pressed. Results Figure 2 shows a photograph of the simulated laparoscopic cholecystectomy. One of two slave devices could be selected and controlled by the Master 1 easily and intuitively. The laparoscopic view was moved to the asked place using the Slave 1, the organ model could be pulled in various directions using the forceps attached to the Slave 2. The forceps held in the left hand could also be used to grasp and pull the organs with sufficient tension, while the scissors held in the right hand could be used to dissect the organ model. Smooth dissection of the organ model was performed by the endoscope specialist. Successful simulated laparoscopic surgery was performed for 17 min. Fig. 1 Proposed locally operated master-slave selectable control system with multiple connection Fig. 2 Simulated laparoscopic cholecystectomy performed by an endoscope specialist using the proposed system connected to one master device and two slave devices with a laparoscope and a forceps selectively S178 Int J CARS (2020) 15 (Suppl 1):S1-S214 We developed a locally operated master-slave selectable control in multiple connection system for robot assisted laparoscopic surgery performed by a surgeon in a sterilized area. The master-slave exchangeable system connected to master and slave devices through each provider and middleware ORiN-based MRLink regulated by the RI application is constructed to facilitate minimally invasive, robotically assisted laparoscopic surgery by a doctor working near the patient. The results of the present study indicate that the proposed device could be used for such applications. The present study was supported in part by Kakenhi (JP18K04065, JP16H01859). Keywords Stereo calibration, Surgical microscopy, Augmented reality, 3D measuremts This work introduces a statistical methodology to assess the 3D measurements performed by a digital surgical microscope. The used surgical microscope (ARRISCOPE, ARRI Medical GmbH, Germany) offers new ways for intraoperative assistance and image-guided interventions. The digital image-processing pipeline resolves many problems related to existing analogue microscopes, when it comes to augmented reality (AR) and 3D reconstruction. For such an imaging device, it is no longer necessary to use a beam splitter to digitize and capture the surgical scene. Nonetheless, the calibration of such a stereoscopic system is still highly challenging and unavoidable for metric 3D measurements. In a previous work [1] , we designed a zoom-independent calibration scheme to solve three main issues: (1) Simplify and automate the calibration process using a hybrid calibration target and a motor controlled lens system to address varying zoom levels. (2) Establish a standardized calibration and statistical evaluation workflow and derive ground-truth comparisons of 3D measurements, 3D planarity constraints and 3D discrepancies values. (3) Create a suitable non-expert tool for recurring re-calibration and maintenance tasks for deployed systems. The developed calibration tool gives a solid objective base when it comes to the possibilities for surgical AR application and 3D measurements. In this work, we present results regarding the statistical evaluation of different zoom levels with fixed focus settings. Camera calibration techniques using checkerboards have been intensively discussed for many applications and are widely accepted within the computer vision community. Nonetheless, these methods rarely address the problem of calibrating optical imaging systems, which have varying focus/zoom settings and very long focal lengths. In addition, the usage of different calibration patterns for different zoom settings is quite cumbersome because of a small field-of-view and a decreasing depth-of-field. For these practical reasons, we use a hybrid calibration target, which just needs to be positioned once for every zoom/focus combination to automate the recording process of calibration and test images. The integrated refocus mechanism also guarantees that all images remain in focus, which shall not be confused with auto-focus. The calibration target is made of ceramic with a printing precision of 10 lm. Ceramic presents the advantage of minimizing the reflection effects, which can lead to calibration problems. Our calibration method follows a model-based approach using synthesized images and applying an image registration via gradient-descent [2] . Figure 1 shows our hybrid target in minimum zoom level and colored corner points used for statistical evaluation based on unique IDs. The calibration dataset consists of three predefined zoom positions (1.58 9 , 2.91 9 , 9.60 9). The test dataset consists of one intermediate zoom position (2.24 9), which is not used for calibration. For every pre-defined zoom level, we captured 25 stereo pairs. We estimate three stereo camera parameters using 20 out of these 25 stereo pairs with low pre-orientation errors. The remaining five pairs are used for evaluation. For the intermediate zoom position, we interpolate our camera parameters using the value from 1.58 9 and 2.91 9 . Statistical evaluation is performed on a grid inside the inner part of the pattern for two reasons: (1) overall visibility in all zoom levels for eased comparison, (2) coverage of the very important surgical area. The metrological quality of the 3D calibration can be assessed by measuring how good the 3D reconstruction of feature points fit to the real object. We use the following statistics: (1) Re-projection errors of 3D reconstructed points. (2) Measurement accuracy: Distances between a preselected set of known ranges are computed for reconstructed points and compared to ground truth-values. For two feature points x j1 , x j2 we use the distance deviation Dd j1,j2 by (3) Planarity: The original object is planar. The distance of the reconstructed point b x ij to the reconstructed object plane is a good indicator for planarity. For all views F i , we make use of the projection P i on the object plane and define a planarity index u i for each view i by Fig. 1 Hybrid calibration target in minimum zoom. Square sizes are from inside out: 0.5 mm, 1.0 mm, and 2.0 mm. Colored points are used for statistical 3D evaluation. These points are identified by a unique ID (4) 3D discrepancy relies on the distances (in 3D) between reconstructed points x ! j1 and their associated point in the estimated view object x ! obj ij , which is very close to the corresponding homographic transformation of the ground truth object, even not knowing the exact position of the latter. We define the 3D point discrepancy by The described method has two major advantages: (1) it is ''cheap'' in terms of computational power and (2) it allows a completely automated calibration. First statistical results are listed in Table 1 showing two important observations: (1) calibration values for zoom level 2.24 9 are computed by interpolating zoom levels 1.589 and 2.919. The measurement accuracy remains at a very acceptable level of 76 lm. (2) No meaningful focal length is estimated for maximum zoom level, as our linear optimizing function did not converge. The investigations of this effect is ongoing work. We presented a 3D accuracy evaluation for a zoom-independent calibration target using a digital surgical microscope. We achieved promising results indicating low re-projection errors and accurate 3D statistics. The usage of a high quality manufactured ceramic calibration target reduced problems of feature detection significantly for large magnification factors. Although calibration of long focal length still needs to be further investigated. Nonetheless, achieved results are an important step to facilitate medical AR applications and 3D measurements. In particular, such quantitative indicators of the 3D measurement accuracy are crucial for medical applications, which require a high level of precision and small tolerances e.g. choosing the correct size of middle ear prosthesis for ENT surgery. The distance between the bifurcations of the tumor feeding artery of oral cancer is unlikely to change with head and neck posture changes Purpose Superselective intra-arterial chemoradiotherapy administered via the superficial temporal and/or occipital artery is an effective clinical treatment for oral cancer. It can provide high concentration of anticancer drugs to the tumor feeding arteries (the target arteries: the maxillary, facial, and lingual arteries). However, this treatment approach requires the surgeon's experience and skills for catheterization into the target artery as the tip of the catheter is not directly visible. In general, surgeons depend on the intraoperative two-dimensional C-arm image. The primary problems associated with this technique include the prolonged operation duration, the increased radiation exposure, and the increased dose of contrast medium, particularly in difficult cases. To overcome these problems, we developed a prototype of an electromagnetic maxillofacial catheter navigation system [1] . This system provides the operator with a threedimensional image of the external carotid artery (ECA) intraoperatively. Previous studies have reported that changes in the head and neck posture can deform the ECA [2] . Therefore, even when the preoperative 3D-CT angiography (3D-CTA) image is superimposed on the intraoperative image of the patient based on the maxilla, the position and posture of the target artery remain unknown. Hence, catheterization should basically be navigated using intraoperative images as reference. However, the preoperative 3D-CTA images that have been referred to by the surgeon in clinical practice to date have certain utility. In previous studies, we measured the distance between the bifurcations to each target artery in the ECA based on the length of the centerline. When the length of the centerline of the target artery does not considerably change regardless of the deformation of the artery owing to head and neck posture changes, we can navigate the catheter to the target bifurcation point by evaluating the displacement along the centerline of the artery using an electromagnetic tracking device from a certain known bifurcation point. To test this hypothesis, the distance between the bifurcations was measured in patients whose 3D-CTA images were obtained twice in the same patient. In this study, we examined how the distance between the bifurcation points to each target artery changes depending on the head and neck posture changes. The purpose of this study was to evaluate whether the length of the centerline of the ECA constructed by preoperative 3D-CTA image is within an acceptable error range that can be referred to during surgery. A multislice CT scanner (Aquilion 64; Toshiba Medical Systems, Tokyo, Japan) with 0.5-mm 9 64-slice collimation was used for scanning the 3D-CTA images. The scanning conditions were 120 kV, 250 mA, slice interval 0.5 mm, slice thickness 1.0 mm, 512 9 512 pixels, and pixel size 0.43 9 0.43 mm 2 . The CTA image data of patients with advanced cancer were obtained twice for each patient, once before and once after the administration of superselective intraarterial chemoradiotherapy. The analysis included the angiographic data of the noncatheterized carotid arteries (left or right) of all patients. Among the patients with oral cancer in whom the right or left carotid artery was not catheterized, 5 patients were randomly selected. The segmentation procedure (binary image processing of CTA images) was performed using Mimics (Materialize, Leuven, Belgium). The ECA centerlines were automatically extracted. The extracted fiducial bifurcation points were vA: internal carotid artery and ECA via the common carotid artery; vB: lingual artery via the ECA; vC: facial artery via the ECA; and vD: maxillary artery via the ECA. The distance between the bifurcations of the target artery in the ECA was measured (Fig. 1) . Next, the difference between the distances measured in the first and second 3D-CTA images was calculated. The research protocol was approved by the IRB of Yokohama City University Hospital. In the two CTA images, the median difference in distance from vA to vB was 0.65 mm and the average difference was  The restoration of intestinal patency after the resection of pathologic tissue is amongst the most frequently performed operations in the digestive tract. Furthermore, the trend of trauma reduction through the minimization of surgical access routes, as well as the further decrease of postoperative complications characterise current aspirations in visceral surgery. Laparoscopic techniques, which are already state of the art for a variety of different indications, are more and more refined towards even less invasive procedures such as single port surgery and operations purely via natural body orifices. Amongst others the most commonly known advantages of minimally invasive surgery for patients are an improved postoperative healing process, shorter hospital stays, reduced postoperative morbidity rates as well as better cosmetic results. Despite various innovative approaches to form the discipline of intestinal anastomosis and to enable endoluminal procedures in the GI tract, no technique has yet been developed that is accepted as a standardized micro invasive alternative to conventional anastomization techniques. In the context of the DFG funded research project ''CONNECT'' we focus on developing a patient adaptable micro invasive anastomization procedure for ubiquitous use in the GI tract, including the design of a compression anastomosis implant and an endoluminal applicator system. Methods Different surgical procedure concepts were derived and implant approaches were designed, 3D printed and tested in an abstracted model and within the scope of an experiment in a pig cadaver to evaluate suitability and applicability of the implant prototypes. In this context we tested anastomosis impermeability and stability. Additionaly first experiments were performed to determine forces required to pierce bowel wall tissue and manipulate proximal and distal bowel endings. These parameters are important for dimensioning the system. For the conceptual development of the implants it has to be considered that anastomotic leakage is amongst the most critical complications of intestinal patency restoration with a high risk of subsequent mortality. That's why it's essential to guarantee a securely sealed bowel connection. In this context we decided to design an Fig. 1 Bifurcations of tumor feeding arteries (target arteries) in the external carotid artery, vA: internal carotid artery and ECA via the common carotid artery, vB: lingual artery via the ECA, vC: facial artery via the ECA, vD: maxillary artery via the ECA implant for the generation of an invagination anastomosis to maximize the anastomotic line [1] , while simultaneously enabling secure contact of serosal tissue, the outermost layer of the intestinal wall, which has a significant influence on complication free anastomosis healing [2] . As there may occur large anatomical variations between the patients concerning bowel wall thickness and lumen diameter, our system will enable patient-adapted size scalability to offer a simple alternative to the currently available size restricted systems, which often imply high storage costs. First experimental results with the implant prototypes showed promising results in terms of applicability and feasibility for endoluminal use in the abstracted model. However, the anastomosis assessment in the pig cadaver still showed some weaknesses. An experimental protocol has to be derived and relevant parameters as basis for an objective comparison between the different models have to be determined. Furthermore, the different tasks of the application device have to be defined and realized by implementing competing solutions. The different approaches have to be experimentally evaluated to assess mutual functionality of the applicator-implant system. In minimally invasive surgery, the computer assisted intervention during the surgical operation is expected to improve surgeon performance [1] . Segmentation of surgical instruments is considered as a key factor in its intervention. For instance, tracking of surgical instruments may lead to surgeon skill assessment and autonomous robotic surgery [2] . In order to distinguish surgical instruments present in surgical videos from organs appeared in background, instruments need to be annotated at pixel level on the movie frame. The high-accuracy segmentation model for distinguishing surgical instrument requires large number of annotated training data, however the manual labeling of each movie frames take costs and time consuming. The purpose of this study is to create a segmentation model from automatically generated annotated data and evaluate this model on whether the surgical instrument is segmented with high accuracy. The proposed method consists of two phases: automatic generation of annotation image, and training of deep neural networks using generated data from the first phase. During the automatic generation phase, two different images were generated: the mask image and the input image. The mask image is created manually within laboratory environment, consist of surgical instrument snapshot, while the input image is synthesized from the mask image and original laparoscopic surgical background picture. The instrument targeted on this study is an ultrasonic coagulation and incision device. To create a surgical instrument snapshot, a green colored paper was prepared as an image background. The images were taken with endoscopic camera with various angles and perspectives for higher-proximity to the real surgical images. According to our hypothesis, the more realistic the image gets, the higher the accuracy may be expected. The image then processed to withdraw only the surgical instrument for further image composition. Once the mask image is processed the laparoscopic image, which does not contain surgical equipment, was randomly selected to synthesize the input image. A total of 800 input images were created for training data. The image contains surgical tools reaching from various sides, angles, near and close distance from camera, with its leading edge opened and closed. In second phase, the segmentation model DeepLabv3 ? were trained with 800 generated images. To test the trained model, 200 test image data-set was prepared from original surgical videos with manual annotation. As a result of evaluating the model trained on 800 synthesized images, the IoU tested with 200 images was 0.695. The proposed method leads to an automatically generation of input data-set and mask images. The result showed automatically synthesized images may perform high recognition in surgical tools with segmentation model. The utilization of this automatic generation of image may support cost-efficient annotation with less amount of time required in the near future. using a force sensing resistor, and in vivo test is performed to evaluate the 4 DOF robot system. In the previous work [1] , the contact force of a guidewire was indirectly measured by using the current of the driving actuator and reflected to the master robot. In addition to the contact force of the guidewire, force feedback of the catheter is also needed. As shown in Fig. 1 , a force sensing resistor (FSR), Flexiforce A201 sensor, is attached to the Y-connector part connected to the end of the catheter in order to measure the contact force of the catheter directly. The measured force is scaled up or down and reflected to the user through the master robot. In order to perform in vivo test, the position and the orientation of the robot should be adjusted and fixed to the bed, so a mounting device has been developed. The mounting device with the robot is moved forward and backward along the rails of a surgical bed by several pairs of ball bearings. The angle of the slave robot is also adjusted by a handle. The master robot was installed at the master site separated from a surgical bed by a radiation shield barrier, and the slave robot was mounted on the surgical bed with the mounting device. Operators manipulate the master robot at the master site with the radiographic images. A 40 kg female domesticated pig was chosen as an experimental subject. The goal of the experiment was insertion of a catheter into the right renal artery (RRA) through the femoral artery. One expert operator and eight non-experts participated in the experiment. Each participant repeats trial three times with haptic function and without haptic function, and the contact force of the guidewire and catheter are measured. The completion time for the installation of the slave robot and assembly of the VI devices are measured. The completion time and the radiation dose of the conventional procedure and the robotic procedure are measured and compared. The doses are measured at the master site and at the slave site, respectively. As an experimental result, the catheter was successfully inserted into the target artery. Figure 2 shows the measured force of the catheter insertion task into RRA. The measured force was transmitted to the master device and the user felt the reflection force. The maximum force for the straight artery and the curved artery are 104 mV and 557 mV, respectively. The maximum feedback force for the straight artery and the curved artery is 82 mV and 158 mV, and the user can feel a larger force at the curved artery. Total time from installation of the robot to the start of the procedure is 465.89 s and total removal time is 97.78 s. Completion time was measured during the catheter was inserted from common iliac artery to RRA. Completion time was measured to be 24.64 s to 180.56 s. Compared to completion time for the conventional procedure, which took 9.71 s, the robotic procedure takes long time because the maximum driving speed of the slave robot was set to be low for the safety. However, it is meaningful that as the number of trials increases, the completion time decreases and a non-expert failed at first procedure, but succeeded in the next trials. The radiation dose was measured with a radiation dosimetry at the bed during the conventional procedure and at the master site during the robot procedure. The dose during the conventional procedure was 2.44 mSv/h and the average dose during the robot procedure was 259.6 lSv/h. The remote control greatly reduces the radiation exposure by 89.3%. As can be seen from the experimental results, the catheter was successfully inserted into the target artery, and the contact force of the catheter was directly measured by the FSR sensor, thereby the contact forces were reflected to the operator through the master device. The ease of use and adaptability of the developed robotic system were also verified through the experiment. The FSR was used for the experiment to measure the catheter contact force, but the sensor should be embedded in the robot in order to secure electronic safety from the sensor because the Y-connector and the guidewire may contact the sensor and it can cause electronic problem. As on going work, we are modifying the robot design to secure the electronic safety. Reference Intraoperative navigation has been proposed to ensure accurate translation of the virtual plan into the operating room using 3D printing and optical tracking [1] . However, navigation information is presented on an external monitor, forcing surgeons to focus their attention on the display and away from the patient. In this context, augmented reality (AR) can offer patient information to the physician by including clinical data in the sight between him and the patient. AR technology has previously shown how physicians can benefit from guidance during clinical interventions, allowing interaction with the virtual 3D models and the real environment [2] . In this study, we propose a new method based on AR to allow surgeons to translate preoperative virtual planning into the operating room, to improve the reproducibility of these surgeries and to reduce inter-surgeon variability. Our objective was to evaluate the use of AR as an additional tool from the navigation workflow proposed in [1] to guide the surgeon while positioning the remodeled bone in the target location. The use of this technique was evaluated in one patient affected by metopic craniosynostosis. The proposed approach was tested and evaluated in the surgical reconstruction of a 12 months old infant presenting metopic craniosynostosis. Surgical correction was performed through frontoorbital advancement. Informed consent was signed by the parent or legal guardian of the patient before the surgical intervention. A preoperative computed tomography scan was acquired to confirm the diagnosis. This study was used as a reference for virtual surgical planning. Surgical osteotomies and the target shape for the resected bones were designed in specific computer-aided software, generating 3D models that are exported in stereolithography file format. Target shape was determined objectively considering the age and gender of the patient. A software application was developed on Unity platform to visualize virtual surgical planning overlaid on the surgical field by means of AR-based visualization. The application is based on an RGB camera (Intel Ò RealSense TM ) for video recording of the surgical field, and a 3D printed tracking marker attached to the patient's bone surface for real-time positioning of the anatomy during surgery. For each video frame, 3D position of the marker was computed using Vuforia Ò development kit. Then, this computed position is used to update the AR visualization (i.e. virtual model position and orientation in the image) according to the current position of the patient's anatomy. The 3D tracking marker was designed to contain a unique pattern and several holes for optimal fixation to the bone surface. This marker was manufactured in polylactic acid material using a dual-extruder desktop 3D printer (Ultimaker B.V., Netherlands). In addition, the 3D printed marker was sterilized prior to surgery to maintain asepsis of the surgical field using a sterilization technique based on ethylene oxide at 37°C. Finally, the marker was fixed rigidly to the healthy bone surface surrounding the affected region using two resorbable pins (Fig. 1A) . After fixation, the relative position of the marker with respect to the patient's anatomy was calculated using the intraoperative navigation system described at [1] and transferred to the AR software application. The developed application was used during the remodeling phase of the surgery to indicate the target position of the remodeled bone fragments according to the preoperative virtual surgical plan (Fig. 1B) . The AR information was displayed to the surgeons on an external screen adjacent to the surgical field. The accuracy of bone fragment positioning using AR guidance was evaluated using intraoperative navigation as gold-standard. After bone fragments were positioned by the surgeons using AR, the real position of the bone was computed by recording multiple 3D points along the bone fragments surface using an optically tracked pointer tool (Fig. 2) . The average AR positioning error was computed as the average Euclidean distance between those points and the preoperative virtual surgical plan. The surgical procedure continued then following the workflow as described in [1] , without the use of AR for guidance. The average positioning error of the bone fragments using AR guidance was 0.89 ± 0.49 mm. The AR application displayed enough information to manually position the bone fragments without requiring navigation assistance. The aesthetic result was satisfactory, showing an improved bitemporal width and correction of the metopic ridging. No postoperative complications occurred. The positioning error, below 1 mm, indicates an accurate match between the remodeled bone fragment position and the virtual surgical plan. This shows the accuracy of the AR visualization and the feasibility of surgical guidance during bone fragment positioning. Int J CARS (2020) 15 (Suppl 1):S1-S214 However, our proposal would benefit from AR head-mounted displays, such as Microsoft HoloLens, that could be an alternative to directly overlay virtual 3D models on the surgeon's field of view without requiring an external camera or monitor. The proposed AR approach allows surgeons to precisely translate preoperative virtual planning into the operating room, overlaying the target bone fragment in the surgical field. This methodology could improve the reproducibility of these surgeries and reduce inter-surgeon variability. Purpose A distal radio-ulnar joint (DRUJ) prosthesis is a treatment option in arthroplasty of the DRUJ. It helps restoring joint function and relieves pain. Unfortunately, complications, such as periprosthetic fractures and aseptic loosening, often occur. Better knowledge of the kinematic properties of the joint in the design and placement of the implant can help reduce stress on the bone and thus reduce complications related to DRUJ prostheses. Four-dimensional (4D) as well as regular computed tomography (CT) have already been used to investigate one of the primary kinematic properties of the DRUJ: the axis of rotation. However, a consensus on the motion of the distal part of the radius around the ulna has not been reached. This may be attributed to the methodological error, which is not quantified in available literature. The goal of this study is therefore to use advanced 3D image analysis technology to investigate motion of the radius about the ulna during pronation while taking into account the methodological error. After an initial CT scan, a cadaver arm with an intact cartilage layer and without signs of previous injury was selected. Placement of the arm in the setup was achieved through limited fixation using an intramedullary pin inserted in the humerus and attachment of the hand to the handle using Velcro straps (Fig. 1) . This ensures natural motion of the joints in the forearm. To evaluate the methodological error the arm was scanned 10 times at 120 mAs without moving the arm between scans. During image processing the radius and ulna were segmented in the first frame, which serves as a reference, and then registered to the remaining 9 frames. Subsequently, the methodological error was determined, represented by the mean target registration error (mTRE), being the mean distance between corresponding points of the bones in the reference position and the 9 observed positions. Forearm rotation was assessed by placing the cadaver in full supination and rotating the handle by 10°increments until full pronation was reached. At every step a scan was made, effectively simulating a 4D scan. After segmentation and registration as described above, the centroid of the distal part of the radius was determined and plotted for every timeframe to visualize the path of the distal radius with respect to the ulna during forearm rotation (Fig. 2) . A circle fit was performed to find the approximate center of rotation. The mTRE for the radius and the ulna was 0.018 ± 0006 mm and 0.015 ± 0.005 mm respectively (Table 1 ). Motion analysis of the centroid of the distal part of the radius revealed a nearly circular motion path suggesting a fixed axis of rotation (Fig. 2)  The small methodological errors enable investigation of the path of motion with a high reliability. Motion of the radius around the ulna was near circular suggesting a static center of rotation. The research is ongoing and focuses on variability observed in the center of rotation  The goal of this project is to develop and evaluate a patient-mounted MRI-compatible robot that allows for accurate needle placement while completely eliminating radiation exposure. We are creating a body-mounted MRI-compatible robot for perineural injections to treat pain in adult and pediatric patients. The robot will enable needle tip and target visualization under MRI in real-time through an integrated imaging coil, built-in fiducials for image registration, and active needle insertion/rotation. MRI provides unmatched soft tissue visualization of the targeted nerves and delivery of the locally injected medications without contrast. Chronic pain management is an important clinical problem in both adult and pediatric patients. Studies indicate that approximately 35% of the US population in adults and 20-35% of children worldwide suffer from chronic pain [1, 2] . Patients with chronic pain often have adverse physical and mental consequences if chronic pain is not managed effectively. Opioid prescriptions have skyrocketed, increasing almost 275% from 1991 to 2013. With increasing concerns related to opioid prescriptions, physicians are seeking non-narcotic solutions to pain management. Perineural injection is an essential tool in the pain management device market for the diagnosis and treatment of chronic pain. High success requires good visualization of the targeted nerve and accurate needle placement. Methods The overall system consists of the robotic device and an MRI imaging coil that mounts to the patient under the robot base, Fig. 1 . The robot is designed with a four-degree of freedom needle alignment module and a two-degree of freedom remotely actuated needle driver module. The six-degree of freedom fully actuated robot can operate inside the scanner bore during imaging; hence, minimizing the need of moving the patient in or out of the scanner during the procedure, and thus, potentially reducing the procedure time and streamlining the workflow. The novel remote actuation design of the needle driver module with beaded chain transmission can reduce the weight and profile on the patient, as well as minimize the imaging degradation caused by the actuation electronics. The needle driver can be operated in the manual or motorized mode by disengaging or engaging the gears transmission of the actuation units; thus, it could increase the safety in case of motors fails as well as facilitate the learning curve since the clinicians could manipulate the needle manually. The integrated imaging coil is a single loop coil built inside a circular case to mount under the robot. The coil is attached the back of the patient using straps. To allow the coil to be used with our Siemens 1.5T Aera scanner, a coil interface box was built by the company Stark Contrast (Erlangen Germany). The interface box has a connector that plugs into the Siemens scanner and then eight BNC connectors for custom coils such as ours. We are also able to combine our custom coil with the Siemens table coil for improved imaging. Hospital approvals including infection control were obtained for a cadaver torso study. The torso was obtained from Science Care Inc. (Phoenix, Arizona) and positioned prone on the table. The single loop coil was strapped to the back and the robot was locked to the coil. MRI images were obtained and sent to the robot control system. The robot has fiducial markers which are recognized by the robot control software and used to register the robot coordinate system to the scanner coordinate system. A target was selected in the back and the robot was commanded to move the needle guide over the entry location and orient the needle guide toward the target. The Interventional Radiologist then used the remote needle drive to manipulate the needle towards the target. While this study showed the validity of our concept, the needle drive was not stiff enough to accurately place the needle and will be improved in future work. We have described the development and first cadaver study of a patient-mounted MRI-compatible robot for pain injections in the back. Purpose Calibration is the first crucial process in an image-guided surgical system. Traditional surgical systems employ bulky and expensive optical or electromagnetic tracking systems to establish real-time correspondence between patient and surgical instruments. Alternatively, some systems may use only vision based tracking systems which typically employ externally placed cameras to determine and track poses of different entities in the surgical setup. However, in case of procedures requiring surgical microscopes, it is not evident how the tracking information from cameras can be transferred onto the microscope view as the fields of view are highly distinct. The purpose of this study was to propose a stereo-calibration technique for calibrating a microscope camera and an external tracking camera with partially or non-overlapping viewpoints. We present a stereo calibration technique for estimating the extrinsic relationship between a surgical microscope and an external tracking camera rigidly attached to each other. The procedure starts by first calibrating the two cameras individually using conventional calibration techniques such as Zhang's algorithm. The calibration parameters were used to undistort the camera images for accurate stereo calibration. A specific calibration was prepared for the setup which consisted of two identical planar chessboard patterns with different sizes printed on a calibration board (Fig. 1) . The calibration pattern was chosen as the world coordinate system such that the first chessboard corner of the left pattern was taken as origin (0, 0) whereas the first chessboard corner of the right pattern was taken as (a, b) where a and b represent the x-y distances between the two chessboards. The calibration pattern was placed in front of the setup such that each chessboard was visible in its corresponding camera view. The chessboard corners were detected in each camera frame simultaneously and the transformation matrices of each camera T M (microscope camera) and T E (external tracking camera) were estimated using: where P i are the detected chessboard corners in image space, P w are their corresponding coordinates in world coordinate system, K is the intrinsic camera matrix and T = [R t] represents the extrinsic parameters. The stereo-calibration matrix T ME between the two cameras can then be computed as: Only one calibration pose could be sufficient to determine the relationship between the two cameras. However, owing to imperfections in calibration and chessboard detection, multiple poses can be recorded and the stereo relationship determined using least means squares'' approach. Based on the evaluation methods proposed in [1, 2] , we performed two series of experiments to validate the calibration technique. First, we placed two identical cameras (C270, Logitech International S.A., Switzerland) 20.0 cm apart from each other. Two 9 9 7 chessboard patterns were printed with 10 mm and 15 mm square sizes and placed in front of the cameras such that each camera can only see one of the patterns. The distance between the two patterns was 14.5 cm. The calibration was carried out by recording images at different poses and the results are presented in Table 1 . Furthermore, since this setup involved partially overlapping camera views, conventional stereo calibration approach was also applied with a 15 mm checkerboard pattern visible in both cameras together. Similar results were obtained as depicted in Table 1 . In the second series of experiments, an HD camera (xiQ MQ013CG-ON, Ximea Gmbh, Munster, Germany) was attached on one of the eyepieces of the surgical microscope (Zeiss OPMI MDO S5 Microscope, Zeiss, US). An external camera was then attached to the microscope such that the two cameras have a rigid relationship as shown in Fig. 2 . Two 10 9 7 chessboard patterns were printed with 3.7 mm and 0.6 mm square sizes and placed in front of the cameras similar to the previous series of experiments. The distance between the two patterns was kept at 6.9 cm. Stereo-calibration result for this setup is also depicted in Table 1 . A relatively higher reprojection Fig. 1 Proposed calibration pattern Int J CARS (2020) 15 (Suppl 1):S1-S214 S187 error was achieved in this setup owing to the motion constraints due to the presence of the microscope. In this study, we proposed a stereo-calibration technique for a pair of microscope and external tracking camera. The system uses two different calibration patterns placed at a fixed distance from each other such that each pattern is visible only in one of the cameras. The stereocalibration was determined using geometrical relationship between cameras. The proposed system can be used to establish correspondence between the tracking camera and the surgical microscope in procedures requiring the two camera vision systems such as in ENT surgeries. The system yielded comparable result as conventional calibration technique which requires a significant overlap between the two camera views. As proved through the two series of experiments, the technique may be used for any system of heterogeneous cameras and can be used to calibrate multiple series of cameras by setting one of the cameras as the origin and computing the relationship between each camera with the base camera. In future, we plan to extend the method to setups that require non-planar calibration patterns. Purpose Patients with drug-resistant epilepsy may benefit from surgical intervention if the epileptogenic zone (EZ) is well localized. To this aim, in stereoelectroencephalography (SEEG) several deep electrodes are placed through percutaneous drill holes to stereotactically defined coordinates in the brain. Thanks to less invasiveness, lower complications rates and the advent of stereotactic robot assistance, SEEG is nowadays the preferred modality, at least in Europe [1] . Correctly placing the electrodes without adversely affecting vessels and specialized brain areas requires interdisciplinary interaction between different medical specialists. In previous works [2] we presented a planning and navigation system for electrode placement that integrated CT, gadolinium-enhanced MRI and EEG data to make it easier for the clinician to safely place the electrodes. In this work we present our early results on exploring the electrode planning using Virtual Reality. Given the good results that Virtual Reality interfaces obtained in other medical fields, we believe that by interacting with a 3D model in space clinicians can better understand the consequences of each choice and ultimately led to better surgical outcomes. Existing surgical planner: The previous software was already able to: (1) load and visualize multimodal CT and MRI scans together with safe volumes for the electrodes to be implanted-see Fig. 1a -and (2) navigate the planned scene-see Fig. 1b for an example with a brain calibration phantom-. These capabilities and the corresponding data were used also for the VR exploration. Virtual reality headset: For this work we used the HTC Vive headset together with the Leap Motion controller in order to freely track the hands of the user without the need for a joystick. Programming environment: To develop the application the main program used was Unity, which is one of the most used frameworks to program user interaction experiences and video games. The code was in C#. Metric: In order to evaluate the results, we run a survey where both clinical and non-expert users tested the platform and rated their simplicity and overall impression. The application we created can display arbitrary volume renders and meshes extracted from medical images and the user can interact virtually with the objects in the 3D space using her own hands, which are detected thanks to the Leap Motion. More complex interaction such as combining different meshes, cropping, changing transparency and activating controls specific for the surgical application in use are accessible from an emergent menu that can be visualized with a hand gesture. More importantly, it is possible to interact with the position and orientation of the electrodes defined in the pre-operative planning stage to directly inspect which brain regions are being touched. We employed a usability test methodology to test the impact of this new VR interfaces on both the clinical users and the non-expert ones. We recruited a group of ten people, including 2 surgeons, expert in SEEG placement and 8 non-experts. All the non-experts had no clinical background and no previous experience on SEEG electrode planning. All of them had an introductory tutorial that allowed them to familiarize with the procedure. The survey covered only the new VR interface, not the planning software. We used the concurrent think-aloud protocol, in which users are encouraged to stream their thoughts during the task so that the examiner can better understand the usability issues. User satisfaction and overall experience were both high (mean 4.5/5 and 4.9/5 respectively), however we found that (a) interaction with the hands can be unreliable sometimes depending on the luminosity of the room used for the test (b) clinical users prefer to use something that does not block the real world view and which does not require putting a full headset device on. For these reasons, we will investigate in the future the use of augmented reality devices instead of full VR ones. In this work we presented an application for the exploration of SEEG electrode placement in virtual reality. We received very positive feedback from the users, showing that virtual reality interaction on medical data is well received. Following the comments of the clinical users, we will focus the next studies on augmented reality devices that allow to combine real world view with 3D rendered objects. Purpose Developing surgical instrumentation is not only challenging but also notoriously resource consuming. Before being able to assess the added-value of the developed technology in a clinically relevant environment, lots of time and money needs to be invested. Providing insight on the potential added-value of the envisioned technology at an earlier phase could help reduce the risks of wasted investments. Surgical simulators, which are well established in the scientific community, could make a huge impact here. Users would be allowed to manipulate existing instruments on the simulator but also a partially developed novel tool, finalized with the means of virtual reality (VR). An assessment of the user's technical performance through the analysis of qualitative and quantitative data can then provide a comparison between existing and novel technology. In the literature, prior work along these lines is scarce. Spiers et al. evaluated a novel virtual tool on a custom laparoscopic simulator [1] . The experiments were conducted with unexperienced users in an environment without any specified validation and with no assessment reference. This thus lowers the confidence that can be placed in the outcome of the study as it is not grounded in reality. We propose in this abstract an assessment framework to help engineers properly evaluate the potential added-value of an early design. This work completes the existing assessment framework for image-guided interventions [2] by proposing a level 0 of assessment, allowing early-on insights on efficacy before the technology has even been developed. Fetal laser surgery (FLS) treating twin-twin transfusion syndrome is used as a case study. FLS consists of accessing the womb in a minimally invasive approach. The surgeon manipulates an instrument equipped with a camera and a laser to search for and coagulate specific targets on the placenta. A complicated case is when the placenta is positioned such that there is no direct line of sight from the insertion point to the placenta. Currently, curved rigid instruments with fixed curvature (instrument 1) allow surgeons to access the placenta. The instrument manipulation is complex and still does not allow full accessibility to the placenta. Two new instruments are proposed: a straight instrument equipped with a flexible distal tip actuated by a lever at the handle (instrument 2), and an instrument with two distal degrees of freedom autonomously controlled (instrument 3). With the latter, the surgeon focuses on positioning the tip while the orientation is handled by the controller. We will investigate if both design paths should be pursued. The framework is composed of seven different elements (Fig. 1) . The user should be clearly described. As there is no consensus on the definition of expert and novice, we propose to describe a user based on his ability to perform the surgery independently, with supervision or none of the above. We assume that the most competent are the most eligible for evaluating value. The currently employed technology is used as a reference when assessing novel technology. VR allows to render features of the studied concept without developing the necessary hardware. The environment consists of surgical trainers located in a lab. Stepping away from clinical reality allows more control on the study parameters. This enforces the necessity of validating the simulator, with face validity as a minimum. The envisioned scenario must be representative of what occurs in reality. Therefore, procedural tasks should be favored over basic laparoscopic tasks. Validation is required, with content validity as a minimum. The hypothesis to be tested is that the novel technology enhances the psychomotor skills of the user compared to the current practice. As of this day, the literature does not propose a unique approach to assess such performance. The data set used for this analysis should be reliable, objective and sufficiently representative in order to capture the full performance. Observation methods could be: questionnaires, motion sensors, force sensors, cameras, instrument action sensors, clinical outcome measurement methods, surgeon or patient monitoring. Metrics are extracted from the different acquired signals and attempt to quantify the quality of the surgical gesture. A statistical hypothesis test is then used to compare the performances. The four surgeons able to operate without supervision affiliated to the unique fetal center of Belgium participated in our case-study. The curved rigid instrument was tested against the two novel instruments on a mixed-reality surgical trainer validated by face and content validity. The users had to accomplish the procedural tasks on a placenta positioned in three different configurations (Fig. 2) . The observation methods were: body wall force sensor, instrument tip motion sensor, instrument lever sensor, foot pedal sensor, screenshots of the placenta. The surgeons underwent a NASA Task Load Index questionnaire and a two-part custom instrument evaluation questionnaire after the campaign. The first part is a 5-point Likert scale survey covering safety, stability and global performance of the instrument in each configuration. The second is a pairwise comparison between the three instruments. The four users accomplished the procedural tasks six times for each instrument. Thus for one instrument, the trials covered twice each placenta configuration. Instrument 3 was discarded quickly as the surgeons were unable to finish all trials. While the autonomous controller performed as programmed, the instrument manipulation lacked of intuitiveness. The surgeon was easily disoriented. As a first phase, we analyzed the qualitative data. Table 1 Table 1 Results of the first part of the custom questionnaire Instrument 1: Curved rigid instrument preferred instrument 2 : 100% (placenta top), 50% (placenta middle), 25% (placenta bottom). Conclusion A framework was proposed to help engineers take early decisions on whether pursuing specific designs aimed to be relevant and valuable. By applying the presented framework to our case-study, we are confident in our decision to discard one of the instruments. However, the first phase of analysis revealed promising results for the other novel instrument operated on the top placenta. We hope to reinforce our decision to pursue this direction with the next phase of analysis dedicated to quantitative data. Presurgical planning is necessary to maximize patient safety, to avoid eloquent cortex, and to reduce postoperative deficits. Traditionally, surgeons rely on the segmentation of anatomical magnetic resonance imaging (MRI) to plan their surgical trajectory, although the incorporation of multimodal data is on the increase. Multimodal MRI techniques such as diffusion weighted derived structural connectivity (SC) and resting-state functional MRI derived functional connectivity (FC), have been shown to be useful metrics for computing minimally invasive approaches. These metrics may be used to provide visuospatial landmarks and context, but also to identify eloquent cortex. Several simulators exist for training, however many of them do not factor in SC and FC and are based mainly on structural representation of the brain. In this study we propose, a low cost, hand-held multi-use augmented reality (AR) simulator that allows the user to test different surgical paths based on multimodal visualization, and evaluate the damage caused by the tested trajectory. The AR application was based on a fully rendered brain derived from anonymized patient data. The application allows full visualization and interaction with the brain's structural components (as rendered brain regions), SC (as the brain's tractography), and FC. In order to derive quantitative assessment of path planning, tumours (represented as ellipsoids), were placed in the 3D workspace in the context of the neuroanatomical structures; the same ellipsoids were placed on the patient's structural MRI. During testing, the user task involves looking at the MRI, and then targeting the tumour within the AR workspace using a tracked targeting tool. Once the user has attained their desired trajectory, they are asked to tap the screen, withdraw the tool, and the brain tumour is displayed within the context of the tractography in AR, by hiding the grey matter of the brain (Figs. 1, 2) . Next, the tumour is covered by grey matter again and the user is asked to target the same tumour. During both insertions, and the user is made aware of the metrics of speed and accuracy. For all insertions, data is collected of the tool's position at every frame, the total time taken, and the components (gray matter, tractography) that were affected by the tool's path. For each path taken, a multimodal connectivity-based eloquence score is calculated based on the tool's distance to the salient FC and SC components traversed (algorithm previously described [1] ) This allows objective evaluation of the user's chosen paths and shows the potential eloquent cortices affected. The user's performance is calculated using Fitts's Law, based on the volume of the tumour and the distance traversed by the user. This offers metrics on both the user's performance in AR, but also to the potential damage caused to the brain. In conclusion, we have designed and evaluated an AR-based application that allows user visualization of a patient's neuroanatomical structures, supports surgical trajectory planning, and facilitates and objective evaluation of these tasks based on the patient's FC and SC. The user's performance is calculated based on their accuracy and time. The path taken is given a damage score allowing evaluation of potential postoperative deficit. The application runs on a cellular device needing on an AR tracker (pattern printed on paper) and AR tool, making it reusable, low cost, and portable. In addition, we hope that our tool can be used as a potential educational platform for neurosurgery trainees. Fig. 1 The transparent brain allowing the user to see the tumour before being asked to target again Fig. 2 A user undergoing the targeting task Int J CARS (2020) 15 (Suppl 1):S1-S214 S191 Mastoidectomy is a surgical procedure in which a portion of the temporal bone is removed to gain access to the inner ear. Performing this surgery involves fine microsurgical skills. Traditionally, residents have learned the surgical techniques by dissecting cadaveric temporal bones. Limited availability of cadaveric specimens, coupled with ethical concerns on their use, has spurred the development of virtual reality simulators with high-fidelity visual, auditory, and force feedback for temporal bone surgery [1] . These allow trainees to practice surgery in an environment where they can make mistakes and learn without jeopardizing the patient's safety. However, without an automatic feedback mechanism, an expert's presence is required to assess the performance, placing a heavy burden on their time. Additionally, automated performance assessment can provide trainees with objective feedback for self-driven learning. Our research group has begun to develop algorithms demonstrating the feasibility of automating the end-product assessment of a virtual temporal bone dissection using the Welling Scale [2] . The purpose of this research is to complete and validate an automatic performance assessment by comparing the computer assessment with an expert's assessment. Methods We aim to provide a self-sustained performance assessment tool by developing algorithms that automatically evaluate selected criteria in the Welling Scale, a validated instrument presently used to assess temporal bone dissection proficiency. A subset of the Welling Scale criteria was chosen based on their relative importance, ascertained from published surveys and from our own discussions with practising otologists. These key criteria can be described in terms of four functional categories: identification, skeletonization, intactness and no extraneous cells. The identification criteria determines whether each critical structure is sufficiently visible from the surgeon's vantage point. Skeletonization refers to leaving a thin protective layer of bone around the critical structures while dissecting the bone (Fig. 1) . The intactness criteria assesses whether or not the surgeon injured critical structures in the dissected anatomy that should have been preserved. Each of these three criteria, on every critical anatomic structure for which they are relevant, are assessed on the dissected virtual temporal bone using image processing and geometric algorithms we devised in [1] and have since refined and improved. The final criterion, which requires the surgeon to completely dissect and remove mastoid air cells surrounding certain structures, was also added to our automated assessment suite. To score this criterion, we first used binary thresholding and few morphological operations to segment the air cells around relevant anatomical structures. The region identified as mastoid air cells requiring excision is then tested against the corresponding air cells in the dissected bone to determine what proportion, if any, remains. To validate our algorithms, we performed an initial study using 15 dissections of 3 different temporal bones, the results of which we present here. Surgeon dissections for two of the three virtual temporal bones were obtained from a nation-wide face and content validity study of the CardinalSim virtual temporal bone surgery simulator which we conducted in June 2019 at the Canadian Society of Otolaryngology-Head and Neck Surgery (CSOHNS) national meeting. Participants were instructed to perform a cortical mastoidectomy, including a posterior tympanotomy, during a 20-min drilling session. Participants were given a 5-min introduction to the simulator and graduate students trained in using CardinalSim were present as surgical assistants to alleviate any technological issues. Therefore, no prior experience with CardinalSim was necessary for participation in the study. The dissections for the third dataset were obtained from the simulation component of a temporal bone boot camp course that was hosted at our institution. Final products of the virtual dissections were saved to disk with the consent of the study participants, and could be loaded again within the surgical simulator for later viewing and evaluation. An expert otolaryngologist scored the final products of the virtual dissections according to the Welling Scale. The same dissections were then again scored automatically using our performance assessment tool for comparison and validation. All the algorithms are integrated into a GUI application [1] which takes the output of the virtual temporal bone dissection and segmentations of the critical structures as input. The performance assessment application automatically generates a scorecard of the selected Welling Scale criteria with a binary result of 1 or 0 indicating, respectively, a pass or fail for each criterion. The application also provides a visualization of the results to aid comprehension (Fig. 2) . To compare the automated assessments with the expert surgeon assessments, we again grouped the criteria by their functional categories (identification, skeletonization, intactness) and created a confusion matrix for each category. The accuracy of our automated performance assessment against the expert scoring, as indicated by simple agreement and by Cohen's Kappa score, are shown in Table 1 . We are currently in the process of finalizing the implementation of the 'no extraneous air cells' category and evaluating its accuracy, and thus results do not appear here yet. We have achieved moderate accuracy for the identification and intactness categories and substantial accuracy for the skeletonization category. Tracking of surgical instruments is an essential step towards the modernization of the surgical workflow. Real-time tracking of a laparoscopic camera used in minimally-invasive surgery is required for applications in surgical workflow documentation, machine learning, image-location, and intra-operative visualization. In order to assist surgeons and clinicians during and after laparoscopic surgeries, we create an interactive tool visualizing the patient's abdomen. An inertial measurement unit (IMU) assists the tool tracking in situations when no line-of-sight is available for infrared (IR) based tracking of the laparoscopic camera. Methods By using sensor-fusion, combining the tracking data from an IMU and an IR optical tracking device, a program for real-time tracking of surgical instruments is developed and tested in several surgical scenarios. IMU-based tracking is tested for use in scenarios when no line-of-sight is present for optical-tracking. IMU-based tracking is compared to the infrared tracking-defined here as the ''gold standard'' for medical instrument tracking-and tested for accuracy and usability in the operating room. Detecting the trocar position without calibration and using it to reduce the degree of freedom results in a better position estimate. Two sensor-fusion implementations for processing IMU sensor data [1] are applied for the data sets and compared. Overall, the IMU-IR-tracking system functions well for laparoscopic tracking scenarios. We show that IMU-based tracking can efficiently bridge the gap when the IR-sensor has no line-of-sight to the tracking tools. By using measurement data from a real laparoscopic surgery, the IMU's orientation drift is on average 3.18°and 7.84°, for calibration frequencies of once every ten seconds and once every minute, respectively. Figure 1 shows the orientation represented as quaternions during laparoscopic surgery. The red line is the measurement obtained by the infrared tracking system, and the blue line is the computed orientation by the IMU sensor data. The red dots mark a time every 10 s when infrared tracking calibrates the IMUs. Figure 2 gives the angular error for each axis. . 1 Excerpt of the surgery, calibration every 10 s, orientation Int J CARS (2020) 15 (Suppl 1):S1-S214 S193 With the created datasets for realistic movement patterns at a significance level of 5%, it was shown that that faster movement patterns do not significantly increase the IMU's orientation error. The tracking program proved to be quite useful for the application tested. Even with high amounts of data, the program was capable of running efficiently in real-time and provided useful applications intraand post-operatively. The obtained error values help to create sensor models for estimating the risk of an incorrect location. Using Bayesfilters [2] is state of the art in mobile robotics. The algorithm needs sensor models, which we can create by the obtained measurements. Bayes Filters give not only an estimated location but a probability distribution for the camera tip in space. Since broader distributions correspond to worse position estimates, surgeons know when localization is not accurate and can provide lineof-sight to the camera for precise navigation or setting points of interest. For further improvement of the localization, we plan to incorporate visual odometry into the tracking system. 1 Technische Universität Dresden, Dresden, Germany Keywords Ankle foot orthoses, Motion element, Process chain, Knowledge-based design The aim is the development of a complete digital process chain as well as the deployment of a tool box for the knowledge-based selection of parameters for the creation of stress-adjusted motion elements of individual ankle foot orthoses. The parametric description and creation of discrete geometries and the numerical construction of a specific number of random more refined parameter sets for initial training phase is the basis for this purpose. Thereby it has to be possible to create geometrical models to be given to the orthopedic technician, which has the opportunity to use them for the subsequent orthosis construction. Furthermore a general instruction for the enhancement of the named toolbox has to be served. This is important to ensure, that the transfer of the research results is given to other projects. In addition the process chain for other cases of application has to be treated with the same equality. To create the work stages of the commercial process chain as user friendly as possible, it is necessary to define the detailed digital process chain and to develop a consistent data model and information model. This includes suitable folder structures, data management and data-name-conventions. Thereby the whole digital workflow has to be considered. It starts with a scan of the patient anatomy, the preparation of these scan data, the construction of the individual orthosis as well as the selection of the convenient motion elements. The last step is based on determined stresses and the final creation of production data for additive manufacturing. The current process chain is getting its input data from individual specific stresses, which are also affected and defined by geometric parameters and metadata of the patient (for example size, weight, activity degree). Afterwards the intervention of the artificial intelligence has the assignment to generate a convenient model of a motion element or at least to choose an initial-model and adjust it to the individual stress-case with these patient-specific data. The adjustment of the CAD model is made by the adjustment of the model parameters based on the patient parameters. Thereby the selection of a convenient motion element is the output as well as the current end of the process chain. First results took place on researches for the considered parameters of patient data and anatomy as well as for the CAD model and for the design of the (parametric) motion elements. Furthermore a few tests for the characterisation of different materials and different additive manufacturing methods as well as the identification of critical points of the latest orthoses were already realized. At last there were first steps for the realization of the artificial intelligence. Positive results: • Good and fast adaptation of different (parameterized) dimensions of the model geometries. • Different designs of motion elements were found and it is possible to use them for parametric and patient-specific adaptation. Negative results (to be improved in the future): • Lack of model adjustment of natural or functioning surfaces in order to be able to adapt anatomical designs. Based on practical tests with the used CAD software carried out on the designed CAD model, it was proven that at least the adjustment process basically works. The general usability of the process chain to be developed in its first steps was proven. The current status represents at least a few first steps towards a continuous process chain for the fast creation of patient-specific ankle foot orthoses with the use of artificial intelligence. Fig. 2 Excerpt of the surgery, calibration every 10 s, angular error S194 Int J CARS (2020) 15 (Suppl 1):S1-S214 • Further development and testing of the parametric motion element models (in CAD). • Further development and testing of more anatomical designs of the elements. • Development of the artificial intelligence and of the toolbox for orthopedic technicians as well as the continuation of the whole process chain. Keywords Tissue-mimicking phantoms, Multimodal imaging, imageguided interventions, Liver ablation Purpose Tissue-mimicking phantoms with realistic anatomical structures are useful for the development of image-guided interventions and validation of image processing techniques, clinical training, and preoperative surgical planning. X-ray computed tomography (CT) is often used to provide a highly accurate ground truth for new ultrasound-based guidance techniques. Recently, computer-assisted navigation and fusion of ultrasound and CT images has been shown to improve the outcome of liver tumour ablations [1, 2] . Currently, it is of significant clinical interest to develop tissue-mimicking phantoms that include realistic vasculature, allow for flow imaging, and maintain sufficient ultrasound contrast. The inclusion of flow could also be useful to simulate the cooling effect from the surrounding vasculature that occurs during in vivo liver tumour ablations. In this study, a methodology to develop a tissue-mimicking liver phantom that satisfies the above criteria, compatible with ultrasound and CT imaging is proposed. Poly(vinyl) alcohol cryogel (PVA-c) is chosen as a tissue-mimicking material due to its similar acoustic properties to human soft tissue and non-toxic nature. Additives are incorporated to provide acoustic backscattering, and X-ray contrast. Computer-Aided Design (CAD), moulding, casting and 3D printing techniques are used to create realistic anatomical structures. A human liver portion that contained an anatomically-realistic vasculature and tumour-like structures was designed in a CAD software (Autodesk Fusion 360). Negative moulds were generated which then, they were 3D printed (Ultimaker 3) using polylactic acid filament. After the generation of the negative moulds, the tissue-mimicking material was prepared and poured into them to create the individual structures. All structures were placed together into a custom-made container to form the final phantom. To prepare the tissue-mimicking material, distilled water was heated to 90°C and mixed with PVA powder for approximately 90 min. A mechanical stirrer was used to dissolve the PVA powder. In order to form a solid structure, the aqueous PVA-c mixture underwent freeze-thaw (FT) cycles; each cycle consisted of 12 h of freezing at -20°C and 12 h of thawing at room temperature. To introduce acoustic backscattering and X-ray contrast, talcum powder and Barium sulphate were mixed with PVA-c, respectively. For evaluation of the ultrasound and X-ray imaging appearances of the developed phantom, a clinical ultrasound imaging system (Ultrasonix Sonix MDP) with a linear array (L14-5) and an intraoperative X-ray imaging system (Medtronic O-Arm) in 3D mode were used, respectively. A 2D ultrasound B-mode image of phantom location that contained cross-sections of vessel and tumour-like structures is presented in Fig. 1 . The vessel appears as hypoechoic with distinct boundaries and can easily be differentiated from the background. A speckle pattern similar to that seen during clinical liver imaging was visible in the background. The tumour-like structure appears as hyperechoic with weak acoustic attenuation artefacts. With CT imaging, there was sufficient contrast to allow differentiation between the vascular and tumour structures and the background tissue. We presented a methodology to develop tissue-mimicking liver phantoms with realistic anatomical structures for multimodal ultrasound and CT imaging. These phantoms are well suited for validation of image processing techniques such as image fusion and registration, and for clinical training of liver ablation techniques. This methodology is broadly applicable to a wide range of clinical contexts in which patient-specific anatomical structures are derived from imaging data. Stereotactic navigation is useful for localizing, targeting, and guiding surgical procedures when a target cannot be seen directly [1] . It has been an established technology in several surgical and interventional radiological fields. Like neuronavigation surgery, stereotactic navigation for pelvic surgery is feasible since the soft tissue organs are confined to the bony pelvis and other important anatomical structures are relatively fixed to, where well defined landmarks and a rigid registration are available [1, 2] . On the other hand, pelvic surgery is technically difficult, and there is a high risk of unexpected complications such as major vessel injury and ureter injury, especially during minimally invasive surgery, which has lack of tactile feedback. The aim of this study was to apply stereotactic navigation for recurrent rectal cancer cases and evaluate clinical feasibility and usefulness by the concept of enhancing surgeon's spatial awareness and eliminating much of guesswork. Preoperative computer tomography scans were used for image data. Before image acquisition, 6-8 skin fiducials were attached along the inguinal ligament and the pubic bone. A StealthStation S8 optical navigation system (Medtronic, Minneapolis, MN) was used with cranial software for the study. Patient lied on the bean bag to minimize sliding movement during position change, and a patient tracker was mounted on the operating table. After preoperative CT scan images were loaded into the system and verified to meet the minimum system requirement, each point of skin fiducials were localized in image space with a registration pointer to complete patient-to-image paired point registration. The instrument tracker was mounted on the distal shaft of laparoscopic instrument and calibrated for instrument tracking. From December 2018 to November 2019, we experienced three cases of pelvic recurrence after rectal cancer surgery and performed minimally invasive surgery under the real time 3 dimensional image guidance. The numbers of used skin fiducials were 8, 8, and 6 in each case, and the registration error were 3.4 mm, 3.2 mm, and 4.2 mm, respectively (Fig. 1 ). Total setup time for navigation surgery was less than 30 min in all cases. Stereotactic navigation guidance contributed to better localization of the tumor. It enabled surgeon to perform his procedure with better spatial awareness and minimize guessing of surrounding anatomy (Fig. 2) . No intraoperative major complication was noted. Application of currently available stereotactic navigation system seems to be feasible with satisfactory accuracy in recurrent rectal cancer in a pelvic cavity by targeting pelvic and retroperitoneal structures. Patient-tailored image-guided navigation surgery especially for challenging cases will enhance surgical quality and patient safety. Keywords Image matching, Periacetabular osteotomy, 3D CAD, Postoperative evaluation Purpose In the field of clinical orthopedics, periacetabular osteotomy (PAO) surgery is a treatment for developmental dysplasia of the hip (DDH). Our group calculated, compared, and evaluated the freedom of movement and accompanying dynamics of the hip joint during deep flexion before and after PAO surgery by using an image matching method [1] we developed. Based on the calculated results, the closest distance between the femur and the acetabular margin was evaluated using a 3D model. The five subjects in this study were diagnosed with acetabular dysplasia and applied for acetabular migration. The target motion was a deep flexion from a standing position. We photographed a series of motions using flat panel detector before and after the operation. A virtual, gray-scale 3D model that could be manipulated in six degrees of freedom was created from the computed tomography image. Based on the correlation of the pixel values between the virtual projection and the X-ray image, the spatial attitude at the time of X-ray imaging was estimated (Fig. 1) . Applicable subjects were the pre and postoperative acetabular and hip joints, and the postoperative acetabular as seen from the preoperative acetabular and the femur as viewed from the pelvis were calculated. Then, the closest distance between the femur and the acetabular margin was measured again using 3D-CAD (Fig. 2) . The calculations from before and after surgery were compared. It was found that the internal rotation and adduction were significantly added to the acetabulum before and after acetabular migration. When the pre-and post-operative dynamics were compared, the external rotation angle of the hip joint after operation decreased, but there was no significant change in dynamics. In addition, when comparing the preand postoperative distance between the femur and periphery acetabular, we found that the post-operative distance is larger. These results indicate that the acetabular inward rotation and adduction movement, which increases the coverage of the femoral head, increases the proximity of the acetabular margin. We analyzed the amount of movement of the acetabulum and the dynamic analysis of the deep flexion motion from the standing position after acetabular movement using the image matching method. Furthermore, we measured the closest approach distance using 3D-CAD based on the dynamic analysis results. In this analysis of the acetabular movement, it was found that the rotation was largest in PAO. As reflected by the 3D-CAD, the dynamics of the pre-and post-operative hip indicate that the closest distance between the acetabular and the femur begins to increase; this is thought to be affected by the rotational movement of the acetabulum. These analysis results will enable more useful postoperative evaluations. Accurate estimation of C-arm position relative to the patient is an important task in many orthopaedic applications in which spatial information of anatomical structures and surgical instrumentations are desired. There exist different technologies for this purpose that either require additional navigation apparatus (i.e. external optical tracking) or precisely fabricated calibration phantoms. In order to display the anatomical structures of interest in a specific viewing angle, the routine intraoperative practice consists of several C-arm repositionings and acquiring X-ray projections from those perspectives. This usually results in a high level of ionizing radiation exposure and added surgical time that has to be ideally avoided. In a previous effort to address this problem, researchers aligned a generic model with the surgical patient using a landmark digitization approach and synthesized non-patient-specific predictive X-ray images based on the measured position of the C-arm [1] ; they demonstrated significant reductions in the number of X-ray images acquired for some selected clinical cases, but their approach did not allow them to incorporate patient-specific information into the procedure. Zheng demonstrated the ability to generate an accurate scaled 3D model of a specific patients pelvis based on a single anteriorposterior radiograph, but was unable to resolve the scale [2] . By combining a similar analysis with a preoperatively acquired CT volume to establish scale, we anticipate that the generation of an accurate estimate of the C-arms position based on a single X-ray shot is possible, which further can be use to generate patient-specific predictive X-rays from arbitrary orientations. Given the emergence of machine learning techniques in recent year, our goal in this study, therefore, is to determine whether we can use a machine learning technique to accurately estimate the C-arm's position relative to the patient's anatomy only based on one X-ray projection. Fig. 1 Bone density-based digitally reconstructed radiographs in our 3D-to-2D model-to-image registration techniques (Left) and hip joint by using an image matching method (Right) Fig. 2 The closest approach distance was measured using 3D-CAD based on the dynamic analysis results In this work, all models were trained on digitally reconstructed radiographs (DRR) of pelvic CT images. A neutral position was chosen by selecting three anatomical landmarks on a CT volume, furthermore the following coordinate frame was defined: • x-axis: dorsal to ventral • y-axis: coronal to caudal • z-axis: lateral to medial On that basis, the following random deviation ranges for the 6DoF were agreed on, as they are in the clinically relevant magnitude: o investigate the effect of training database size on the performance of a Convolutional Neural Network (CNN) for this task, a common CNN, VGG19, was trained on different database sizes by monitoring a testing accuracy metric. This was a combined metric that included both translation and rotation errors as: RotationError (rad)*100 ? TranslationError (mm). For a followup study, a series of 30016 synthetic X-rays along with their corresponding spatial parameters was used to train a series of CNN with different architectures and their performance assessed on a separate (unseen) synthetic test-set. For this study, we used the following architectures: InceptionV3, MobileNetV2, NASNetMobile, ResNet50V2, VGG19. All the networks were trained from scratch for 15 h and the convergence was monitored based on a validation dataset. Results Figure 1 illustrates the performance of the trained network, VGG19, in predicting the C-arm position from five input images. Using the predicted transforms, a new image was generated to visualize the result. The increase in performance is also shown in Fig. 2 . Starting from about 10,000 to 30,000 images the accuracy stopped increasing with given the 15 h training time. Based on that, we identified that the ResNet50 network achieved the highest accuracy based on the combined accuracy metric (10.51 mm) as illustrated in Table 1 . In this study we implemented a patient-specific algorithm for intraoperative C-arm position estimation. If trained preoperatively, our model can predict the C-arm position with a high accuracy (translation error \ 1 mm -rotation error \ 1°) based on a single X-ray projection. We showed that this clinically acceptable accuracy could be reached within a potential preoperative time window. Therefore, this study has high potential to improve treatment by allowing a new, easy to setup ct to patient aliment for surgical guidance systems. Keywords Laser guidance, Minimally invasive surgery, C-arm fluoroscopy, Radiation exposure, Intramedullary nail Purpose Minimally invasive surgery is one of the most commonly used methods for fracture surgery, with a particular advantage of rapid recovery because the surgery is performed through a small incision. During minimally invasive fracture surgery, the fractured bone fragments are not directly visible, and therefore the surgeons confirm reduction status using a C-arm (C-arm fluoroscopy), which can show two dimensional fluoroscopic images. If surgeon choose an intramedullary nailing technique for bone fixation, he need to find out interlocking holes based on fluoroscopic images. The freehand technique involves a procedure in which the shoot of a C-arm must be repeated until the shape of the distal hole taken by the C-arm reaches a perfect circle. In this process, large numbers of radiographs are required, and it is also time-consuming to specify the exact insertion position. Although various studies have attempted to solve such problems of distal interlocking or radiation exposure, the free-hand technique remains the predominant method [1] . In this study, we developed a laser guidance system (ATLAS: Aiming and Targeting system by LASer) to show real position corresponding to any point on a fluoroscopic image. The system can be applied to the widely used freehand technique fracture surgery. We introduce the system overview, hardware design, calibration and its control methods. We evaluated its clinical usefulness from phantom experiments. The ATLAS is fixed to the C-arm's image intensifier side as shown in Fig. 1 and calibration with dedicated markers is required prior to using the system. The ATLAS rotates two line lasers to indicate the insertion position according to the intersection. The insertion position can be selected in the fluoroscopy image taken by the C-arm and the intersection moves to the corresponding part, then a procedure such as K-wire, drill or screw insertion can be performed according to the intersection of the lasers. The display range of ATLAS is designed for a 9-inch C-arm. Its range is extended by attaching a mirror to the laser source such that the full a 9-inch range can be displayed. For the calibration process of the ATLAS, a plastic marker with a metal ball is utilized. The process is divided into two parts: (1) determining the focal length and principal point of the C-arm, and (2) identifying the position of the laser module, nail or table using the results of part 1 along with the marker image. After calibration, the software can calculate rotation angle of two lasers using coordinates of the selected point and two lasers. We evaluated the control accuracy of the ATLAS and the accuracy in the C-arm environment. 100 9 100 mm grids and a circle with the same size as the actual shooting range of the 9-inch C-arm were printed and used for the control accuracy experiment. Grids are drawn at 5 mm intervals, and experiments were carried out on a total of 25 points at 25 mm intervals horizontally and vertically. In the C-arm environment test, a steel wire of 1 mm thickness was placed in the form of a grid, and circles of a constant size were printed on the translucent paper at each intersection between the wires. The distance between each intersection is 10 mm, the inner red circle has a radius of 1.5 mm, and the outer blue circle has a 3 mm radius. The average control error of the ATLAS was 0.57 mm with standard deviation of 0.32 mm. Maximum error was 1.28 mm. In the experimental environment using C-arm fluoroscopy, the accuracy was within 1.5 mm at 23 of 25 measurement points, and the remaining two points were within a 3 mm accuracy. Testing with an intramedullary nail was also conducted. The result showed that a single C-arm image could be used to locate the hole of the nail and insert the K-wire into the hole. We proposed ATLAS that shows corresponding points on real space with respect to fluoroscopic images with cross-points of lasers and verified its clinical usefulness for assisting insertion of interlocking screws. The ATLAS is easy to use and can be applied to various surgeries using C-arm fluoroscopy. Currently, the ATLAS is designed to show positional information but its function will be extended to show postural information and we expect that it enable more accurate marking and enhanced convenience. which is adopt passive and active modes respectively. Detailed design, finite element analysis, force feedback closed-loop control algorithm and a series of experiments are then performed based on our robotic system. Methods Take traditional MIS as an example, the topological configuration for surgical robot can be briefly described. After the workspace for preoperative positioning is optimized analysis by simulation, the design demands for positioning manipulator modular joint is summarized. In this paper, two kinds of modular joint are proposed: one is adopted passive joint design method, which is compact, lightweight and no motor; another belongs to active joint design concept, which has complex motor driver and compliant force feedback mode. In order to achieve the locking torque, the reverse drag smoothly, accurate positioning accuracy, the passive modular joint adopt the method of reverse assembly of harmonic reducer to enhance the locking force, as shown in Fig. 1 . The hollow structure in the passive joint is facilitating wiring. The bending moment of the modular is determined by the selectin of bearing; and the locking force is depending on the ratio of harmonic reducer and the torque of selected brake. As shown in Fig. 2 , the transmission of active modular joint includes servo motor, gear reducer, belt drive steering, harmonic reducer which is always the last stage for zero backlash. Here, one force sensor is assembled at the end of the output side. Moreover, similar to the passive modular joint, many bearings can convert the load into rolling friction. Drag control, also known as compliant control, refers to the movement of the robot in the direction of traction after being pulled by external forces. When the robot works normally, the motor needs to control the joint to start or stop, and the gravity, centrifugal force, Coriolis force and inertial force in the whole process will change with the change of motion state. In order to realize the detection of external forces, it is necessary to calculate the robot joint torque without external forces through dynamics, so as to eliminate the influence of this force. The torque deviation value can be obtained by subtracting the actual measured torque from the theoretical calculated torque (including gravity, centrifugal force, Coriolis force, inertial force, etc.). By analyzing the magnitude, positive and negative of the torque deviation value, the robot can be determined whether it is subject to external traction. Finally, the excellent performance of two modular joints were studied quantitatively and qualitatively by tension test and human reverse drag experiment. Both active and passive joints can match the required braking torque according to the selection of brake or motor (Especially, the locked torque is 50 Nm for MIS robot). The reverse drag force of passive modular joint depends on the magnitude of the reverse friction force of harmonic reducer selected. For the active modular joint, the reverse drag force can be set in according to the resolution of torque sensor selected. Compared with the passive modular joint, the active joint with force feedback have the characteristic of collision detection. In this test, when the external force is removed, the manipulator will continue to operate according to the pre-set mode. The optional mode includes: (1) stop working and power off; (2) continue to move according to the original planned path. The experiments outcome demonstrated the correctness and effectiveness of two modular joint of positioning manipulator proposed in this paper. The passive modular joint can be reverse dragged smoothly when the brake is on (the maximum drag force is 35 N). And the active modular joint can fulfill better drag effects, but the disadvantage is extra motor, higher weight, expensive fee and larger dimension. Performance comparison between passive joint and active joint is shown in Table 1 . That is, the dragging test verified that the reverse drag torque is smaller and easier to setting in according with the actual design demands. However, the passive modular joint is more stable and reliable by virtue to traditional mechanical transmission. In recent years, convolutional neural networks (CNNs) have found increasingly active application in computer-aided diagnosis (CAD) research fields. Typically, to design a general-use, high performance detector using machine learning, training is conducted by applying comprehensive sets of case images having a variety of variations. It has been shown that, when configuring CNN training data, rather than providing the data uniformly, dividing the data into multiple subsets and adjusting their ratios offers greater potential for effective learning. We propose in this study a learning method that features incrementally-repeated CNN training using these subsets. In this study, subsets of hepatic tumor training data were created based on mass size and intensity. Utilizing multiple data sets prepared for use in evaluating a trained CNN, optimal ratios were considered and performance evaluations using actual unknown data were conducted. Next, the ratios of subsets that presented numerous detection errors with evaluation data were raised and retraining was conducted. A high-performance CNN was then designed by repeating this process as long as increases in the area under curve (AUC) were observed. As a result of applying unknown data to this CNN, we found that it exhibited a higher AUC than a CNN to which comprehensive training data was applied, demonstrating the effectiveness of the proposed learning method. (1) Procedural flow of the incremental learning method Figure 1 shows the procedural flow of the proposed incremental learning method. (2) Creating subsets We created subsets by dividing hepatic tumor case images into 9 classifications according to size and contrast. Contrast was defined as the difference between the tumor interior and border. The classification names shown in the classification table, and used hereinafter in this paper, were created by connecting the initial letters of the words Small, Medium, and Large (for indicating size) and Pale, Middle, and Deep (for indicating contrast). (3) Incremental CNN training for application in hepatic tumor detection [1, 2] First, we created CNNs for each subset and detection CNNs that comprehensively covered all subsets. Next, evaluations were conducted with these detectors using evaluation data sets to find their detection performance. For evaluation, the area under curve (AUC) of an ROC curve was calculated, thus obtaining the detection performance of each training. The average detector AUC by subset was 0.787, with subsets SD, SP, MM, MP, and LP exceeding average performance. In addition, set B had an average AUC of 0.790, with subsets SD, SP, MM, and MP exceeding average detection performance. From these, we therefore created detector Z1. Following this, adding subsets that presented numerous detection errors with the Z-type detectors, we incrementally created high performance detectors. Subsets to add were determined by calculating scores using formula (1) . Excepting MM, only the most effective subsets were added for detector reconfiguration. Score ¼ detection errors of target subset2 ð Þ þ detection errors of surrounding 3 subsets ð Þ ð 1Þ We conducted evaluations by applying the CNNs that were created to unknown data. Hundred cancer cases and 100 non-cancer cases were used in the unknown data set. Table 1 shows AUCs obtained from the incremental learning method using evaluation data sets and unknown data sets. In Table 1 , significant detection performance improvement was also observed with unknown data. For both unknown sets A and B, compared to the comprehensive-type X, Z2 and Z4, which obtained optimal values with the evaluation data, obtained maximum AUC values, suggesting the effectiveness of the present learning method.  We used subsets to conduct incremental learning for CNN-based CAD development and proposed a method for creating high-detection-performance CNNs. Focusing on size and contrast differences, we divided case images into 9 subset classifications, used data sets purposed for evaluation to consider optimal combinations, and then evaluated performance improvement using unknown data sets. As a result, detectors created with multiple data sets exhibited incremental performance improvements, demonstrating the effectiveness of this method for CAD development. The diagnosis of depression has always had the problem of not being able to employ objective indexes [1] . To address this, a system was developed for objective depression diagnosis that distinguishes whether or not depression is present by using image and voice engineering technologies. The purpose is to have health practitioners use CAD as an aid to depression diagnosis. In this study, depression classifiers were created with focus on information obtained from the voice. This study also considered a system configuration that is capable of higher determination accuracy by combining the present study's classifiers with classifiers reported last fiscal year that focus on the eye direction and facial expression of subjects with depression [2] . (1) Determination of depression using fundamental frequencies extracted from voice signals Observations of the voice signals of a large number of subjects with depression have shown that these subjects exhibit fewer utterances and much less modulated voices than healthy subjects. These characteristics are particularly conspicuous in women. Using these characteristics and the fundamental frequency F0, multiple features were created. In addition to standard deviation and other basic statistics, the average bandpower at the time of Fourier transformation and other parameters were calculated from the F0 and the time variance of F0's variance. This may be considered as the respective quantification of the intensity and amount of voice modulation. Labeling was performed that indicated whether a certain point in voice is an utterance or not and the proportion of non-voice sections in a 4-min voice signal were calculated. After dimensional compression of these features (total 20), they were entered into a support vector machine (SVM), and training and classification were performed. In order to raise classifier accuracy through effective selection of features while accounting for differences attributable to gender differences, data was created by dividing it up between male and female, and after performing 5 evaluations using leave-one-out cross validation, their average was deemed the accuracy. (2) Determination of depression using combinations of classifiers Classifiers created by method (1) and eye direction and facial expression classifiers reported at CARS2019 [2] were combined. Using 4 min of video data that allows tracking of eye direction and facial expression from the above-mentioned data, stacking-a type of ensemble learning-was used to perform evaluation with 4-fold cross-validation. Figure 1 shows the configuration of the proposed system. Results Table 1 shows the classification accuracy obtained by each method. For method (1), the classifier that utilized features calculated from voice signals, accuracy was 68% for classifiers created and evaluated with only male data and 75% for those created and evaluated with only female data. Weighting by the number of data points for males and females respectively and then averaging obtained an overall classifier accuracy of 72%. In addition, when classifiers were combined using method (2) , an overall classifier accuracy of 88% was obtained. This is summarized as follows: The accuracy of classifier systems that employed voice information was 72%. On the other hand, the accuracy of classifier systems that used video information was 76% for both eye direction and facial expression, while combining them yielded an accuracy of 83%. However, by combining all voice and video information, the highest accuracy of 88% was achieved. It was verified that depression determination accuracy increased by introducing classifiers that employed voice information to classifier [2] , which was reported at CARS2019 combines eye direction and facial expression. Future activities aimed at further accuracy improvement will clearly benefit from adding features that are based on observations of target subject voice. They will also require detailed considerations of dimensional compression aimed at the selection and optimization of features. The diagnosis of depression has always had the problem of not being able to employ objective indexes [1] . To address this, a system was developed for objective depression diagnosis that distinguishes whether or not depression is present by using image engineering technology. The purpose is to have health practitioners use CAD as an aid to depression diagnosis. Up to the previous fiscal year, focus was on eye direction and facial expression, creating data sets for each and using separate classifiers to determine the presence of depression [2] . Both classifiers achieved a detection accuracy of approximately 76%. This paper presents our considerations on the configuration of a classifier system capable of higher accuracy by combining these two types of classifiers. The data used comprised 254-s video image of 30 cases in total, 15 of subjects with depression and 15 of healthy subjects. Changes to eye direction and facial expression were extracted from this video in 1-s intervals. Each of these was specified successively as eye direction data and facial expression data. (The last fiscal year's CARS2019 abstract may be referenced for details on eye direction and facial expression [2] ) These two types of data were used for machine learning classification. The stacking technique was used as the method of classification. Figure 1 shows the configuration of the 2-stage stacking technique used in this study. In the first stage, a support vector machine (SVM) was trained with eye direction data and a layer recurrent neural network (LRN) was trained with facial expression data. In the second stage, using the classifier scores obtained in stage 1, training and classification were conducted with the four methods of SVM, neural network (NN), soft voting, and successive selection. The SVM and NN (middle nodes: 10; transfer function: softmax and purelin) were used independently and were evaluated according to their classifier scores. With soft voting and successive selection, 69 classifiers (NN, SVM, k-nearest neighbors (KNN), etc.) were used. Soft voting was evaluated using the average classification score of the 69 classifiers. For successive selection, the classifier with the best classification performance among all the classifiers was extracted and then combined with other classifiers. If classification performance improved, combination was continued. If a combination in this process exhibited either no change or a degradation in classification performance, this procedure was discontinued, and the accuracy in the state prior to this combination was set as the final classification score for evaluation. 5-fold cross-validation was used to evaluate classification accuracy. Results Table 1 shows the classification accuracy, sensitivity, and specificity obtained for the four algorithms. Looking only at classification accuracy, that of the SVM was 80%, NN and soft voting about 77%, and successive selection about 83%. Successive selection and SVM exceeded 80% for both sensitivity and specificity, suggesting a good balance. On the other hand, looking only at NN's sensitivity, at about S204 Int J CARS (2020) 15 (Suppl 1):S1-S214 86% it was equal to that of successive selection. However, because of its low specificity of approximately 66%, its classification accuracy was about 77%. In contrast, soft voting's low sensitivity of about 73% resulted in a classifier accuracy of about 77%. As a result, even in training using the same multiple classifiers, successive selection, in which classifiers were selected and combined, exhibited better improvement in classification accuracy than soft voting. A system was developed that combines multiple classifiers to deliver high classification accuracy. Adopting a stacking configuration, and using SVM, NN, soft voting, and successive selection methods, classification was performed. Ultimately, a classification method capable of determining the presence or absence of depression with approximately 83% accuracy was verified, indicating the successful creation of a system with meaningful performance. Future improvements to accuracy are expected to depend on a deepening of the stacking structure through the use of other features besides eye direction and facial expression, such as voice. Purpose Body fat mainly consists of subcutaneous fat (SAT) and visceral fat (VAT). Obesity associated with the increase of body fat is divided into SAT type obesity and VAT type obesity. SAT type obesity has been reported as a risk factor such as sleep apnea syndrome and joint pain. Also, VAT type obesity has been reported as a risk factor such as cardiovascular disease and type II diabetes. Separate measurements of SAT and VAT are important to prevent these diseases. Substitute indices such as body mass index (BMI) and waist circumference are often used in clinical practice, but these methods are difficult to measure body fat directly. Dixon sequenced magnetic resonance imaging (MRI) is useful for measuring SAT and VAT because of the non-radiation exposure and good contrast between water and fat. Recently, many automatic body fat extraction methods using deep learning have been proposed. Therefore, we developed the automatic body fat extraction methods in whole-body (WB) MRI using deep learning. Although many images with ground-truth (GT) images are required to train deep learning, manual creation of GT images is timeconsuming. In a previous study, to reduce the time-consuming work of creating GT images (artificial GTs), we created the artificial GTs of SAT and VAT in MRI by multi-atlas segmentation using WB computed tomography (CT) images and artificial GTs of SAT and VAT in CT. However, the accuracy of the artificial GTs of SAT and VAT were poor. We also have manually labeled images (mLabels) of thoracoabdominal cavity in CT and used them with Dixon MR images to create more accurate artificial GTs of SAT and VAT in MRI. In this study, we propose a new method for creating artificial GTs for deep learning-based fat segmentation in MR images to reduce the time-consuming work. Methods A total of 24 sets of WB CT images and mLabels of thoracoabdominal cavity were used, with three sets for each of four BMI categories (under 20, 20-25, 25-30, and over 30) in both sexes. 16-row CT scanner (GE, Light Speed CT) was employed for the acquisition of CT images. Also, a total of 80 sets of WB MR images were obtained, with ten sets for each of the same categories. A 3-Tesla MR scanner (Siemens Biograph mMR) was employed for the acquisition of MR images. The Dixon method was employed for the acquisition of four types of MR images (in-phase, out-of-phase, fatonly and water-only). Figure 1 shows the process of creating the artificial GTs of SAT and VAT for deep learning-based fat segmentation in MR images using multi-atlas segmentation. Firstly, WB CT and MR images were cropped from the base of the neck to the groin and were rescaled with a matrix size of 384 9 312 9 258. Secondly, the in-phase MR image was selected as the target image, and 24 sets of WB CT images were non-rigidly registered to the target image using DROP 3D registration software [1] . The artificial GTs of thoracoabdominal cavity in MRI were created by multi-atlas segmentation with mLabels of thoracoabdominal cavity registered by the deformation field. Finally, the artificial GTs were created using the artificial GTs of thoracoabdominal cavity and the fat-only MR images. Then, we trained a 3D FC-ResNet [2] based body fat Fig. 1 The process of creating the artificial GTs of SAT and VAT in MR images by multi-atlas segmentation segmentation model using the four types of MR images and the artificial GTs of SAT and VAT. Among the 80 sets of MR images, 48 sets were used for training, 16 sets were used for hyper-parameter tuning, and 16 sets were used for evaluation. The segmentation accuracy was evaluated by the Dice coefficient. To evaluate the accuracy of the extracted results from the learning model (ERs) and the artificial GTs, we compared them to the mLabels of SAT and VAT. Results Figure 2 shows the result of the ERs of SAT and VAT, the artificial GTs of SAT and VAT, and the mLabels of SAT and VAT. Table 1 shows the Dice coefficient of the ERs and artificial GTs in 16 sets for 3D FC-ResNet and comparison with the mLabels of SAT and VAT. The artificial GTs of SAT were more accurate than the artificial GTs of VAT. We proposed the artificial GT creation method for deep learningbased body fat segmentation in MRI using multi-atlas segmentation. It was suggested to be useful for the GT image creation cost reduction. Keywords Lung Cancer, Convolutional Neural Network, Dilated Convolution, Attention Network Purpose Lung cancer is the second most common cancer [1] . More people die of lung cancer every year than of colon, breast, and prostate cancers combined [1] . One of the main issues facing Computer-Aided Diagnosis (CAD) schemes for lung nodule detection and classification is the wide variation of nodule sizes. In our initial analysis, we observed that nodules that are malignant have bigger sizes/diameters than benign nodules. However, mid-range nodules between 5 and 12 mm represent the nodules that are ''difficult'' to classify. There is no easy way to classify these nodules as they contain a mixture of both benign and malignant nodules. In this study, we present a novel method to classify these nodules, which has not been examined in previous studies. Methods Figure 1 depicts the entire network architecture of our new Dilated Convolution and Gated-Dilated (GD) sub-network method. To classify the mid-range nodules more accurately, we implemented a context-aware sub-layer to generate signals that are responsible for closing or opening the gate that is in front of each dilated convolution, K 1 and K 2 . This gives the network the ability to choose the right dilation for each nodule, depending on the nodule size (K 1 is applicable to smaller nodules, whereas K 2 classifies bigger nodules better). We applied five consecutive GD layers with 32, 32, 64, 64, and 64 channels, respectively as depicted in Fig. 1 . For each GD layer, the number of channels is equally divided between the two dilated convolutional layers, e.g., the number of channels is 32 in the first GD layer. This means that the convolutional layer with dilation rate of 1and 2 have 16 channels, equally. After concatenating the outputs of the two layers, the total number of channels sums up to 32. Fig. 2 The mLabel, artificial GT, and ER of SAT and VAT Fig. 1 Our new Gated-Dilated (GD) network framework consists of five GD layers, a Max-Pooling layer, and a Fully-Connected layer. The design of the GD layer is shown in the magnified image. The bottom part of the diagram illustrates how the 2 implemented dilated convolutions, K 1 and K 2 operate on 5 9 5 input features After five GD layers, global max-pooling was implemented to summarize the feature space. Max-pooling is required to extract the most prominent features extracted from all the feature maps, to detect distinguishing features that classify benign/malignant nodules. Maxpooling is also required to reduce the number of trainable parameters in the fully-connected network layer, which comes after the maxpooling layer. The output of the fully-connected layer is the probability of the nodule in question being malignant. If the output of the sigmoid exceeds 0.5, this means that the network predicts that the nodule in question has a higher probability of being malignant; otherwise, it is benign. We also used the binary cross-entropy loss function and the Adam optimizer to train our network. The dataset used in this study is the public LIDC-IDRI dataset. This dataset is the largest public lung nodule dataset and consists of 1,018 CT scans collated from 1010 patients altogether. We implemented eight other networks as baseline comparison methods, to compare them with our GD network. The first baseline method is a conventional CNN network that has the same number of layers and channels as the GD network. We also designed two ablation studies to analyze the contributions of the dilated convolution and the Context-Aware Sub-network, respectively: (1) GD-No-Dilation, which is similar to our proposed method, except both K 1 and K 2 have the same dilation rate; (2) GD-No-Gate, which is similar to GD except that there is no Context-Aware Sub-network in the network architecture. We also implemented a state-of-the-art lung nodule classification model, Multi-Crop Convolutional Neural Network (Multi-Crop) [2] , Resnet-50 and Densenet-161 pre-trained with the Imagenet dataset. We studied two methods of transfer learning: (1) we fine-tuned all the network parameters (Resnet-Full and Densenet-Full); (2) we only fine-tuned the last layer (Resnet and Densenet). We validated all nine methods using ten-fold cross-validation, whereby the sum of 406 malignant and 442 benign nodules in the dataset where randomly divided into 10 exclusive partitions. The area under the receiver operating characteristic curve (AUC), accuracy, precision, and sensitivity results of all nine methods are tabulated in Table 1 . We observe that GD outperforms all other methods except CNN in terms of the sensitivity. To analyze the performance of our GD network on nodules of different diameters within the dataset, we compared the accuracies of the four top-performing models on different nodule diameters in Fig. 2 . The results show that the accuracy of our GD model exceeds all other methods in most of the diameters. Our proposed GD network outperforms eight popular baseline stateof-the-art methods including Multi-Crop, Densenet and Resnet in the challenging task of lung nodule benign/malignant classification on the public LIDC-IDRI dataset. Our results show that significant improvements can be achieved with our method in classifying ''difficult'' nodules that lie in the range of 5-12 mm within the dataset. A. Tanaka 1 , M. Nemoto 2 , H. Kaida 3 , Y. Kimura 1 , T. Nagaoka 1 , T. Yamada 1 , K. Ushifusa 1 , K. Hanaoka 4 , K. Kitajima 5 , T. Tsuchitani 6 , K. Ishii 3,4 1 Kindai University, Graduate School of Biology Oriented Science and Technology, Kinokawa-shi, Japan 2 Kindai University, Biology Oriented Science and Technology, Kinokawa-shi, Japan 3 Kindai University, Medical, Osakasayama-shi, Japan 4 Kindai University Hospital, Osakasayama-shi, Japan 5 Hyogo College of Medicine, Medical, Nishinomiya city, Japan 6 Hyogo College of Medicine Hospital, Nishinomiya city, Japan Keywords One-class SVM, Anomaly detection, FDG-PET/CT, Computer aided diagnosis Purpose The purpose of this study is to evaluate the feasibility of a proposed lesion enhance/detection method based on an anomaly detection for FDG-PET/CT images. In the proposed method, the one-class SVM (OCSVM) based anomaly detection is used to estimate the ''voxel abnormality'' for each voxel on input FDG-PET/CT images. The OCSVM is trained by voxel data from only normal cases. Generally, it is not easy to collect enough lesion data for machine learning. Since small training dataset often includes a severe deviation of lesions, the two/multi-class classifier trained by small dataset often leads to the limited performance of lesion classification [1] . Otherwise, the OCSVM training needs only the normal voxels being easy to collect, instead of the lesion voxels. Employing OCSVM avoids collecting lesion data. Also, the OCSVM has the potential to enhance unexpected/ rare lesions that have characteristics different from normal patterns. For the experimental evaluation, we applied the novel automatic detection method using OCSVM to the clinical FDG-PET/CT images, including primary and metastatic lesions of the neck, lung and mediastinum. As the flowchart of proposed method shown in Fig. 1 , first, CT and FDG-PET (PET) images are scaled to 2.4 mm isotropic resolution. Second, bilateral lungs, and mediastinum region are automatically extracted. The mediastinum region is extracted including the cervical region. The upper limit of the cervical region is set to the lower edge of the mandibula. The lower limit of the mediastinum region is set to an approximate surface of the diaphragm estimated from the inferior surfaces of the bilateral lungs by the radial basis function interpolation. Third, the degree of voxel abnormality is measured in each voxel within the cervix, the lung, and the mediastinum. In the measurement, multiple OCSVMs are used selectively for different anatomical regions. Last, a thresholding process is applied to the obtained voxel abnormality to extract candidate area of the lesion. In the measurement of the voxel abnormality for a specific region, an OCSVM dedicated to the specific region is adequately used. The OCSVM has been trained by a normal voxel set from the specific region. In this method, three types of OCSVM (for bilateral lungs, and mediastinum) are used. The lung OCSVMs use CT value, SUV, and the distance from the lung contour as features. The feature vector for the mediastinum OCSVM includes CT value, SUV, and the normalized three-dimensional coordinates. This study consists of 80 cases of clinical PET/CT data. These data have been obtained at Kindai University Hospital and Hyogo College of Medicine Hospital. On the training of multiple OCSVM, 40 normal cases are used. Remaining malignant 40 cases with 105 lesions are used to evaluate the feasibility of this method. The proposed process successfully detected to every malignant case. The enhancement performances for each anatomical region are shown as free-response receiver operating characteristic curve (FROC curves) (Fig. 2) . When the number of false positive voxels per case (nFP/case) was 1000, the sensitivity of detecting pulmonary lesions was 88.1% (right lung) and 87.5% (left lung) respectively. The detection rate of left lung is less sensitive because it is affected by SUV spillover from myocardium. On the other hand, the sensitivity in the neck and mediastinum was 83.7% when nFP/case was 20,000. The performance of detecting lesions in mediastinum was inferior to that in other regions, because the mediastinum region includes various tissue and is more difficult to classify lesion patterns than other areas. We proposed automated detection algorithm of lesion candidates on lungs and mediastinum regions, and evaluated this method with 40 cases of clinical FDG-PET/CT. We confirmed the feasibility and effectiveness of lesion enhancement by multiple OCSVMs. When nFP/case was 1000, in both lungs region, the lesion sensitivity was more than 87%. Also, the sensitivity in the neck and mediastinum region was 83.7% when nFP/case was 20,000. The future direction is to improve the detection accuracy in both neck and mediastinal regions by reviewing the features used for OCSVM training. The breast cancer (BC) remains the important problem for the worldwide healthcare system. Each year more than one million BC cases are diagnosed, and BC itself represents almost a quarter of all malignancies in women. The highest morbidity values are typical for developed countries and they correspond to more than 360,000 new cases per year in the Europe and more than 200,000 new cases per year in the USA [1] . That is why the breast cancer screening programs are widely used worldwide with the aim to find tumor in the earliest possible stage and decrease the cancer-related mortality and morbidity. Mammography is the main modality that is used for breast cancer screening for decades. It has a lot of advantages, including the high sensitivity that reaches 98% in the fatty breast, cost-effectiveness and the ability to increase the patient survival up to 14%. However there is a significant overlap between benign and malignant features during the visual assessment. That is why the specificity of this method is only moderate (about 83%) and strongly depends on the structure of the studied population. This leads to the significant biopsy rate (13.02-20.12 per 1000 screening mammographies), of which only 27% are positive (i.e. 72% biopsies are unnecessary performed for benign lesions) [2] . Therefore in the USA the mean cost of breast-care following a false positive mammogram was $527 and total false-positive breast biopsies cost the healthcare system is more than $2 billion per year. Meanwhile, in the era of artificial intelligence any digital technologies that increase the specificity of mammography become extremely important for the healthcare system. Regions of interest (ROI) of screening mammograms marked by a radiologist (see Fig. 1a ) were used as the source data. These ROIs corresponded the rectangular parts of the 8-bit mammograms with benign and malignant lesions. There were three steps of differentiation: (1) search of a lesion contour, (2) calculation of statistical, textural and morphological features of both the selected area of the lesion, and beyond its contours, (3) application of classification methods based on the obtained features and machine learning methods. The search for contour points is based on the search for local maxima of gradients of image brightness. The fact that the obtained contour should be closed was taken into account, those local maxima that would form a closed contour of lesion were taken into consideration. For this, the Cartesian coordinates x i , y i of the maxima points are transformed into the polar coordinate system r = x i Á cosu ? y i Á sinu relative to the center of the ROI. And a search is made for the maxima of the extrema that belong to the angle u. Since the mammogram is a projection of the entire mammary gland, the superimposed artifacts on the mammograms may have gradients of brightness comparable to those on the boundary of the lesion itself, and the boundary of the lesion itself may not be clear. A contour approximation was constructed by calculating third degree polynomials to eliminate those local maxima that are not related to the lesion contour. The Cartesian distance between the found maximum point of gradient and the polynomial approximation was estimated using the threshold Niblack method. Figure 2b shows the result of the lesion contour searching. The features of the selected area were calculated both for the initial brightness values of the image pixels and for their derivatives: the module and direction of the brightness gradient function, the Laplace operator, Local Binary Patterns (LBP) descriptors. The features of Tamura, Haralick, Hu's moments, and other statistical features were used as classifying features. The methods of Principal Component Analysis (PCA), Latent Dirichlet allocation (LDA), Baves were also used for dimension reduction. Machine learning methods as the Support vector machine (SVM) and Neural networks (NN) with various architectures were used in the experiment to lesion differentiation. Figure 2 shows a scheme of research methods. A set of 200 mammogram images was used (100 benign and 100 malignant cases) for the study. All cases were marked by a radiologist and confirmed histo-and/or cytologically. As a result of the studies, it was found that the Haralick features are more sensitive in the task of lesion differentiation. Probabilistic comparisons are more suitable for dimension reduction. Similar results are obtained when classifying using the SVM and NN with an average error probability of 8-10% depending on the part of the set used for training. The specificity of the automated approaches varied from 89.4 to 92.3%. On the contrary, with the bare eye it was possible to reach the specificity 85.2%. The use of ResNet-50 and GoogLeNet deep learning networks for image classification gave a classification accuracy of less than 80%. Conclusion Automated approach surpasses the visual assessment performed by trained specialist. This data may have a clinical significance. The purpose of this study is to propose a generalized image feature generation based on an unsupervised learning technique with a small normal dataset. In a computer-aided detection (CADe) system, the local image features extracted from a partial region such as a lesion candidate are used to identify the lesion likelihood of the partial area. The local image feature is one of the essential factors that significantly affect the performance of the CADe system. Recently, many feature generation methods based on supervised deep learning techniques have been proposed. However, supervised deep learning techniques generally require a large scale of training data. It is well known that collecting much lesion data and its clinical annotations are difficult. Otherwise, there is also a great interest in the unsupervised deep learning techniques with normal data that can be collected easier than lesion data. In this study, we use multiple-deep convolutional autoencoders (DCAEs) [1] . The DCAEs are trained by an unsupervised deep learning technique with only a normal dataset. In this study, we apply the proposed method to the detection of cerebral aneurysm on head MRA. We also analyze the relationships between the lesion identification performance of the generated features and the scale of the training dataset. Our generalized feature generation method is to extract optimal local image features from the local image patch and uses four kinds of DCAEs trained by a normal dataset. We expect that the DCAEs lead a difference between the feature from normal data and the feature from lesion data not used in the DCAE learning. The structure of the DCAE is shown in Table 1 . It includes three convolutional layers, two max-pooling layers, and a full connection layer. The input is a three channeled 2.5 dimensional (2.5D) image patch extracted from a three-dimensional (3D) local image patch. The 2.5D patch includes axial, coronal, and sagittal slices. The size of the 2.5D image patch is 32 9 32 pixels. We experimentally know that the principal dimension extraction of the input data often leads to the Convolution (Conv) 3 9 3 1 3 29 32 9 6 Max Pooling (Pool) 2 9 2 1 1 69 16 9 6 Conv 3 9 3 1 1 69 16 9 9 Pool 2 9 2 1 89 8 9 9 Conv 3 9 3 1 89 8 9 12 Full connection (FC) --n stability of DCAEs learning. To apply various kinds of lesion and image modality, we employ four types of 2.5D projection techniques: the maximum intensity projection (MIP), the minimum intensity projection (Min-IP), the mean intensity projection (Mean-IP), and the extraction of center slices (Cent). These four kinds of 2.5D image patches extracted from a 3D patch are inputted to their DCAE. The number of latent variable dimensions, shown as ''n'' in Table 1 , is optimized experimentally by each DCAEs. In the learning of every DCAEs, the cross-entropy loss function and the mini-batch learning are used. The mini-batch size and the maximum number of epochs are 256 and 500, respectively. The optimal epoch number for each DCAE is obtained by validation with a training dataset. The features generated by a DCAE are described as follows: (1) The latent variables of the input image patch (output of the FC layer). (2) The mean squared error between pixel values of the input 2.5D patch and the reproduced patch from the latent variables. (3) The Mahalanobis distance from trained a normal dataset in the latent variable space. (4) Nine kinds of pixel value statistics of difference image between the input image patch and the reproduced image patch (maximum, minimum, average, skewness, kurtosis, standard deviation, first-/second-/ third-quartiles). In this study, the proposed method is evaluated by the classification performance of local image patches using the generated features. The classifier is the AdaBoosted ensemble, which are constructed 100 decision stump weak classifiers. In this study, we use 450 cases of head MRA data with cerebral aneurysms. The dataset has originally 0.47 9 0.47 9 0.60 mm 3 resolution and is scaled to 0.47 mm isotropic resolution previously. The image patches are obtained from the cerebral artery regions that are extracted by [2] previously. The average number of generated 2.5D image patches from a case is 248 with 273 standard deviation. The dataset has been taken at the University of Tokyo Hospital and are given lesion annotations by experienced radiologists. In the evaluation, we analyze the relationships between the lesion classification performance of the generated features and the scale of the training dataset. We follow the three-fold cross-validation theory. Furthermore, in the experiment, we evaluate the performances of not only the DCAE trained by 300 (2 9 450/3) cases but also the DCAEs trained by less than 300 cases (from 5 to 200 cases). Each of these small datasets is randomly extracted from the training dataset of 300 cases. Each extraction trial is run three times to reduce random time differences. The scale of test data is fixed to 150 cases irrespective of the training dataset scale. In the experiments, the feature generation was successfully performed irrespective of the training dataset scale. We evaluated the classification performances of the generated features for every dataset scale. Figures 1 and 2 show the FROC curves and the average ANODE scores, respectively. The ANODE score is the average sensitivity when the number of false positives per case is 1/8, 1/4, 1/2, one, two, four, or eight. In Fig. 2 , the error bars show the standard deviations. These figures show that the classification performance declined with the reduction of the scale of the training dataset. However, there were the statistical differences only between the average ANODE score obtained from 300 cases training and the scores obtained from 5 or 10 cases training by the Welch's t-test (p \ 0.05). In this study, we proposed a generalized image feature generation method using multiple DCAEs trained by unsupervised deep learning with a small normal dataset. In the experiments, our method could generate practical local image features for classifying cerebral aneurysms. Furthermore, we also confirmed that our method could create useful image features, even if the training dataset is small.  Breast cancer screening is a public health issue. Knowing that one in nine women will be affected by this disease, the detection of its first signs is crucial. Computer Assisted Diagnostic Systems (CADx) can help the radiologist to read mammograms and play a key role in the early detection of breast cancer. Nowadays, machine learning algorithms are evolving to the point where they sometimes surpass some radiologists in cancer diagnosis. The main objective of this work is to validate a mammography image classifier and explore the U-Net deep learning architecture for segmentation using the CBIS-DDSM public dataset [1, 2] . The proposed algorithm identifies the presence of abnormalities and locates the relevant region in the image to assist the radiologist in his diagnosis. The first part of this work consists of developing a binary classifier using deep neural networks for the classification of mammography images. The classifier should be able to identify 2 classes: malignant and benign. All our models will be trained with the CBIS-DDSM public dataset. This dataset contains 3785 images in dicom format from 2620 patient files. It contains normal, benign and malignant cases with verified pathological information. The major advantage is that it uses the same American College of Radiology (ACR) standardized lexicon of the BIRADS classification. In order to facilitate the processing of these data, we have classified them into two separate folders: malignant (1364 mammographies) and benign (2421 mammographies) and we divided the dataset into three parts: training 75%, validation 20% and test 5%. The second part of this work consists of locating the tumour. For that, we examined the different methods that are possible but also feasible with the data we are working on. In the CBIS-DDSM database, we have no information about tumour location, their coordinates or the radius that encompasses them. Therefore, we cannot apply a known object detection algorithm like Yolo or RCNN. However, the CBIS-DDSM public database provides us with interesting additional information: the ROI annotation and its segmentation called ''mask'', i.e. the cropping of the region of interest in the image for each anomaly, to which a lesion segmentation algorithm has been applied for more precision. We decided to develop a deep neural network for the segmentation of masses in a set of mammograms. The segmentation of mammographic images consists here in obtaining a binary image as output, which allows us to distinguish the mass from the rest of the image. In order to perform semantic segmentation, we need to retain spatial information. That is why we use a fully convolutional networks. The convolutional layers, coupled with the MaxPooling subsampling layers, produce a very large feature vector. We used the U-Net architecture, a very powerful network and often used in this kind of studies. It presents an ''encoder-decoder'' architecture, where the encoder and decoder layers are symmetrical to each other, as shown in Fig. 1 , hence the ''U'' in its name. The decoder can access to more general characteristics produced by the decoder layers, to reduce information loss. Intermediate outputs of the encoder are thus concatenated with the inputs of the intermediate layers of the decoder, depending on the symmetry. ResNet, also known as Residual Network, allows extremely deep neural networks to be formed relatively efficiently. We then tried to use it as the ''backbone'' of our U-Net model in order to combine the advantages of both models. First, we present the results obtained with our binary classifier for the different parameters tested are shown in Table 1 . It should be noted that ES corresponds to EarlyStopping, i.e. the number of epochs during which the model trained, FC to Fully Connected and AUC to the area under the ROC curve. The best classification result we achieved was with the Incep-tionV3 model. We obtain a training accuracy of 99.27%, validation accuracy of 72.46% and test accuracy of 75.26%. The ResNet50 model provided a slightly higher training accuracy (99.38%) but InceptionV3's performance remains higher in terms of validation, test accuracy and area under the ROC curve (method based on transfer learning and fine tuning). We could improve our model by using a larger database and by reworking the dataset in which we have mixed masses and calcifications to constitute our two classes (benign and malignant). For the segmentation results, we can also see in Fig. 2 that by stacking the two neural networks, U-Net and ResNet, the prediction of the location of the mass is even more accurate and accentuated. In this paper, we addressed the problem of breast cancer screening from mammograms using three convolutional neural network architectures Inception V3, ResNest50 and InceprionResNetV2. We achieved an accuracy score of 99% and 75% for the training and for the testing respectively with our binary classifier. After that, all images containing masses and calcifications were mixed together to create two classes (benign and malignant). Further investigations can be done using a bigger dataset thanks to a collaboration with a Belgian hospital to improve screening results and also by separating the masses and calcifications to create the dataset. Concerning the segmentation of medical images, U-Net is one of the most powerful networks. We have also shown that by stacking the two neural networks, U-Net and ResNet, the prediction of the location of the mass is more accurate and accentuated. Int J CARS (2020) 15 (Suppl 1):S1-S214 phantom were scanned by use of a CT scanner (SOMATOM Definition Flash, Siemens Healthcare) with 0.6-mm slice thickness and 0.6-mm reconstruction interval at 120 kVp energy for SE-CTC volumes and at 140 kVp and 80 kVp energies for the corresponding DE-CTC volumes. To simulate different concentrations of fecal tagging, the phantom was filled partially between CT scans by simulated fecal materials with contrast concentrations of 20 mg/ml, 40 mg/ml, and 60 mg/ml. Figure 1a shows the design of the proposed 3D-ResNet-GAN EC. The 3D-ResNet-GAN is a 3D generalization of the 2D pix2pix GAN [2] , where the generator network that is based on the 3D-U-Net architecture has six downsampling and six upsampling convolution layers. Our proposed generator has a new ResBlock on each convolutional layer in the downsampling path and between the downsampling and upsampling paths. Figure 1b shows an overview of the self-supervised EC training scheme. In a clinical case, the desired cleansed target image volume is not available. To address this issue, the 3D-ResNet-GAN is pretrained with 200 paired volumes of interest (VOIs) with 128 3 voxels from precisely matching lumen locations of the CTC scans of the colon phantom acquired without and with 20 mg/ml and 60 mg/ml contrast concentrations. During this pre-training, the GAN is trained to generate the output VOI (EC VOI) of the native phantom from the provided corresponding tagged phantom VOIs. After this initial pretraining, we simulated a clinical case by use of the 100 previously unseen tagged phantom VOIs that had been acquired at the 40 mg/ml contrast concentration. To obtain an estimate for the corresponding target EC VOIs of these 100 new VOIs, the pre-trained GAN is applied to the new VOIs to yield a corresponding self-generated set of target EC VOIs. The training of the GAN is continued with this total set of 300 paired VOIs that includes the original 200 paired phantom VOIs as well as the new 100 self-generated paired VOIs. The training continues iteratively by replacing the target EC VOIs of the 100 new VOIs at each iteration with those of the most recent output of the 3D-ResNet-GAN. For evaluation, we assessed the difference in image quality between the EC VOIs and the corresponding VOIs of the native phantom. The peak signal-to-noise ratio (PSNR) was used as the metric of the image quality. To evaluate the effect of ResBlocks, we compared the performance of the proposed 3D-ResNet-GAN EC scheme on SE-CTC datasets to those of our previous 3D-GAN EC on SE-CTC and DE-CTC datasets. The use of DE-CTC datasets has been previously shown to improve the performance of EC over that of SE-CTC datasets in CTC. The image quality assessment was performed for the initial training step and for the first two self-supervised training steps (Fig. 2a) . The mean and standard deviation of the PSNR for the proposed 3D-ResNet-GAN EC scheme were 35.86 ± 1.04 after the initial training step, 36.15 ± 1.00 after the second training step, and 36.16 ± 1.01 after the third training step. For the 3D-GAN EC scheme, these metrics were 35.64 ± 1.05 for the initial step, 35.92 ± 1.03 for the second step, and 35.95 ± 1.03 for the third step. When the 3D-GAN EC scheme was applied to the DE-CTC datasets, the metrics were 35.97 ± 1.01 for the initial step, 36.17 ± 1.01 for the second step, and 36.18 ± 1.01 for the third step. Figure 2b shows a 2D image region extracted from a 3D VOI. As shown by these images, the quality of the EC images generated by the 3D-ResNet-GAN EC with SE-CTC datasets is similar to those generated by 3D-GAN EC with DE-CTC datasets. We developed a self-supervised 3D-ResNet-GAN scheme that uses ResBlocks to enhance the performance of EC based on partially selfgenerated training samples. Our results indicate that the scheme can be used with SE-CTC images to generate high-quality EC images that are similar to those obtained by our previous 3D-GAN EC method with DE-CTC images. Therefore, the proposed method shows promise in providing an effective EC scheme for clinical use in CTC. Fig. 1 Overview of a self-supervised EC scheme. a Proposed 3D-ResNet-GAN EC with ResBlock. b Overview of the self-supervised EC scheme for the training of 3D-ResNet-GAN EC scheme Fig. 2 a Improvement of the image quality of EC by use of the selfsupervised 3D-GAN based EC with ResBlock in comparison to the EC scheme without ResBlock. b Examples of EC images generated by different methods 
