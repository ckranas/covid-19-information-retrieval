5bc05260a8a81cb27484ffab9c14466bfa1079e5
Forecasting: theory and practice
Fotios  Petropoulos Daniele  Apiletti Vassilios  Assimakopoulos Mohamed Zied Babai Devon K Barrow Christoph  Bergmeir Ricardo J Bessa John E Boylan Jethro  Browell Claudio  Carnevale Jennifer L Castle Pasquale  Cirillo Michael P Clements Clara  Cordeiro Fernando  Luiz Cyrino  Oliveira Shari  De Baets Alexander  Dokumentov Piotr  Fiszeder Philip Hans Franses Michael  Gilliland M  Sinan Gönül Paul  Goodwin Luigi  Grossi Yael  Grushka-Cockayne Mariangela  Guidolin Massimo  Guidolin Ulrich  Gunter Xiaojia  Guo Renato  Guseo Nigel  Harvey David F Hendry Ross  Hollyman Tim  Januschowski Jooyoung  Jeon Richmond R Victor   Jose Yanfei  Kang Anne B Koehler Stephan  Kolassa Nikolaos  Kourentzes Sonia  Leva Feng  Li Konstantia  Litsiou Spyros  Makridakis Andrew B Martinez Sheik  Meeran Theodore  Modis Konstantinos  Nikolopoulos   Dilekönkal Alessia  Paccagnini Ioannis  Panapakidis Jose M Pavía Manuela  Pedio Diego J Pedregal Pierre  Pinson Patrícia  Ramos David E Rapach J James Reade Bahman  Rostami-Tabar Michał  Rubaszek Georgios  Sermpinis Han Lin Shang Evangelos  Spiliotis Aris A Syntetos Priyanga  Dilini Talagala Thiyanga S Talagala Len  Tashman Dimitrios  Thomakos Thordis  Thorarinsdottir Ezio  Todini Juan  Ramón Trapero  Arenas Xiaoqian  Wang Robert L Winkler Alisa  Yusupova Florian  Ziel 

University of Bath Politecnico di Torino National Technical University of Athens University of Birmingham Monash University INESC TEC -Institute for Systems and Computer Engineering, Technology and Science Lancaster University University of Strathclyde University of Brescia University of Oxford University of Nicosia University of Reading Universidade do Algarve Pontifical Catholic University of Rio de Janeiro (PUC-Rio) Universiteit Gent Nicolaus Copernicus University Erasmus School of Economics SAS Northumbria University University of Bath University of Verona University of Virginia University of Padua Bocconi University and Baffi-CAREFIN Centre MODUL University Vienna University College London University of Padua University College London Beihang University University of Bath Miami University Lancaster University Lancaster University Politecnico di Milano Manchester Metropolitan University University of Nicosia University of Bath Northumbria University University of Thessaly UMMICS University of Bristol ETSI Industrial Technical University of Denmark Saint Louis University University of Bologna National Technical University of Athens Italian Hydrological Society ETSI Industrial Duke University Lancaster University University of Duisburg 
Forecasting has always been in the forefront of decision making and planning. The uncertainty that surrounds the future is both exciting and challenging, with individuals and organisations seeking to minimise risks and maximise utilities. The lack of a free-lunch theorem implies the need for a diverse set of forecasting methods to tackle an array of applications. This unique article provides a non-systematic review of the theory and the practice of forecasting. We offer a wide range of theoretical, state-of-the-art models, methods, principles, and approaches to prepare, produce, organise, and evaluate forecasts. We then demonstrate how such theoretical concepts are applied in a variety of real-life contexts, including operations, economics, finance, energy, environment, and social good. We do not claim that this review is an exhaustive list of methods and applications. The list was compiled based on the expertise and interests of the authors. However, we wish that our encyclopedic presentation will offer a point of reference for the rich work that has been undertaken over the last decades, with some key insights for the future of the forecasting theory and practice. Forecasting has come a long way since early humans looked at the sky to see if the weather would be suitable for hunting, and even since mid-19th-century hunters could get a forecast such as "a high of 40 with a chance of rain". Now a hunter can look at a smartphone to instantly get hour-by-hour forecasts of temperatures and probabilities of rain at multiple locations as well as videos of maps showing forecasted weather patterns over the coming hours. Tailored forecasts of increasing sophistication can be generated to inform important decisions of many different types by managers, public officials, investors, and other decision makers. Rapid advances in computing have enabled the analysis of larger and more complex data sets and stimulated interest in analytics and data science. As a result, the forecaster's toolbox of methods has grown in size and sophistication. Computer science has led the way with methods such as neural networks and other types of machine learning, which are getting a great deal of attention from forecasters and decision makers. Other methods, including statistical methods such as Bayesian forecasting and complex regression models, have also benefited from advances in computing. And improvements have not been limited to those based on computing advances. For example, the literature on judgmental forecasting has expanded considerably, driven largely by the "wisdom of crowds" notion. The combining, or aggregation, of forecasts, which is not a new idea, has received increased attention in the forecasting community recently and has been shown to perform well. For example, the top-performing entries in the M4 forecasting competition run by Spyros Makridakis combined forecasts from multiple methods. Many models have been developed to forecast the number of deaths that will be caused by , and combining the forecasts makes sense because it is hard to know which one will be the most accurate. It is consistent with Bayesian ideas since it can be viewed as updating, with each individual forecast added to the combined forecast (also called an ensemble) contributing some new information. Despite the excitement surrounding these new developments, older methods such as exponential smoothing and ARIMA are still valuable too. Exponential smoothing, along with other simple approaches like a random walk, are quite robust and not as prone to overfitting as more complex methods. In that sense, they are useful not only on their own merits, but as part of an ensemble that also includes more sophisticated methods. Combined forecasts are more valuable if the forecasts come from methods that are diverse so that their forecast errors are not highly correlated. The conditions leading to larger, more sophisticated toolboxes for forecasters have also led to larger data sets with denser grids and improved models in areas of application. This has happened with models of the atmosphere, which are important in formulating improved weather forecasts. More detailed information about customers and their preferences allows the development of improved models of customer behaviour for managers. In turn, forecasting methods that can handle all of that information quickly are valuable for decision-making purposes. This process has spurred an explosion in trying to gather information on the internet. In the Bayesian approach, inferences and forecasts are probabilistic in nature, and predictive probabilities are forecasts. Risk is an important consideration in decision making, and probability forecasts can 1 This subsection was written by Robert L. Winkler. quantify such risks. Theoretical work in probability forecasting has been active for some time, and decision makers in many areas of practice have embraced the use of probability forecasts. The U.S. National Weather Service began issuing probabilities of precipitation to the public in the 1960s. Yet extensive widespread use and dissemination of probabilities has only developed since the turn of the century. Now probability forecasts are increasingly communicated to the public and used as inputs in decision making. Nate Silver's FiveThirtyEight.com report, gives probability forecasts for elections, medicine and science, sporting events, economic measures, and many other areas, often looking at multiple forecasting models individually and also combining them. It is natural for people to desire certainty. When probability forecasts of precipitation were first disseminated widely, many were very sceptical about them, with some accusing the forecasters of hedging and saying "Don't give me a probability. I want to know if it's going to rain or not". Of course, point forecasts often are given along with probability forecasts. Frequent exposure to probabilities helps the general public better understand, appreciate, and feel more comfortable with them. And the current situation in the world with COVID-19, increases in huge fires, big storms, political polarisation, international conflicts, etc., should help them realise that we are living in an age with huge uncertainties, and forecasts that quantify these uncertainties can be important. Where possible, visualisation can help, as indicated by the saying that a picture is worth a thousand words. Examples are the cones of uncertainty on maps in forecasts of the speed, severity, and future path of hurricanes, and the time line of the probability of a team winning a game, updated quickly after each play. Put simply, this is an exciting time for the field of forecasting. With all of the new theoretical developments and forecasting applications in practice, this article is very timely. Forecasting is so ubiquitous that it is not possible to cover all of these developments in a single article. This article manages to cover quite a few, and a good variety. Using short presentations for each one from an expert "close to the ground" on that theoretical topic or field of practice works well to provide a picture of the current state of the art in forecasting theory and practice. The theory of forecasting is based on the premise that current and past knowledge can be used to make predictions about the future. In particular for time series, there is the belief that it is possible to identify patterns in the historical values and successfully implement them in the process of predicting future values. However, exact prediction of futures values is not expected. Instead, among the many options for a forecast of a single time series at a future time period are an expected value (known as a point forecast), a prediction interval, a percentile and an entire prediction distribution. This set of results collectively could be considered to be "the forecast". There are numerous other potential outcomes of a forecasting process. The objective may be to forecast an event, such as equipment failure, and time series may play only a small role in the forecasting process. Forecasting procedures are best when they relate to a problem to be solved in practice. The theory can then be developed by understanding the essential features of the problem. In turn, the theoretical results can lead to improved practice. In this introduction, it is assumed that forecasting theories are developed as forecasting methods and models. A forecasting method is defined here to be a predetermined sequence of steps that produces forecasts at future time periods. Many forecasting methods, but definitely not all, have corresponding stochastic models that produce the same point forecasts and can also be used to generate prediction intervals and prediction distributions. A stochastic model makes assumptions about the process and the associated probability distributions. Even when a forecasting method has an underlying stochastic model, the model is not necessarily unique. For example, the simple exponential smoothing method has multiple stochastic models, including state space models that may or may not be homoscedastic (i.e., possess constant variance). The combining of forecasts from different methods has been shown to be a very successful forecasting method. The combination of the corresponding stochastic models, if they exist, is itself a model. Forecasts can be produced by a process that incorporates new and/or existing forecasting methods/models. Of course, these more complex processes would also be forecasting methods/models. Consideration of the nature of the variables and their involvement in the forecasting process is essential. In univariate forecasting, the forecasts are developed for a single time series by using the information from the historical values of the time series itself. While in multivariate forecasting, other time series variables are involved in producing the forecasts, as in time series regression. Both univariate and multivariate forecasting may allow for interventions (e.g., special promotions, extreme weather) . Relationships among variables and other types of input could be linear or involve nonlinear structures (e.g., market penetration of a new technology). When an explicit functional form is not available, methodologies such as simulation or artificial neural networks might be employed. Theories from fields, such as economics, epidemiology, and meteorology, can be an important part of developing these relationships. Multivariate forecasting could, also, mean forecasting multiple variables simultaneously (e.g., econometric models). The data or observed values for time series come in many different forms that may limit or determine the choice of a forecasting method. In fact, there may be no historical observations at all for the item of interest, when judgmental methods must be used (e.g., time taken to complete construction of a new airport). The nature of the data may well require the development of a new forecasting method. The frequency of observations can include all sorts of variations, such as every minute, hourly, weekly, monthly, and yearly (e. g., the electricity industry needs to forecast demand loads at hourly intervals as well as long term de- mand for ten or more years ahead). The data could be composed of everything from a single important time series to billions of time series. Economic analysis often includes multiple variables, many of which affect one another. Time series for businesses are likely to be important at many different levels (e.g., stock keeping unit, common ingredients, or common size container) and, consequently, form a hierarchy of time series. Some or many of the values might be zero; making the time series intermittent. The list of forms for data is almost endless. Prior to applying a forecasting method, the data may require pre-processing. There are basic details, such as checking for accuracy and missing values. Other matters might precede the start of the forecasting method or be incorporated into the methods/models themselves. The treatment of seasonality is such a case. Some forecasting method/models require de-seasonalised time series, while others address seasonality within the methods/models. Making it less clear when seasonality is considered relative to a forecasting method/model, some governmental statistical agencies produce forecasts to extend time series into the future in the midst of estimating seasonal factors (i.e., X-12 ARIMA). Finally, it is extremely important to evaluate the effectiveness of a forecasting method. The ultimate application of the forecasts provides guidance in how to measure their accuracy. The focus is frequently on the difference between the actual value and a point forecast for the value. Many loss functions have been proposed to capture the "average" of these differences. Prediction intervals and percentiles can be used to judge the value of a point forecast as part of the forecast. On the other hand, the quality of prediction intervals and prediction distributions can themselves be evaluated by procedures and formulas that have been developed (e.g., ones based on scoring rules). Another assessment tool is judging the forecasts by metrics relevant to their usage (e.g., total costs or service levels). In the remaining subsections of section 2, forecasting theory encompasses both stochastic modelling and forecasting methods along with related aspects. Time series models that are often used for forecasting are of the ARMA class, where ARMA is short for autoregressive-moving average (Box et al., 2008) . Before one can start with an iterative model-building process (Franses et al., 2014) , a forecaster needs to decide whether s/he creates models for the levels or the differences of the data. Note that when the data are log-transformed, the differences of the logs approximate the growth rates. The choice between growth rates and levels is guided by so-called unit root tests (Dickey and Fuller, 1979; Dickey and Pantula, 1987) . Another way of phrasing it is that one needs to decide if the data have a stochastic trend (if yes, then convert to growth rates) or a deterministic trend (stick to levels). Under the null hypothesis of a stochastic trend, the data are non-stationary, meaning that they have no fixed mean nor variance, and hence the test involves non-standard statistical theory (Phillips, 1987) . There is also the possibility to swap the hypotheses and take the levels as the null model (Kwiatkowski et al., 1992; Hobijn et al., 2004) . Non-stationary properties may also follow from structural breaks (Perron, 1989) , so it is reasonable to allow for potential breaks in the tests. The power of unit root tests is not large, and in practice one often finds signals to consider growth rates (Nelson and Plosser, 1982) . For forecasting, this is not so much of a problem, as forecasts improve when growth rates are considered instead of levels (Franses and Kleibergen, 1996) . For seasonal data, like quarters and months, one also needs to choose between levels and growth rates, but now there are more choice options for the growth rates. For example for quarterly data, one can opt for the current quarter minus the previous quarter (first differences with seasonal dummies, Miron, 1996) , for the current quarter minus the same quarter in the previous year (seasonal differences, Hylleberg, 1994) , for both at the same time (double differencing, Box et al., 2008) , or for the current quarter minus a parameter times the previous quarter (periodic differencing, Franses, 1994) . In practice this means that one relies on tests for a unit root like before and now also on tests for so-called seasonal unit roots (Hylleberg et al., 1990; Franses, 1991; Ghysels et al., 1994) and/or on tests for periodic unit roots (Boswijk and Franses, 1996; Boswijk et al., 1997) . Making the proper choice improves forecast accuracy (Kawasaki and Franses, 2004) . Like structural breaks in non-seasonal data, it can happen that seasonal means shift abruptly, and it is thus relevant to include these in the test regression (Franses and Vogelsang, 1998; Franses et al., 1997) . Allowing for mean shifts, if there are any, improves the out-of-sample forecasts . Overall, the conclusion is that the proper choice for the type of growth rates is important for forecast quality. Most tests are available in standard software, and asymptotic theory and critical values of the tests are available for any seasonal format. Time series decomposition is an important building block for various forecasting approaches and a crucial tools for statistical agencies. Seasonal decomposition is a way to present a time series as a function of other time series, called components. Commonly used decompositions are additive and multiplicative, where such functions are summation and multiplication correspondingly. If logs can be applied to time series, any additive decomposition method can serve as multiplicative after applying log transformation to the data. The simplest additive decomposition of a time series with single seasonality comprises three components: trend, seasonal component, and the "remainder". It is assumed that the seasonal component has a repeating pattern (thus sub-series corresponding to every season are smooth or even constant), the trend component describes the smooth underlying mean and the remainder component is small and contains noise. The first attempt to decompose time series into trend and seasonality is dated to 1847 when Buys-Ballot (1847) performed decomposition between trend and seasonality, modelling the trend by a polynomial and the seasonality by dummy variables. Then, in 1884 Poynting (1884) proposed price averaging as a tool for eliminating trend and seasonal fluctuations. Later, his approach was extended by Hooker (1901 ), Spencer (1904 and Anderson and Nochmals (1914) . Copeland (1915) was the first who attempted to extract the seasonal component, and Macaulay (1931) proposed a method which is currently considered "classical". The main idea of this method comes from the observation that averaging a time series with window size of the time series seasonal period leaves the trend almost intact, while effectively removes seasonal and random components. At the next step, subtracting the estimated trend from the data and averaging the result for every season gives the seasonal component. The rest becomes the remainder. Classical decomposition led to a series of more complex decomposition methods such as X-11 (Shishkin et al., 1967) , X-11-ARIMA (Dagum, 1988; Ladiray and Quenneville, 2001 ), X-12-ARIMA (Findley et al., 1998) , and X-13-ARIMA-SEATS (Findley, 2005) . Seasonal trend decomposition using Loess (STL: Cleveland et al., 1990) takes iterative approach and uses smoothing to obtain a better estimate of the trend and seasonal component at every iteration. Thus, starting with an estimate of the trend component, the trend component is subtracted from the data, the result is smoothed along sub-series corresponding to every season to obtain a "rough" estimate of the seasonal component. Since it might contain some trend, it is averaged to extract this remaining trend, which is then subtracted to get a detrended seasonal component. This detrended seasonal component is subtracted from the data and the result is smoothed again to obtain a better estimate of the trend. This cycle repeats a certain number of times. Another big set of methods use a single underlining statistical model to perform decomposition. The model allows computation of confidence and prediction intervals naturally, which is not common for iterative and methods involving multiple models. The list of such methods includes TRAMO/SEATS procedure (Monsell et al., 2003) , the BATS and TBATS models (De Livera et al., 2011) , various structural time series model approaches (Harvey, 1990; Commandeur et al., 2011) , and the recently developed seasonal-trend decomposition based on regression (STR: Dokumentov, 2017; Dokumentov and Hyndman, 2018) . The last mentioned is one of the most generic decomposition methods allowing presence of missing values and outliers, multiple seasonal and cyclic components, exogenous variables with constant, varying, seasonal or cyclic influences, arbitrary complex seasonal schedules. By extending time series with a sequence of missing values the method allows forecasting. Temporal data are often subject to uncontrolled, unexpected interventions, from which various types of anomalous observations are produced. Even though an anomaly is typically considered as an unusual, sudden deviation from the norm (Hand, 2009) , it is difficult to find a unified definition for an anomaly and mostly application-specific (Unwin, 2019) . Anomaly detection in forecasting literature has two main focuses, which are conflicting in nature: one demands special attention be paid to anomalies as they can be the main carriers of significant and often critical information such as fraud activities, disease outbreak, natural disasters, while the other down-grades the value of anomalies as it causes data quality issues (Talagala et al., 2020a) . In time series and forecasting literature anomaly detection problems can be identified under three major umbrella themes: detection of (i) contextual anomalies (point anomalies, additive anomalies) within a given series, (i) anomalous sub-sequences within a given series, and (iii) anomalous series within a collection of series (Gupta et al., 2013; Talagala et al., 2020b) . According to previous studies forecast intervals are quite sensitive to contextual anomalies and the greatest impact on forecast are from anomalies occurring at the forecast origin (Chen and Liu, 1993a) . The anomaly detection methods in forecasting applications can be categorised into two groups: (i) model-based approaches and (ii) feature-based approaches. Model-based approaches compare the predicted values with the original data. If the deviations are beyond a certain threshold, the corresponding observations are treated as anomalies (Luo et al., 2018b,a; Sobhani et al., 2020) . Contextual anomalies and anomalous sub-sequences are vastly covered by model-based approaches. Limitations in the detectability of anomalous events depend on the input effects of external time series. Examples of such effects are included in SARIMAX models for polynomial approaches. In nonlinear contexts an example is the generalised Bass model (Bass et al., 1994) for special life cycle time series with external control processes. SARMAX with nonlinear perturbed mean trajectory as input variable may help separating the mean process under control effects from anomalies in the residual process. Feature-based approaches, on the other hand, do not rely on predictive models. Instead, they are based on the time series features measured using different statistical operations that differentiate anomalous instances from typical behaviours (Fulcher and Jones, 2014) . Feature-based approaches are commonly used for detecting anomalous time series within a large collection of time series. Under this approach, it first forecasts an anomalous threshold for the systems typical behaviour and new observations are identified as anomalies when they fall outside the bounds of the established anomalous threshold (Talagala et al., , 2020b . Most of the existing algorithms involve a manual anomalous threshold. In contrast, Burridge and Robert Taylor (2006) and Talagala et al. (2020b) use extreme value theory based data-driven anomalous thresholds. Approaches to the problem of anomaly detection for temporal data can also be divided into two main scenarios: (i) batch processing and (ii) data streams. The data stream scenario poses many additional challenges, due to nonstationarity, large volume, high velocity, noisy signals, incomplete events and online support (Luo et al., 2018b; Talagala et al., 2020b) . The performance evaluation of the anomaly detection frameworks is typically done using confusion matrices (Luo et al., 2018b; Sobhani et al., 2020) . However, these measures are not enough to evaluate the performance of the classifiers in the presence of imbalanced data (Hossin and Sulaiman, 2015) . Following Ranawana and Palade (2006) and Talagala et al. (2019) , Leigh et al. (2019) have used some additional measures such as negative predictive value, positive predictive value and optimised precision to evaluate the performance of their detection algorithms. Exogenous variables are those included in a forecasting system because they add value but are not being predicted themselves, and are sometimes called 'features'. For example, a forecast of county's energy demand may be based on the recent history of demand (an endogenous variable), but also weather forecasts, which are exogenous variables. Many time series methods have extensions that facilitate exogenous variables, such as autoregression with exogenous variables (ARX). However, it is often necessary to prepare exogenous data before use, for example so that it matches the temporal resolution of the variable being forecast (hourly, daily, and so on). Exogenous variables may be numeric or categorical, and may be numerous. Different types of predictor present different issues depending on the predictive model being used. For instance, models based on the variable's absolute value can be sensitive to extreme values or skewness, whereas models based on the variable value's rank, such as tree-based models, are not. Exogenous variables that are correlated with one another also poses a challenge for some models, and techniques such as regularisation and partial leased squares have been developed to mitigate this. Interactions between exogenous variables my also be important when making predictions. For example, crop yields depend on both rainfall and sunlight: one without the other or both in excess will result in low yields, but the right combination will result in high yields. Interactions may be included in linear models by including the product of the two interacting exogenous variables as a feature in the model. This is an example of feature engineering, the process of creating new features based on domain knowledge or exploratory analysis of available data. In machine learning, many features may be created by combining exogenous variables speculatively and passed to a selection algorithm to identify those with predictive power. Combinations are not limited to products, or only two interacting variables, and where many exogenous variables are available, could include summary statistics (mean, standard deviation, range, quantiles...) of groups of variables. Where exogenous variables are numerous, dimension reduction may be applied to reduce the number of features in a forecasting model. Dimension reduction transforms multivariate data into a lower dimensional representation while retaining meaningful information about the original data. Principal component analysis (PCA) is a widely used method for linear dimension reduction, and non-linear alternatives are also available. In retail forecasting, for example, sales of thousands of products may be recorded but including them all in a sales forecasting model may be impractical. Dimension reduction offers an alternative to only using a subset of the available features. Preparation of data for forecasting tasks is increasingly important as the volume of available data is increasing in many application areas. Further details and practical examples can be found in Kuhn and Johnson (2019) and Albon (2018) among other texts in this area. For deeper technical discussion and range of non-linear dimension reduction algorithms, see Hastie et al. (2009) . Exponential smoothing is one of the workhorses of business forecasting and despite the many advances in the field, it is always a tough benchmark to bear in mind. The development of exponential smoothing dates back to 1944, where Robert G. Brown through a mechanical computing device estimated key variables for fire-control on the location of submarines (Gardner, 1985) . More details about the state of the art of exponential smoothing can be found in Gardner (2006) . The idea behind exponential smoothing relies on the weighted average of past observations, where that weight decreases exponentially as one moves away from the present observations. The appropriate exponential smoothing method depends on the components that appear in the time series. For instance, in case that no clear trend or seasonal pattern is present, the simplest form of exponential smoothing 7 This subsection was written by Juan Ramón Trapero Arenas. methods known as Simple (or Single) Exponential Smoothing (SES) is adequate. In some references, is also known as Exponentially Weighted Moving Average (Harvey, 1990) . The formula for SES can be obtained from minimising the discounted least squares error function and expressing the resulting equation in a recursive form (Harvey, 1990) . If observations do not have the same weight, the ordinary least squares cannot be applied. On the other hand, the recursive form is very well-suited for saving data storage. In order to use SES, we need to initialise the algorithm and to define the exponential smoothing parameter. Those values can be obtained by minimising the sum of squares of the one-step ahead forecast errors. Traditionally, the initialisation is done by using either ad hoc values or a heuristic scheme . The estimation of the smoothing parameter usually is restricted to values between 0 and 1. Once SES is defined, the method only provides point forecasts, i.e., forecasts of the mean. Nonetheless, for SES as well as the rest of exponential smoothing methods, it is of vital importance for many applications to provide density (probabilistic) forecasts. To that end, Hyndman et al. (2002) extended exponential smoothing methods under state space models to equip them with a statistical framework capable of providing future probability distributions. Note the difference between traditional exponential smoothing methods and exponential smoothing models (under the state space approach). The former only provide point forecasts, meanwhile the latter also offers probabilistic forecasts, which obviously includes prediction intervals. So far, we have introduced the main concepts using SES, however, real time series can include other components as trends, seasonal patterns, cycles, and the irregular (error) component. Fortunately, for many combinations of components a particular exponential smoothing can be chosen. Pegels (1969) proposed a first classification of exponential smoothing methods, later extended by Gardner (1985) and Taylor (2003a) . The state space framework mentioned above, developed by Hyndman et al. (2002) , allowed to compute the likelihood for each exponential smoothing model and, thus, model selection criteria such as AIC could be used to automatically identify the appropriate exponential smoothing model. Note that the equivalent state space formulation was derived by using a single source of error instead of a multiple source of error (Harvey, 1990) . Hyndman et al. (2008) utilised the notation (E,T,S) to classify the exponential smoothing models, where those letters refer to the following components: Error, Trend, and Seasonality. This notation has gained popularity because the widely-used forecast package for R statistical software, and nowadays exponential smoothing is frequently called ETS. The key idea of linear regression models is that a target (or dependent, forecast, explained, regress) variable, i.e., a time series of interest, can be forecast through other regressor (or independent, predictor, explanatory) variables, i.e., time series or features, assuming that a linear relationship exists between them. For example, daily product sales may be forecast using information related with past sales, prices, advertising, promotions, special days, and holidays. In order to estimate the model, forecasters typically minimise the sum of the squared errors (least squares estimation), using the observations available for fitting the model to the data (Ord et al., 2017) . If the model is simple, consisting of a single regressor, then two coefficients are computed, which are the 8 This subsection was written by Vassilios Assimakopoulos. coefficient of the regressor and a constant. When more regressor variables are considered, the model is characterised as a multiple regression one and additional coefficients are estimated. A common way to evaluate how well a linear regression model fits the data, is through the coefficient of determination, indicating the proportion of variation in the target variable explained by the model. Values close to one indicate sufficient goodness-of-fit, while values close to zero insufficient fitting. However, goodness-of-fit should not be confused with forecastability (Harrell, 2015) . When the complexity of the model is increased, i.e., more regressors are considered, the value of the coefficient will also rise, even if such additions lead to overfitting. Thus, regression models should be evaluated using cross-validation approaches, approximating the post-sample accuracy of the model, or measures that account for model complexity, such as information criteria and the adjusted coefficient of determination (James et al., 2013) . Other diagnostics are the standard deviation of the residuals and the t-values of the regressors. Residual standard error summarises the average error produced by the model given the number of regressors used, thus accounting for overfitting. The t-values measure the impact of excluding regressors from the model in terms of error, given the variation in the data, thus highlighting the importance of the regressors. To make sure that the produced forecasts are reliable, the main assumptions made by the model must be fulfilled. This means that (i) the residuals must be normally distributed with an average value of zero, (ii) the residuals must display insignificant autocorrelation, (iii) the variability of the residuals should be equal across time (no heteroscedasticity present), and (iv) the correlation between the residuals and the observations of the regressor must be zero. If these assumptions are violated, that may mean either that part of the variance of the target variable has not been explained by the model (other or more regressors are needed), or that outliers and influential observations have negatively affected the estimation of the model. Apart from time series regressors, regression models can also exploit categorical (dummy or indicator) variables which may e.g., inform the model about promotions, special events, and holidays (binary variables), the day of the week or month of the year (seasonal dummy variables provided as one-hot encoded vectors), trends and structural changes, and the number of trading/working days included in the examined period. In cases where the target series is long and displays complex seasonal patterns, additional regressors such as Fourier series and lagged values of both the target and the regressor variables may become useful. In the age of vast computing power and computational intelligence, the contribution of simple forecasting methods is possibly not en vogue; the implementation of complicated forecasting systems becomes not only expedient but possibly desirable. Nevertheless forecasting, being a tricky business, does not always favour the complicated or the computationally intensive. Enter the theta method. From its beginnings 20 years back in Assimakopoulos and Nikolopoulos (2000) to recent advances in the monograph of Nikolopoulos and Thomakos (2019) , to other work in-between and recently too, the theta method has emerged as not only a powerfully simple but also enduring method in modern time series forecasting. The original idea has been now fully explained and understood and, as Nikolopoulos and Thomakos (2019) have shown, even the reverend AR(1) model forecast is indeed a theta forecast -and it has already been shown by Hyndman and Billah (2003) that the theta method can represent simple exponential smoothing 9 This subsection was written by Dimitrios Thomakos. (SES) with drift forecasts as well. In its simplest form the method generates a forecast from a linear combination of the last observation and some form of "trend" function, be that a constant, a linear trend, a non-parametric trend or a non-linear trend. The weights on this linear combination, the number of trend functions and their nature and other aspects on expanding the method have been recently researched extensively. The main literature has two strands. The first one details the probabilistic background of the method and derives certain theoretical properties, as in Hyndman and Billah (2003) , Nikolopoulos (2012, 2015) and a number of new theoretical results in Nikolopoulos and Thomakos (2019) . The work of Thomakos and Nikolopoulos provided a complete analysis of the theta method under the unit root data generating process, explained its success in the M3 competition (Makridakis and Hibon, 2000) , introduced the multivariate theta method and related it to cointegration and provided a number of other analytical results for different trend functions and multivariate forecasting. The second strand of the literature expands and details various implementation (including hybrid approaches) of the method, as in the theta approach in supply chain planning of Nikolopoulos et al. (2012) , the optimised theta models and their relationship with state space models in Fioruci et al. (2015) and Fiorucci et al. (2016) , hybrid approaches as in Theodosiou (2011) and Spiliotis et al. (2019a) , to the very latest generalised theta method of Spiliotis et al. (2020a) . These are major methodological references in the field, in addition to many others of pure application of the method. The theta method is also part of the family of adaptive models/methods, and a simple example illustrates the point: the AR(1) forecast or the SES forecast are both theta forecasts but they are also both adaptive learning forecasts, as in the definitions of the recent work by Kyriazi et al. (2019) . As such, the theta forecasts contain the basic building blocks of successful forecasts: simplicity, theoretical foundations, adaptability and performance enhancements. Further research on the usage of the theta method within the context of adaptive learning appears to be a natural next step. Given the simplicity of its application, the freely available libraries of its computation, its scalability and performance, the theta method should be considered as a critical benchmark henceforth in the literature -no amount of complexity is worth its weight if it cannot beat a single Greek letter! The fundamental planks of a forecasting task are collecting the required data and finding a pattern in the data. Then comes the job of identifying a model to represent the pattern, i.e., fitting the model to the data. The procedure has been the staple of forecasters in many of the methods they used. Sometimes when the pattern is not conspicuous, it becomes a challenge. A wise next step would be exploring the data for any hidden patterns. Box et al. (1976) did one such exploration with regards to time series data. They realised that although there may not be any visible patterns in some data series, a pattern could emerge if they correlated the series with itself, i.e., by finding autocorrelations. As an exemplar for finding an autocorrelation, one can offset the data by one period (i.e., making the series to lag by one period) to get a different series and then find the correlation between these -which results in an autocorrelation of lag 1. If the second series is obtained by offsetting the original series by two periods, then one gets an autocorrelation of lag 2. This process continues for other lags. 10 This subsection was written by Sheik Meeran. Box et al. (1976) made use of this 'autocorrelation phenomenon' and incorporated the same into a self-regressing model, i.e., an autoregressive model, represented by the AR part of ARIMA. They further expanded the idea of extracting hidden patterns by finding a relationship between a data in one period and the error in the previous period, and thus came the Moving Average model, i.e., the MA portion of ARIMA. Curiosity about the name 'Moving Average' is palpable among forecasting enthusiasts. Some think the name is arbitrary while others think the history behind this name is lost. Yet another view is "each value of y t can be thought of as a weighted moving average of the past few forecasts" , where y is the forecast variable. Sometimes, the pattern in the data could be even harder to spot as the pattern is 'Integrated' (submerged) with the trend in the data. Such data are called non-stationary. Hence a need for differencing the series to remove the trend and extract the pattern. 'I' in ARIMA denotes the integrated nature of the data. There are alternative views on this, namely 'integrated' could mean 'all-inclusive'. Some consider it could have come from the need for the differenced data to be integrated to get the values at the same level as in the original series. ARIMA has three parameters, p, d, and q, indicating the number of AR terms, the number of differencing, and the number of MA terms respectively. An ARIMA model is denoted as ARIMA (p, d, q) . The data series could have, besides, seasonal patterns that could overlap with the non-seasonal pattern that is seen above. Such models are called Seasonal ARIMA (SARIMA) models represented by ARIMA(p, d, q) (P , D, Q)  where P , D, and Q are the seasonal parameters and the s is the periodicity. Enumerating such models could be quite demanding. Some software use autocorrelation function (ACF) and partial autocorrelation function (PACF) to identify suitable models. The function auto.arima of the forecast package for R statistical software compares models using information criteria, and has been found to be very effective and increasingly being used in ARIMA modelling. A popular extension to ARIMA models is called ARIMAX, implemented by incorporating additional exogenous variables (regressors) that are external to and different from the forecast variable. Volatility has been recognised as a primary measure of risks and uncertainties (Markowitz, 1952; Sharpe, 1964; Taylor et al., 2009; Gneiting, 2011a) . Estimating future volatility for measuring the uncertainty of forecasts is imperative for probabilistic forecasting. Yet, the right period in which to estimate future volatility has been controversial as volatility based on too long a period will make irrelevant the forecast horizon of our interests, whereas too short a period results in too much noise (Engle, 2004 ). An alternative to this issue is the dynamic volatility estimated through the autoregressive conditional heteroscedasticity (ARCH) proposed by Engle (1982) , and the generalised autoregressive conditional heteroscedasticity (GARCH) model proposed by Bollerslev (1987) . The ARCH model uses the weighted average of the past squared forecast error whereas the GARCH model generalises the ARCH model by further adopting past squared conditional volatilities. The GARCH model is the combination of (i) a constant volatility, which estimates the long-run average, (ii) the volatility forecast(s) in the last steps, and (iii) the new information collected in the last steps. The weightings on these components are typically estimated with maximum likelihood. The models assume a residual distribution allowing for producing density forecasts. One of the benefits of the GARCH model is that it can model heteroscedasticity, the volatility clustering characteristics of time series (Mandelbrot, 1963) , a phenomenon common to many time series where uncertainties are predominant. Volatility clustering comes about as new information tends to arrive time clustered and a certain time interval is required for the time series to be stabilised as the new information is initially recognised as a shock. The GARCH model has been extended in the diverse aspects of non-linearity, asymmetry and long memory. Among many such extensions, the Exponential GARCH (EGARCH) model by Nelson (1991) uses log transformation to prevent negative variance; the Threshold GARCH (TGARCH) model by Zakoian (1994) allows for different responses on positive and negative shocks. A small piece of information can have more impact when the time series is under stress than under a stable time series (Engle, 2004) . Another pattern often observed in the volatility time series is slowly decaying autocorrelation, also known as a long memory pattern, which Baillie et al. (1996) capture using a slow hyperbolic rate of decay for the ARCH terms in the fractionally integrated GARCH (FIGARCH) model. Separately, in a further approach to directly estimating long term volatility, the GARCH-MIDAS (Mixed Data Sampling) model proposed by Engle et al. (2013) decomposes the conditional volatility into the short-term volatility, as captured by the traditional GARCH, and the long-term volatility represented by the realised volatilities. The univariate GARCH models surveyed so far have been exended to multivariate versions, in order to model changes in the conditional covariance in multiple time series, resulting in such examples as the VEC (Bollerslev, 1987) and BEKK (Engle and Kroner, 1995) , an acronym derived from Baba, Engle, Kraft, and Kroner. The VEC model, a direct generalisation of the univariate GARCH, requires more parameters in the covariane matrices and provides better fitness at the expense of higher estimation costs than the BEKK. The VEC model has to ensure the positivity of the covariance matrix with further constraints, whereas the BEKK model and its specific forms, e.g., factor models, avoid this positivity issue directly at the model specification stage. In an effort to further reduce the number of parameters to be estimated, the linear and non-linear combinations of the univariate GARCH models, such as the constant conditional correlation model of Bollerslev (1990) and the dynamic conditional correlation models of Tse and Tsui (2002) and of Engle (2002) , were investigated. With the advances in digital data technologies, data is recorded more frequently in many sectors such as energy , healthcare (Whitt and Zhang, 2019) , transportation (Gould et al., 2008) , and telecommunication (Meade and Islam, 2015a) . This often results in time series that exhibit multiple seasonal cycles (MSC) of different lengths. Forecasting problems involving such series have been increasingly drawing the attention of both researchers and practitioners leading to the development of several approaches. Multiple Linear Regression (MLR) is a common approach to model series with MSC (Kamisan et al., 2018; Rostami-Tabar and Ziel, 2020) . While MLR is fast, flexible, and uses exogenous regressors, it does not allow to decompose components and change them over time. Building on the foundation of the regression, Facebook introduced Prophet (Taylor and Letham, 2018) , an automated approach that utilises the Generalised Additive Model (Hastie and Tibshirani, 1990 (Taylor, 2003b) . MSARIMA allows for exogenous regressors and terms can evolve over time, however, it is not flexible, and the computational time is high. Svetunkov and Boylan (2020) introduced the Several Seasonalities ARIMA (SSARIMA) model which constructs ARIMA in a state-space form with several seasonalities. While SSARIMA is flexible and allows for exogenous regressors, it is computationally expensive, especially for high frequency series. Taylor (2003b) introduced Double Seasonal Holt-Winters (DSHW) to extend ETS for modelling daily and weekly seasonal cycles. Following that, Taylor (2010) proposed a triple seasonal model to consider the intraday, intraweek and intrayear seasonalities. Gould et al. (2008) and Taylor and Snyder (2012) instead proposed an approach that combines a parsimonious representation of the seasonal states up to a weekly period in an innovation state space model. With these models, components can change, and decomposition is possible. However, the implementation is not flexible, the use of exogenous regressors is not supported, and the computational time could be high. An alternative approach for forecasting series with MSC is TBATS (De Livera et al., 2011) . TBATS uses a combination of Fourier terms with an exponential smoothing state space model and a Box-Cox transformation, in an entirely automated manner. It allows for terms to evolve over time and produce accurate forecasts. Some drawbacks of TBATS, however, are that it is not flexible, can be slow, and does not allow for covariates. In response to shortcomings in current models, Forecasting with Additive Switching of Seasonality, Trend and Exogenous Regressors (FASSTER) has been proposed by O'Hara-Wild and Hyndman (2020). FASSTER is fast, flexible and support the use of exogenous regressors into a state space model. It extends state space models such as TBATS by introducing a switching component to the measurement equation which captures groups of irregular multiple seasonality by switching between states. In recent years, Machine Learning (ML) approaches have also been recommended for forecasting time series with MSC. MultiLayer Perceptron (MLP: Dudek, 2013; Zhang and Qi, 2005) , Recurrent Neural Networks (RNN: Lai et al., 2018 ), Generalised Regression Neural Network (GRNN: Dudek, 2015 , and Long Short-Term Memory Networks (LSTM Zheng et al., 2017) have been applied on real data (Bandara et al., 2020a; Xie and Ding, 2020) with promising results. These approaches are flexible, allow for any exogenous regressor and suitable when non-linearity exists in series, however interpretability might be an issue for users (Makridakis et al., 2018) . State Space (SS) systems are a very powerful and useful framework for time series and econometric modelling and forecasting. Such systems were initially developed by engineers, but have been widely adopted and developed in Economics as well (Harvey, 1990; Durbin and Koopman, 2012) . The main distinguishing feature of SS systems is that the model is formulated in terms of states, which are a set 13 This subsection was written by Diego J. Pedregal. of variables usually unobserved, but which have some meaning. Typical examples are trends, seasonal components or time varying parameters. A SS system is built as the combination of two sets of equations: (i) state or transition equations which describe the dynamic law governing the states between two adjacent points in time, and (ii) observation equations which specify the relation between observed data (both inputs and outputs) and the unobserved states. Once a SS system is fully specified, the core problem is to provide optimal estimates of states and their covariance matrix over time. This can be done in two ways, either by looking back in time using the well-known Kalman filter (useful for online applications) or taking into account the whole sample provided by smoothing algorithms (typical of offline applications) (Anderson and Moore, 1979) . Given any set of data and a specific model, the system is not fully specified in most cases because it usually depends on unknown parameters scattered throughout the system matrices that define the SS equations. Estimation of such parameters is normally carried out by Maximum Likelihood defined by prediction error decomposition (Harvey, 1990) . Non-linear and non-Gaussian models are also possible, but at the cost of a higher computational burden because more sophisticated recursive algorithms have to be applied, like the extended Kalman filters and smoothers of different orders, particle filters (Doucet and Gordon, 2001) , Unscented Kalman filter and smoother (Julier and Uhlmann, 1997) , or simulation of many kinds, like Monte Carlo, bootstrapping or importance sampling (Durbin and Koopman, 2012) . The paramount advantage of SS systems is that they are not a particular model or family of models strictly speaking, but a container in which many very different model families may be implemented, indeed many treated in other sections of this paper. The following is a list of possibilities, not at all exhaustive: • Univariate models with or without inputs: regression, ARIMAX, transfer functions, non-multiplicative exponential smoothing, structural unobserved components, Hodrick-Prescott filter, spline smoothing. • Fully multivariate: natural extensions of the previous ones plus echelon-form VARIMAX, Structural VAR, VECM, Dynamic Factor models, panel data. • Non-linear and non-Gaussian: TAR, ARCH, GARCH, Stochastic Volatility (Durbin and Koopman, 2012) , Dynamic Conditional Score (Harvey, 2013) , Generalised Autoregressive Score (Creal et al., 2013) , multiplicative exponential smoothing, multiplicative unobserved components. • Other: periodic cubic splines, periodic unobserved components models, state dependent models, Gegenbauer long memory processes (Dissanayake et al., 2018) . Once any researcher or practitioner becomes acquainted to a certain degree with the SS technology, some important advanced issues in time series forecasting may be comfortably addressed (Casals et al., 2016) . It is the case, for example, of systems block concatenation, systems nesting in errors or in variables, treating errors in variables, continuous time models, time irregularly spaced data, mixed frequency models, time varying parameters, time aggregation, hierarchical and group forecasting (time, longitudinal or both: Villegas and Pedregal, 2018) , homogeneity of multivariate models (proportional covariance structure among perturbations), etc. All in all, the SS systems offer a framework capable of handling many modelling and forecasting techniques available nowadays in a single environment. Once the initial barriers are overcome, a wide panorama of modelling opportunities opens up. Since the late 1980s, especially in macroeconomics and finance, the applications of dynamic econometric modelling and forecasting techniques have increasingly relied on a special class of models that accommodate regime shifts, Markov switching models (for short MSMs, also called hidden state Markov models). The idea of MSMs is to relate the parameters of otherwise standard dynamic econometric frameworks (such as systems of regressions, vector autoregressions, and vector error corrections) to one or more unobserved state variables (say S t , V t ) that take K values and would capture the notion of systems going through phases or "regimes". Such states follow a simple, discrete stochastic process, such as a M-th order, recurrent, ergodic, non-reducible Markov chain, and are independent of the shocks of the model. The typical assumption of M = 1 is without loss of generality, because higher order Markov chains can be re-parameterised as a higher dimensional M = 1 chain by increasing K. The mainstream literature (see, e.g., Hamilton 1990 , or the textbook treatments by Kim et al. 1999, and Pedio 2018) initially focused on time-homogeneous Markov chains (when the probabilities of the state transitions are constant). However, the finance literature (Gray, 1996) has moved towards time-heterogeneous MSMs, in which the transition matrix of the regimes may change over time, reacting to lagged values of the endogenous variables, to lagged exogenous variables, or to the lagged values of the state (in a self-exciting fashion). For instance, in a K-regime MS VAR(p), all the parameters in the conditional mean (the vector of intercepts and the p autoregressive matrices) may be assumed to take different, estimable values as a function of S t . Moreover, the covariance matrix of the system shocks may be assumed to depend on some state variable, either the same as the mean parameters (S t ) or an additional, specific one (V t ), which may also depend on lags of S t . When a MS VAR model is extended to include exogenous regressors, we face a MS VARX, of which MS regressions are just a special case. Even though multivariate MSMs may suffer from issues of over-parameterisations that must be kept in check, their power of fitting complex non-linearities is unquestioned because, as discussed by Marron and Wand (1992) , mixtures of normal distributions provide a flexible family that can be used to approximate many distributions. Moreover, MSMs are known to capture key features of many time series (Timmermann, 2000) . For instance, differences in conditional means across regimes enter the higher moments such as variance, skewness, and kurtosis; differences in means in addition to differences in variances across regimes may generate persistence in levels and squared values of the series. MSMs may be estimated by maximum likelihood, although other estimation methods cannot be ruled out, like GMM (Lux, 2008) . Typically, estimation and inference are based on the Expectation-Maximisation algorithm proposed by Dempster et al. (1977) , a filter that allows the iterative calculation of the onestep ahead forecast of the state vector given the information set and a simplified construction of the log-likelihood of the data. However, there is significant evidence of considerable advantages offered by Bayesian approaches based on Monte Carlo Markov chain techniques to estimating multivariate MSMs (see, for example, Hahn et al., 2010) . MSMs have been recently generalised in a number of directions, such as including regimes in conditional variance functions (e.g., of a GARCH or DCC type, which poses significant statistical challenges in writing the likelihood function; see Pelletier, 2006) , and relating different sub-sets of parameters to state vectors that follow a hierarchical structure, following Hamilton and Lin (1996) . It is a well-known fact that financial and economic time series often display non-linear patterns. The most common source of non-linearity is structural instability that may appear in the form of abrupt shifts (breaks) or of recurrent regimes in the model parameters. In the latter case, such instability is stochastic, it has a structure, and as such, it can be predicted. Accordingly, modelling economic and financial instability has become an essential goal for econometricians since the 1970s. One of the first and most popular models is the threshold autoregressive (TAR) model developed by Tong (1978) . A TAR model is an autoregressive model in which the parameters are driven by a state variable S t , which is itself a random variable taking K distinct values. In turn, the value assumed by S t depends on the value of the threshold variable q t resulting from a comparison with a set of K − 1 threshold levels, q * k . The threshold variable q t can be exogenous or can be a lagged value of the dependent variable. In the latter case, we speak of self-exciting threshold autoregressive (SETAR) models. Other choices of q t include linear (see, for example, Chen et al., 2003; Chen and So, 2006; Gerlach et al., 2006) or non-linear (see, for example, Chen, 1995; Wu and Chen, 2007) combinations of the lagged dependent variable or of exogenous variables. The TAR model has also been extended to account for different specifications of the conditional mean function leading to the development of the threshold moving average (TMA: see, for example, Tong, 1978; De Gooijer, 1998; Ling et al., 2007) and the threshold autoregressive moving average (TARMA: see, for example, Ling, 1999; Amendola et al., 2006)  A criticism of TAR models is that they imply a conditional moment function that fails to be continuous. To address this issue, Chan and Tong (1986) proposed the smooth transition autoregressive (STAR) model. The main difference between TAR and STAR models is that, while a TAR imposes an abrupt shift from one regime to the others at any time that the threshold variable crosses above (or below) a certain level, a STAR model allows for gradual changes among regimes. In its simplest form, a STAR is a two-regime model where the dependent variable y t is determined as the weighted average of two autoregressive (AR) models. The weights are given by the transition function G, which is a continuous function bounded between 0 and 1. The transition variable of G, x t , can be the lagged endogenous variable y t−d for d ≥ 1 (see, for example, Teräsvirta, 1994) , a (possibly non-linear) function of it, or an exogenous variable. The transition variable can also be a linear time trend (x t = t), which generates a model with smoothly changing parameters (see, for example, Lin and Teräsvirta, 1994) . Popular choices for G are the logistic function (which gives rise to the LSTAR model) and the exponential function (ESTAR), although additional cumulative distribution functions are also possible. Notably, the simple STAR model we have described can be generalised to have multiple regimes (see, for example, Van Dijk et al., 1999) . Threshold models are also applied to modelling and forecasting volatility; for instance, the GJR-GARCH model of Glosten et al. (1993) can be interpreted as a special case of a threshold model. A few multivariate extensions of threshold models also exist, such as vector autoregressive threshold models, threshold error correction models (see, for example, Balke and Fomby, 1997) , and smooth transition vector error correction models (see, for example, Granger and Swanson, 1996) . In the forecasting literature, Nikolopoulos (2020) argues that great attention has been given to modelling fast-moving time series with or without cues of information available . Less attention has been given to intermittent/count series (see section 2.7), which are more difficult to forecast given the presence of two sources of uncertainty: demand volume, and timing. Historically there have been few forecasting methods developed specifically for such data (Syntetos et al., 2015b) . We believe that through a time series decomposition approachá la Leadbetter (1991) we can isolate 'peaks over threshold' (POT) data points, and create new intermittent series from any time series of interest. The derived series present almost identical characteristics with the series that Croston (1972) analysed. In essence one could use such decomposition forecasting techniques to tackle much more difficult phenomena and problems coming from finance, politics, healthcare, humanitarian logistics, business, economics, and social sciences. Any time series can be decomposed into two sub-series: one containing the baseline (white swans) and one containing the extreme values over an arbitrary-set or rule-based-set threshold (grey and black swans) as proposed by Taleb (2008) . Unfortunately, major decision-related risks and most of the underlying uncertainty lay with these extremes. So, it is very important to be able to effectively model and forecast them. It is unlikely that any forecasting approach will accurately give the exact timing of the forthcoming extreme event, but it will instead provide a satisfactory cumulative forecast over a long period of time. The question still stands what can one do with this forecast? For earthquake data, although even if we know that a major earthquake is going to hit a region, it is almost impossible to decide to evacuate cities, but still we can influence and legislate the way structures are built and increase the awareness, training, preparedness and readiness of the public; and also ensure enough capital on hold to cope with the aftermath of the major event. For epidemics/pandemics there are clear implications, as we have evidenced with COVID-19, on how proactively we can source and secure human resources, medical supplies, etc. What is the current doctrine when forecasting in such a context: advanced probabilistic models. These methods typically require a lot of data and reconstruct the distributions of the underlying phenomena. These come with common successes and a plethora of constraints: big data sets needed for training the models, high mathematical complexity, and invisibility to practitioners how these methods do actually work and thus less acceptance in practice. Yet again, forecasting accuracy is the name of the game and thus these forecasting methods are serious contenders for the task in hand. Extreme Value Theory (EVT) analyses extreme deviations from statistical measures of central location to estimate the probability of events that are more extreme than anything observed in the time series. This is usually done in the following two ways (Nikolopoulos, 2020) Forecasts are often very simple models. The simplest are usually functions of deterministic terms, or the forecast variable itself in previous time periods. Often these kinds of models can forecast better than more complicated forecast models. By more complicated, we usually mean multivariate modelsregression models with multiple explanatory variables. Such models are often based on available theories regarding the determination of the variable to be forecast, and are often referred to as structural models. In a stationary world without structural change, then it would be anticipated that the best structural model would provide the best forecasts, since it would provide the conditional mean of the data process (see, for example, Clements and Hendry, 1998) . In a non-stationary world of unit roots and structural breaks, however, this need not be the case. In such situations, often simple forecast models can outperform structural models, especially at short forecast horizons (see, for example, Hendry and Clements, 2001) . Multivariate forecast models require that explanatory variables also be forecast -or at least, scenarios be set out for them. These may be simplistic scenarios, for example all explanatory variables take their mean values. Such scenarios can play a useful role in formulating policy making since they illustrate in some sense the outcomes of different policy choices. Since the 1980s and Sims (1980) , vector autoregressive (VAR) models have become ubiquitous in macroeconomics, and common in finance (see, for example, Hasbrouck, 1995 (Johansen and Nielsen, 2009a; Castle et al., 2015a) . The flexibility of the approach is such that it has been applied in a wide variety of contexts, from volcanic eruptions (Pretis et al., 2016) to prediction markets and social media trends (Vaughan Williams and Reade, 2016) . A particularly important and ever-expanding area of empirical analysis involves the use of panel data sets. Outliers, structural breaks and split trends undoubtedly also exist in panel time series. The potential to test for common outliers and structural changes across cross sectional units would be useful, as would the ability to allow individual units to vary individually, e.g., time-varying fixed effects. Nymoen and Sparrman (2015) is the first application of indicator saturation methods in a panel context, looking at equilibrium unemployment dynamics in a panel of OECD countries, but applications into the panel context are somewhat constrained by computer software packages designed for indicator saturation. The gets R package of Pretis et al. (2017 can be used with panel data. Functional time series consist of random functions observed at regular time intervals. Functional time series can be classified into two categories depending on if the continuum is also a time variable. On the one hand, functional time series can arise from measurements obtained by separating an almost continuous time record into consecutive intervals (e.g., days or years, see Horváth and Kokoszka, 2012) . We refer to such data structure as sliced functional time series, examples of which include daily precipitation data (Gromenko et al., 2017) . On the other hand, when the continuum is not a time variable, functional time series can also arise when observations over a period are considered as finite-dimensional realisations of an underlying continuous function (e.g., yearly age-specific mortality rates, see Li et al., 2020b) . Thanks to recent advances in computing storage, functional time series in the form of curves, images or shapes is common. As a result, functional time series analysis has received increasing attention. For instance, Bosq (2000) and Bosq and Blanke (2007) proposed the functional autoregressive of order 1 (FAR(1)) and derived one-step-ahead forecasts that are based on a regularised Yule-Walker equations. FAR(1) was later extended to FAR(p), under which the order p can be determined via Kokoszka and Reimherr's (2013) hypothesis testing. Horváth et al. (2020) compared the forecasting performance between FAR(1), FAR(p), and functional seasonal autoregressive models of Chen et al. (2019b) . To overcome the curse of dimensionality, a dimension reduction technique, such as functional principal component analysis (FPCA), is often used. Aue et al. (2015) showed asymptotic equivalence between a FAR and a vector autoregressive (VAR) model. Via an FPCA, Aue et al. (2015) proposed a forecasting method based on the VAR forecasts of principal component scores. This approach can be viewed as an extension of Hyndman and Shang (2009) (Nelsen, 2006; Trivedi and Zimmer, 2007; Joe, 1997) , copula modelling techniques (Durante and Sempi, 2015) , vine copulas (Joe, 2014) , review on frequentist approaches for copula-based forecasting (Patton, 2013) , see the aforementioned references and the references therein. The advantages of the Bayesian copula approach compared to the frequentist treatments are (i) the Bayesian approach allows for jointly modelling the marginal models and the copula parameters, which improves the forecasting efficiency (Joe, 2005; Li and Kang, 2018) , (ii) probabilistic forecasting is naturally implied with the Bayesian predictive density, and (iii) experts information can be seamlessly integrated into forecasting via the priors' setting. Forecasting with copulas involves selecting between a different class of copulas. Common approaches include Bayesian hypothesis testing where copula parameters are treated as nuisance variables (Huard et al., 2006) , or parsimonious modelling of covariance structures using Bayesian selection and model averaging (Pitt et al., 2006; Smith, 2010; Min and Czado, 2011) . One particular interest in copulas is to forecast the dynamic dependencies between multivariate time series. Time-varying copula construction is possible via (i) an autoregressive or GARCH form of de- pendence parameters (Patton, 2006a; Lucas et al., 2014) , (ii) factor copula construction (Oh and Patton, 2018; Tan et al., 2019) that simplifies the computation, (iii) the stochastic copula autoregressive model (Almeida and Czado, 2012 ) that dependence is modelled by a real-valued latent variable, and (iv) covariatedependent copulas approach by parameterising the dependence as a function of possible time-dependent covariates (Li and Kang, 2018 ) that also improves the forecasting interpretability. ARMA-like and GARCHlike dependences in the tail can be considered as special cases of Li and Kang (2018) . In multivariate time series forecasting, unequal length of data is a common issue. One possible approach is to partition the copula parameters into elements relating only to the marginal distributions and elements only relating to the copula (Patton, 2006b) . For mixed frequency data, it is possible to decompose the copula dependence structure into linear and nonlinear components. Then the high and low frequency data is used to model the linear and nonlinear dependencies, respectively (Oh and Patton, 2016) . Bayesian 19 This subsection was written by Feng Li. data augmentation is also used to forecast multivariate time series with mixed discrete and continuous margins (Smith and Khaled, 2012) . For other treatments for discrete and continuous time series (see, for example, Panagiotelis et al., 2012 Panagiotelis et al., , 2017 . Bayesian approach for lower dimensional copula forecasting (d < 10) is straightforward with traditional Gaussian copulas, Student's-t copulas, Archimedean copulas, or pair copula combinations. In higher dimensional settings, special considerations are required to save the computational burden, such as low rank approximation of covariates matrix (Salinas et al., 2019a) or factor copula models with stochastic loadings (Creal and Tsay, 2015) . In the Bayesian setup, forecasting model performance is typically evaluated based on a K-fold out-ofsample log predictive score (LPS: Geweke and Amisano, 2010) , and out-of-sample Value-at-Risk (VaR) or Expected Shortfall (ES) are particularly used in financial applications. The LPS is an overall forecasting evaluation tool based on predictive densities, serving out-of-sample probabilistic forecasting. LPS is ideal for decision makers (Geweke, 2001; Geweke and Amisano, 2010 Blanchard and Kahn, 1980; Sims, 2002) , researchers can write the model using the state-space representation composed by the transition equation and the measurement equation. The latter matches the observed data (in the Smets and Wouters: output growth rate, consumption, investment, wages, worked hours, inflation, and short-term interest rate) with the model latent variables. DSGE models forecasting performance is investigated along two dimensions: point forecast and density forecast. The point forecast is implemented by conducting both static and dynamic analysis, as described in Cardani et al. (2019) . If the static analysis provides a unique forecast value, the dynamic analysis describes the evolution of the prediction along the time dimension to investigate possible time-varying effects. Usually, point predictions are compared using the Diebold and Mariano (1995) and the Clark and West (2006) tests that compare predictions from two competing models The accuracy of the static analysis is based mainly on Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). MAE and RMSE are used to provide a relative forecasting evaluation compared to other competitors. Following Clements and Hendry (1998) , Kolasa et al. (2012) apply the standard forecast unbiased test to assess if DSGEs are good forecasters in the absolute sense. The accuracy of the dynamic analysis is based on the Fluctuation Test (for some DSGE applications, see: Giacomini and Rossi, 2016; Cardani et al., 2019; Boneva et al., 2019) . This test is based on the calculation of RMSEs that are assessed to investigate if the forecasting performance can be influenced by instabilities in the model parameters. The density forecast is based on the uncertainty derived by the Bayesian estimation and it is commonly evaluated using the probability integral transform and the log predictive density scores (as main references, Wolters, 2015; Kolasa and Rubaszek, 2015a) . The statistical significance of these predictions is evaluated using the Amisano and Giacomini (2007) test that compares log predictive density scores from two competing models. Volatility models of financial instruments are largely based solely on closing prices; meanwhile, daily low and high (LH) prices significantly increase the amount of information about the variation of returns during a day. LH prices were used for the construction of highly efficient estimators of variance, so called the range-based (RB) estimators (e.g., Parkinson, 1980; Garman and Klass, 1980; Rogers and Satchell, 1991; Yang and Zhang, 2000; Fiszeder and Perczak, 2013) . They, however, have a fundamental drawback, as they neglect the temporal dependence of returns (like conditional heteroscedasticity) and do not allow for the calculation of multi-period dynamic volatility forecasts. In the last dozen or so years, numerous univariate dynamic volatility models have been constructed based on LH prices. Some of them were presented in the review paper of Chou et al. (2015) . These models can be divided into four groups. The first one comprises simple models, used traditionally to describe returns, but they are based on the price range or on the mentioned earlier RB variance estimators. They include such models as random walk, moving average, exponentially weighted moving average (EWMA), autoregressive (AR), autoregressive moving average (ARMA), and heterogeneous autoregressive (HAR). The second group contains models which describe the conditional variance (or standard deviation) of returns. It comprises models like GARCH-PARK-R (Mapa, 2003) , GARCH-TR (Fiszeder, 2005) , REGARCH (Brandt and Jones, 2006) , RGARCH (Molnár, 2016) . The third group includes models which describe the conditional mean of the price range. It means that in order to forecast variance of returns the results have to be scaled. This group contains models like RB SV (Alizadeh et al., 2002) , CARR (Chou, 2005) , TARR , CARGPR (Chan et al., 2012) , STARR (Lin et al., 2012) , and MSRB (Miao et al., 2013) . The last group is methodologically different because the estimation of model parameters is based on the 21 This subsection was written by Piotr Fiszeder. sets of three prices, i.e., low, high and closing. This approach comprises the GARCH models (Lildholdt, 2002; Venter et al., 2005; Fiszeder and Perczak, 2016 ) and the SV model (Horst et al., 2012) . The development of multivariate models with LH prices has taken place in the last few years. They can be divided into three groups. The first one includes models, used traditionally to describe returns or prices, but they are based on the price range or RB variance estimators. They comprise such models like multivariate EWMA, VAR, HVAR, and vector error correction (VEC). It is a simple approach, however most models omit modelling the covariance of returns. The second group is formed by the multivariate RB volatility models like RB-DCC , DSTCC-CARR (Chou and Cai, 2009 ), RR-HGADCC (Asai, 2013) , RB-MS-DCC (Su and Wu, 2014) , DCC-RGARCH (Fiszeder et al., 2019) , RB-copula (Chiang and Wang, 2011; Wu and Liang, 2011) . The third group includes the multivariate co-range volatility models like multivariate CARR (Fernandes et al., 2005) , BEKK-HL (Fiszeder, 2018) and co-range DCC (Fiszeder and Fałdziński, 2019) . These models apply LH prices directly not only for the construction of variances of returns but also for covariances. Estimators of time series processes can be dramatically affected by the presence of few aberrant observations which are called differently in the time series literature: outliers, spikes, jumps, extreme observations. If their presence is neglected, coefficients could be biasedly estimated. Biased estimates of ARIMA processes will decrease the efficiency of predictions (Bianco et al., 2001) . Moreover, as the optimal predictor of ARIMA models is a linear combination of observed units, the largest coefficients correspond to observations near the forecast origin and the presence of outliers among these units can severely affect the forecasts. The issue was first raised in the seminal paper by Fox (1972) , who suggests a classification of outliers in time series, separating additive outliers (AO) from innovation outliers (IO). The influence of different types of outliers on the prediction errors in conditional mean models (ARIMA models) is studied by Chen and Liu (1993a,b) and Ledolter (1989 Ledolter ( , 1991 , while the GARCH context is explored by Franses and Ghijsels (1999) and Catalán and Trívez (2007) . Abraham and George E. P. Box (1979) propose a Bayesian model which reflects the presence of outliers in time series and allows to mitigate their effects on estimated parameters and, consequently, improve the prediction ability. The main idea is to use a probabilistic framework allowing for the presence of a small group of discrepant units. Model specification for stationary and nonstationary processes is an issue to be considered very carefully when extreme observations are present. A procedure for the correct specification of models is introduced by Tsay (1986) relying on iterative identification-detection-removal of cycles in the observed time series contaminated by outliers. The same issue is tackled by Abraham and Chuang (1989) ; in this work non-influential outliers are separated from influential outliers which are observations with high residuals affecting parameter estimation. Tsay's procedure has been later modified (Balke, 1993 ) to effectively detect time series level shifts. The impulse-and step-indicator saturation approach is used by Marczak and Proietti (2016) for detecting additive outliers and level shifts estimating structural models in the framework of nonstationary seasonal series. They find that timely detection of level shifts located towards the end of the series can improve the prediction accuracy. 22 This subsection was written by Luigi Grossi. All these works are important because outlier and influential observations detection are crucial for improving the forecasting performance of models. Alternatively, the presence of extreme values can be included in the process specification and, once parameters are estimated, the model can be used to obtain bootstrap forecast intervals reflecting the possibility of large stochastic events in the future (Phillips, 1996) . The robust estimation of model parameters is another way to improve predictive accuracy without correcting or removing outliers. Sakata and White (1998) introduce a new two-stage estimation strategy for the conditional variance based on Hampel estimators and S-estimators. Park (2002) proposes a robust GARCH model, called RGARCH exploiting the idea of least absolute deviation estimation. The robust approach is also followed for conditional mean models by Gelper et al. (2009) who introduce a robust version of the exponential and Holt-Winters smoothing technique for prediction purposes and by Cheng and Yang (2015) who propose an outlier resistant algorithm developed starting from a new synthetic loss function. Very recently, Beyaztas and Shang (2019) have introduced a robust forecasting procedure based on weighted likelihood estimators to improve point and interval forecasts in functional time series contaminated by the presence of outliers. The use of equilibrium-correction models is ubiquitous in forecasting. Hendry (2010) notes that in economics this class commonly includes models with explicit equilibrium-correction mechanisms such as vector equilibrium-correction models (VEqCM) as well as models with implicit equilibrium-correction (or long-run mean reversion) mechanisms such as vector auto-regression (VAR) models, dynamic factor models (DFM), dynamic stochastic general-equilibrium (DSGE) models, most models of the variance, and almost all regression equations. This class of forecast model is also prevalent across many other disciplines. For example, Pretis (2020) illustrates that there is an equivalence between physical energy balance models, which are used to explain and predict the evolution of climate systems, and VEqCMs. Despite their wide-spread use in economic modelling and forecasting, equilibrium-correction models often produce forecasts that exhibit large and systematic forecast errors. Hendry (1998, 1999) showed that forecasts from equilibrium-correction models are not robust to abrupt changes in the equilibrium. These types of regime changes are very common in macroeconomic time series (Hamilton, 2016) and can cause the forecasts from many models to go off track. Therefore, if for example, there is a change in the equilibrium towards the end of the estimation sample, forecasts from this class of models will continue to converge back to the previous equilibrium. In general, the forecasts from equilibrium-correction models can be robustified by estimating all model parameters over smaller or more flexible sub-samples. Several studies have proposed general procedures that allow for time-variation in the parameters; see, for example, Pesaran et al. (2013) , Giraitis et al. (2013) , and Inoue et al. (2017) . This allows for an optimal or more adaptive selection of model estimation windows in order generate forecasts after structural breaks have occured. An alternative approach for robustifying forecasts from equilibrium-correction models is to focus on the formulation and estimation of the equilibrium. Hendry (2006) shows that differencing the equilibriumcorrection mechanism can improve the forecasts by removing aspects which are susceptible to shifts. However, differencing the equilibrium also induces additional forecast-error variance. Castle et al. (2010) show that it is beneficial to update the equilibrium or to incorporate the underlying structural break process. Alternatively, Castle et al. (2015b) show that there can be large forecast gains from smoothing over estimates of the transformed equilibrium. Building on this, Martinez et al. (2019) show that there are many possible transformations of the equilibrium that can improve the forecasts. Several of these transformations imply that the equilibrium-correction model collapses to different naive forecast devices whose forecasts are often difficult to beat. By replacing the equilibrium with smooth estimates of these transformations, it is possible to outperform the naive forecasts at both short and long forecast horizons while retaining the underlying economic theory embedded within the equilibrium-correction model. Thus, it is possible to dramatically improve forecasts from equilibrium-correction models using targeted transformations of the estimated equilibrium so that it is less susceptible to the shifts which are so damaging to the model forecasts. When a forecast is made today of the future value of a variable, the forecast is necessarily 'real time' -only information available at the time the forecast is made can be used. The forecasting ability of a model can be evaluated by mimicking this setup -generating forecasts over some past period (so outcomes known) only using data known at each forecast origin. As noted by Clements and Hendry (2005) , out-ofsample forecast performance is the gold standard. Sometimes the analysis is pseudo real time. At a given forecast origin t, forecasts are constructed only using data up to the period t, but the data are taken from the latest-available vintage at the time the study is undertaken. The improved availability of real-time databases has facilitated proper real-time studies. 25 At time t the data are taken from the vintage available at time t. Data revisions are often important, and occur because statistical agencies strive to provide timely estimates which are based on incomplete source data (see, for example, Grimm, 2005, 2008; Zwijnenburg, 2015) . There are a number of possible real-time approaches. The conventional approach is to estimate the forecasting model using the latest vintage of data available at time t. Suppose the vintage-t contains data for time periods up to t − 1, denoted . . . , y t t−3 , y t t−2 , y t t−1 . The observation for time t − 1 is a first estimate, for t − 2 a second estimate, and so on, such that data for earlier periods will have been revised many times. Hence the model will be estimated on data of different maturities, much of which will have been revised many times. But the forecast will typically be generated by feeding into the model 'lightly-revised' data for the most recent time periods. The accuracy of the resulting forecasts can be improved upon (in principle) by taking into account data revisions (see, for example, Koenig et al., 2003; Kishor and Koenig, 2012; Clements and Galvão, 2013b) . Koenig et al. (2003) suggest using real-time-vintage (RTV) data to estimate the model. The idea is to use early estimates of the data to estimate the model, so that the model is estimated on 'lightly-revised' data that matches the maturity of the forecast-origin data that the forecast is conditioned on. Other approaches seek to model the data revisions process along with the fully-revised true values of the data, as in Kishor and Koenig (2012) , Cunningham et al. (2009), and Jacobs and van Norden (2011) . Reduced form models that avoid the necessity of estimating unobserved components have adapted the vector autoregression (VAR) of Sims (1980) to jointly model different observed vintages of data. Following Patterson (1995) , Garratt et al. (2008) work in terms of the level of the log of the variable, Y t+1 t , and model the vector given by Galvão (2012, 2013a) and Carriero et al. (2015) minimise the effects of benchmark revisions and re-basing by modelling 'samevintage-growth rates', namely Z t+1 = y t+1 t , y t+1 t−1 , . . . , y t+1 t−q+1 , where y t+1 t = Y t+1 t − Y t+1 t−1 , and q denotes the greatest data maturity. Galvão (2017) shows how forecasts of fully-revised data can be generated for Dynamic stochastic general equilibrium (DSGE) models (for example, Del Negro and Schorfheide, 2013), by applying the approach of Kishor and Koenig (2012) . Clements (2017) argues that improvements in forecast accuracy might be expected to be greater for interval or density forecasts than point forecasts, and this is further explored by Clements and Galvão (2017) . Surveys on data revisions and real-time analysis, including forecasting, are provided by Croushore (2006, 2011b,a) and Clements and Galvão (2019) . Forecasting the diffusion of innovations is a broad field of research, and influential reviews on the topic have highlighted its importance in many disciplines for strategic or anticipative reasons (Mahajan et al., 1990; Meade and Islam, 2006; Peres et al., 2010) . Large-scale and fast diffusion processes of different nature, ranging from the spread of new epidemics to the adoption of new technologies and products, from the fast diffusion of news to the wide acceptance of new trends and social norms, are demanding a strong effort in terms of forecasting and control, in order to manage their impact into socio-economic, technological and ecological systems. The formal representation of diffusion processes is often based on epidemic models, under the hypothesis that an innovation spreads in a social system through communication among people just like an epidemics does through contagion. The simplest example is represented by the (cumulative) logistic equation that describes a pure epidemic process in a homogeneously mixing population (Verhulst, 1838) . The most famous and employed evolution of the logistic equation is the Bass model (Bass, 1969) developed in the field of quantitative marketing and soon become a major reference, due to its simple and powerful structure. The Bass model (BM) describes the life-cycle of an innovation, depicting its characterising phases of launch, growth/maturity, and decline, as result of the purchase decisions of a given cohort of potential adopters. Mathematically, the model is represented by a first order differential equation, describing a diffusion process by means of three parameters: the maximum market potential, m, assumed to be constant along the whole diffusion process, and parameters p and q, referring respectively to two distinct categories of consumers, the innovators, identified with parameter p, adopting for first, and the imitators, adopting at a later stage by imitating others' behaviour and thus responsible for word-of-mouth dynamics. In strategic terms, crucial forecasts are referred to the point of maximum growth of the life cycle, the peak, and the point of market saturation. Innovation diffusion models may be also used for post-hoc explanations, helping understand the evolution of a specific market and its response to different external factors. Indeed, one of the most appealing 26 This subsection was written by Mariangela Guidolin. characteristics of this class of models is the possibility to give a simple and nice interpretation to all the parameters involved. In this perspective, a valuable generalisation of the BM was proposed in Bass et al. (1994) with the Generalised Bass Model (GBM). The GBM enlarges the BM by multiplying its hazard rate by a very general intervention function x(t), assumed to be non-negative, which may account for exogenous shocks able to change the temporal dynamics of the diffusion process, like marketing strategies, incentive mechanisms, change in prices and policy measures. Another generalisation of the BM and the GB¡, relaxing the assumption of a constant market potential was proposed in Guseo and Guidolin (2009) with the GGM. This model postulates a time-dependent market potential, m(t), which is function of the spread of knowledge about the innovation, and thus assumes that a diffusion process is characterised by two separate phases, information and adoption. The GGM allows a significant improvement in forecasting over the simpler BM, especially through a more efficient description of the first part of the time series, often characterised by a slowdown pattern, as noticed by Guseo and Guidolin (2011) . As early as in 1925 Alfred J. Lotka demonstrated that manmade products diffuse in society along Sshaped patterns similar to those of the populations of biological organisms (Lotka, 1925) . Since then S curve logistic descriptions have made their appearance in a wide range of applications from biology, epidemiology and ecology to industry, competitive substitutions, art, personal achievement and others (Fisher and Pry, 1971; Marchetti, 1983; Meade, 1984; Modis, 1992) . In fact, logistic growth can be detected whenever there is growth in competition, and competition can be generalised to a high level of abstraction, e.g., diseases competing for victims and all possible accidents competing for the chance to be materialised. S curves enter as modular components in many intricate natural patterns. One may find S curves inside S curves because logistics portray a fractal aspect; a large S curve can be decomposed in a cascade of smaller ones (Modis, 1994) . One may also find chaos by rendering the logistic equation discrete (Modis and Debecker, 1992) . Finally, logistics sit in the heart of the Volterra-Lotka equations, which describe the predator-prey relations and other forms of competition. In its full generality, the logistic equation, in a discrete form, with cross terms to account for all interrelations between competing species, would give a complete picture in which growth in competition, chaos, self-organisation, complex adaptive systems, autopoiesis, and other such academic formulations, all ensue as special cases (Modis, 1997) . Each S curve has its own life cycle, undergoing good and bad "seasons" (see figure 1) . A large set of behaviours have been tabulated, each one best suited for a particular season (Modis, 1998) . Becoming conservative -seeking no change -is appropriate in the summer when things work well. But excellence drops in second place during the difficult times of winter -characterised by chaotic fluctuations -when fundamental change must take place. Learning and investing are appropriate for spring, but teaching, tightening the belt, and sowing the seeds for the next season's crop belong in the fall. S curves can also serve simply qualitatively to obtain rare insights and intuitive understanding. The seasons metaphor dictates that in spring the focus is on what to do, whereas in fall the emphasis shifts to the how. For example, the evolution of classical music can be visualised as a large-timeframe S curve beginning sometime in the fifteenth century and reaching a ceiling in the twentieth century (see figure   27 This subsection was written by Theodore Modis. Winter Spring Summer Fall Winter Figure 1 : Typical attributes of a growth cycle's "seasons". Adopted from Modis (1998) with the permission of the author. 2). In Bach's time composers were concerned with what to say. The value of their music is in its architecture and as a consequence it can be interpreted by any instrument, even by simple whistling. But two hundred years later composers such as Debussy wrote music that depends crucially on the interpretation, the how. Classical music was still "young" in Bach's time but was getting "old" by Debussy's time. No wonder Chopin is more popular than Bartók. Chopin composed during the "summer" of music's S curve when public preoccupation with music grew fastest. Around that time composers were rewarded more handsomely than today. The innovations they made in music were assimilated by the public within a short period of time because the curve rose steeply and would rapidly catch up with each innovation. But today the curve has flattened and composers are given limited space. If they make even small innovations they find themselves above the curve and there would not be any time in the future when the public will appreciate their work. On the other hand, if they do not innovate, they will not be saying anything new. In either case today's composers will not be credited with an achievement. An S curve constructed using only qualitative arguments seems accurate and informative. Competition in a social system refers to the presence of antagonists that contend a resource. Separate time series describe rate variables of the individually acquired resource representing local oscillatory behaviour with nonlinear trends, unexpected deviations and saturating effects. The use of standard time series analysis may be misleading for crucial omissions. A nonlinear trend cannot be 'absorbed' by low-order differences. Consequently, seasonal and autoregressive components cannot be correctly identified. The analytic representation of competition follows different approaches. Systems analysis exploits differential equations describing rate variables as functions of all variables and part of their interactions. Complex systems analysis (Boccara, 2004) refers to a class of agents that interact through local transition rules producing competition as an emergent behaviour. Through mean field approximations, previous approach 28 This subsection was written by Renato Guseo. Figure 2 : The evolution of classical music. The vertical axis could be something like "importance", "public appreciation", or "public preoccupation with music", (cumulative). Adopted from Modis (2013) with the permission of the author. may be frequently reduced to a system of differential equations. Competition is synchronic if antagonists reach a common environment simultaneously; diachronic when the late entrant is introduced later. First historical contributions are due to Lotka (1920) and Volterra (1926) , which independently obtained a class of predator-prey synchronic models. A generalised version of the Lotka-Volterra model, LV, is provided by Abramson and Zanette (1998) . In a duopoly (Morris and Pratt, 2003) the equations explicit the role of local carrying capacities or market potentials, the inhibiting strength of the rivals in accessing the residual resource, and the rate modulation of interaction or word-of-mouth (WOM) effects between the specific cumulative resource and its untransformed residual. The residual resource is essentially specific and interactions exclude first and second order components of the rivals. Usually, Lotka-Volterra models do not have closed form solutions. In both cases, staked form of equations or solutions allows a first-stage nonparametric inference based on nonlinear least squares (NLS) avoiding strong assumptions on the stochastic distributions of error components. Short-term refining may be grounded on a Seasonal Autoregressive Moving Average with exogenous input (SARMAX) representation. Outstanding forecasts are obtained including the first-stage solution as 'exogenous' input. A different synchronic approach, Givon-Bass (GB) that extends diffusion of innovations theory (Bass, 1969; Givon et al., 1995) is given in Bonaldo (1991) through equations that involve parametric components of global interaction. The residual resource is completely accessible to all competitors, and the rate equations introduce distributed seeding effects. The GB model has a closed form solution that was independently published by Krishnan et al. (2000) . The model and related advances by Savin and Terwiesch (2005) and Guseo and Mortarino (2010) were generalised to the diachronic case in Guseo and Mortarino (2012) , defining a competition and regime change diachronic model (CRCD). The carrying capacity has two fixed regimes before and during competition. Similar aspects concern seeding and interaction effects. CRCD has a closed form solution. A relevant extension of CRCD is discussed in Guseo and Mortarino (2014) introducing within-brand and cross-brand WOM effects not included in Lotka-Volterra approach. The unrestricted unbalanced diachronic competition model (UCRCD) is defined with these new competing effects. It does not have a closed form solution for general 'discriminations' parameters. The restricted UCRCD case, for restricted discriminations, has two different solutions. The model assumes a local invariance of carrying capacity. In Guseo and Mortarino (2015) this assumption was weakened, introducing a dynamic carrying capacity approach Guidolin, 2009, 2011) . The CRCD is a special submodel of UCRCD. Forecasting uncertainty consists in estimating the possible range of forecast errors (or true values) in the future and the most widely adopted representation is a forecast interval (Patel, 1989) . The forecast interval indicates a range of values and the respective probability, which is likely to contain the true value (which is yet to be observed) of the response variable. Since for a specific lead-time the forecast interval only encompasses information from a marginal distribution, it can also be named marginal forecast interval (MFI). A MFI can be obtained from: parametric distribution, such as a Gaussian distribution with conditional mean and variance estimated with a Generalised ARCH model (Baillie and Bollerslev, 1992) ; non-parametric distribution, e.g., obtained with conditional kernel density estimation ; directly estimated with statistical learning methods, such as quantile regression (Taylor and Bunn, 1999) or bootstrapping (Masarotto, 1990) , or with machine learning algorithms like quantile random forests (Meinshausen, 2006) . For multi-step ahead forecasting problems, in particular when information about forecast uncertainty is integrated in multi-period stochastic optimisation (Dantzig and Infanger, 1993) , information about the temporal dependency structure of forecast errors (or uncertainty) is a fundamental requirement. In this case, the concept of simultaneous forecast intervals (SFI) can be found in the statistical literature (Chew, 1968 ). SFI differ from MFI since take into account the temporal interdependency of forecast errors and are constructed to have the observed temporal trajectory of the response variable fully contained inside the forecast intervals during all lead-times of the time horizon. The number of works that cover SFI is lower when compared to the MFI, but some examples are: methods based on Bonferroni-and product-type inequalities applied time series forecasting models likes ARIMA and Holt-Winters (Ravishanker et al., 1991) ; combination of bootstrap replications and an heuristic optimisation procedure to find an envelope of the temporal trajectories closest to the deterministic forecast (Staszewska-Bystrova, 2011); sampling forecast errors at different horizons and estimate the SFI with the empirical Mahalanobis distance (Jordá et al., 2013) . Advances in Operations Research for decision-making problems under uncertainty imposed new requirements in forecast uncertainty estimation and representation. On one hand, stochastic optimisation requires a scenario representation for forecast uncertainty (Powell, 2019) . This motivated research in methods that generate uncertainty forecasts represented by random vectors (term used in statistics) or path forecasts (term used in econometrics), such as parametric copula combined with MFI (Pinson et al., 2009) , parametric dynamic stochastic model (Li and Chan, 2011) or epi-spline basis functions (Rios et al., 2015) . On the other hand, robust optimisation does not make specific assumptions on probability distributions 29 This subsection was written by Ricardo Bessa. and the uncertain parameters are assumed to belong to a deterministic uncertainty set. Hence, some authors proposed new methods to shape forecast uncertainty as polyhedral or ellipsoidal regions to enable a direct integration of forecasts in this type of optimisation problem (Bertsimas and Pachamanova, 2008; Golestaneh et al., 2019) . Finally, communication of forecast uncertainty (e.g., MFI, SFI, random vectors) to decision-makers requires further attention since it remains as a major bottleneck for a wide adoption by industry, particularly in uses cases with multivariate time series (Akram et al., 2015) and adverse weather events (Ramos et al., 2010) . Please also see section 3.7.8. A non-negative continuous random variable X is fat-tailed, if its survival function L(x) = 1 for t > 0 (Embrechts et al., 2013) . The parameter α is known as the tail parameter, and it governs the thickness of the tail -the smaller α the fatter the tail -and the existence of moments, so that E[X p ] < ∞ if and only if α > p. Often α is re-parametrised as ξ = 1/α. Fat tails are omnipresent in nature, from earthquakes to floods, and they are particularly common in human-related phenomena like financial markets, insurance, pandemics, and wars (see, for example, Mandelbrot, 1983; Taleb, 2020 , and references therein). Forecasting fat-tailed random variables is therefore pivotal in many fields of life and science. However, while a basic coverage of the topic is available in most time series and risk management manuals (e.g., Shumway and Stoffer, 2017; McNeil et al., 2015) , the profound implications of fat tails are rarely taken into consideration, and this can generate substantial errors in forecasts. As observed in Taleb et al. (2020) , any statistical forecasting activity about the mean -or another quantity -of a phenomenon needs the law of large numbers (LLN), which guarantees the convergence of the sample mean at a given known rate, when the number of observations n grows. Fat-tailed phenomena with tail parameter α ≤ 1 are trivially not predictable. Since their theoretical mean is not defined, the LLN does not work, for there is nothing the sample mean can converge to. This also applies to apparently infinite-mean phenomena, like pandemics and wars, i.e., extremely tricky objects of study, as discussed in Cirillo and Taleb (2016a) . In similar situations, one can rely on extreme value theory to understand tail risk properties, but should refrain from standard forecasting. For random variables with 1 < α ≤ 2, the LLN can be extremely slow, and an often unavailable number of observations is needed to produce reliable forecasts. Even for a well-behaved and non-erratic phenomenon, we all agree that a claim about the fitness or non-fitness of a forecasting approach, just on the basis of one single observation (n = 1), would be considered unscientific. The fact is that with fat-tailed variables that "n = 1" problem can be made with n = 10 6 observations (Embrechts et al., 2013; Taleb, 2020) . In the case of events like operational losses, even a larger n → ∞ can still be just anecdotal (Cirillo and Taleb, 2016a) . According to Taleb (2020) , owing to preasymptotics, a conservative heuristic is to manage variables with α ≤ 2.5 as practically unpredictable. Their sample average is indeed too unstable and needs too many 30 This subsection was written by Pasquale Cirillo. observations for forecasts to be reliable in a reasonable period of time. For α > 2.5, conversely, forecasting can take place, and the higher α the better. In any case, more research is strongly needed and desirable. Observe that even discussing the optimality of any alarm system (see, for example, Turkman and Turkman, 1990; Svensson et al., 1996) based on average forecasts would prove meaningless under extremely fat tails (α ≤ 2), when the LLN works very slowly or does not work. In fact, even when the expected value is well-defined (i.e., 1 < α < 2), the non-existence of the variance would affect all the relevant quantities for the verification of optimality (De Mare, 1980) , like for instance the chance of undetected events. For all these quantities, the simple sample estimates commonly used would indeed be misleading. Other leading indicators consist of one variable only and are purely survey-based, such as the monthly ifo Business Climate Index published by the ifo Institute for Germany, which covers the appraisal of current and future expected business of executives of approximately 9,000 German firms across industries (ifo Institute, 2020). If a specific leading indicator is able to capture certain economic changes before they happen, such a leading indicator is said to Granger-cause (or predictively cause) a forecast variable (Granger, 1969) , implying that a "cause" cannot happen after an effect. In (theoretical) econometric terms and following the description of Lütkepohl (2005) , a forecast variable is Granger-caused by a leading indicator if for at least one forecast horizon h the following inequality holds: the mean square error of the optimal h-stepahead predictor of the forecast variable, which includes the information contained in the leading indicator in the set of all relevant information to the forecaster available at the forecast origin, must be smaller than the mean square error of that h-step-ahead predictor of the forecast variable without the information contained in said leading indicator (Lütkepohl, 2005) . Nonetheless, the concept of Granger causality also has its limitations. The notion that a "cause" cannot happen after an effect is only a necessary, yet not a sufficient condition for causality, hence the reason why "cause" has been written with quotation marks in this section. If one ignored this fact and equated causality with Granger causality, they would commit an informal logical fallacy called Post hoc ergo propter hoc (i.e., after this, therefore because of this; Walton et al., 2008) . A bold example of this fallacy would be, "Because 31 This subsection was written by Ulrich Gunter. the birds migrate, winter is coming". This is a fallacy, as winter would come at about the same time every year, no matter if there were migrating birds or not. Moreover, hardly any economic activity is monocausal. There are also different types of formal statistical Granger causality tests available for different data structures that are implemented in typical statistics/econometrics software packages. For the simple case of two variables (Granger, 1969) , say the forecast variable and the leading indicator, the null hypothesis of a bivariate Granger causality test is that the leading indicator does not Granger-cause the forecast variable. Under this null hypothesis, the F-test statistic on the joint impact of the coefficients of the past realisations of the leading indicator employed as explanatory variables in a multiple linear regression with the forecast variable as dependent variable and its past realisations as additional explanatory variables will not be statistically significantly different from zero. The maximum lag for past realisations would be optimally determined, for instance, by some information criterion (e.g., AIC, BIC). A simple model must be easily understood by decision-makers. On the contrary, relationships in a complex model are opaque for its users. In this context, complexity is not measured solely by the number of parameters, but also by the functional form of the relationships among variables. Complex models are commonly believed to deliver better forecasts, as they well describe sophisticated economic structures and offer good fit the data. Consequently, they are favoured by researchers, decisionmakers and academic journals. However, empirical studies provide little evidence about their forecasting superiority over simple models. This can be explained using the bias-variance trade-off framework, in which the mean squared error can be decomposed into Noise is driven by the random term in the model. It cannot be avoided, even if we know the true DGP. Variance is caused by the need to estimate the model parameters, hence its value increases with model complexity and declines with the sample size. Bias is predominantly related to model mis-specification, which is most likely to occur for simple methods. The implications of the above framework are twofold: (i) the relationship between model complexity and MSE is U-shaped and (ii) the optimal model complexity increases with the sample size. The illustration of the bias-variance trade-off for a simple autoregressive model is provided by Ca' Zorzi et al. (2016) . For a stationary DGP, the accuracy of forecasts from the random walk and a calibrated AR(1) model tend to be higher than that from an estimated AR(1). This result explains why in numerous studies the random walk is a tough benchmark as well as why a simple, calibrated AR(1) model can be successful in forecasting inflation (Faust and Wright, 2013) , exchange rates (Ca' Zorzi et al., 2017) or oil prices (Rubaszek, 2020) compared to a number of complex competitors. Wide support to the view that model simplicity improves forecasting performance is presented by Green and Armstrong (2015) , in an introductory article to the special issue of Journal of Business Research "Simple versus complex forecasting", as well as the results of M1 and M2 competitions (Makridakis et al., 1982 (Makridakis et al., , 1993 . Stock and Watson (1998) also show that for most US monthly series complex non-linear autoregressive models deliver less accurate forecasts than their linear counterparts. On the contrary, the results of the M3 and M4 competitions tend to favour more complex models (Makridakis and Hibon, 2000; Makridakis et al., 2020c) . Why are then complex models preferred to simple ones, if the latter deliver more accurate forecasts? Brighton and Gigerenzer (2015) claim that there is a tendency to overemphasize the bias component and downplay the role of variance. This behaviour is leading to an implicit preference towards more complex forecasting methods, which is called by the authors as "bias bias". To avoid it, one can follow the golden rules of forecasting, which says: be conservative in the choice of over-ambitious models and be wary of the difficulty to forecast with complex methods . References to 'big data' have become somewhat ubiquitous both in the media and in academic literature in recent years. Whilst in some disciplines (for example Finance) it has become possible to observe time series data at ever higher frequency, it is in the cross section that the amount of data available to analysts has seen exponential growth. Ordinary Least Squares (OLS) is the standard tool for data analysis and prediction, but is well known to perform poorly when there many potential explanatory variables; Bishop (2006) sets out clearly why this is so. In situations where there is no obvious underlying model to suggest which of a potentially large set of candidate variables to focus on, the researcher needs to add new tools to the tool kit. There are two principal sets of approaches to this problem. The first seeks to reduce the model dimensions by summarising the predictors in to a much smaller number of aggregated indices or factors. The second is to employ a regularisation strategy to reduce the effective dimension of the problem by shrinking the size of the regression coefficients. Some strategies reduce a subset of these coefficients to zero, removing some variables from the model entirely and these can hence be described as variable selection procedures. Such procedures may be applicable either when a problem is believed to be truly sparse, with only a small number of variables having an effect, or alternatively when a sparse model can provide effective forecasts, even though the underling system is in fact more complex. In the frequentist framework, the Least Absolute Shrinkage and Selection Operator (LASSO) procedure of Tibshirani (1996) has proven effective in many applications. The LASSO requires choice of an additional regularisation parameter (usually selected by some statistical criteria or via a cross validation process). Various refinements to the original LASSO procedure have been developed, see in particular Rapach et al. Where the underlying process is not properly sparse (see Giannone and Primiceri, 2017 George and McCulloch (1993) and Mitchell and Beauchamp (1988) , which use binary indicators to select variables in to the model. More recent approaches use continuous random variables to achieve a similar effect, making computation more tractable. Examples include the Horeshoe Prior (Carvalho et al., 2010; Piironen and Vehtari, 2017) and the LN-CASS prior of Thomson et al. (2019) . Cross (2020) is a recent example of an economic forecasting exercise using several such models. Even when selecting for forecasting facing multiple breaks at unknown times, the natural target for model selection is the data generating process (DGP: Hendry, 1995) . However, taxonomies of all possible sources of forecast errors from estimated models that are not the DGP show only a couple of mistakes determine forecast failures (systematic departures between forecasts and later outcomes), namely mismeasured forecast origins and unanticipated location shifts (Clements and Hendry, 1999; Hendry and Mizon, 2012) . Failure to capture these can distort estimation and forecasts. When facing an unknown number of in-sample contaminating outliers and multiple breaks at unknown times, selection must be jointly over both observations and variables, requiring computer learning methods (Hendry and Doornik, 2014; . Autometrics, a multiple-path block-search algorithm (see Doornik, 2018) , uses impulse (IIS: Hendry et al., 2008 and Nielsen, 2009b) and step (SIS: Castle et al., 2015c) indicator saturation for discovering outliers and breaks, as well as selecting over other variables. Although knowing the in-sample DGP need not suffice for successful forecasting after out-of-sample shifts like financial crises and pandemics, 'robust variants' of selected models can then avoid systematic mis-forecasts (see, for example, Martinez et al., 2019; Doornik et al., 2020a) . Saturation estimation has approximately K = k+2n candidates for k regressors, with 2 K possible models, requiring selection with more candidate variables, K, than observations, n. Selection criteria like AIC (Akaike, 1973) , BIC (Schwarz, 1978) , and HQ (Hannan and Quinn, 1979) are insufficient in this setting. For saturation estimation, we select at a tight significance level, α = 1/K, retaining subject-theory variables and other regressors. When forecasting is the aim, analyses and simulations suggest loose significance for then selecting other regressors, close to the 10% to 16% implied significance level of AIC, regardless of location shifts at or near the forecast origin (Castle et al., 2018) . At loose levels, Autometrics can find multiple undominated terminal models across paths, and averaging over these, a univariate method and a robust forecasting device can be beneficial, matching commonly found empirical outcomes. The approach applies to systems, whence selection significance of both indicators and variables is judged at the system level. Capturing in-sample location shifts remains essential . There are costs and benefits of selection for forecasting. Selection at a loose significance level implies excluding fewer relevant variables that contribute to forecast accuracy, but retaining more irrelevant variables that are adventitiously significant, although fewer than by simply averaging over all sub-models (Hoeting et al., 1999) . Retaining irrelevant variables that are subject to location shifts worsens forecast performance, but their coefficient estimates are driven towards zero when updating estimates as the forecast origin moves forward. Lacking omniscience about future values of regressors that then need to be 34 This subsection was written by David F. Hendry. forecast, not knowing the DGP need not be costly relative to selecting from a general model that nests it (Castle et al., 2018) . Overall, the costs of model selection for forecasting are small compared to the more fundamental benefit of finding location shifts that would otherwise induce systematic forecast failure. When building a predictive model, its purpose is usually not to predict well the already known samples, but to obtain a model that will generalise well to new, unseen data. To assess the out-of-sample performance of a predictive model we use a test set that consists of data not used to estimate the model. However, as we are now using only parts of the data for model building, and other parts of the data for model evaluation, we are not making the best possible use of our data set, which is a problem especially if the amount of data available is limited. Cross-validation (CV) , first introduced by Stone (1974) , is a widely used standard technique to overcome this problem (Hastie et al., 2009 ) by using all available data points for both model building and testing, therewith enabling a more precise estimation of the generalisation error and allowing for better model selection. The main idea of (k-fold) cross-validation is to partition the data set randomly into k subsets, and then use each of the k subsets to evaluate a model that has been estimated on the remaining subsets. An excellent overview of different cross-validation techniques is given by Arlot and Celisse (2010) . Despite its popularity in many fields, the use of CV in time series forecasting is not straightforward. Time series are often non-stationary and have serial dependencies. Also, many forecasting techniques iterate through the time series and therewith have difficulties dealing with missing values (withheld for testing). Finally, using future values to predict data from the past is not in accordance with the normal use case and therewith seems intuitively problematic. Thus, practitioners often resort to out-of-sample evaluation, using a subset from the very end of a series exclusively for evaluation, and therewith falling back to a situation where the data are not used optimally. To overcome these problems, the so-called time-series cross-validation ) extends the out-of-sample approach from a fixed origin to a rolling origin evaluation (Tashman, 2000) . Data is subsequently moved from the out-of-sample block from the end of the series to the training set. Then, the model can be used (with or without parameter re-estimation) with the newly available data. The model re-estimation can be done on sliding windows with fixed window sizes or on expanding windows that always start at the beginning of the series (Bell and Smyl, 2018) . However, these approaches extending the out-of-sample procedure make again not optimal use of the data and may not be applicable when only small amounts of data are available. Adapting the original CV procedure, to overcome problems with serial correlations, blocked CV approaches have been proposed in the literature, where the folds are chosen in blocks (Racine, 2000; Bergmeir and Benítez, 2012) and/or data around the points used for testing are omitted (Racine, 2000; Burman et al., 1994) . Finally, it has been shown that with purely autoregressive models, CV can be used without modifications, i.e., with randomly choosing the folds (Bergmeir et al., 2018) . Here, CV estimates the generalisation error accurately, as long as the model errors from the in-sample fit are uncorrelated. This especially holds when models are overfitting. Underfitting can be easily detected by checking the residuals for serial correlation, e.g., with a Ljung-Box 35 This subsection was written by Christoph Bergmeir. test (Ljung and Box, 1978) . This procedure is implemented in the forecast package in R (R Core Team, 2020), in the function CVar. Given N forecasts of the same event, forecast combination involves estimation of so called combination weights assigned to each forecast, such that the accuracy of the combined forecast generally outperforms the accuracy of the forecasts included. Early statistical approaches adopted a range of strategies to estimate combination weights including (i) minimizing in-sample forecast error variance among forecast candidates (Bates and Granger, 1969; Newbold and Granger, 1974; Min and Zellner, 1993) , (ii) formulation and estimation via ordinary least squares regression (Granger and Ramanathan, 1984; MacDonald and Marsh, 1994) , (iii) use of approaches based on Bayesian probability theory (Bunn, 1975; Bordley, 1982; Clemen and Winkler, 1986; Diebold and Pauly, 1990; Raftery, 1993) , (iv) and the use of regime switching and time varying weights recognising that weights can change over time (Diebold and Pauly, 1987; Elliott et al., 2005; Lütkepohl, 2011; Tian and Anderson, 2014) . Barrow and Kourentzes (2016) contains a very good documentation and empirical evaluation of a range of these early approaches, while De Menezes et al. (2000) and Armstrong (2001b) contain guidelines on their use. Recent statistical approaches use a variety of techniques to generate forecasts and/or derive weights. Kolassa (2011) apply so called Akaike weights based on Akaike Information Criterion (Sakamoto et al., 1986) , while bootstrapping has been used to generate and combine forecast from exponential smoothing (Cordeiro and Neves, 2009; Barrow et al., 2020; Bergmeir et al., 2016) , artificial neural networks (Barrow and Crone, 2016a,b) , and other forecast methods Hillebrand and Medeiros, 2010; Inoue and Kilian, 2008) . Barrow and Crone (2016b) developed cross-validation and aggregating (Crogging) using cross-validation to generate and average multiple forecasts, while more recently, combinations of forecasts generated from multiple temporal levels has become popular Athanasopoulos et al., 2017; Kourentzes and Athanasopoulos, 2019) . These newer approaches recognise the importance of forecast generation in terms of uncertainty reduction (Petropoulos et al., 2018a) , the creation of diverse forecasts (Brown et al., 2005a; Lemke and Gabrys, 2010) , and the pooling of forecasts Lichtendahl Jr and Winkler, 2020) . Now nearly 50 years on from the seminal work of Bates and Granger (1969) , the evidence that statistical combinations of forecasts improves forecasting accuracy is near unanimous, including evidence from competitions (Makridakis et al., 1982; Makridakis and Hibon, 2000; Makridakis et al., 2020c) , and empirical studies (Elliott et al., 2005; Andrawis et al., 2011; . Still, researchers have tried to understand why and when combinations improve forecast accuracy (Palm and Zellner, 1992; Petropoulos et al., 2018a; Timmermann, 2006) , and the popularity of the simple average (Chan and Pauwels, 2018; Smith and Wallis, 2009a; Claeskens et al., 2016) . Others have investigated properties of the distribution of the forecast error beyond accuracy considering issues such as normality, variance, and in out-of-sample performance of relevance to decision making (Makridakis and Winkler, 1989; Chan et al., 1999; Barrow and Kourentzes, 2016) . 36 This subsection was written by Devon K. Barrow. Looking forward, evidence suggests that the future lies in the combination of statistical and machine learning generated forecasts (Makridakis et al., 2020c) , and in the inclusion of human judgement (Gupta, 1994; Wang and Petropoulos, 2016; Petropoulos et al., 2018b) . Additionally, there is need to investigate such issues as decomposing combination accuracy gains, constructing prediction intervals (Koenker, 2005;  Grushka-Cockayne and Jose, 2020), and generating combined probability forecasts (Raftery et al., 1997; Ranjan and Gneiting, 2010; Hall and Mitchell, 2007; Clements and Harvey, 2011) . Finally, there is need for the results of combined forecasts to be more interpretable and suitable for decision making (Bordignon et al., 2013; Graefe et al., 2014; Barrow and Kourentzes, 2016; Todini, 2018 Tay and Wallis, 2000; Berkowitz, 2001; Guidolin and Timmermann, 2006; Shackleton et al., 2010, inter alia) . Initial work on forecast combination focused on the combination of point forecasts. This literature goes back to Bates and Granger (1969) and a summary of more recent work is provided in (Timmermann, 2006) , while the ability of such methods to improve prediction accuracy is extensively documented (see, for example, Stock and Watson, 2004; Hansen, 2008; Guidolin and Timmermann, 2009, inter alia) . In recent years attention has shifted towards evaluation, comparison and combination of density forecasts with empirical applications that are mostly encountered in the areas of macroeconomics and finance. The improved performance of combined density forecasts stems from the fact that pooling of forecasts allows to mitigate potential misspecifications of the individual densities when the true population density is nonnormal. Combining normal densities yields a flexible mixture of normals density which can accommodate heavier tails (and hence skewness and kurtosis), as well as approximate non-linear specifications (Hall and Mitchell, 2007; Jore et al., 2010) . The predictive density combination schemes vary across studies and range from simple averaging of individual density forecasts to complex approaches that allow for time-variation in the weights of prediction models, also called experts (see Aastveit et al., 2019 , for a comprehensive survey of density forecast combination methods). A popular approach is to combine density forecasts using a convex combination of experts' predictions, so called 'linear opinion pools' (see, for example, Hall and Mitchell, 2007; Kascha and Ravazzolo, 2010; Geweke and Amisano, 2011) . In order to determine the optimal combination weights this method relies on minimising Kullback-Leibler divergence of the true density from the combined density forecast. These linear approaches have been extended by Gneiting and Ranjan (2013) to allow non-linear transformations of the aggregation scheme and by Kapetanios et al. (2015) , whose 'generalised pools' allow the combination weights to depend on the (forecast) value of the variable of interest. 37 This subsection was written by Alisa Yusupova. Billio et al. (2013) developed a combination scheme that allows the weights associated with each predictive density to be time-varying, and propose a general state space representation of predictive densities and combination schemes. The constraint that the combination weights must be non-negative and sum to unity implies that linear and Gaussian state-space models cannot be used for inference and instead Sequential Monte Carlo methods (particle filters) are required. More recently, McAlinn and West (2019) developed a general Bayesian formulation for estimating time-varying linear (instead of convex) combinations of predictive densities. The method implemented in this work uses a pool of dynamic linear models to generate individual predictive densities, and then combines these through another dynamic linear model. Improved rational decisions are the final objective of modelling and forecasting. Relatively easy decisions among a number of alternatives with predefined and known outcomes become hard when they are conditioned by future unknown events. This is why one resorts to modelling and forecasting, but this is insufficient. To be successful, one must account for the future conditioning event uncertainty to be incorporated into the decision-making process using appropriate Bayesian approaches, as described by the decision theory literature (Berger, 1985; Bernardo, 1994; DeGroot, 2004) . The reason is that taking a decision purely based on model forecasts is equivalent to assuming the future event (very unlikely, as we know) to equal the forecasted value. Therefore, the estimation of the predictive probability density is the essential prerequisite to estimating the expected value of benefits (or of losses) to be compared and traded-off in the decision-making process (Draper and Krnjajić, 2013) . This highly increases the expected advantages together with the likelihood of success and the robustness of the decision (Todini, 2017 (Todini, , 2018 . In the past, the assessment of the prediction uncertainty was limited to the evaluation of the confidence limits meant to describe the quality of the forecast. This was done using continuous predictive densities, as in the case of the linear regression, or more frequently in the form of predictive ensembles. These probabilistic predictions, describing the uncertainty of the model forecasts given (knowing) the observations can be used within the historical period to assess the quality of the models (Todini, 2016) . When predicting into the future, observations are no more available and what we look for (known as predictive probability) is the probability of occurrence of the unknown "future observations" given (knowing) the model forecasts. This can be obtained via Bayesian inversion, which is the basis of several uncertainty post-processors used in economy (Diebold et al., 1998) , hydrology (Krzysztofowicz, 1999; Todini, 1999 Todini, , 2008 Schwanenberg et al., 2015) , meteorology (Granger and Pesaran, 2000; Katz and Lazo, 2011; Economou et al., 2016;  Reggiani and Boyko, 2019), etc. Accordingly, one can derive a predictive probability from a single model forecast to estimate the expected value of a decision by integrating over the entire domain of existence all the possible future outcomes and theirs effects, weighted with their appropriate probability of occurrence. When several forecasts are available to a decision maker, the problem of deciding on which of them one should rely upon becomes significant. It is generally hard to choose among several forecasts because one model could be best under certain circumstances but rather poor under others. Accordingly, to improve the available knowledge on a future unknown event, predictive densities are extended to multiple forecasts to provide decision makers with a single predictive probability, conditional upon several model's forecasts (Raftery et al., 1997; Coccia and Todini, 2011) . A number of available uncertainty post processors can cope with multi-model approaches, such as Bayesian model averaging (Raftery, 1993; Raftery et al., 1997) , model output statistics (Glahn and Lowry, 1972; Wilkd, 2005) , ensemble model output statistics (Gneiting et al., 2005) , and model conditional processor (Todini, 2008; Coccia and Todini, 2011) . Finally, important questions such as: (i) "what is the probability that an event will happen within the next x hours?" and (ii) "at which time interval it will most likely occur?" can be answered using the multitemporal approach (Krzysztofowicz, 2014; Coccia, 2011) and results of its applications were presented in Coccia (2011 ), Todini (2017 , and Barbetta et al. (2017) . Multiple experts' forecasts are collected in a wide variety of situations: medical diagnostics, weather prediction, forecasting the path of a hurricane, predicting the outcome of an election, macroeconomic forecasting, and more. One of the central findings from the forecasting literature is that there is tremendous power in combining such experts' forecasts into a single forecast. The simple average, or what Surowiecki refers to as 'the wisdom of crowds' (Surowiecki, 2005) , has been shown to be a surprisingly robust combined forecast in the case of point forecasting (Clemen and Winkler, 1986; Clemen, 1989; Armstrong, 2001a) . The average forecast is more accurate than choosing a forecast from the crowd at random and is sometimes even more accurate than nearly all individuals (Mannes et al., 2012) . The average point forecast also often outperforms more complicated point aggregation schemes, such as weighted combinations (Smith and Wallis, 2009b; Soule et al., 2020) . Mannes et al. (2012) highlight two crucial factors that influence the quality of the average point forecast: individual expertise and the crowd's diversity. Of the two: "The benefits of diversity are so strong that one can combine the judgments from individuals who differ a great deal in their individual accuracy and still gain from averaging" (Mannes et al., 2012, page 234) . Larrick and Soll (2006) define the idea of 'bracketing': In the case of averaging, two experts can either bracket the realisation (the truth) or not. When their estimates bracket, the forecast generated by taking their average performs better than choosing one of the two experts at random; when the estimates do not bracket, averaging performs equally as well as the average expert. Thus, averaging can do no worse than the average expert, and with some bracketing, it can do much better. Modern machine learning algorithms such as the random forest exploit this property by averaging forecasts from hundreds of diverse experts (here, each "expert" is a regression tree; Grushka-Cockayne et al., 2017a). Only when the crowd of forecasts being combined has a high degree of dispersion in expertise, some individuals in the crowd might stand out, and in such cases, there could be some benefits to chasing a single expert forecaster instead of relying on the entire crowd. Mannes et al. (2014) suggest that combining a small crowd can be especially powerful in practice, offering some diversity among a subset of forecasters with an minimum level of expertise. When working with probabilistic forecasting, averaging probabilities is the most widely used probability combination method Cooke (1991) ; Hora (2004) ; Clemen (2008) . Stone (1961) Although diversity benefits the average point forecast, it can negatively impact the average probability forecast. As the crowd's diversity increases, the average probability forecast becomes more spread out, or more underconfident (Dawid et al., 1995; Hora, 2004; Ranjan and Gneiting, 2010) . Averaging quantiles, instead of probabilities, can offer sharper and better calibrated forecasts (Lichtendahl et al., 2013) . Trimmed opinion pools can be applied to probability forecasts, also resulting in better calibrated forecasts (Jose et al., 2014) . The ubiquity of data and the increased sophistication of forecasting methods results in more use of probabilistic forecasts. While probabilities are more complex to elicit, evaluate, and aggregate compared to point estimates, they do contain richer information about the uncertainty of interest. The wisdom of combining probabilities, however, utilises diversity and expertise differently than combining point forecasts. When relying on a crowd, eliciting point forecasts versus eliciting probabilities can significantly influence the type of aggregation one might choose to use. Are advances in computing and data availability going to pave the way for a revolution in forecasting? Is big data going to significantly increase the forecast accuracy of macroeconomic forecasts? The last two decades have seen a proliferation of literature on forecasting using big data (Varian, 2014; Swanson and Xiong, 2018; Hassani and Silva, 2015) . But the jury is still out as to whether the promised improvements in forecast accuracy can be realised for macroeconomic phenomena. The tools used to analyse big data focus on regularisation techniques to achieve dimension reduction, see Kim and Swanson (2014 Swanson ( , 2018 for a summary of the literature. This can be achieved through selection (such as Autometrics, Doornik, 2018) , shrinkage (including ridge regression: Hoerl and Kennard, 1970; Lasso: Tibshirani, 1996; Zou, 2006; and Elastic Nets: Zou and Hastie, 2005) , variable combination (such as Principal Components Analysis and Partial Least Squares: Bai and Ng, 2008; Bai and Ng, 2009; Stock and Watson, 2012; Ding and Hwang, 1999; Forni et al., 2000; Zou et al., 2006) , and machine learning methods (including Artificial Neural Networks: Zhang et al., 1998; Medeiros et al., 2006) . Many of these methods are 'black boxes' where the algorithms are not easily interpretable, and so they are mostly used for forecasting rather than for policy analysis. Machine learning methods are seen as an efficient approach to dealing with big data sets (Athey, 2018) . Big data has been effectively used to estimate the forecast origin which, if improved, would lead to better forecasts absent any later shifts. Nowcasting can benefit from large data sets as the events have happened and the information is available; see Castle et al. (2017) for a nowcasting application. However, the benefits of big data are not as evident in a forecasting context where the future values of all added variables also need to be forecast and are as uncertain as the variable of interest. Macroeconomic time series data are highly non-stationary, with stochastic trends and structural breaks. Regularisation methods (such as cross-validation and hold-back) often assume that the data generating process does not change over time. Forecasting models that assume the data are drawn from a stationary distribution (even after differencing) do not forecast well ex ante. So while there seems to be lots of mileage 40 This subsection was written by Jennifer L. Castle. in improving forecasts using big data, as they allow for more flexible models that nest wider information sets, more general dynamics and many forms of non-linearity, the statistical problems facing 'small' data forecasting models do not disappear (Harford, 2014; Doornik and Hendry, 2015) . do not find improvements in forecasting from big data sets over small models. It is essential to keep in mind the classical statistical problems of mistaking correlation for causation, ignoring sampling biases, finding excess numbers of false positives and not handling structural breaks and non-constancies both in-and out-of-sample, in order to guard against these issues in a data abundant environment. Big data is normally accompanied by the nature that observations are indexed by timestamps, giving rise to big data time series characterised by high frequency and long-time span. Processing big data time series is obstructed by a wide variety of complications, such as significant storage requirements, algorithms' complexity and high computational cost (L'heureux et al., 2017; Wang et al., 2018a; Galicia et al., 2018; Wang et al., 2020b) . These limitations accelerate the great demand for scalable algorithms. Nowadays, increasing attention has been paid to developing data mining techniques on distributed systems for handling big data time series, including but not limited to processing (Mirko and Kantelhardt, 2013) , decomposition (Bendre and Manthalkar, 2019) , clustering (Ding et al., 2015) , classification (Triguero et al., 2015) , and forecasting (Galicia et al., 2018) . Distributed systems, initially designed for independent jobs, do not support to deal with dependencies among observations, which is a critical obstacle in time series processing (Li et al., 2014; Wang et al., 2020b) . Various databases ( library to conduct multi-step forecasting by splitting the multi-step forecasting problem into h sub-problems (h is the forecast horizon). Another strand of the literature on forecasting big data time series is to improve time-consuming estimation methods using a MapReduce framework. Sheng et al. (2013)  Time series forecasting involves use of historical data to predict values for a specific period time in future. This approach assumes that recent and historical patterns in the data will continue in the future. This assumption is overly ingenuous. However, this is not reliable in some situations. For example, (i) forecasting COVID-19 cases where, due to interventions and control measures taken by the governments and due to the change in personal behaviour, the disease transmission pattern changes rapidly, and (ii) forecasting sales of a new product: external factors such as advertisement, promotions, social learning, and imitation of other individuals change the system behaviour. In such circumstances to make reliable forecasts it is important to take into account all information that might influence the variable that is being forecast. This information includes a variety of environmentallevel and individual-level factors. An agent-based modelling is a powerful tool to explore such complex systems. Agent-based modelling approach is useful when, (ii) data availability is limited, (ii) uncertainty of various interventions in place and a rapidly changing social environment, and (iii) limited understanding of the dynamics of the variable of interest. Agent-based modelling disaggregates systems into individual level and explores the aggregate impact of individual behavioural changes on the system as a whole. In other words, the key feature of agentbased modelling is the bottom-up approach to understand how a system's complexity arises, starting with individual level. As opposed to this, the conventional time series forecasting approaches are considered top-down approaches. Agent-based models have two main components: (i) Agents, and (ii) Rules, sometimes referred as procedures and interactions. Agents are individuals with autonomous behaviour. Agents are heterogeneous. Each agent individually assesses on the basis of a set of rules. An agent-based modelling approach simulates how heterogeneous agents interact and behave to assess the role of different activities on the target variable. A general framework for Agent-based modelling involves three main stages (See Despite these limitations and challenges, agent-based modelling has been used extensively to model infectious disease transmission and forecasting (Tracy et al., 2018; Venkatramanan et al., 2018) . Agentbased modelling approaches have been widely used in early phases of the COVID-19 outbreak, to assess the impact of different interventions on disease spread and forecasts (Wallentin et al., 2020) . In a review paper, Weron (2014) states some applications of agent-based models for electricity demand forecasting. Xiao and Han (2016) use agent-based models to forecast new product diffusion. Furthermore, thinking the other way around, Hassan et al. (2013) explain how forecasting principles can be applied in agent-based modelling. A time series feature can be any statistical representation of time series characteristics. A vast majority of time series mining tasks are based on similarity quantification using their feature representations, including but not limited to time series clustering Kang et al., 2014 Kang et al., , 2015 , classification (Nanopoulos et al., 2001; Fulcher et al., 2013) , anomaly detection (Kang, 2012; Talagala et al., 2020a) , and forecasting (Kang et al., 2017; Montero-Manso et al., 2020) . The choice of features depends on the nature of the data and the application context. The state-of-the-art time series feature representation methods quantify a wide range of time series characteristics, including simple summary statistics, stationarity (Montero-Manso et al., 2020; Wang et al., 2020c) , model fits (Fulcher and Jones, 2014; Christ et al., 2018) , time series imaging (Li et al., 2020f) , and others. In the forecasting community, two lines of forecasting approaches have been developed using time series features, namely feature-based model selection and feature-based model combination. The motivation behind them is no single model always performs the best for all time series. Instead of choosing one model for all the data, features can be used to obtain the most appropriate model or the optimal combination of candidate models, per series. As early as in 1972, Reid (1972) argues that time series characteristics provide valuable information in forecast model selection, which is further echoed by Makridakis and Hibon (1979) . One way to forecast an extensive collection of time series is to select the most appropriate method per series according to its features. Pioneer studies focus on rule-based methods (for example, Arinze, 1994; Wang et al., 2009b) to recommend the "best" forecasting model per series based on its features. Another line of approaches apply regression to study how useful features are in predicting which forecasting method performs best (for example, Meade, 2000; . With the advancement of machine learning, more recent literature uses "meta-learning" to describe the process of automatically acquiring knowledge for forecast model selection. The first such study is by Prudêncio and Ludermir (2004) , who apply decision trees for forecast model selection. Lemke and Gabrys (2010) compare different meta-learning approaches to investigate which model works best in which situation. Kang et al. (2017) propose using feature spaces to visualise the strengths and weaknesses of different forecasting methods. Other algorithms such as neural networks and random forecasts are also applied to forecast model selection (Kück et al., 2016; Talagala et al., 2018) . One of the pioneering studies in feature-based forecast combination is the rule-based approach by Collopy and Armstrong (1992)  Bootstrap methodology has been widely applied in many areas of research, including time series analysis. The bootstrap procedure (Efron, 1979 ) is a very popular methodology for independent data because of its simplicity and nice properties. It is a computer-intensive method that presents solutions in situations where the traditional methods fail or are very difficult to apply. However, Efron's bootstrap (iid bootstrap) has revealed itself inefficient in the context of dependent data, such as in the case of time series, where the dependence structure arrangement has to be kept during the resampling scheme. Most of the resampling for dependent data consider segments of the data to define blocks, such that the dependence structure within each block can be kept. Different versions of blocking differ in the way as blocks are constructed: the nonoverlapping block bootstrap (Carlstein, 1990) , the moving block bootstrap (Künsch, 1989) , the circular block bootstrap (Politis and Romano, 1992) , and the stationary block bootstrap (Politis and Romano, 1994) . But, if the time series process is driven from iid innovations, another way of resampling can be used. The iid Bootstrap can then be easily extended to a dependent setup. That was the spirit of sieve bootstrap proposed by Bühlmann (1997) . This method is based on the idea of fitting parametric models first and resampling from the residuals. Such models include, for example, the linear regression (Freedman, 1981) and autoregressive time series (Efron and Tibshirani, 1986) . This approach is different from the previous bootstrap methods for dependent data; the sample bootstrap is (conditionally) stationary and does not present a structure of dependence. Another different feature is that the sieve bootstrap sample is not a subsample from the original data, as in the previous methods. Observe that even if the sieve bootstrap is based on a parametric model, it is nonparametric in its spirit. The AR model here is just used to filter the residuals series. A few years ago, the sieve bootstrap was used for estimating forecast intervals (Zagdański, 2001; Andrés et al., 2002) . Motivated by these works, Cordeiro and Neves (2006 ) developed a procedure to estimate point forecasts. The idea of these authors was to fit a model to the time series, extract the residuals and then apply the sieve bootstrap to the residuals. Further developments of this procedure include the estimation of forecast intervals (Cordeiro and Neves, 2014) and also the detection, estimation and imputation of missing data (Cordeiro and Neves, 2013) . In a recent work (Bergmeir et al., 2016) a similar approach was also consider, the residuals were extracted and resampled using moving block bootstrap. Bickel and Freedman (1981) and later in Angus (1992) showed that in extreme value theory, the bootstrap version for the maximum (or minimum) does not converge to the extremal limit laws. Zelterman (1993) pointed out that "to resample the data for approximating the distribution of the k largest observations would not work because the 'pseudo-samples' would never have values greater than X n:n " 49 . A method considering to resample a smaller size than the original sample was proposed in Hall (1990) . Recently, Neves and Cordeiro (2020) used this idea and developed a preliminary work in modelling and forecasting extremes in time series. The term bagging was proposed by Breiman (1996) to describe the generation of several versions of a predictor, via Bootstrap procedures, with a final aggregation stage. Thus, "bootstrap aggregating" was established as bagging. The main idea is to improve predictors' accuracy once the data sets, draw randomly with replacement, will approximating the original distribution. The author argues that bagging works for unstable procedures, but it was not tested for time series. Years after, Kilian and Inoue (2004) suggested 49 max(X 1 , · · · , X n ). 50 This subsection was written by Fernando Luiz Cyrino Oliveira. the first attempts for temporal dependent data. For data-driven methods, to forecasting and simulation time series and deal with predictors ensembles, bagging has shown as a powerful tool. A general framework for ensemble forecasting methods involves four main stages: (i) data treatment, (ii) resampling, (iii) forecasting, and (iv) aggregation. However, for time series, bootstrap should be done carefully, as the serial dependence and non-stationarity must be considered. This led Bergmeir et al. (2016) to propose a bagging version for exponential smoothing, the Bagged ETS. As pre-treatment, after a Box-Cox transformation, the series is decomposed into trend, seasonal, and remainder components via STL decomposition (Cleveland et al., 1990) . The resampling stage uses moving block bootstrap (MBB: Lahiri and Lahiri, 2003) , applied to the remainder. There are several discussions in the literature about this procedure, mainly regarding the size of the blocks. MBB resampling the collection of overlapping (consecutive) blocks of observations. The idea is to keep the structure still present in the remainder. The forecasts are obtained via ETS methods and, for the final aggregation, the authors adopted the median. Their method is evaluated on the M3 data set and outperformed the original benchmarks. The work of Bergmeir et al. (2016) inspired many others: Dantas et al. (2017) applied the idea for air transport demand data and de Oliveira and Cyrino Oliveira (2018) for energy consumption, proposing the so-called remainder sieve bootstrap (RSB). Dantas and Cyrino Oliveira (2018) proposed an extension to the Bagged ETS where bagging and exponential smoothing are combined with clustering methods. The approach aims to consider and reduce the covariance effects among the ensemble time series, creating clusters of similar forecasts -since it could impact the variance of the group. A variety of forecasts are selected from each cluster, producing groups with reduced variance. In light of the aforementioned, there are several possibilities for each stage of the mentioned framework. Finally, Meira et al. (2020) proposed "treating and pruning" strategies to improve the performance of prediction intervals for both model selection and forecast combinations. Testing over a large set of real time series from the M forecasting competitions, their results highlighted the importance of analysing the prediction intervals of the ensemble series before the final aggregation. Neural Networks (NNs) or Artificial Neural Networks (ANNs) are mathematical formulations inspired by the work and functioning of biological neurons. They are characterized by their ability to model nonstationary, nonlinear and high complex datasets. This property along with the increased computational power have put NNs in the frontline of research in most fields of science (De Gooijer and Hyndman, 2006; Zhang et al., 1998) . A typical NN topology is consisted by three types of layers (input, hidden and output) and each layer is consisted by nodes. The first layer in every NN, is the input layer and the number of its nodes corresponds to the number of explanatory variables (inputs). The last layer is the output layer and the number of nodes corresponds to the number of response variables (forecasts). Between the input and the output layer, there is one or more hidden layers where the nodes define the amount of complexity the model is capable of fitting. Most NN topologies in the input and the first hidden layer contain an extra node, called the bias node. The bias node has a fixed value of one and serves a function similar to the intercept in traditional regression models. Each node in one layer has connections (weights) with all or a subset (for example, for the convolutional neural network topology) of the nodes of the next layer. NNs process the information as follows: the input nodes contain the explanatory variables. These variables are weighted by the connections between the input and the first hidden nodes, and the information reaches to the hidden nodes as a weighted sum of the inputs. In the hidden nodes, there is usually a nonlinear function (such as the sigmoid or the RelU) which transform the information received. This process is repeated until the information reaches the output layer as forecasts. NNs are trained by adjusting the weights that connect the nodes in a way that the network maps the input value of the training data to the corresponding output value. This mapping is based on a loss function, the choice of which depends on the nature of the forecasting problem. The most common NN procedure, is the back-propagation of errors (Shapiro, 2000) . The simpler and most common NN topology, is the Multilayer Forward Perceptron (MLP). In MLP, the hidden nodes contain the sigmoid function and the information moves in forward direction (from the inputs to the output nodes). An another NN topology where the information moves also only in a forward direction is the Radial Basis Function NN (RBF). Now the hidden neurons compute the Euclidean distance of the test case from the neuron's centre point and then applies the Gaussian function to this distance using the spread values. Recurrent Neural Networks (RNNs) are NN topologies that allow previous outputs to be used as inputs while having hidden states. The information moves both forwards and backwards. RNNs have short-term memory and inputs are taken potentially from all previous values. MLPs, RBFs and RNNs are universal function approximators (Hornik, 1991; Schäfer and Zimmermann, 2006; Park and Sandberg, 1991) . However, the amount of NN complexity in terms of hidden layers and nodes to reach this property, might make the NN topology computationally unfeasible to train. Neural networks can be equipped to provide not only a single-valued forecast, but rather the entire range of values possible in a number of ways. We will discuss three selected approaches in the following, but remark that this is a subjective selection and is by far not comprehensive. 53 1. Analogously to linear regression and Generalised Linear Models, obtaining probabilistic forecasts can be achieved by the neural network outputting not the forecasted value itself but rather parameters of probability distribution or density (Bishop, 2006) . In forecasting, a prominent example is the DeepAR model (Salinas et al., 2019b) , which uses a recurrent neural network architecture and assumes the probability distribution to be from a standard probability density function (e.g., negative binomial or Student's t). Variations are possible, with either non-standard output distributions in forecasting such as the multinomial distribution (Rabanser et al., 2020) or via representing the probability density as cumulative distribution function (Salinas et al., 2019a) or the quantile function . 2. An alternative approach is to apply concepts for quantile regression (Koenker, 2005) to neural networks, e.g., by making the neural network produce values for selected quantiles directly (Wen et al., 2017) . 3. It is possible to combine neural networks with existing probabilistic models. For example, neural networks can parametrise state space models (Durbin and Koopman, 2012) as an example for another class of approaches (Rangapuram et al., 2018) , dynamic factor models (Geweke, 1977) with neural networks (Wang et al., 2019b) or deep temporal point processes (Turkmen et al., 2019) . The appeals of using neural networks for point forecasts carry over to probabilistic forecasts, so we will only comment on the elegance of modern neural network programming frameworks. To the forecasting model builder, the availability of auto gradient computation, the integration with highly-tuned optimisation algorithms and scalability considerations built into the frameworks, means that the time from model idea to experimental evaluation has never been shorter. In the examples above, we brushed over the need to have loss functions with which we estimate the parameters of the neural networks. Standard negative log-likelihood based approaches are easily expressible in code as are approaches based on non-standard losses such as the continuous ranked probability score . With open source proliferating in the deep learning community, most of the above examples for obtaining probabilistic forecasts can readily be test-driven (see, for example, Alexandrov et al., 2019) . For the future, we see a number of open challenges. Most of the approaches mentioned above are univariate, in the following sense. If we are interested in forecasting values for all time series in a panel, we may be interested in modelling the relationship among these time series. The aforementioned approaches mostly assume independence of the time series. In recent years, a number of multivariate probabilistic forecasting models have been proposed (Salinas et al., 2019a; Rangapuram et al., 2020) , but much work remains to obtain a better understanding. Another counter-intuitive challenge for neural networks is to scale them down. Neural networks are highly parametrised, so in order to estimate parameters correctly, panels with lots of time series are needed. However, a large part of the forecasting problem landscape (Januschowski and Kolassa, 2019) consists of forecasting problems with only a few time series. Obtaining good uncertainty estimates with neural networks in these settings is an open problem. Categorising forecasting methods into statistical and machine learning (ML) is not trivial, as various criteria can be considered for performing this task . Nevertheless, more often than not, forecasting methods are categorised as ML when then they do not prescribe the data-generating process, e.g., through a set of equations, thus allowing for data relationships to be automatically learned (Barker, 2020) . In this respect, methods that build on unstructured, non-linear regression algorithms, such as Neural Networks (NN), Decision Trees, Support Vector Machines, and Gaussian Processes, are considered as ML (Makridakis et al., 2018) . 54 This subsection was written by Evangelos Spiliotis. Since ML methods are data-driven, they are more generic and easier to be adapted to forecast series of different characteristics (Spiliotis et al., 2020b) . However, ML methods also display some limitations. First, in order for ML methods to take full advantage of their capacity, sufficient data are required. Thus, when series are relatively short and display complex patterns, such as seasonality and trend, ML methods are expected to provide sub-optimal forecasts if the data are not properly pre-processed (Zhang et al., 1998; Makridakis et al., 2018) . On the other hand, when dealing with long, high-frequency series, typically found in energy (Chae et al., 2016) , stock market (Moghaddam et al., 2016) , and demand (Carmo and Rodrigues, 2004 ) related applications, ML methods can be applied with success. Second, computational intensity may become relevant (Makridakis et al., 2020c) , especially when forecasting numerous series at the weekly and daily frequency (Seaman, 2018) or long-term accuracy improvements over traditional methods are insignificant (Nikolopoulos and Petropoulos, 2018) . Third, given that the effective implementation of ML methods strongly depends on optimally determining the values of several hyper-parameters, related both with the forecasting method itself and the training process, considerable complexity is introduced, significant resources are required to set up the methods, and high experience and a strong background in other fields than forecasting, such as programming and optimisation, are needed. In order to deal with these limitations, ML methods can be applied in a cross-learning (CL) fashion instead of a series-by-series one, i.e., allow the methods to learn from multiple series how to accurately forecast the individual ones (Makridakis et al., 2020c) . The key principle behind CL is that, although series may differ, common patterns may occur among them, especially when data are structured in a hierarchical way and additional information, such as categorical attributes and exogenous/explanatory variables, is provided as input (Fry and Brundage, 2020) . The CL approach has several advantages. First, computational time can be significantly reduced as a single model can be used to forecast multiple series simultaneously. Second, methods trained in a particular dataset can be effectively used to provide forecasts for series of different datasets that display similar characteristics (transfer-learning), thus allowing the development of generalised forecasting methods (Oreshkin et al., 2020a) . Third, data limitations are mitigated and valuable information can be exploited at global level, thus allowing for patterns shared among the series, such as seasonal cycles (Dekker et al., 2004) and special events (Huber and Stuckenschmidt, 2020) , to be effectively captured. Based on the above, CL is currently considered the most effective way of applying ML for batch time series forecasting. Some state-of-the-art implementations of CL include long short-term memory NNs (Smyl, 2020) , deep NNs based on backward and forward residual links (Oreshkin et al., 2020b) , featurebased XGBoost (Montero-Manso et al., 2020), and gradient boosted decision trees (Bojer and Meldgaard, 2020 ). With the advent of big data, machine learning now plays a leading role in forecasting. 56 There are two primary reasons for this. First, conventional ordinary least squares (OLS) estimation is highly susceptible to overfitting in the presence of a large number of regressors (or features). OLS maximises the fit of the model over the estimation (or training) sample, which can lead to poor out-of-sample performance; in essence, OLS over-responds to noise in the data, and the problem becomes magnified as the number of features grows. A class of machine-learning techniques, which includes the popular least absolute shrinkage and selection operator (LASSO, Tibshirani, 1996) and elastic net (ENet, Zou and Hastie, 2005) , employs penalised regression to improve out-of-sample performance with large numbers of features. The LASSO and ENet guard against overfitting by shrinking the parameter estimates toward zero. Very noisy data -data with a very low signal-to-noise ratio -exacerbate the overfitting problem. In such an environment, it is vital to induce adequate shrinkage to guard against overfitting and more reliably uncover the predictive signal amidst all the noise. For LASSO and ENet estimation, a promising strategy is to employ a stringent information criterion, such as the Bayesian information criterion (BIC, Schwarz, 1978) , to select (or tune) the regularisation parameter governing the degree of shrinkage (often denoted by λ). Wang et al. (2009a) and Fan and Tang (2013) A second reason for the popularity of machine learning in the era of big data is the existence of powerful tools for accommodating complex predictive relationships. In many contexts, a linear specification appears overly restrictive, as it may neglect important nonlinearities in the data that can potentially be exploited to improve forecasting performance. Neural networks (NNs) are perhaps the most popular machine-learning device for modelling nonlinear predictive relationships with a large number of features. Under a reasonable set of assumptions, a sufficiently complex NN can approximate any smooth function (for example, Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989; Barron, 1994) . By design, NNs are extremely flexible, and this flexibility means that a large number of parameters (or weights) need to be estimated, which again raises concerns about overfitting, especially with very noisy data. The weights of a NN are typically estimated via a stochastic gradient descent (SGD) algorithm, such as Adam (Kingma and Ba, 2015) . The SGD algorithm itself has some regularising properties, which can be strengthened by adjusting the algorithm's hyperparameters. We can further guard against overfitting by imposing a dropout rate (Hinton et al., 2012; Srivastava et al., 2014) for the weights. Perhaps the quintessential example of a noisy data environment is forecasting asset returns, especially at short horizons (e.g., monthly). Because many asset markets are reasonably efficient, most of the fluctuations in returns are inherently unpredictable -they reflect the arrival of new information, which, by definition, is unpredictable. This does not mean that we should not bother trying to forecast returns, as even a seemingly small degree of return predictability can be economically significant (for example, Campbell and Thompson, 2008) . Instead, it means that we need to be particularly mindful of overfitting when forecasting returns in the era of big data. The robustness of the forecasting process depends mainly on the characteristics of the target variable. In cases of high nonlinear and volatile time series, a forecasting model may not be able to fully capture 57 This subsection was written by Ioannis Panapakidis. and simulate the special characteristics, a fact that may lead to poor forecasting accuracy (Pradeepkumar and Ravi, 2017) . Contemporary research has proposed some approaches to increase the forecasting performance (Sardinha-Lourenço et al., 2018) . Clustering-based forecasting refers to the application of unsupervised machine learning in forecasting tasks. The scope is to increase the performance by employing the information of data structure and of the existing similarities among the data entries (Goia et al., 2010) . Clustering is a proven method in pattern recognition and data science for deriving the level of similarity of data points within a set. The outputs of a clustering algorithm are the centroids of the clusters and the cluster labels, i.e., integer numbers that denote the number of cluster that a specific data entry belongs to (Xu and Wunsch, 2005) . There are two approaches in clustering-based forecasting: (i) Combination of clustering and supervised machine learning, and (ii) solely application of clustering. In the first case, a clustering algorithm is used to split the training set into smaller sub-training sets. These sets contains patterns with high similarity. Then for each cluster a dedicated forecaster is applied (Chaouch, 2014; Fan et al., 2008) . Thus, the number of forecasting algorithms is equal to the number of clusters. This approach enables to train forecasters with more similar patterns and eventually achieve better training process. The forecasting systems that involve clustering are reported to result in lower errors (Fan et al., 2006; Mori and Yuihara, 2001) . In the second case, a clustering algorithm is used to both cluster the load data set and perform the forecasting (López et al., 2012) . In the sole clustering applications, either the centroids of the clusters can be utilised or the labels. Pattern sequence-based forecasting is an approach that employs the cluster labels. In this approach, a clustering of all days prior to the test day is held and this results in sequences of labels of a certain length. Next, the similarity of the predicted day sequence with the historical data sequences is examined. The load curve of the predicted day is the average of the curves of the days following the same sequences (Martinez Alvarez et al., 2011) . There is variety of clustering algorithms that have been proposed in forecasting such as the k-means, fuzzy C-means (FCM), self-organising map (SOM), deterministic annealing (DA), ant colony clustering ACC, and others. Apart from the clustering effectiveness, a selection criterion for an algorithm is the complexity. k-means and FCM are less complex compared to the SOM that needs a large number of variables to be calibrated prior to its application. Meta-heuristics algorithms, such as ACC, strongly depend on the initialisation conditions and the swarm size. Therefore, a comparison of clustering algorithms should take place to define the most suitable one for the problem under study (Mori and Yuihara, 2001; Li et al., 2008; Elangasinghe et al., 2014; Wang et al., 2015a) . The assessment of clustering-based forecasting is held via common evaluation metrics for forecasting tasks. The optimal number of clusters, which is a crucial parameter of a clustering application, is selected via trial-and-error, i.e., the optimal number corresponds to the lowest forecasting error (Nagi et al., 2011) . Hybrid approaches combine two or more of the above-mentioned advanced methods. In general, when methods based on AI-based techniques, physical, and statistical approaches are combined together, the result is often improved forecasting accuracy as a benefit from the inherent integration of the single methods. The idea is to mix diverse methods with unique features to address the limitations of individual techniques, 58 This subsection was written by Sonia Leva. thus enhancing the forecast performance (Nespoli et al., 2019; Mandal et al., 2012) . The performance of the hybrid methods depends on the performance of the single methods, and these single methods should be specifically selected for the problem that has to be addressed. Hybrid methods can be categorised based on the constituent methods, but also considering that these base methods may not necessarily act only on the forecasting stage but also on data treatment and parameters identification stages. In data pre-processing combined approaches, different methods can be used for decomposing the time series into subseries (Son et al., 2019) or the signal into different frequencies (Zang et al., 2018) , and for classifying the historical data (Huang et al., 2015a ). An advantage of such hybrid methods is robustness against sudden changes in the values of the main parameters. However, they require additional knowledge and understanding of the base methods, and have the disadvantage of slow response time to new data. The purpose of the parameter selection stage is to optimise the parameters of the model, in terms of extracting nonlinear features and invariant structures (Behera et al., 2018; Ogliari et al., 2018) but also in terms of estimation of the parameter adopted for the prediction; for example, meteorological factors such as temperature, humidity, precipitation, snowfall, cloud, sunshine, wind speed, and wind direction (Qu et al., 2016) . Hybrid methods feature straightforward determination of the parameters with relatively basic structures. However, the implementation is sometimes challenging, and depends on the knowledge and expertise of the designer. Finally, the data post-processing hybrid approaches forecast the residual errors resulted from the forecasting model. Since these hybrid methods consider residual errors from the model, they aim in further improving the predictions of the base methods by applying corrections in the forecasts. However, a disadvantage of these hybrid methods is the increased calculation time, as the residual errors must also be estimated. Also, such hybrid methods are not general and will depend on the field of application. In many cases, hybrids approaches outperform other (single) approaches such as kNN, NN, and ARIMAbased models (Mellit et al., 2020) . A great example is the hybrid method by (Smyl, 2020) , which achieved the best performance in the M4 forecasting competition. In particular, in energy applications, a combination of physical and AI-based techniques can lead to improved forecasting performance. Furthermore, machine learning methods based on historical data of meteorological variables combined with an optimal learning algorithm and weather classification can further improve the forecasting accuracy of single methods. However, in general, the weak point of such hybrid approaches is that they underperform when meteorological conditions are unstable (Chicco et al., 2015) . Demand forecasting is the basis for most planning and control activities in any organisation. Demand will typically be accumulated in some pre-defined 'time buckets' (periods), such as a day, a week or a month. On many occasions, demand may be observed in every time period, resulting in what is sometimes referred to as 'non-intermittent demand'. Alternatively, demand may appear sporadically, with no demand at all in some periods, leading to an intermittent appearance of demand occurrences. Intermittent 59 This subsection was written by Aris A. Syntetos. demand items monopolise the stock bases in the after sales industry and are prevalent in many other industries, including the automotive, IT, and electronics sectors. Their inventory implications are dramatic and forecasting their requirements is a very challenging task. Methods to forecast intermittent demand may broadly be classified as parametric and non-parametric. The former suppose that future demand can be well represented by a statistical distribution (say Poisson or Negative Binomial) which has parameters that are unknown but may be forecasted using past data. These methods are discussed in this sub-section. In the latter, the data are not assumed to follow any standard probability distribution. Instead, direct methods are used to assess the distributions required for inventory management. Such methods are discussed in the following sub-section. Simple Exponential Smoothing (SES) is often used in practice to forecast intermittent demand series. However, SES fails to recognise that intermittent demand is built from two constituent elements: (i) the inter-demand intervals, which relate to the probability of demand occurring, and (ii) the demand sizes, when demand occurs. The former indicates the degree of intermittence, whereas the latter relates to the behaviour of the positive demands. Croston (1972) showed that this inherent limitation leads to SES being (positively) biased after a demand occurring period; this is sometimes referred to as an 'issue point' bias. Subsequently, he proposed a method that forecasts separately the sizes of demand, when demand occurs, and the inter-demand intervals. Both forecasts are produced using SES, and the ratio of the former over the latter gives a forecast of the mean demand per period. Croston's method was shown by Syntetos and Boylan (2001) to suffer from another type of bias (inversion bias) and the same researchers proposed a modification to his method that leads to approximately unbiased estimates. This method is known in the literature as the Syntetos-Boylan Approximation (SBA). It has been found repeatedly to account for considerable empirical inventory forecasting improvements (Eaves and Kingsman, 2004; Gutierrez et al., 2008; van Wingerden et al., 2014; Nikolopoulos et al., 2016) and, at the time of writing, it constitutes the benchmark against which other (new) proposed methodologies in the area of intermittent demand forecasting are assessed. Croston's method is based upon the assumption of a Bernoulli demand arrival process. Alternatively, demand may be assumed to arrive according to a Poisson process. It is also possible to adapt Croston's method so that sizes and intervals are updated based on a simple moving average (SMA) procedure instead of SES. Boylan and Syntetos (2003) , Shale et al. (2006) , and Syntetos et al. (2015a) presented correction factors to overcome the bias associated with Croston's approach under a Poisson demand arrival process and/or estimation of demand sizes and intervals using an SMA. For a detailed review of developments in intermittent demand forecasting interested readers are referred to Boylan and Syntetos (2021) . In contrast to parametric forecasting, where a demand distribution is assumed and point forecasts of the unknown distribution parameters are estimated, in the non-parametric forecasting approach, the demand is not assumed to follow any standard probability distribution. This approach leads directly to an estimation of the empirical distribution function based on the past data. Two main non-parametric forecasting approaches have dominated the intermittent demand literature: the bootstrapping approach and the Overlapping/Non-Overlapping aggregation Blocks approach (Boylan and Syntetos, 2021) . The bootstrapping approach relies upon a resampling (with or without replacement) of the historical demand data to build the empirical distribution of the demand over a specified interval. This approach was initially introduced by Efron (1979) . Since then, it has been developed by Willemain et al. (2004) and Zhou and Viswanathan (2011) to deal with intermittent demand items (Babai et al., 2020) . Willemain et al. (2004) have proposed a method that resamples demand data by using a Markov chain to switch between no demand and demand periods. The empirical outperformance of this method has been shown when Porras and Dekker (2008) were the first to consider aggregation with overlapping and non-overlapping blocks (OB and NOB) approach in forecasting the demand of spare parts. In the NOB approach, a demand series is divided into consecutive non-overlapping blocks of time, whereas in OB, at each period the oldest observation is dropped and the newest is included (Rostami-Tabar et al., 2013) . Boylan and Babai (2016) have compared the statistical and inventory performance of the OB and NOB methods. They found that, unless the demand history is short, there is a clear advantage of using OB instead of NOB. More recently, based on extreme value theory (EVT), Zhu et al. (2017) have proposed an improvement of the OB method that models better the tail of lead-time demand. They have shown that the empirical-EVT method leads to higher achieved target cycle service levels when compared to the original method proposed by Porras and Dekker (2008) . In many application areas, forecasts are required across a wide collection of products, services or locations. In this situation, it is convenient to introduce classification rules that allow subsets of time series to be forecasted using the same approaches and methods. Categorisation rules, such as the ABC inventory classification, serve the forecasting function only coincidentally. They do not necessarily align to the selection of the best forecasting method. Bartezzaghi et al. (1999) identified five factors that contribute towards intermittence and 'lumpiness' (intermittence with highly variable demand sizes): number of potential customers, frequency of customers' requests, heterogeneity of customers, variety of individual customer's requests, and correlations between customers' requests. These may contribute towards useful classifications, for example by the number of customers for an item. When this number is low and some of the customers are large, then direct 61 This subsection was written by John E. Boylan. communication with the customers can inform judgemental forecasts. Similarly, if a customer's requests are highly variable, then 'advance demand information' from customers can help to improve judgemental estimates. These strategies can be very useful in a business-to-business environment, where such strategies are feasible. Within certain modelling frameworks, classification of time series is well established. For example, within an ARIMA framework (Box et al., 2008) , or within a state-space framework for exponential smoothing (Hyndman et al., 2002) , series may be classified, for example based on the AIC (Akaike, 1973) . It is more challenging to classify series according to their recommended forecasting method if some of the candidate methods, such as Croston's method, lack a fully satisfactory model base. In the field of intermittent demand forecasting, proposed the SBC classification scheme, enabling time series to be classified according to their length of average demand interval and coefficient of variation of demand sizes (when demand occurs). These rules were based on assumptions of independent and identically distributed (iid) demand, and a comparison of expected mean square error between methods. The scheme has been extended by Kostenko and Hyndman (2006) and by Petropoulos and Kourentzes (2015) . In an empirical case-study, Boylan et al. (2008) examined series not necessarily conforming to iid assumptions and found the rules to be robust to inexact specification of cut-off values. Moon et al. (2013) used logistic regression to classify time series of demand for spare parts in the South Korean Navy. The classification was designed to identify superior performance (accuracy and inventory costs) of direct and hierarchical forecasting methods, based on the serial correlation of demands, the coefficient of variation of demand volume of spare parts, and the functionality of the naval equipment. An alternative approach to classification is combination of forecasts. Petropoulos and Kourentzes (2015) investigated combining standard forecasting methods for intermittent demand (e.g., SES, Croston, Syntetos-Boylan Approximation). They did not find this to improve accuracy directly, but obtained good results from the use of combinations of forecasts at different temporal frequencies, using methods selected from the extended SBC classification scheme. The "classical" Boolean logic is not able to handle for uncertainties and/or vagueness that are necessary when dealing with many real world problems. This is in part due to the fact that the Boolean logic is based on only two values (i.e., a statement can only be true or false). Fuzzy logic tries to overcome this issue by admitting that a statement/variable could be partially true or partially false. Mathematically, the fuzzy logic framework is based on the work of Zadeh (1965) who introduced the theory of fuzzy sets. The main point of this theory is the definition of two kinds of sets: 1. Crisp sets are the "classical" sets in the Boolean logic. An element can belong (or not) to a certain set. 2. Fuzzy sets, where an element can belong to the sets with a certain membership grade, with a value that varies in the interval [0, 1]. The definition of the fuzzy sets allows the framework to take into account the uncertainty and vagueness of information. An extension of this approach is related to the fact that a certain variable can assume 62 This subsection was written by Claudio Carnevale. a crisp value (classical theory) or can belong to different fuzzy sets with different membership grade. For example, in a system implemented to forecast the daily pollutant concentration in atmosphere, one of the inputs could relate to the weather conditions, such as the wind speed. In the classical approach, the system must have as an input the value of the wind speed at a certain day. In the fuzzy approach, the input of the system could be the membership grade of the input variable to three different fuzzy sets: (i) "Not windy", (ii) "average windy", and (iii) "strong windy". On the other hand, the user of the forecasting system may be only interested in a classification of the output variable instead of the crisp value. In this case, the fuzzy approach is applied to the pollutant concentration which could belong with a certain degree to the fuzzy sets (i) "not polluted day", (ii) "medium polluted day", (iii) "high polluted day", and (iv) "critical polluted day". In fuzzy theory, each fuzzy set is characterised by a (generally nonlinear) function, called the membership function, linking crisp values to the membership of the different sets. The association of a crisp value to its membership for a set is called fuzzyfication, while the inverse operation (from a membership value to a crisp value) is called defuzzification. As with the logic theory, the inference system assumes a key role in the fuzzy theory. A Fuzzy Inference System (FIS) allows the interpretation of the membership grades of the input variable(s) and, given some sets of fuzzy rules, assigns the corresponding values to the output variable(s). In the literature, two main fuzzy inference systems are presented: 1. Mamdani system (Mamdani and Assilian, 1975) , where both the input and output of the inference system are membership functions. 2. Sugeno system (Sugeno, 1985) , where the output of the inference system is a crisp value, usually obtained by applying a linear function to the defuzzified value of the input. Association rule mining is an exploratory data-driven approach which is able to automatically and exhaustively extract all existing correlations in a data set of categorical features. It is a powerful but computationally intensive technique, successfully applied in different forecasting contexts (Acquaviva et al., 2015; Di Corso et al., 2018) . Its results are in a human-readable form. The data set must be in the form of transactions, i.e., a collection of events, each described by categorical features. If the phenomena under analysis are modelled by continuous-valued variables, discretisation can be applied to obtain a suitable data set. Association rule mining core task is the frequent itemset extraction, which consists in finding frequentlyoccurring relationships among items in a data set (Han et al., 2011) . Given a data set of records characterised by several attributes, an item refers to a pair of (attribute=value), while a set of items is called itemset. The support count of an itemset is the number of records r containing that itemset. The support of an itemset is the percentage of records containing it with respect to the total number of records in the data set. An itemset is frequent when its support is greater than or equal to a minimum support threshold. An association rule is an implication in the form A → B, where A and B are disjoint itemsets (i.e., (Tan et al., 2005) . A is called rule body or antecedent and B rule head or consequent. To evaluate the quality of an association rule, the support, confidence, and lift metrics are commonly exploited (Han et al., 2011) . Rule support is the fraction of records containing both A and B, indicating the probability that a record contains every item in these itemsets. The support of the rule is computed as the support of the union of A and B. Rule confidence represents the strength of the implication, and is the conditional probability that a transaction containing A also contains B, P (B|A), i.e., the proportion of records that contain A with respect to those that also contain B. Finally, the lift of a rule measures the correlation between antecedent and consequent. It is defined as the ratio between the rule A → B confidence and the support of B. A lift ratio equal to 1.0 implies that itemsets A and B are not correlated. A lift higher than 1.0 indicates a positive correlation, meaning that the occurrence of A likely leads to the occurrence of B with the given confidence. The greater the lift, the stronger the association. Finally, a lift lower than 1.0 indicates a negative correlation between A and B. The problem of association rule mining consists in the extraction of all the association rules having rule support and confidence greater than the respective support and confidence thresholds, MinConf and MinSup, defined as parameters of the mining process (Tan et al., 2005) . These thresholds allow to control the statistical relevance of the extracted rules. The process of rule mining consists of two steps. The first step is the computation of frequent itemsets, i.e., itemsets with support greater or equal to MinSup. The second step is the extraction of association rules from frequent itemsets. Let be F a frequent itemset, hence having a support higher than MinSup, pairs A and B = F − A are derived so that the confidence of A → B is higher than MinConf . The first step of the process is the most computationally expensive. Thus, several algorithms have been proposed to solve the problem of frequent itemset extraction (Zaki, 2000) , some specifically addressing high-dimensionality issues (Apiletti et al., 2017 . Despite being computationally demanding, association rule mining is an exhaustive approach, i.e., all and only statistically relevant correlations are extracted. Text data, such as social media posts, scholar articles and company reports, often contains valuable information that can be used as predictors in forecasting models (Aggarwal and Zhai, 2012) . Before extracting useful features from the data, a text document needs to be cleaned and normalised for further processing. The first step of preparing the data is to filter out stop words -the words that do not add much meaning to a sentence, e.g., "a", "is", and "me". For grammatical reasons, it is necessary for documents to use different forms of words. Stemming and lemmatisation can be applied to reduce inflectional forms or relate different forms of a word to a common base form. Stemming often chops off the end of a word, while lemmatisation uses vocabularies and morphological analysis to return the base form (or the lemma) of a word (Lovins, JB, 1968; Manning et al., 2008) . For example, the word "industries" will be turned into "industri" or "industry" if stemming or lemmatisation is applied. To model and analyse text data, we need to transform it into numerical representations so the forecasting models can process them as predictors. One way of transforming the data is through sentiment analysis. Sentiment analysis is often applied to detect polarity within customer materials such as reviews and social media posts (Das and Chen, 2007; Archak et al., 2011 ). An easy way to obtain the sentiment score of a word is to look it up in a happiness dictionary (for example, the hedonometer dictionary, Hedonometer, 2020). Another common way of representing the sentiment is to use a vector of numeric values 64 This subsection was written by Xiaojia Guo. that denote the word's positivity, negativity and neutrality based on existing lexical databases such as the WordNet (Godbole et al., 2007; Baccianella et al., 2010) . Once the sentiment of each word is calculated, we can apply an aggregation algorithm (e.g., simple average) to measure the sentiment of an entire sentence or paragraph. In scholar articles and company reports, context features might be more important than sentiments. The bag-of-words model and word embeddings are often applied to generate numeric representations of such text. A bag-of-words model simply returns a matrix that describes the occurrence of words within a document (Goldberg, 2017) . When we use this matrix as input to a forecasting model, each word count can be considered as a feature. The Word2Vec method is a widely used embedding method that is built based on the context of a word. Specifically, it trains a two-layer neural network that takes each word as an input to predict its surrounding words. The weights from the input layer to the hidden layer are then utilised as the numerical representation for the input word (Le and Mikolov, 2014) . Once the text is turned into numeric representations, they can be used as predictors in any forecasting models. The most challenging part in this process is to find the right technique to extract features from the text. In terms of software implementation, the Natural Language Toolkit (NLTK) and SpaCy library in Python can be applied to remove stop words and stem or lemmatise text (Loper and Bird, 2002; Honnibal, 2015) . The bag-of-words technique is also available in NLTK. A particular implementation of the Word2Vec model is available on Google code (2013). Moreover, a public data set of movie reviews that is commonly studied in literature is available from the Stanford NLP Group (2013). (Pennings and van Dalen, 2017; Oliveira and Ramos, 2019; Villegas and Pedregal, 2018) . The forecasts of hierarchical time series produced independently of the hierarchical structure generally will not add up according to the aggregation constrains of the hierarchy, i.e., they are not coherent. Therefore, hierarchical forecasting methods that generate coherent forecasts should be considered to allow appropriate decision-making at the different levels. Actually, by taking advantage of the relationships between the series across all levels these methods have shown to improve forecast accuracy (Athanasopoulos et al., 2009; Shang and Hyndman, 2017; Yagli et al., 2019) . The most common approaches to hierarchical forecasting are bottom-up and top-down. In the bottomup approach forecasts for each time series at the bottom-level are first produced and then these are added up to obtain forecasts for all other series at the hierarchy (Dunn et al., 1976) . Since forecasts are obtained at the bottom-level no information is lost due to aggregation. In the top-down approach forecasts for the top-level series are first generated and then these are disaggregated generally using historical proportions to obtain forecasts for the bottom-level series, which are then aggregated (Gross and Sohl, 1990). Hyndman et al. (2011) showed that this approach introduces bias to the forecasts. Recent research on hierarchical forecasting tackles the problem using a two-stage approach. Forecasts for the series at all levels of the hierarchy are first obtained independently without considering any aggregation constrains (we refer to these as base forecasts). Then, base forecasts are adjusted so that they become coherent (we refer to these as reconciled forecasts). This adjustment is achieved by a matrix that maps the base forecasts into new bottom-level forecasts which are then added up . Wickramasuriya et al. (2019) found the optimal solution for this matrix, which minimises the trace of the covariance matrix of the reconciled forecast errors (hence MinT reconciliation). This optimal solution is based on the covariance matrix of the base forecast errors which incorporates the correlation structure of the hierarchy. Wickramasuriya et al. (2019) presented several alternative estimators for this covariance matrix: (i) proportional to the identity which is optimal only when base forecast errors are uncorrelated and equivariant (referred to as OLS), (ii) proportional to the sample covariance estimator of the in-sample one-step-ahead base forecast errors with off-diagonal elements null accounts for the differences in scale between the levels of the hierarchy (referred to as WLS), (iii) proportional to the previous estimator unrestricted also accounts for the relationships between the series (referred to as MinT-Sample), and (iv) proportional to a shrinkage estimator based on the two previous estimators, parameterising the shrinkage in terms of variances and correlations, accounts for the correlation across levels (referred as MinT-Shrink). More recently these techniques were extended to probabilistic forecasting . When base forecasts are probabilistic forecasts characterised by elliptical distributions, Panagiotelis et al. (2020) showed that reconciled probabilistic forecasts also elliptical can be obtained analytically. When it is not reasonable to assume elliptical distributions, a non-parametric approach based on bootstrapping in-sample errors can be used. Temporal aggregation is the transformation of a time series from one frequency to another of lower frequency. As an example, a time series of length n that is originally sampled at a monthly frequency can be transformed to a quarterly series of length n/3 by using equally-sized time buckets of three periods each. It is usually applied in an non-overlapping manner, but overlapping aggregation can also be considered. The latter is preferred in the case when the original series is short, but has the disadvantage of applying lower weights on the few first and last observations of the series and introducing autocorrelations (Boylan and Babai, 2016) . Temporal aggregation is appealing as it allows to investigate the original series through different lenses. By changing the original frequency of the data, the apparent series characteristics also change. In the case of slow-moving series, temporal aggregation leads to decrease of intermittence . In the case of fast-moving series, higher levels of aggregation (i.e., lower frequencies) allow for better modelling of trend patterns, while lower aggregation levels (i.e., higher frequencies) are more suitable for capturing seasonal patterns (Spithourakis et al., 2014; Kourentzes et al., 2014) . Research has found evidence of improved forecasting performance with temporal aggregation for both slow and fast (Spithourakis et al., 2011) moving time series. This led to characterise temporal aggregation as a "self-improving mechanism". The good performance of temporal aggregation was reconfirmed by Babai et al. (2012) , who focused on its utility performance rather than the forecast 66 This subsection was written by Fotios Petropoulos. error. However, one challenge with single levels of aggregation is the choice of a suitable aggregation level for each series . Instead of focusing on a single aggregation level, Andrawis et al. (2011) , Kourentzes et al. (2014) , Petropoulos and Kourentzes (2014) , and Petropoulos and Kourentzes (2015) suggested the use of multiple levels of aggregation, usually abbreviated as MTA (multiple temporal aggregation). This not only tackles the need to select a single aggregation level, but also partly addresses the issue of model uncertainty, instead of relying on model selection and parametrisation at a single aggregation level. Using this property, Kourentzes et al. (2017) showed that MTA will typically lead to more accurate forecasts, even if in theory suboptimal. Different frequencies allow for better identification of different series patterns, so it is intuitive to consider multiple temporal levels and benefit from the subsequent forecast combination across frequencies. Kourentzes and Petropoulos (2016) showed how multiple temporal aggregation can be extended to incorporate exogenous variables. However, forecasting at a single level of aggregation can still result in better performance when the seasonal pattern is strong Spiliotis et al. (2019b Spiliotis et al. ( , 2020c . Athanasopoulos et al. (2017) expressed multiple temporal aggregation within the hierarchical forecasting framework using the term "temporal hierarchies". Temporal hierarchies allow for the application of established hierarchical reconciliation approaches directly to the temporal dimension. Jeon et al. (2019) show how temporal hierarchies can be used to obtain reconciled probabilistic forecasts, while Spiliotis et al. (2019b) explored empirical bias-adjustment strategies and a strategy to avoid excessive seasonal shrinkage. Nystrup et al. (2020) proposed estimators for temporal hierarchies suitable to account for autocorrelation in the data. Finally, Kourentzes and Athanasopoulos (2020) applied temporal hierarchies on intermittent data, and showed that higher aggregation levels may offer structural information which can improve the quality of the forecasts. In the last two subsections (2.9.1 and 2.9.2), we saw two complimentary hierarchical structures, crosssectional and temporal. Although the machinery behind both approaches is similar, often relying on the hierarchical framework by Hyndman et al. (2011) and Athanasopoulos et al. (2009) , and work that followed from these (particularly Wickramasuriya et al., 2019) , they address different forecasting problems. Crosssectional hierarchies change the unit of analysis but are fixed in the period of analysis. For example, a manufacturer may operate using a hierarchy across products. The different nodes in the hierarchy will correspond to different products, product groups, super-groups, and so on, but will all refer to the same period, for example, a specific week. Temporal hierarchies do the opposite, where the unit of analysis is fixed, but the period is not. For example, we may look at the sales of a specific Stock Keeping Unit (SKU) at a daily, weekly, monthly, quarterly and annual levels. However, one can argue that having annual forecasts at the SKU level may not be useful. Similarly, having aggregate sales across an organisation at a weekly frequency is also of little value. In connecting these to organisational decisions, we can observe that there is only a minority of problems that either cross-sectional or temporal hierarchies are natural, as typically decisions can differ across both the unit and the period (planning horizon) of analysis. In the latter case, both hierarchical approaches 67 This subsection was written by Nikolaos Kourentzes. are more akin to statistical devices that can improve forecast accuracy through the use of forecast combinations, rather than satisfy the motivating argument behind hierarchical forecasting that is to provide coherent predictions for decisions at different levels of the hierarchy. Cross-temporal hierarchies attempt to overcome this limitation, providing coherent forecasts across all units and periods of analysis, and therefore a common outlook for the future across decision-makers at different functions and levels within an organisation. The literature remains sparse on how to construct cross-temporal forecasts, as the size of the hierarchy can easily become problematic. Kourentzes and Athanasopoulos (2019) propose a heuristic approach to overcome the ensuing estimation issues. The approach works by compartmentalising the estimation. First, they obtain estimates of the cross-sectional reconciliation weights for each temporal level of the hierarchy. Then, these are combined across temporal levels, to a unique set that satisfies all coherency constraints. Using these combined weights, they obtain the reconciled bottom level forecasts, which can be aggregated as needed. Although they recognise that their approach can result in suboptimal results in terms of reconciliation errors, it guarantees coherent forecasts. Cross-temporal forecasts are more accurate than either temporal or cross-sectional hierarchical forecasts and provide a holistic view of the future across all planning levels and demarcations. Spiliotis et al. (2020c) also identify the problem, however, they do not focus on the coherency of forecasts and propose a sequential reconciliation across the two dimensions. This is shown to again be beneficial, but it does not achieve coherency. Arguably one can adapt the iterative correction algorithm by Kourentzes and Athanasopoulos (2020) to enforce coherency in this approach as well. Ecological inference forecasting (EIF) aims to predict the inner-cells values of a set of contingency tables when only the margins are known. It defines a fundamental problem in disciplines such as political science, sociology and epidemiology (Salway and Wakefield, 2004) . Cleave et al. (1995) , Greiner (2007) and Pavia et al. (2009) describe other areas of application. The fundamental difficulty of EIF lies in the fact that this is a problem with more unknowns than observations, giving rise to concerns over identifiability and indeterminacy: many sets of substantively different internal cell counts are consistent with a given marginal table. To overcome this issue, a similarity hypothesis (and, sometimes, the use of covariates) is routinely assumed. The basic hypothesis considers that either conditional row (underlying) probabilities or fractions are similar (related) among contingency tables (Greiner and Quinn, 2010) . The covariations among row and column margins of the different tables are then used to learn about the internal cells. The above hypothesis is not a cure-all to the main drawback of this approach. EIF is exposed to the socalled ecological fallacy (Robinson, 1950) : the presence of inconsistencies in correlations and association measures across different levels of aggregation. This is closely related to the well-known Simpson's Paradox (Simpson, 1951) . In this setting, the ecological fallacy is manifested through aggregation bias (Wakefield, 2004) due to contextual effects and/or spatial autocorrelation (Achen and Phillips Shively, 1995) . This has led many authors to disqualify ecological inference forecasts (see, for example, Freedman et al., 1998; Tam Cho, 1998; Anselin and Tam Cho, 2002; Herron and Shotts, 2004 ) and many others to study under which circumstances ecological inference predictions would be reliable (Firebaugh, 1978; Gelman et al., 2001; Forcina and Pellegrino, 2019) . Despite the criticisms, many algorithms for solving the EIF problem can be found in the literature, mainly from the ecological regression and mathematical programming frameworks (some of them available in functions of the R statistical software). The ecological regression literature has been prolific since the seminal papers of Goodman (1953 Goodman ( , 1959 and Duncan and Davis (1953) and is undergoing a renaissance after King (1997) : new methods easily generalisable from 2 × 2 tables to R × C tables have been proposed (King et al., 1999; Rosen et al., 2001) , the geographical dimension of the data is being explicitly considered (Calvo and Escolar, 2003; Puig and Ginebra, 2015) , and new procedures combining aggregated and individual level data, including exit polls, are introduced (Glynn and Wakefield, 2010; Greiner and Quinn, 2010; Klima et al., 2019) . See King et al. (2004) for a wide survey and Klima et al. (2016) and Plescia and De Sio (2018) for an extensive evaluation of procedures. In mathematical programming exact and inequality constraints for the inner-cell values are incorporated in a natural way. Hence, this approach has shown itself to be a proper framework for generating ecological inference forecasts. The proposals from this approach can be traced back to Hawkes (1969) and Irwin and Meeter (1969) . After them, some key references include McCarthy and Ryan (1977), Tziafetas (1986) , Corominas et al. (2015) , and Romero et al. (2020) . Solutions based on other strategies, for instance, entropy maximization, have been also suggested (see, for example, Johnston and Pattie, 2000;  Bernardini Papalia and Fernandez Vazquez, 2020). People may use judgment alone to make forecasts or they may use it in combination with statistical methods. Here the focus is on pure judgmental forecasting. Different types of judgment heuristic (mental 'rules of thumb') can be used to make forecasts. The heuristic used depends on the nature of the information available to the forecaster (Harvey, 2007) . Consider cases where the only relevant information is held in the forecaster's memory. For example, someone might be asked whether Manchester United or Burnley will win next week's match. Here one memory-based heuristic that might be applicable is the recognition heuristic: if one recognises one object but not the other, then one should infer that the recognised object has higher value (Goldstein and Gigerenzer, 2002 ). In the above example, most people who recognise just one of the teams would be likely to make a correct forecast that Manchester United will win (Ayton et al., 2011) . The availability heuristic is another memory-based heuristic that may be applicable: objects that are most easily brought to mind are those which are more likely. Thus, if we are asked which team is likely to come top of the premier league, we would say Manchester United if that is the one that most easily comes to mind. The availability heuristic is often effective because more likely events are encountered more often and more recently and are hence better remembered. However, it can be disrupted by, for example, greater media coverage of more unlikely (and hence more interesting) events. Tversky (1973) suggested that people use the representativeness heuristic to deal with this type of situation. Forecasters first select a variable that they think is able to represent the one that must be predicted. For example, a teacher may consider that frequency in attending voluntary revision classes represents a student's ability in the final examination. Thus, if a student attended 15 of the 20 revision classes, they are likely to obtain 75% in the final examination. Finally, consider situations in which people forecast future values of a variable on the basis of a record of previous values of that variable. There is some evidence that, when forecasting from time series, people use anchor-and-adjustment heuristics (Hogarth and Makridakis, 1981; Lawrence and O'Connor, 1992) . For example, (i) when forecasting from an upward trended series, they anchor on the last data point and then make an upward adjustment to take the trend into account and (ii) when forecasting from an untrended series containing autocorrelation, they anchor on the last data point and make an adjustment towards the mean to take the autocorrelation into account. Kahneman (2011) and others have divided cognitive processes into those which are intuitive (System 1) and those which are deliberative (System 2). We have discussed only intuitive processes underlying judgmental forecasting (Gigerenzer, 2007) . However, they can be supplemented by deliberative (System 2) processes (Theocharis and Harvey, 2019) in some circumstances. Judgmental adjustments to algorithmic computer-based forecasts can enhance accuracy by incorporating important extra information into forecasts (McNees, 1990; Fahimnia et al., 2020; Perera et al., 2019) . However, this information is sometimes used inefficiently (Fildes et al., 2019a) . Adjustments are also often made for other reasons and these can be detrimental to accuracy (Fildes et al., 2009; Franses and Legerstee, 2009a) . People may 'egocentrically discount' a computer's forecasts when its rationale is less clear than their own reasoning (Bonaccio and Dalal, 2006) . They can also be less tolerant of errors made by algorithms than those made by humans (Önkal et al., 2009; Dietvorst et al., 2015) . The random errors associated with algorithmic forecasts, and the salience of rare large errors, can therefore lead to an unjustified loss of trust in computer forecasts. Adjustments may also give forecasters a sense of ownership of forecasts or be used to justify their role (Önkal and Gönül, 2005) . Computer-based forecasts are designed to filter randomness from time-series. In contrast, humans tend to perceive non-existent systematic patterns in random movements (O'Connor et al., 1993; Reimers and Harvey, 2011) and apply adjustments to reflect them. This can be exacerbated by the narrative fallacy (Taleb, 2008) , where people invent stories to explain these random movements, and hindsight bias (Fischhoff, 2007) , where they believe, in retrospect, that these movements were predictable. Recent random movements, and events, are particularly likely to attract undue attention, so long-run patterns identified by the computer are given insufficient weight (Bolger and Harvey, 1993) . Damaging interventions are also probable when they result from political interference (Oliva and Watson, 2009) How can interventions be limited to occasions when they are likely to improve accuracy? Requiring people to document reasons justifying adjustments can reduce gratuitous interventions (Goodwin, 2000b) . Explaining the rationale underlying statistical forecasts also improved adjustment behaviour when series had a simple underlying pattern in a study by Goodwin and Fildes (1999) . However, providing guidance on when to adjust was ineffective in an experiment conducted by (Goodwin et al., 2011) , as was a restriction preventing people from making small adjustments. When determining the size of adjustments required, decomposing the judgment into a set of easier tasks improved accuracy in a study by Webby et al. (2005) . Providing a database of past outcomes that occurred in circumstances analogous to those expected in the forecast period also improved adjustments in a study by Lee et al. (2007) . Outcome feedback, where the forecaster is informed of the most recent outcome is unlikely to be useful since it contains noise and exacerbates the tendency to over-focus on recent events (Goodwin and Fildes, 1999; Petropoulos et al., 2016) . However, feedback on biases in adjustments over several recent periods may improve judgments . Feedback will be less useful where interventions are rare so there is insufficient data to assess performance. Forecasters -practitioners and researchers alike -use Forecast Support Systems (FSS) in order to perform their forecasting tasks. Usually, such an FSS allows the forecaster to load their historical data and they can then apply many different types of forecasting techniques to the selected data. The idea is that the forecaster selects the method which leads to highest forecast accuracy. Yet, there is no universally 'best' method, as it depends on the data that is being forecasted. Thus, selection is important in achieving high accuracy. But how does this selection occur? Research on judgmental selection of statistical forecasts is limited in quantity. Lawrence et al. (2002) found that participants were not very adept at selecting good forecasting algorithms from a range offered to them by an FSS and had higher error than those who were presented with the optimal algorithm by an FSS. Petropoulos et al. (2018b) compared judgmental selection of forecasting algorithms with automatic selection based on predetermined information criteria. They found that judgmental selection was better than automatic selection at avoiding the 'worst' models, but that automatic selection was superior at choosing the 'best' ones. In the end, overall accuracy of judgmental selection was better than that of algorithmic selection. If their experiment had included more variation of the data (trends, fractals, different autoregressive factors) and variation of proposed models, this could possibly have led to better algorithmic than judgmental performance (Harvey, 2019) . Time series that are more complex will place a higher cognitive load on judgmental selection. This was confirmed in a study by Han et al. (2019) , who used an electroencephalogram (EEG) for the comparison of judgmental forecast selection versus (judgmental) pattern identification. They found that pattern identification outperformed forecast selection, as the latter required a higher cognitive load, which in turn led to a lower forecasting accuracy. It is likely that, in practice, judgmental selection is much more common than automatic selection. This preference for human judgment over advice from an algorithm has been shown in an experiment bÿ Onkal et al. (2009) Gigerenzer (2014) . The performance of the participants improved with a larger difference in quality between models and a lower level of noise in the data series. In a second experiment, De Baets and Harvey (2020) found that participants adjusted more towards the advice of what they perceived to be a good quality model than a medium or bad quality one. Importantly, in selecting an algorithm and seeing it err, people are quick to abandon it. This phenomenon is known as 'algorithm aversion' (Dietvorst et al., 2015) and is due to a 'perfection schema' we have in our heads where algorithms are concerned (Madhavan and Wiegmann, 2007) . We do not expect them to 'fail' and thus react strongly when they do. While a model may not perform as it should for a particular dataset and may thus elicit algorithm aversion for that particular method, one should not abandon it for all datasets and future forecasts. Panels of experts are often used in practice to produce judgmental forecasts. This is especially true in cases with limited available quantitative data and with the level of uncertainty being very high. In this section, three methods for eliciting judgmental forecasts from panels of experts are presented: the Delphi method, interaction groups (IG), and structured analogies (SA). The Delphi method is centred around organising and structuring group communication , which aims to achieve a convergence of opinion on a specific real-world issue. It is a multipleround survey in which experts participate anonymously to provide their forecasts and feedback (Rowe and Wright, 2001) . At the end of each round, the facilitator collects and prepares statistical summaries of the panel of experts' forecasts. These summaries are presented as feedback to the group, and may be used towards revising their forecasts. This loop continues until a consensus is reached, or the experts in the panel are not willing to revise their forecasts further. In some implementations of the Delphi method, justification of extreme positions (forecasts) is also part of the (anonymous) feedback process. The Delphi method results in a more accurate outcome in the decision-making process (Dalkey, 1969; Steurer, 2011) . Rowe and Wright (2001) mentioned that, by adopting the Delphi method, groups of individuals can produce more accurate forecasts than simply using unstructured methods. A drawback of the Delphi method is the additional cost associated with the need to run multiple rounds, extending the forecasting process as well as increasing the potential drop-out rates. On the other hand, the anonymity in the Delphi method eliminates issues such as groupthink and the 'dominant personalities' effects ( Van de Ven and Delbeco, 1971 ). The IG method suggests that the members of the panel of experts actively interact and debate their points to the extent they have to reach an agreement on a common forecast (Litsiou et al., 2019) . Sniezek and Henry (1989) found that members of interacting groups provide more accurate judgments compared to individuals. However, there is mixed evidence about the forecasting potential of IG (Scott Armstrong, 2006; Boje and Murnighan, 1982; Graefe and Armstrong, 2011) . Besides, the need for arranging and facilitating meetings for the IG makes it a less attractive option. 72 This subsection was written by Konstantia Litsiou. Another popular approach to judgmental forecasting using panels of experts is SA, which refers to the recollection of past experiences and the use analogies (Green and Armstrong, 2007) . In the SA method, the facilitator assembles a panel of experts. The experts are asked to recall and provide descriptions, forecasts, and similarities/differences for cases analogous to the target situation, as well as a similarity ranking for each of these analogous cases. The facilitator gathers the lists of the analogies provided by the experts, and prepares summaries, usually using weighted averages of the recalled cases based on their similarity to the target situation. Semi-structured analogies (sSA) have also been proposed in the literature, where the experts are asked to provide a final forecasts based on the analogous cases they recalled, which essentially reduces the load for the facilitator . Nikolopoulos et al. (2015) supported that the use of SA and IG could result to forecasts that are 50% more accurate compared to unstructured methods (such as unaided judgment). One common caveat of using panels of experts is the difficulty to identify who a real expert is. Engaging experts with high level of experience, and encouraging the interaction of experts are also supported by Armstrong and Green (2018) . Scenarios provide exhilarating narratives about conceivable futures that are likely to occur. Through such depictions they broaden the perspectives of decision makers and act as mental stimulants to think about alternatives. Scenarios enhance information sharing and provide capable tools for communication within organisations. By virtue of these merits, they have been widely used in corporate planning and strategy setting since 1960's (Godet, 1982; Schoemaker, 1991; Goodwin, 1999, 2009; Goodwin and Wright, 2010) . Even though utilisation of scenarios as decision advice to judgmental forecasting has been proposed earlier (Schnaars and Topol, 1987; Bunn and Salo, 1993) , The prospects of receiving a single scenario versus multiple scenarios were further explored in Goodwin et al. (2019b) . The researchers investigated whether assimilation or contrast effects will occur when decision makers see optimistic (pessimistic) forecasts followed by pessimistic (optimistic) ones compared against receiving a single scenario in solitude. In case of assimilation, a scenario presenting an opposing world view with the initial one would cause adjustments in the opposite direction creating an offset effect. On the other hand, in case of contrast, the forecasts generated after the initial scenarios would be adjusted to more extremes when an opposing scenario is seen. In two experiments conducted in different contexts the researchers found resilient evidence for contrast effects taking place. Interestingly, seeing an opposing scenario also increased the confidence of the forecasters in their initial predictions. In terms of the effects of scenario presence on the forecasting performance, however, the experimental evidence indicates the benefits are only circumstantial. Goodwin et al. (2019a) found that providing scenarios worsened forecast accuracy and shifted the resultant production order decisions further away from optimality. Despite this performance controversy, the decision makers express their fondness in receiving scenarios and belief in their merits (Önkal et al., 2013; Goodwin et al., 2019b) . Therefore, we need more tailored research on scenarios and judgmental forecasting to reveal the conditions when scenarios can provide significant improvements to the forecasting accuracy. Defined as "firm belief in the reliability, truth, and ability of someone/something" (Oxford English Dictionary), trust entails accepting vulnerability and risk (Rousseau et al., 1998) . Given that forecasts are altered or even discarded when distrusted by users, examining trust is a central theme for both forecasting theory and practice. Studies examining individual's trust in model versus expert forecasts show that individuals often distrust algorithms (Meehl, 2013; Burton et al., 2020) and place higher trust on human advice (Diab et al., 2011; Eastwood et al., 2012) . We live in an era where we are bombarded with news about how algorithms get it wrong, ranging from COVID-19 forecasts affecting lockdown decisions to algorithmic grade predictions affecting university admissions. Individuals appear to prefer forecasts from humans over those from statistical algorithms even when those forecasts are identical (Önkal et al., 2009) . Furthermore, they lose trust in algorithms quicker when they see forecast errors (Dietvorst et al., 2015; Prahl and Van Swol, 2017 ). Such 'algorithm aversion' and error intolerance is reduced when users have opportunity to adjust the forecasting outcome, irrespective of the extent of modification allowed (Dietvorst et al., 2018) . Feedback appears to improve trust, with individuals placing higher trust in algorithms if they can understand them (Seong and Bisantz, 2008) . Overuse of technical language may reduce understanding of the forecast/advice, in turn affecting perceptions of expertise and trustworthiness (Joiner et al., 2002) . Explanations can be helpful (Goodwin et al., 2013b) , with their packaging affecting judgments of trustworthiness (Elsbach and Elofson, 2000) . Algorithmic appreciation appears to easily fade with forecasting expertise (Logg et al., 2019) , emphasising the importance of debiasing against overconfidence and anchoring on one's own predictions. Trusting experts also presents challenges (Hendriks et al., 2015; Hertzum, 2014; Maister et al., 2012) . Expert forecasts are typically seen as predisposed to group-based preconceptions (Brennan, 2020; Vermue 74 This subsection was written by DilekÖnkal. et al., 2018), along with contextual and motivational biases (Burgman, 2016) . Misinformed expectations, distorted exposures to 'forecast failures', and over-reliance on one's own judgments may all contribute to distrusting experts as well as algorithms. Credibility of forecast source is an important determinant in gaining trust (Önkal et al., 2019) . Studies show that the perceived credibility of system forecasts affects expert forecasters' behaviours and trust (Alvarado-Valencia and Barrero, 2014), while providing information on limitations of such algorithmic forecasts may reduce biases (Alvarado-Valencia et al., 2017) . Previous experience with the source appears to be key to assessing credibility (Hertzum, 2002) and trust (Cross and Sproull, 2004) . Such 'experienced' credibility appears to be more influential on users' acceptance of given forecasts as opposed to 'presumed' credibility (Önkal et al., 2017) . Source credibility can be revised when forecast (in)accuracy is encountered repetitively (Jiang et al., 1996) , with forecaster and user confidence playing key roles (Sah et al., 2013) . Trust is critical for forecasting efforts to be translated into sound decisions (Choi et al., 2020; Özer et al., 2011) . Further work on fostering trust in individual/collaborative forecasting will benefit from how trusted experts and models are selected and combined to enhance decision-making. Point forecasts are single number forecasts for an unknown future quantity also given by a single number. Interval forecasts take the form of two point forecasts, an upper and a lower limit. Finally, a less common type of forecast would be a predictive Highest Density Region (pHDR), i.e., an HDR (Hyndman, 1996) for the conditional density of the future observable. pHDRs would be interesting for multimodal (possibly implicit) predictive densities, e.g., in scenario planning. Once we have observed the corresponding realisation, we can evaluate our point, interval and pHDR forecasts. There are many common point forecast error measures (PFEMs), e.g., the mean squared error (MSE), mean absolute error (MAE), mean absolute scaled error (MASE), mean absolute percentage error (MAPE) or many others (see section 3.4 in Hyndman and Athanasopoulos, 2018) . Which one is most appropriate for our situation, or should we even use multiple different PFEMs? Let us take a step back. Assume we have a full density forecast and wish to "condense" it to a point forecast that will minimise some PFEM in expectation. The key observation is that different PFEMs will be minimised by different point forecasts derived from the same density forecast (Kolassa, 2020b) . • The MSE is minimised by the expectation. • The MAE and MASE are minimised by the median (Hanley et al., 2001) . • The MAPE is minimised by the (−1)-median (Gneiting, 2011a, p. 752 with β = −1). • The hinge/tick/pinball loss is minimised by the appropriate quantile (Gneiting, 2011b) . • In general, there is no loss function that is minimised by the mode (Heinrich, 2014) . Our forecasting algorithm may not output an explicit density forecast. It is nevertheless imperative to think about which functional of the implicit density we want to elicit (Gneiting, 2011a) , and tailor our error measure -and forecasting algorithm! -to it. It usually makes no sense to evaluate a point forecast with multiple PFEMs (Kolassa, 2020b) . 75 This subsection was written by Stephan Kolassa. Interval forecasts can be specified in multiple ways. We can start with a probability coverage and require two appropriate quantiles -e.g., we could require a 2.5% and a 97.5% quantile forecast, yielding a symmetric or equal-tailed 95% interval forecast. Interval forecasts of this form can be evaluated by the interval score (Winkler, 1972; Brehmer and Gneiting, 2020) , a proper scoring rule (section 6.2 in . We can also use the hinge loss to evaluate the quantile forecasts separately. Alternatively, we can require a shortest interval subject to a specified coverage. This interval is not elicitable relative to practically relevant classes of distributions (Brehmer and Gneiting, 2020 ). Yet another possibility is to maximise the interval forecast's probability coverage, subject to a maximum length 2 . This modal interval is elicitable by an appropriate -zero-one-loss (Brehmer and Gneiting, 2020) . The pHDR is not elicitable even for unimodal densities (Brehmer and Gneiting, 2020) . In the multimodal case, the analysis is likely difficult. Nevertheless, a variation of the Winkler score has been proposed to evaluate pHDRs on an ad hoc basis (Hyndman, 2020) . One could also compare the achieved to the nominal coverage, e.g., using a binomial test -which disregards the volume of the pHDR (Kolassa, 2020a) . In conclusion, here is a bewildering array of PFEMs, which require more thought in choosing among than is obvious at first glance. The difficulties involved in evaluating interval and pHDR forecasts motivate a stronger emphasis on full density forecasts (cf. Askanazi et al., 2018) . Probabilistic forecasting is a term that is not strictly defined, but usually refers to everything beyond point forecasting (Gneiting, 2011a) . However, in this section we consider only the evaluation of full predictive distributions or equivalent characterisations. For the evaluation of prediction of quantiles, intervals and related objects, see section 2.11.1. One crucial point for evaluating probabilistic forecasts is the reporting, which is highly influenced from meteorologic communities. From the theoretical point of view, we should always report the predicted cumulative distribution function F of our prediction target F. Alternatively for continuous data, reporting the probability density function is a popular choice. For univariate prediction problems a common alternative is to report quantile forecast on a dense grid of probabilities, as it approximates the full distribution . For multivariate forecasts, it seems to become standard to report a large ensemble (a set of simulated trajectories/paths) of the full predictive distribution. The reason is that the reporting of a multivariate distribution (or an equivalent characterisation) of sophisticated prediction models is often not feasible or practicable, especially for non-parametric or copula-based forecasting methods. In general, suitable tools for forecasting evaluation are proper scoring rules as they address calibration and sharpness simultaneously Gneiting and Katzfuss, 2014) . Preferably, we consider strictly proper scoring rules which can identify the true predicted distribution among a set of forecast candidates that contains the true model. In the univariate case the theory is pretty much settled and there is quite some consensus about the evaluation of probabilistic forecasts (Gneiting and Katzfuss, 2014) . The continuous ranked probability score (CRPS) and logarithmic scores (log-score) are popular strictly proper scoring rules, while the quadratic and pseudospherical score remain strictly proper alternatives. The CRPS can be well approximated by averaging across quantile forecasts on an equidistant grid of probabilities (Nowotarski and Weron, 2018) . 76 This subsection was written by Florian Ziel. For multivariate forecast evaluation the situation is more complicated and many questions remain open . The multivariate version of the log-score is a strictly proper scoring rule, but it requires the availability of a multivariate density forecast. This makes it impracticable for many applications. discuss the energy score, a multivariate generalisation of the CRPS, that is strictly proper. Still, it took the energy score more than a decade to increase its popularity in forecasting. A potential reason is the limited simulation study of Pinson and Tastu (2013) that concludes that the energy score can not discriminate well differences in the dependency structure. In consequence other scoring rules were proposed in literature, e.g., the variogram score (Scheuerer and Hamill, 2015) which is not strictly proper. Ziel and Berk (2019) consider a strictly proper scoring method for continuous variables using copula techniques. In contrast to Pinson and Tastu (2013) , recent studies (Ziel and Berk, 2019; Lerch et al., 2020) show that the energy score discriminates well when used together with significance tests like the Diebold-Mariano (DM) test. In general, we recommended scoring be applied with reliability evaluation (see section 2.11.3) and significance tests (see section 2.11.4). Additionally, if we want to learn about the performance of our forecasts it is highly recommended to consider multiple scoring rules and evaluate on lower-dimensional subspaces. For multivariate problems, this holds particularly for the evaluation of univariate and bivariate marginal distributions. A probabilistic forecast takes the form of a predictive distribution over future quantities or events of interest. Probabilistic forecasts are central in risk-based decision making where reliability, or calibration, is a necessary condition for the optimal use and value of the forecast. A probabilistic forecast is calibrated if the observation cannot be distinguished from a random draw from the predictive distribution or, in the case of ensemble forecasts, if the observation and the ensemble members look like random draws from the same distribution. In the univariate setting, several alternative notions of calibration exist for both a single forecast Tsyplakov, 2013 ) and a group of forecasts (Strähl and Ziegel, 2017) . The notion most commonly used in applications is probabilistic calibration (Dawid, 1984) ; the forecast system is probabilistically calibrated if the probability integral transform (PIT) of a random observation, that is, the value of the predictive cumulative distribution function in the observation, is uniformly distributed. If the predictive distribution has a discrete component, a randomised version of the PIT should be used (Gneiting and Ranjan, 2013) . Probabilistic calibration is assessed visually by plotting the histogram of the PIT values over a test set. A calibrated forecast system will return a uniform histogram, a ∩-shape indicates overdispersion and a ∪shape indicates underdispersion, while a systematic bias results in a biased histogram (e.g. Thorarinsdottir and Schuhen, 2018) . The discrete equivalent of the PIT histogram, which applies to ensemble forecasts, is the verification rank histogram (Anderson, 1996; Hamill and Colucci, 1997) . It shows the distribution of the ranks of the observations within the corresponding ensembles and has the same interpretation as the PIT histogram. 77 This subsection was written by Thordis Thorarinsdottir. For small test sets, the bin number of a PIT/rank histogram must be chosen with care. With very few bins, the plot may obscure miscalibration while with many bins, even perfectly calibrated forecasts can yield non-uniformly appearing histograms (Thorarinsdottir and Schuhen, 2018; Heinrich, 2020) . The bin number should be chosen based on the size of the test set, with the bin number increasing linearly with the size of the test set (Heinrich, 2020) . More specifically, the uniformity of PIT/rank values can be assessed with statistical tests (Delle Monache et al., 2006; Taillardat et al., 2016; Wilks, 2019) , where the test statistics can be interpreted as a distance between the observed and a flat histogram (Wilks, 2019; Heinrich, 2020) . Testing predictive performance is further discussed in section 2.11.4. Calibration assessment of multivariate forecasts is complicated by the lack of a unique ordering in higher dimensions and the many ways in which the forecasts can be miscalibrated (Wilks, 2019) . Gneiting et al. (2008) propose a general two-step approach where an ensemble forecast and the corresponding observation are first mapped to a single value by a pre-rank function. Subsequently, the pre-rank function values are ranked in a standard manner. The challenge here is to find a pre-rank function that yields informative and discriminative ranking (Wilks, 2004; Gneiting et al., 2008; Thorarinsdottir et al., 2016) , see Thorarinsdottir et al. (2016) and Wilks (2019) for comparative studies. Alternatively, Ziegel and Gneiting (2014) propose a direct multivariate extension of the univariate setting based on copulas. In addition to calibration, forecasts should also be sharp, or specific, to ensure their utility in decision making (e.g. . This is discussed in sections 2.11.1 and 2.11.2. The rich domain of forecasting has created numerous methods for generating competing forecasts. A natural consequence of such was the development of statistical tests for predictive ability in the last thirty years. These tests provided forecasters some formal reassurance that the predictive superiority of a leading forecast is statistically significant and is not merely due to random chance. One of the early papers that undoubtedly sparked growth in this field was Diebold and Mariano (1995, DM hereafter) . In their seminal paper, DM provided a simple yet general approach for testing equal predictive ability, i.e., if two forecasting sources are equally accurate on average. Their population-level predictive ability test has very few assumptions (e.g., covariance stationary loss differential) and is applicable to a wide range of loss functions, multi-period settings, and wide class of forecast errors (e.g., non-Gaussian, serially and/or contemporaneously correlated). This test though not originally intended for models has been widely used by others to test forecasting models' accuracy (Diebold, 2015) . Modifications were later introduced by Harvey et al. (1998) to improve small sample properties of the test. Generalisations and extensions have emerged to address issues that DM tests encountered in practice such as nested models (Clark and McCracken, 2001, 2009) , parameter estimation error (West, 1996) , cointegrated variables (Corradi et al., 2001) , high persistence (Rossi, 2005) , and panel data (Timmermann and Zhu, 2019) . Finite-sample predictive ability tests also emerged from the observation that models may have equal predictive ability in finite samples, which generated a class called conditional predictive accuracy tests (Giacomini and White, 2006; Clark and McCracken, 2013 ). An alternative approach to comparing forecast accuracy is through the notion of forecast encompassing, which examines if a forecast encompasses all useful information from another with respect to predictions 78 This subsection was written by Victor Richmond R. Jose. (Chong and Hendry, 1986; Harvey et al., 1998; Clark and McCracken, 2001) . Though it has a few more assumptions, forecast encompassing tests in certain contexts might be preferable to the mean square prediction error testsà la Diebold-Mariano (Busetti and Marcucci, 2013) . Another stream of statistical tests that are available looks at multiple forecasts simultaneously instead of pairs. Addressing a need for a reality check on "data snooping", White (2000) later modified by Hansen (2005) developed a multiple model test that uses a null hypothesis of "superior predictive ability" instead of the equal predictive ability used in DM tests. These have also been generalised to deal with issues such as cointegrated variables (Corradi et al., 2001) and multi-horizon forecasts (Quaedvlieg, 2019) . Recently, Li et al. (2020e) proposed a conditional superior predictive ability test similar to Giacomini and White (2006) 's innovation to the DM test. A different approach for studying performance of multiple forecasting models is through the use of multiple comparison tests such as multiple comparison with a control and multiple comparison with the best (Hsu, 1981; Edwards and Hsu, 1983; Horrace and Schmidt, 2000) . These tests often are based on jointly estimated confidence intervals that measure the difference between two parameters of interest such as the forecast accuracies of a model and a benchmark. Koning et al. (2005) illustrates how they can be ex post used to analyse forecasting performance in the M3 forecasting competition (Makridakis and Hibon, 2000) using model ranking instead of forecast accuracy scores as its primitives. The multiple comparison of the best was used in the analysis of the subsequent M4 and M5 Competitions (Makridakis et al., 2020c,d) . The theory of forecasting appears mature today, based on dedicated developments at the interface among a number of disciplines, e.g., mathematics and statistics, computer sciences, psychology, etc. A wealth of these theoretical developments have originated from specific needs and challenges in different application areas, e.g., in economics, meteorology and climate sciences, as well as management science among others. In this section, many aspects of the theory of forecasting were covered, with aspects related to data, modelling and reasoning, forecast verification. Now, the fact that forecasting is mature does not mean that all has been done -we aim here at giving a few pointers at current and future challenges. First of all, it is of utmost importance to remember that forecasting is a process that involves both quantitative aspects (based on data and models) and humans, at various levels, i.e., from the generation of forecasts to their use in decision-making. A first consequence is that we always need to find, depending on the problem at hand, an optimal trade-off between data-driven approaches and the use of expert judgement. In parallel, forecasting is to be thought of in a probabilistic framework in a systematic manner (Gneiting and Katzfuss, 2014) . This allows us to naturally convey uncertainty about the future, while providing the right basis to make optimal decisions in view of the characteristics of the decision problem, as well as the loss (or utility) function and risk aversion of the decision maker. Another consequence is that using forecasts as input to decision-making often affects the outcome to be predicted itself -a problem known as self-negating forecasts (possibly also self-fulfilling) or the prophet dilemma. With advances in the science of dynamic systems and game theory, we should invest in modelling those systems as a whole (i.e., forecasting and decision-making) in order to predict the full range of possible outcomes, based on the decisions that could be made. 79 This subsection was written by Pierre Pinson. In parallel, it is clear that today, the amount of data being collected and possibly available for forecasting is growing at an astounding pace. This requires re-thinking our approaches to forecasting towards high-dimensional models, online learning, etc. Importantly, the data being collected is distributed in terms of ownership. And, due to privacy concerns and competitive interests, some may not be ready to share their data. Novel frameworks to learning and forecasting ought to be developed with that context in mind, for instance focusing on distributed and privacy-preserving learning -an example among many others is that of Google pushing forward federated learning (Abadi et al., 2016) , an approach to deep learning where the learning process is distributed and with a privacy layer. Eventually the access and use of data, as well as the contribution to distributed learning (and collaborative analytics, more generally), may be monetised, bringing a mechanism design component to the future theory of forecasting. A simple and pragmatic example is that of forecast reconciliation: if asking various agents to modify their forecasts to make them coherent within a hierarchy, such modifications could be monetised to compensate for accuracy loss. A large part of today's modelling and forecasting approaches uses a wealth of data to identify and fit models, to be eventually used to forecast based on new data and under new conditions. Different approaches have been proposed to maximise the generalisation ability of those models, to somewhat maximise chances to do well out-of-sample. At the root of this problem is the effort to go beyond correlation only, and to identify causality (see, e.g., Pearl (2009) for a recent extensive coverage). While causality has been a key topic of interest to forecasters for a long time already, new approaches and concepts are being pushed forward for identification of and inference in causal models , which may have a significant impact on the theory of forecasting. Eventually, the key question of what a good forecast is will continue to steer new developments in the theory of forecasting in the foreseeable future. The nature of goodness of forecasts (seen from the meteorological application angle) was theorised a few decades ago already (Murphy, 1993) , based on consistency, quality and value. We still see the need to work further on that question -possibly considering these 3 pillars, but possibly also finding other ways to define desirable properties of forecasts. This will, in all cases, translates to further developing frameworks for forecast verification, focusing on the interplay between forecast quality and value, but also better linking to psychology and behavioural economics. In terms of forecast verification, some of the most pressing areas most likely relate to (multivariate) probabilistic forecasting and to the forecasting of extreme events. When it comes to forecast quality and value, we need to go beyond the simple plugging of forecasts into decision problems to assess whether this yields better decisions, or not. Instead, we ought to propose suitable theoretical frameworks that allow assessing whether certain forecasts are fundamentally better (than others) for given classes of decision problems. Finally, the link to psychology and behavioural economics should ensure a better appraisal of how forecasts are to be communicated, how they are perceived and acted upon. The purpose of forecasting is to improve decision making in the face of uncertainty. To achieve this, forecasts should provide an unbiased guess at what is most likely to happen (the point forecast), along with a measure of uncertainty, such as a prediction interval (PI). Such information will facilitate appropriate decisions and actions. Forecasting should be an objective, dispassionate exercise, one that is built upon facts, sound reasoning, and sound methods. But since forecasts are created in social settings, they are influenced by organisational politics and personal agendas. As a consequence, forecasts will often reflect aspirations rather than unbiased projections. In organisations, forecasts are created through processes that can involve multiple steps and participants. The process can be as simple as executive fiat (also known as evangelical forecasting), unencumbered by what the data show. More commonly, the process begins with a statistical forecast (generated by forecasting software), which is then subject to review and adjustment, as illustrated in figure 4. In concept, such an elaborate multi-stage process allows "management intelligence" to improve forecast quality, incorporating information not accounted for in the statistical model. In reality, however, benefits are not assured. Lawrence et al. (2006) reviewed more than 200 studies, concluding that human judgment can be of significant benefit but is also subject to significant biases. Among the many papers on this subject, there is general agreement on the need to track and review overrides, and the need to better understand the psychological issues around judgmental adjustments. The underlying problem is that each human touch point subjects the forecast to the interests of the reviewers -and these interests may not align with creating an accurate, unbiased forecast. To identify where such problems are occurring, Forecast Value Added (FVA) analysis is an increasingly popular approach among practitioners. FVA is defined as the change in a forecasting performance metric that can be attributed to a particular step or participant in the forecasting process (Gilliland, 2002) . Any activity that fails to deliver positive FVA (i.e., fails to improve forecast quality) is considered process waste. Starting with a naive forecast, FVA analysis seeks to determine whether each subsequent step in the process improves upon the prior steps. The "stairstep report" of table 1 is a familiar way of summarising results, as in this example from Newell Rubbermaid (Schubert and Rickard, 2011) . Here, averaged across all products, naive (random walk) achieved forecast accuracy of 60%. The company's statistical forecast delivered five percentage points of improvement, but management review and adjustment delivered negative value. Such findings -not uncommon -urge further investigation into (2017) created a Stochastic Value Added (SVA) metric to assess the difference between actual and forecasted distributions, knowledge of which is valuable for inventory management. Including an indication of uncertainty around the point forecast remains an uncommon practice. Prediction intervals in software generally underestimate uncertainty, often dramatically, leading to unrealistic confidence in the forecast. And even when provided, PIs largely go unused by practitioners. Goodwin (2014) summarised the psychological issues, noting that the generally poor calibration of the PIs may not explain the reluctance to utilise them. Rather, "an interval forecast may accurately reflect the uncertainty, but it is likely to be spurned by decision makers if it is too wide and judged to be uninformative" (Goodwin, 2014, page 5) . It has long been recognised (Chatfield, 1986; Lawrence, 2000) that the practice of forecasting falls well short of the potential exhibited in academic research, and revealed by the M forecasting competitions. In the M4, a simple benchmark combination method (the average of Single, Holt, and Damped exponential smoothing) reduced the overall weighted average (OWA) error by 17.9% compared to naive. The top six performing methods in M4 further reduced OWA by over 5% compared to the combination benchmark (Makridakis et al., 2020c) . But in forecasting practice, just bettering the accuracy of naive has proven to be a surprising challenge. Morlidge's (2014b) study of eight consumer and industrial businesses found 52% of their forecasts failed to do so. And, as shown, Newel Rubbermaid beat naive by just two percentage points after management adjustments. Ultimately, forecast accuracy is limited by the nature of the behaviour being forecast. But even a highly accurate forecast is of little consequence if overridden by management and not used to enhance decision making and improve organisational performance. Practitioners need to recognise limits to forecastability and be willing to consider alternative (nonforecasting) approaches when the desired level of accuracy is not achievable (Gilliland, 2010) . Alternatives include supply chain re-engineering -to better react to unforeseen variations in demand, and demand smoothing -leveraging pricing and promotional practices to shape more favourable demand patterns. Despite measurable advances in our statistical forecasting capabilities , it is questionable whether forecasting practice has similarly progressed. The solution, perhaps, is what Morlidge (2014a, page 39) suggests that "users should focus less on trying to optimise their forecasting process than on detecting where their process is severely suboptimal and taking measures to redress the problem". This is where FVA can help. For now, the challenge for researchers remains: To prompt practitioners to adopt sound methods based on the objective assessment of available information, and avoid the "worst practices" that squander resources and fail to improve the forecast. Demand management is one of the dominant components of supply chain management (Fildes et al., 2006) . Accurate demand estimate of the present and future is a first vital step for almost all aspects of supply chain optimisation, such as inventory management, vehicle scheduling, workforce planning, and distribution and marketing strategies (Kolassa and Siemsen, 2016) . Simply speaking, better demand forecasts can yield significantly better supply chain management, including improved inventory management and increased service levels. Classic demand forecasts mainly rely on qualitative techniques, based on expert judgement and past experience (e.g., Weaver, 1971) , and quantitative techniques, based on statistical and machine learning modelling (e.g., Taylor, 2003b; Bacha and Meyer, 1992) . A combination of qualitative and quantitative methods is also popular and proven to be beneficial in practice by, e.g., judgemental adjustments (Turner, 1990; Önkal and Gönül, 2005; Syntetos et al., 2016b) , judgemental forecast model selection (Petropoulos et al., 2018b; Han et al., 2019) , and other advanced forecasting support systems (Baecke et al., 2017; Arvan et al., 2019) . The key challenges that demand forecasting faces vary from domain to domain. They include: 1. The existence of intermittent demands, e.g., irregular demand patterns of fashion products. According to Nikolopoulos (2020) , limited literature has focused on intermittent demand. The seminal work by Croston (1972) was followed by other representative methods such as the SBA method by Syntetos and Boylan (2001) , the aggregate-disaggregate intermittent demand approach (ADIDA) by Nikolopoulos et al. (2011) , the multiple temporal aggregation by Petropoulos and Kourentzes (2015) , and the k nearest neighbour (kNN) based approach by Nikolopoulos et al. (2016) . See section 2.7 for more details on intermittent demand forecasting. 3. The existence of short-life-cycle products, e.g., smartphone demand (e.g., Szozda, 2010; Chung et al., 2012; Shi et al., 2020) . 81 This subsection was written by Yanfei Kang. 4. The hierarchical structure of the data such as the electricity demand mapped to a geographical hierarchy (e.g., Athanasopoulos et al., 2009; Hyndman et al., 2011; , see section 2.9.1). With the advent of the big data era, a couple of coexisting new challenges have drawn the attention of researchers and practitioners in the forecasting community: the need to forecast a large volume of related time series (e.g., thousands or millions of products from one large retailer: Salinas et al., 2019a) , and the increasing number of external variables that have significant influence on future demand (e.g., massive amounts of keyword search indices that could impact future tourism demand (Law et al., 2019) ). Recently, to deal with these new challenges, numerous empirical studies have identified the potentials of deep learning based global models, in both point and probabilistic demand forecasting (e.g., Wen et al., 2017; Rangapuram et al., 2018; Salinas et al., 2019a; Bandara et al., 2020b) . With the merits of cross-learning, global models have been shown to be able to learn long memory patterns and related effects (Montero-Manso and Hyndman, 2020), latent correlation across multiple series (Smyl, 2020) , handle complex real-world forecasting situations such as data sparsity and cold-starts , include exogenous covariates such as promotional information and keyword search indices (Law et al., 2019) , and allow for different choices of distributional assumptions (Salinas et al., 2019a) . A supply chain is 'a network of stakeholders (e.g., retailers, manufacturers, suppliers) who collaborate to satisfy customer demand' (Perera et al., 2019) . Forecasts inform many supply chain decisions, including those relating to inventory control, production planning, cash flow management, logistics and human resources. Typically, forecasts are based on an amalgam of statistical methods and management judgment (Fildes and . Hofmann and Rutschmann (2018) have investigated the potential for using big data analytics in supply chain forecasting but indicate more research is needed to establish its usefulness. In many organisations forecasts are a crucial element of Sales and Operations Planning (S&OP), a tool that brings together different business plans, such as those relating to sales, marketing, manufacturing and finance, into one integrated set of plans (Thomé et al., 2012) . The purposes of S&OP are to balance supply and demand and to link an organisation's operational and strategic plans. This requires collaboration between individuals and functional areas at different levels because it involves data sharing and achieving a consensus on forecasts and common objectives (Mello, 2010) . Successful implementations of S&OP are therefore associated with forecasts that are both aligned with an organisation's needs and able to draw on information from across the organisation. This can be contrasted with the 'silo culture' identified in a survey of companies by Moon et al. (2003) where separate forecasts were prepared by different departments in 'islands of analysis'. Methods for reconciling forecasts at different levels in both crosssectional hierarchies (e.g., national, regional and local forecasts) and temporal hierarchies (e.g., annual, monthly and daily forecasts) are also emerging as an approach to break through information silos in organisations. Cross-temporal reconciliation provides a data-driven approach that allows information to be drawn from different sources and levels of the hierarchy and enables this to be blended into coherent forecasts (Kourentzes and Athanasopoulos, 2019) . 82 This subsection was written by Paul Goodwin. In some supply chains, companies have agreed to share data and jointly manage planning processes in an initiative known as Collaborative Planning, Forecasting, and Replenishment (CPFR) (Seifert, 2003) . CPFR involves pooling information on inventory levels and on forthcoming events, like sales promotions. Demand forecasts can be shared, in real time via the Internet, and discrepancies between them reconciled. In theory, information sharing should reduce forecast errors. This should mitigate the 'bullwhip effect' where forecast errors at the retail-end of supply chains cause upstream suppliers to experience increasingly volatile demand, forcing them to hold high safety stock levels . Much research demonstrating the benefits of collaboration has involved simulated supply chains (Fildes, 2017). Studies of real companies have also found improved performance through collaboration (e.g., Boone and Ganeshan, 2008; Hill et al., 2018; Eksoz et al., 2019) , but case study evidence is still scarce (Syntetos et al., 2016a) . The implementation of collaborative schemes has been slow with many not progressing beyond the pilot stage (Panahifar et al., 2015; Galbreth et al., 2015) . Barriers to successful implementation include a lack of trust between organisations, reward systems that foster a silo mentality, fragmented forecasting systems within companies, incompatible systems, a lack of relevant training and the absence of top management support (Fliedner, 2003; Thomé et al., 2014) . Initiatives to improve supply chain forecasting can be undermined by political manipulation of forecasts and gaming. Examples include 'enforcing': requiring inflated forecasts to align them with sales or financial goals, 'sandbagging': underestimating sales so staff are rewarded for exceeding forecasts, and 'spinning': manipulating forecasts to garner favourable reactions from colleagues (Mello, 2009  Three aspects of the interaction between forecasting and inventory management have been studied in some depth and are the subject of this review: the bullwhip effect, forecast aggregation, and performance measurement. The 'bullwhip effect' occurs whenever there is amplification of demand variability through the supply chain (Lee et al., 2004) , leading to excess inventories. This can be addressed by supply chain members sharing downstream demand information, at stock keeping unit level, to take advantage of less noisy data. Analytical results on the translation of ARIMA demand processes have been established for order-up-to inventory systems (Gilbert, 2005) . There would be no value in information sharing if the wholesaler can use such relationships to deduce the retailer's demand process from their orders (see, for example, Graves, 1999) . Such deductions assume that the retailer's demand process and demand parameters are common knowledge to supply chain members. Ali and Boylan (2011) showed that, if such common knowledge is lacking, there is value in sharing the demand data itself and Ali et al. (2012) are generated at the lower frequency level and then disaggregated, if required, to the higher frequency level. For inventory replenishment decisions, the level of aggregation may conveniently be chosen to be the lead time, thereby taking advantage of the greater stability of data at the lower frequency level, with no need for disaggregation. The variance of forecast errors over lead time is required to determine safety stock requirements for continuous review systems. The conventional approach is to take the variance of one-step-ahead errors and multiply it by the lead time. However, this estimator is unsound, even if demand is independent and identically distributed, as explained by Prak et al. (2017) . A more direct approach is to smooth the mean square errors over the lead time . Strijbosch and Moors (2005) showed that unbiased forecasts will not necessarily lead to achievement, on average, of target cycle service levels or fill rates. Wallström and Segerstedt (2010) proposed a 'Periods in Stock' measure, which may be interpreted, based on a 'fictitious stock', as the number of periods a unit of the forecasted item has been in stock or out of stock. Such measures may be complemented by a detailed examination of error-implication metrics . For inventory management, these metrics will typically include inventory holdings and service level implications (e.g., cycle service level, fill rate). Comparisons may be based on total costs or via 'exchange curves', showing the trade-offs between service and inventory holding costs. Comparisons such as these are now regarded as standard in the literature on forecasting for inventories and align well with practice in industry. Retail companies depend crucially on accurate demand forecasting to manage their supply chain and make decisions concerning planning, marketing, purchasing, distribution, and labour force (Ma and Fildes, 2017) . Inaccurate forecasts lead to unnecessary costs and poor customer satisfaction. Inventory levels should be neither too high to avoid waste and extra costs (of storage and labour force) that reduce sales profits, nor too low to prevent stock-outs and sales lost. Forecasting product demand in retail can be seen in a three-dimensional space: the position in the supply chain hierarchy (store, distribution centre, or chain), the level in the product hierarchy (SKU, brand, category, or total) and the time granularity (day, week, month, quarter, or year); see Syntetos et al. (2016a) . In general, the higher is the position in the supply chain, the lower is the time granularity. Typically, retailers need daily forecasts for store replenishment and weekly forecasts for the distribution centre (DC) activities at the SKU-level (Fildes et al., 2019b). Hierarchical forecasting is a quite promising tool to generate coherent demand forecasts on multiple levels over different dimensions (Oliveira and Ramos, 2019) . Several factors may affect retail sales at the product-level. Data often exhibit multiple seasonal patterns namely weekly and annual cycles which can be modelled by Fourier terms or dummies (Huang et al., 2019) . Demand increases substantially during holidays, festivals, and other special events such as Christmas and Easter which can be captured by dummies. Weather conditions may also affect sales namely high temperature. In contrast to calendar events whose dates are known weather variables need to be forecasted. Price reductions and feature advertisement can boost sales considerably. Different types of promotions are used by retailers such as free units for multiple purchases, direct discounts, loyalty card bonuses, etc. A promotion on one product may affect both the sales of competing products and its own 84 This subsection was written by Patrícia Ramos. futures sales. The high dimensionality of the explanatory variable space makes product sales difficult to forecast using traditional approaches since overfitting may easily occur or models cannot even be estimated (Trapero et al., 2015) . Univariate forecasting models are the most basic methods retailers may use to forecast demand at the SKU level. They range from simple techniques such as simple moving averages or exponential smoothing to Fourier analysis or ARIMA models. These methods should only be used to forecast demand at higher aggregation levels or to forecast products with no promotional intensity (Ramos et al., 2015; Ramos and Oliveira, 2016) . Regression-type models allow the inclusion of external effects that may explain some of the variation Zero sales due to stock-outs or low demand are very often at the SKU × store level either at weekly or daily granularity. Methods suitable for forecasting intermittent data (see section 2.7) have been widely used in spare part sales forecasting but have not been not yet evaluated in the retail context. Also, the effective use of forecasts should couple point estimates with quantile predictions or prediction intervals that are particularly important for determining safety stock amounts needed for replenishment. However, this is an under-investigated aspect of retail forecasting (Taylor, 2007; Kolassa, 2016) . Online sales account for an ever-increasing proportion of retail sales, posing unique challenges to forecasting. There is not much specifically relevant literature, so we will call for more research and collect aspects that should be kept in mind when forecasting online retail demand. Many aspects are similar between brick and mortar (B&M) and online retail forecasting (cf. Fildes et al., 2019b): demand is driven by multiple seasonalities and product lifecycles. Promotions are important in both cases. However, differences are important. First, online retailing is not monolithic. There are huge retailers with enormous assortments and "Long Tails" of highly intermittent SKUs (even though they have fewer large distribution centres than B&Ms have stores). Conversely, there are many small online retailers with much smaller assortments. Here, turnovers may be so small that a large investment in forecasting is not worthwhile. Forecast use strongly depends on the retailer's omnichannel strategy (Armstrong, 2017; Sopadjieva et al., 2017; Melacini et al., 2018 ): e.g., for "order online, collect in store", we need forecasts for both online demand and for the demand fulfilled at the store. This and similar use cases can be addressed through hierarchical forecasting with a large location hierarchy. Online retailers, especially in fashion, have a much bigger problem with product returns. They may need to forecast how many products are returned overall (e.g., Shang et al., 2020) , or whether a specific 85 This subsection was written by Stephan Kolassa. customer will return a specific product. Online retailers can much more easily fine-tune interactions with the customer, e.g., the landing page, product recommendations, personalised promotions, pricing or other offers, or other communications. For this, retailers leverage the customer's purchasing, browsing or returns history, current shopping cart contents, or the retailer's stock position in particular products. For instance, online retailers frequently inform customers if stocks run low, which may either reduce (Park et al., 2020) or increase demand (Cui et al., 2019) . On the other hand, product reviews are a type of interaction of the customer with the retailer which drives future demand and should be included in forecasts. Most importantly, B&M retailers decouple pricing/promotion decisions and optimisation from the customer interaction, and therefore from forecasting. Online, this is not possible, because the customer has total transparency to competitors' offerings. Thus, online pricing needs to react much more quickly to competitive pressures -faster than the forecasting cycle. Thus, the specific value of demand drivers is usually not known at the time of forecasting: we don't know yet which customer will log on to our online store, so we do not know yet how many people will see a particular product displayed on their personalised landing page. (Nor do we know today what remaining stock will be displayed to the customer, per above.) Thus, changes in drivers needs to be "baked into" the forecasting algorithm -but not even the coupling between the promotional or interaction strategy and forecasting is obvious. Feedback loops between forecasting and other processes are thus even more important online: yesterday's forecasts drive today's stock position, driving today's personalised recommendations, driving demand, driving today's forecasts for tomorrow. Overall, online retail forecasting needs to be more agile and responsive to the latest interactional decisions taken in the web store, and more tightly integrated into the retailer's interactional tactics and omnichannel strategy. We are unaware of systematic research on forecasting online retail demand that addresses the points above. Promotional forecasting is central for retailing, but also relevant for many manufacturers, particularly of Fast Moving Consumer Goods (FMCG). In principle, the objective is to forecast sales, as in most business forecasting cases. However, what sets promotional forecasting apart is that we also make use of information about promotional plans, pricing, and sales of complementary and substitute products (Bandyopadhyay, 2009; Zhang et al., 2008) . Other relevant variables may include store location and format, variables that capture the presentation and location of a product in a store, proxies that characterise the competition, and so on (Van Heerde et al., 2002; Andrews et al., 2008) . Three modelling considerations guide us in the choice of models. First, promotional (and associated effects) are proportional. For instance, we do not want to model the increase in sales as an absolute number of units, but instead, as a percentage uplift. We do this to not only make the model applicable to both smaller and larger applications, for example, small and large stores in a retailing chain, but also to gain a clearer insight into the behaviour of our customers. Second, it is common that there are synergy effects. For example, a promotion for a product may be offset by promotions for substitute products. Both these considerations are easily resolved if we use multiplicative regression models. However, instead of working 86 This subsection was written by Nikolaos Kourentzes. with the multiplicative models, we rely on the logarithmic transformation of the data and proceed to construct the promotional model using the less cumbersome additive formulation. Third, the objective of promotional models does not end with providing accurate predictions. We are also interested in the effect of the various predictors: their elasticity. This can in turn provide the users with valuable information about the customers, but also be an input for constructing optimal promotional and pricing strategies . Promotional models have been widely used on brand-level data (for example, Divakar et al., 2005) . However, they are increasingly used on Stock Keeping Unit (SKU) level data (Trapero et al., 2015; Ma et al., 2016) , given advances in modelling techniques. Especially at that level, limited sales history and potentially non-existing examples of past promotions can be a challenge. (Trapero et al., 2015) consider this problem and propose using a promotional model that has two parts that are jointly estimated. The first part focuses on the time series dynamics and is modelled locally for each SKU. The second part tackles the promotional part, which pools examples of promotions across SKUs to enable providing reasonable estimates of uplifts even for new SKUs. To ensure the expected heterogeneity in the promotional effects, the model is provided with product group information. Another recent innovation is looking at modelling promotional effects both at the aggregate brand or total sales level, and disaggregate SKU level, relying on temporal aggregation (Kourentzes and Petropoulos, 2016) . Ma et al. (2016) concern themselves with the intra-and inter-category promotional information. The challenge now is the number of variables to be considered for the promotional model, which they address by using sequential LASSO. Although the aforementioned models have shown very promising results, one has to recognise that in practice promotions are often forecasted using judgemental adjustments, with inconsistent performance (Trapero et al., 2013) . Obtaining an accurate sales forecast for new products is crucial to the successful introduction of these products to the market, which in turn decides the survivability of the organisations . Traditionally forecasting is about finding an underlying pattern in the past data and projecting that into the future. Obviously, for a new product, this mechanism would not work as there is no past data. 88 Assuming the new product is totally new, to forecast in such situations forecasters historically resorted to qualitative methods -which come with inevitable biases and the forecasts produced invariably are off the mark with the forecast error that is seldom anywhere near acceptable. factors that could explain the growth and in turn, the shape of the growth curves. The parameters in the growth curves operationalise such mechanism. One such work is promulgated by Bass (1969) , which is extended multiple times to refine and enrich the growth-curve model in order to reflect the actual growth closely (Islam and Meade, 2012) . For more details on the Bass model (BM) and growth curves, please refer to sections 2.3.19 and 2.3.20. Through these growth models, the complexity of the task of new product forecasting is reduced to just identifying a historic product whose growth pattern is likely to be followed by the new product. Such product in forecasting parlance is called as 'an analogous product'. In modelling terms, the product will have a growth curve defined by the same parameters as that of the new product. Much work has been done in identifying the analogous products and finding the parameter ranges for different products in different industry sectors. Identifying a truly 'analogous product' has proven to be challenging. Many growth curve implementations have shown that forecasting error could easily be more than 50% (Kahn, 2002; Goodwin et al., 2013a) . Hence the continued research in this area. In predicting new product demand based on extracting the intention and expected behaviour of the customers, various old and new techniques had been tried. The Delphi method, customer intention surveys, and test marketing are some of the methods purely based on estimating the intentions directly. However, some other techniques try to extract the same information indirectly and in a subtle way. Conjoint analysis and choice models, agent-based models and prediction and preference markets are some of the successful implementations in this strand. Spare parts are ubiquitous in modern societies. Their demand arises whenever a component fails or requires replacement. Demand for spare parts is typically intermittent, which means that it can be forecasted using the plethora of parametric and non-parametric methods presented in section 2.7. In addition to the intermittence of demand, spare parts have two additional characteristics that make them different from Work-In-Progress and final products, namely: (i) they are generated by maintenance policies and part breakdowns, and (ii) they are subject to obsolescence (Bacchetti and Saccani, 2012; Kennedy et al., 2002) . The majority of forecasting methods do not link the demand to the generating factors, which are often related to maintenance activities. The demand for spare parts originates from the replacement of parts in the installed base of machines (i.e., the location and number of products in use), either preventively or upon breakdown of the part (Kim et al., 2017). Fortuin (1984) claims that using installed base information to forecast the spare part demand can lead to stock reductions of up to 25%. An overview of the literature that deals with spare parts forecasting with installed base information is given by Van der Auweraer and Boute (2019). Spare parts demand can be driven by the result of maintenance inspections and, in this case, a maintenance-based forecasting model should then be considered to deal with this issue. Such forecasting models include the Delay Time (DT) model analysed in Wang and Syntetos (2011) . Using the fitted values of the distribution parameters of a data set related to a hospital pumps, Wang and Syntetos (2011) have shown that when the failure and fault arriving characteristics of the items can be captured, it is recommended to use the DT model to forecast the spare part demand with a higher forecast accuracy. However, when such information is not available, then time series forecasting methods are recommended. 89 This subsection was written by Mohamed Zied Babai. Given the life cycle of products, spare parts are associated with a risk of obsolescence. Molenaers et al. (2012) discussed a case study where 54% of the parts stocked at a large petrochemical company had seen no demand for the last 5 years. Hinton (1999) reported that the US Department of Defence was holding 60% excess of spare parts, with 18% of the parts (with a total value of $1.5 billion) having no demand at all. To take into account the issue of obsolescence in spare parts demand forecasting, Teunter et al. (2011) The role of forecasting in industrial maintenance is of paramount importance. One application is to forecast spare parts, whose demands are typically intermittent, usually required to carry out corrective and preventive maintenances (Wang and Syntetos, 2011 ; Van der Auweraer and Boute, 2019). On the other hand, it is crucial for PdM the forecast of the remaining useful time, which is the useful life left on an asset at a particular time of operation (Si et al., 2011) . This work will be focused on the latter, which is usually found under the prognostic stage (Jardine et al., 2006) . them in three groups: physics-based models, knowledge-based models, and data-driven models. Physicsbased models require high skills on the underlying physics of the application. Knowledge-based models are based on facts or cases collected over the years of operation and maintenance. Although, they are useful for diagnostics and provide explicative results, its performance on prognostics is more limited. In this sense, data-driven models are gaining popularity for the development of computational power, data acquisition, and big data platforms. In this case, data coming from vibration analysis, lubricant analysis, thermography, ultrasound, etc. are usually employed. Here, well-known forecasting models as 90 This subsection was written by Juan Ramón Trapero Arenas. VARIMAX/GARCH are successfully used (García et al., 2010; Cheng et al., 2012; Gomez Munoz et al., 2014; Baptista et al., 2018) . State Space models based on the Kalman Filter are also employed (Pedregal and Carmen Carnero, 2006; Pedregal et al., 2009) . Recently, given the irruption of the Industry 4.0, physical and digital systems are getting more integrated and Machine Learning/Artificial Intelligence are drawing the attention of practitioners and academics alike (Carvalho et al., 2019) . In that same reference, it is found that the most frequently used Machine Learning methods in PdM applications were Random Forest, Artificial Neural Networks, Support Vector Machines and K-means. As logistics and supply chain operations rely upon accurate demand forecasts, reverse logistics and closed loop supply chain operations rely upon accurate forecasts of returned items. Such items (usually referred as cores) can be anything from reusable shipping or product containers to used laptops, mobile phones or car engines. If some (re)manufacturing activity is involved in supply chains, it is both demand and returned items forecasts that are needed since it is net demand requirements (demand -returns) that drive remanufacturing operations. Forecasting methods that are known to work well when applied to demand forecasting, such as SES for example, do not perform well when applied to time-series of returns because they assume returns to be a process independent of sales. There are some cases when this independence might hold, such as when a recycler receives items sold by various companies and supply chains (Goltsos and Syntetos, 2020) . In these cases, simple methods like SES applied on the time series of returns might prove sufficient. Typically though, returns are strongly correlated with past sales and the installed base (number of products with customers). After all, there cannot be a product return if a product has not first been sold. This lagged relationship between sales and returns is key to the effective characterisation of the returns process. Despite the increasing importance of circular economy and research on closed loop supply chains, returns forecasting has not received sufficient attention in the academic literature (notable contributions in this area include Goh and Varaprasad, 1986; Toktay et al., 2000; Toktay, 2003; de Brito and van der Laan, 2009; Clottey et al., 2012) . The seminal work by Kelle and Silver (1989) offers a useful framework to forecasting that is based on the degree of available information about the relationship between demand and returns. Product level (PL) information consists of the time series of sales and returns, alongside information on the time each product spends with a customer. The question then is how to derive this time to return distribution. This can be done through managerial experience, by investigating the correlation of the demand and the returns time series, or by serialising and tracking a subset (sample) of items. Past sales can then be used in conjunction with this distribution to create forecasts of returns. Serial number level (SL) information, is more detailed and consists of the time matching of an individual unit item's issues and returns and thus exactly the time each individual unit, on a serial number basis, spent with the customer. Serialisation allows for a complete characterisation of the time to return distribution. Very importantly, it also enables tracking exactly how many items previously sold remain with customers, providing time series of unreturned past sales. Unreturned past sales can then be extrapolated -along with a time to return distribution --to create forecasts of returns. Goltsos et al. (2019) offered empirical evidence in the area of returns forecasting by analysing a serialised data set from a remanufacturing company in North Wales. They found the Beta probability distribution to best fit times-to-return. Their research suggests that serialisation is something worthwhile pursuing for low volume products, especially if they are expensive. This makes a lot of sense from an investment perspective, since the relevant serial numbers are very few. However, they also provided evidence that such benefits expand in the case of high volume items. Importantly, the benefits of serialisation not only enable the implementation of the more complex SL method, but also the accurate characterisation of the returns process, thus also benefiting the PL method (which has been shown to be very robust). Macroeconomic survey expectations allow tests of theories of how agents form their expectations. Expectations play a central role in modern macroeconomic research (Gali, 2008) . Survey expectations have been used to test theories of expectations formation for the last 50 years. Initially the Livingston survey data on inflationary expectations was used to test extrapolative or adaptive hypothesis, but the focus soon turned to testing whether expectations are formed rationally (see Turnovsky and Wachter, 1972 , for an early contribution). According to (Muth, 1961, p.316) , rational expectations is the hypothesis that: 'expectations, since they are informed predictions of future events, are essentially the same as the predictions of the relevant economic theory.' This assumes all agents have access to all relevant information. Instead, one can test whether agents make efficient use of the information they possess. This is the notion of forecast efficiency (Mincer and Zarnowitz, 1969) , and can be tested by regressing the outturns on a constant and the forecasts of those outturns. Under forecast efficiency, the constant should be zero and the coefficient on the forecasts should be one. When the slope coefficient is not equal to one, the forecast errors will be systematically related to information available at the forecast origin, namely, the forecasts, and cannot be optimal. The exchange between Figlewski and Wachtel (1981, 1983) and Dietrich and Joines (1983) clarifies the role of partial information in testing forecast efficiency (that is, full information is not necessary), and shows that the use of the aggregate or consensus forecast in the individual realisation-forecast regression outlined above will give rise to a slope parameter less than one when forecasters are efficient but possess partial information. Zarnowitz (1985) , Keane and Runkle (1990) and Bonham and Cohen (2001) consider pooling across individuals in the realisation-forecast regression, and the role of correlated shocks across individuals. Recently, researchers considered why forecasters might not possess full-information, stressing informational rigidities: sticky information (see, inter alia, Mankiw and Reis, 2002; Mankiw et al., 2003) , and noisy information (see, inter alia, Woodford, 2002; Sims, 2003) . Gorodnichenko (2012, 2015) test these models using aggregate quantities, such as mean errors and revisions. Forecaster behaviour can be characterised by the response to new information. Over or under-reaction would constitute inefficiency. Broer and Kohlhas (2018) and Bordalo et al. (2018) find that agents overreact, generating a negative correlation between their forecast revision and error. The forecast is revised by more than is warranted by the new information (over-confidence regarding the value of the new information  their responses to news. The empirical evidence is often equivocal, and might reflect: the vintage of data assumed for the outturns; whether allowance is made for 'instabilities' such as alternating over-and under-prediction (Rossi and Sekhposyan, 2016) and the assumption of squared-error loss (see, for example, Patton and Timmermann, 2007; Clements, 2014b) . Research has also focused on the histogram forecasts produced by a number of macro-surveys. Density forecast evaluation techniques such as the probability integral transform 93 have been applied to histogram forecasts, and survey histograms have been compared to benchmark forecasts (see, for example, Bao et al., 2007; Hall and Mitchell, 2009; Clements, 2018) . Research has also considered uncertainty measures based on the histograms Clements (2014a). Engelberg et al. (2009) and Clements (2009 Clements ( , 2010 considered the consistency between the point predictions and histogram forecasts. Reporting practices such as 'rounding' have also been considered (Binder, 2017; Manski and Molinari, 2010; Clements, 2011) . Clements (2019) reviews macroeconomic survey expectations. As soon as Bayesian estimation of DSGEs became popular, these models have been employed in forecasting horseraces to predict the key macro variables, for example, Gross Domestic Product (GDP) and inflation, as discussed in Del Negro and Schorfheide (2013). The forecasting performance is evaluated using rolling or recursive (expanded) prediction windows (for a discussion, see Cardani et al., 2015) . DSGEs are usually estimated using revised data, but several studies propose better results estimating the mod- (2004) Moreover, several studies discuss how prediction performance could depend on the parameters' estimation. Kolasa and Rubaszek (2015b) suggest that updating DSGE model parameters only once a year is enough to have accurate and efficient predictions about the main macro variables. Unemployment in the UK has fluctuated greatly over the last 160 years, ranging from near zero throughout the two world wars, to over 18% during the UK's Great Depression, with more than 3.5 million people registered as unemployed; see figure 5. The data show many different 'regimes', both in levels and volatility. Forecasting unemployment is challenging when the data are highly non-stationary with abrupt distributional shifts, but persistence within regimes. There is a huge literature forecasting the US unemployment rate, which is characterised by more regular business cycle patterns of expansion and contraction compared to the UK. Non-linear statistical models tend to outperform within contractions or expansions, but perform worse across business cycles, see, e.g., Montgomery et al. (1998) , Rothman (1998) , and Koop and Potter (1999) , whereas Proietti (2003) finds that linear models characterised by higher persistence perform significantly better. For the UK, evidence of non-linearities is found by Peel and Speight (2000) , Milas and Rothman (2008) , and Johnes (1999) , and Gil-Alana (2001) finds evidence of long-memory. Barnichon and Garda (2016) applies a flow approach to unemployment forecasting and finds improvements, as does Smith (2011) . See also the studies by Acemoglu and Scott (1994), Floros (2005) , and Smith (2016). One approach by Hendry (2001) assumes unemployment will fall when hiring is profitable. Profitability is proxied by the gap between the real interest rate (reflecting costs) and the real growth rate (reflecting the demand side), such that the unemployment rate rises when the real interest rate exceeds the real growth rate, and vice versa. A dynamic equilibrium correction model, using impulse indicator saturation Given the inconclusive state of the literature, how can we make progress in forecasting aggregate UK unemployment? Forecasting models could be improved with either (i) better economic theories of aggregate unemployment, 96 or (ii) more general empirical models that tackle stochastic trends, breaks, dynamics, non-linearities and interdependence, 97 or better still, both. The COVID-19 pandemic and subsequent lockdown policies highlight just how important forecasts of unemployment are, see . 96 There are many relevant theories based on microfoundations, including search and matching, loss of skills, efficiency wages, and insider-outsider models, see Layard et al. (1991) for a summary. However, our focus is on aggregate unemployment. 97 See Hendry and Doornik (2014) for an approach to jointly tackling all of these issues. Figure 6 shows forecasts from a model of unemployment which would have predicted a 1% rise in the unemployment rate in April and May 2020 given a 30% decline in real GDP, based on the scenario of no furlough scheme. Also included are extrapolative forecasts (Cardt: Doornik et al., 2020a) which show no such rise, reflecting the success of the furlough scheme in maintaining employment during lockdown. As these policies come to an end forecasting models are needed that can handle sudden shifts and changes in policy.  The growth of labour productivity in the UK, measured by the percent change in output per hours worked, has varied dramatically over the last 260 years ranging from -5.8% at the onset of the post WW1 economic crash to just over 7% in 1971; see panel A of figure 7. The data show that productivity growth is very volatile and has undergone large historical shifts with productivity growth averaging around 1% between 1800-1950 followed by an increase in the average annual growth to 3% between 1950-1975. Since the mid-1970's productivity growth has fluctuated around 2%; see panel B of figure 7. However, over the last decade since 2009, 2% annual growth has proved to be an upper bound rather than the average. The most common approach for forecasting productivity, both in the UK and elsewhere, is to estimate the trend growth in productivity using aggregate data. For example, Gordon (2003) considers three separate approaches for calculating trend labour productivity in the United States based on (i) average historical growth rates outside of the business cycle, (ii) filtering the data using the Hodrick-Prescott (HP) filter (Hodrick and Prescott, 1997) , and (iii) filtering the data using the Kalman filter (Kalman, 1960) . In the UK, the Office for Budget Responsibility (OBR) follows a similar approach for generating its forecasts of productivity based on the average historical growth rate as well as judgements about factors that may 98 This subsection was written by Andrew B. Martinez. In the last few decades there have been several attempts to test and allow for time-varying trends in productivity. However, the focus of these approaches has been primarily on the United States (Hansen, 2001; Roberts, 2001) , which saw a sharp rise in productivity growth in the 1990's that was not mirrored in the UK (Basu et al., 2003) . Tests for shifts in productivity growth rates in advanced economies did not find evidence of a change in productivity growth in the UK until well after the financial crisis in 2007 (Benati, 2007; Turner and Boulhol, 2011; Glocker and Wegmüller, 2018) . ing for the long-term trend forecast to adjust based on more recent historical patterns. By taking a local average of the last four years of growth rates, Martinez et al. (2019) generate productivity forecasts whose RMSE is on average 75% smaller than OBR's forecasts extending five-years-ahead and is 84% smaller at the longest forecast horizon. Recent economic recessions have led to a renewed interest in fiscal forecasting, mainly for deficit and debt surveillance. This was certainly true in the case of the 2008 recession, and looks to become even more important in the current economic crisis brought on by the COVID-19 pandemic. This is particularly important in Europe, where countries are subject to strong fiscal monitoring mechanisms. Two main themes 99 See https://obr.uk/forecasts-in-depth/the-economy-forecast/potential-output-and-the-output-gap. (Accessed: 2020-09-05) 100 This subsection was written by Diego J. Pedregal. can be detected in the fiscal forecasting literature (Leal et al., 2008) . First, investigate the properties of forecasts in terms of bias, efficiency and accuracy. Second, check the adequacy of forecasting procedures. The first topic has its own interest for long, mainly restricted to international institutions (Artis and Marcellino, 2001) . Part of the literature, however, argue that fiscal forecasts are politically biased, mainly because there is usually no clear distinction between political targets and rigorous forecasts (Strauch et al., 2004; Frankel and Schreger, 2013) . In this sense, the availability of forecasts from independent sources is of great value (Jonung and Larch, 2006) . But it is not as easy as saying that independent forecasters would improve forecasts due to the absence of political bias, because forecasting accuracy is compromised by complexities of data, country-specific factors, outliers, changes in the definition of fiscal variables, etc. Very often some of these issues are known by the staff of organisations in charge of making the official statistics and forecasts long before the general public, and some information never leaves such institutions. So this insider information is actually a valuable asset to improve forecasting accuracy (Leal et al., 2008) . As for the second issue, namely the accuracy of forecasting methods, the literature can be divided into two parts, one based on macroeconomic models with specific fiscal modules that allows to analyse the effects of fiscal policy on macro variables and vice versa (see Favero and Marcellino (2005) and references therein), and the other based on pure forecasting methods and comparisons among them. This last stream of research basically resembles closely what is seen in other forecasting areas: (i) there is no single method outperforming the rest generally, (ii) judgemental forecasting is especially important due to data problems, and (iii) combination of methods tends to outperform individual ones (Leal et al., 2008) . Part of the recent literature focused on the generation of very short-term public finance monitoring systems using models that combine annual information with intra-annual fiscal data (Pedregal and Pérez, 2010) . The idea is to produce global annual end-of-year forecasts of budgetary variables based on the most frequently available fiscal indicators, so that changes throughout the year in the indicators can be used as early warnings to infer the changes in the annual forecasts and deviations from fiscal targets . The level of disaggregation of the indicator variables are established according to the information available and the particular objectives. The simplest options are the accrual National Accounts annual or quarterly fiscal balances running on their cash monthly counterparts. A somewhat more complex version is the previous one with all the variables broken down into revenues and expenditures. Other disaggregation schemes have been applied, namely by region, by administrative level (regional, municipal, social security, etc.), or by items within revenue and/or expenditure (VAT, income taxes, etc. Paredes et al., 2014; Asimakopoulos et al., 2020) . Unfortunately, what is missing is a comprehensive and transparent forecasting system, independent of Member States, capable of producing consistent forecasts over time and across countries. This is certainly a challenge that no one has yet dared to take up. The (spot) rate on a (riskless) bond represents the ex-ante return (yield) to maturity which equates its market price to a theoretical valuation. Interest rates and their term structure reflect two economic forces. First, they depend on expected economic conditions over the life of the bonds; second, rates are affected by the stance of monetary policy. Because business cycle conditions are characterised by recurring states (like expansions and recessions) and monetary policy is subject to regimes, recent research has reported that dynamic econometric models with regime shifts in parameters, and in particular Markov switching models (MSMs), are useful at forecasting rates. This occurs not despite the non-linear nature of MSMs but because of such nature (Ang and Bekaert, 2002b ) that allows them to capture the pervasive of nonlinearities in the term structure (Stanton, 1997) . The usefulness of MS VAR models with term structure data had been established since Hamilton (1988) and Garcia and Perron (1996) : single-state, VARMA models are overwhelmingly rejected in favour of multi-state models. Subsequently, a literature has emerged that has documented that MSMs are required to successfully forecast the yield curve, with emphasis on short rates. Lanne and Saikkonen (2003) show that a mixture of autoregressions with two regimes improves the predictions of US T-bill rates. Ang and Bekaert (2002a) investigate the MS dynamics of short-term rates for the US, the UK, and Germany. Bekaert et al. (2001) show that while single-state VARs cannot generate distributions consistent with the expectations hypothesis (EH), Markov regimes weaken the evidence against the EH. In fact, because the EH and rational expectations imply that forward rates represent the optimal forecast of spot rates (apart from a risk premium), Guidolin and Timmermann (2009) find that the optimal combinations of lagged short and forward rates depend on regimes so that the EH holds only in such states. A number of papers have also investigated the presence of regimes in the typical factors (level, slope, and convexity) that characterise the no-arbitrage dynamics of the term structure, showing the predictive benefits of incorporating MS (see, for example, Guidolin and Pedio, 2019; Hevia et al., 2015) . As widely documented (Guidolin and Thornton, 2018) , the predictable component in the mean rates is hardly significant. As a result, the random walk tends to predict well. However, density forecasts reflect all moments and the models that capture the dynamics of higher-order moments tend to perform best. MSMs appear at the forefront of a class of non-linear models that produce accurate density predictions (Hong et al., 2004; Maheu and Yang, 2016) . Another literature has strived to fit rates not only under the physical measure, i.e., in time series, but to predict rates when MS enters the pricing kernel, the fundamental pricing operator. A few papers have also assumed that regimes represent a new risk factor (Dai and Singleton, 2003) . This literature reports that MSMs lead to a range of shapes for nominal and real term structures (Veronesi and Yared, 1999) . Often the model specifications that are not rejected by formal tests include MS (Ang et al., 2008; Bansal and Zhou, 2002) . MSMs can reproduce the empirical puzzles that plague the EH (Bansal et al., 2004; Dai et al., 2007) . Finally, MSMs were proposed to forecast conditional variances. Cai (1994) developed a MS ARCH model to examine volatility persistence because this may be inflated by regimes. Gray (1996) generalised this attempt to MS GARCH and reported improvements in pseudo out-of-sample predictions. Further advances in the methods and applications of MS GARCH are Haas et al. (2004) and Smith (2002) . Modelling and predicting default-free, short-term interest rates (also called ex-ante yields) are crucial tasks in asset pricing and risk management. Indeed, the value of interest rate-sensitive securities depends on the value of the riskless rate. Besides, the short interest rate is a fundamental ingredient in the formulation and transmission of the monetary policy. However, many popular models of the short rate (for instance, continuous time, diffusion models) fail to deliver accurate out-of-sample forecasts. Their poor predictive performance may depend on the fact that the stochastic behaviour of short interest rates may be time-varying (for instance, it may depend on the business cycle and on the stance of monetary policy). Notably, the presence of nonlinearities in the conditional mean and variance of the short-term yield influences the behaviour of the entire term structure of spot rates implicit in riskless bond prices, the relationship linking rates to their residual time-to-maturity. For instance, the level of the short-term rate directly affects the slope of the yield curve. More generally, nonlinear rate dynamics imply a nonlinear equilibrium relationship between short and the long-term yields. A few studies have tried to capture the time-varying, nonlinear dynamics of interest rates using threshold models. As discussed by Pai and Pedersen (1999) threshold models show a key advantage compared their closest competitor, i.e., the Markov switching models: the regimes are not determined by an unobserved latent variable, thus fostering interpretability. In most of the applications to interest rates, the regimes are determined by the lagged level of the short rate itself, in a self-exciting fashion. For instance, Pfann et al. (1996) explored nonlinear dynamics of the US short-term interest rate using a (selfexciting) threshold autoregressive model augmented by conditional heteroscedasticity (namely, a TAR-GARCH model) and found strong evidence of the presence of two regimes. More recently, also Gospodinov (2005) has used a TAR-GARCH to predict the short-term rate and has shown that this model can capture some well-documented features of the empirical data, such as high persistence and strong conditional heteroscedasticity. Lemke and Archontakis (2008) have derived the analytical solution for arbitrage-free bond yields when the short-term riskless rate follows a threshold autoregressive model and have found that self-exciting autoregressive models are especially suitable to capture the near unit-root behaviour typically observed in the evolution of short rates. Other papers have tried to model the riskless yield curve using multivariate threshold models. For instance, extending seminal work by Enders and Granger (1998) , Clements and Galvão (2003) have tested the expectation theory of the term structure of the interest rates in a nonlinear system that allows the response of the change in short rates to past values of the spread between long and short term rates to depend on the level of the spread. They have found that the spread predicts change in the short-term interest rate only when its current level is high. As typical applications of the threshold regime switching frameworks to empirical finance, a distinct literature exists that estimate TAR models to also forecast conditional higher order moments, see, for example, Pfann et al. (1996) and more recently Dellaportas et al. (2007) , and all reported reasonable accuracy. Despite their appeal and their good empirical performance, the use of threshold models to predict interest rates has so far remained limited. This is mainly due to the fact that statistical inference for this kind of regime switching models poses some challenges, because the likelihood function is discontinuous with respect to the threshold parameters. The boom and bust in housing markets in the early and mid 2000s and its decisive role in the Great Recession has generated a vast interest in the dynamics of house prices and emphasised the importance of accurately forecasting property price movements during turbulent times. International organisations, central banks and research institutes have become increasingly engaged in monitoring the property price developments across the world. 104 At the same time, a substantial empirical literature has developed that deals with predicting future house price movements (for a comprehensive survey see Ghysels et al., 2013) . Although this literature concentrates almost entirely on the US (see, for example, Rapach and Strauss, 2009; Bork and Møller, 2015) , there are many other countries, such as the UK, where house price forecastability is of prime importance. Similarly to the US, in the UK, housing activities account for a large fraction of GDP and of households' expenditures; real estate property comprises a significant component of private wealth and mortgage debt constitutes a main liability of households (Office for National Statistics, 2019). The appropriate forecasting model has to reflect the dynamics of the specific real estate market and take into account its particular characteristics. In the UK, for instance, there is a substantial empirical literature that documents the existence of strong spatial linkages between regional markets, whereby the house price shocks emanating from southern regions of the country, and in particular Greater London, have a tendency to spread out and affect neighbouring regions with a time lag (see, for example, Cook and Thomas, 2003; Holly et al., 2010; Antonakakis et al., 2018, inter alia) . Recent evidence also suggests that the relationship between real estate valuations and conditioning macro and financial variables displayed a complex of time-varying patterns over the previous decades (Aizenman and Jinjarak, 2013) . Hence, predictive methods that do not allow for time-variation in both predictors and their marginal effects may not be able to capture the complex house price dynamics in the UK (see Yusupova et al., 2019 , for a comparison of forecasting accuracy of a battery of static and dynamic econometric methods). An important recent trend is to attempt to incorporate information from novel data sources (such as newspaper articles, social media, etc.) in forecasting models as a measure of expectations and perceptions of economic agents. It has been shown that changes in uncertainty about house prices impact on housing investment and real estate construction decisions (Cunningham, 2006; Banks et al., 2015; Oh and Yoon, 2020) , and thus incorporating a measure of uncertainty in the forecasting model can improve the forecastability of real estate prices. For instance in the UK, the House Price Uncertainty (HPU) index (Yusupova et al., 2020) , constructed using the methodology outlined in Baker et al. (2016) , 105 was found to be important in predicting property price inflation ahead of the house price collapse of the third quarter of 2008 and during the bust phase (Yusupova et al., 2019) . Along with capturing the two recent recessions (in the early 1990s and middle 2000s) this index also reflects the uncertainly related to the EU Referendum, Brexit negotiations and COVID-19 pandemic. Exchange rates have long fascinated and puzzled researchers in international finance. The reason is that following the seminal paper of Meese and Rogoff (1983) , the common wisdom is that macroeconomic models cannot outperform the random walk in exchange rate forecasting (see Rossi, 2013 , for a survey). This view is difficult to reconcile with the strong belief that exchange rates are driven by fundamentals, such as relative productivity, external imbalances, terms of trade, fiscal policy or interest rate disparity (MacDonald, 1998; Lee et al., 2013; Couharde et al., 2018) . These two contradicting assertions by the academic literature is referred to as "exchange rate disconnect puzzle". The literature provides several explanations for this puzzle. First, it can be related to the forecast estimation error. The studies in which models are estimated with a large panels of data (Mark and Sul, 2001; Engel et al., 2008; Ince, 2014) , long time series (Lothian and Taylor, 1996) or calibrated (Ca' Zorzi and Rubaszek, 2020) deliver positive results on exchange rate forecastability. Second, there is ample evidence that the adjustment of exchange rates to equilibrium is non-linear (Taylor and Peel, 2000; Curran and Velic, 2019) , which might diminish the out-of-sample performance of macroeconomic models (Kilian and Taylor, 2003; Lopez-Suarez and Rodriguez-Lopez, 2011) . Third, few economists argue that the role of macroeconomic fundamentals may be varying over time and this should be accounted for in a forecasting setting (Byrne et al., 2016; Beckmann and Schussler, 2016) . The dominant part of the exchange rate forecasting literature investigates which macroeconomic model performs best out-of-sample. The initial studies explored the role of monetary fundamentals to find that these models deliver inaccurate short-term and not so bad long-term predictions in comparison to the random walk (Meese and Rogoff, 1983; Mark, 1995) . In a comprehensive study from mid-2000s, Cheung et al. (2005 showed that neither monetary, uncovered interest parity (UIP) nor behavioural equilibrium exchange rate (BEER) model are able to outperform the no-change forecast. A step forward was made by Molodtsova and Papell (2009) , who proposed a model combining the UIP and Taylor rule equations and showed that it delivers competitive exchange rate forecasts. This result, however, has not been confirmed by more recent studies (Cheung et al., 2019; Engel et al., 2019) . In turn, Ca' Zorzi and Rubaszek (2020) argue that a simple method assuming gradual adjustment of the exchange rate towards the level implied forecasting. Overall, at the current juncture it might be claimed that "exchange rate disconnect puzzle" is still puzzling, with some evidence that methods based on PPP and controlling the estimation forecast error can deliver more accurate forecast than the random walk benchmark. The range-based (RB) volatility models is a general term for the models constructed with high and low prices, and most often with their difference i.e., the price range. A short review and classification of such models is contained in subsection 2.3.15. From practical point of view, it is important that low and high prices are almost always available with daily closing prices for financial series. The price range (or its logarithm) is a significantly more efficient estimator of volatility than the estimator based on closing prices (Alizadeh et al., 2002) . Similarly the co-range (the covariance based on price ranges) is a significantly more efficient estimator of the covariance of returns than the estimator based on closing prices (Brunetti and Lildholdt, 2002) . For these reasons models based on the price range and the co-range better describe variances and covariances of financial returns than the ones based on closing prices. Forecasts of volatility from simple models like moving average, EWMA, AR, ARMA based on the RB variance estimators are more accurate than the forecasts from the same models based on squared returns of closing prices (Vipul and Jacob, 2007; Rajvanshi, 2015) . Forecasts of volatility from the AR model based on the Parkinson estimator are more precise even than the forecasts from the standard GARCH models based on closing prices (Li and Hong, 2011) . In plenty of studies it was shown that forecasts of volatility of financial returns from the univariate RB models are more accurate than the forecasts from standard GARCH models based on closing prices (see, for example, Mapa, 2003 The RB models were used in many financial applications. They lead for example to more precise fore-  In this section, we focus on the practical advances on jointly forecasting multivariate financial time series with copulas. In the copula framework, because marginal models and copula models are separable, point forecasts are straightforward with marginal models, but dependence information is ignored. A joint probabilistic forecast with copulas involves both estimations of the copula distribution and marginal models. In financial time series, an emerging interest is to model and forecast the asymmetric dependence. A typical asymmetric dependence phenomenon is that two stock returns exhibit greater correlation during market downturns than market upturns. Patton (2006a) employs the asymmetric dependence between exchange rates with a time-varying copula construction with AR and GARCH margins. A similar study for measuring financial contagion with copulas allows the parameters of the copula to change with the states of the variance to identify shifts in the dependence structure in times of crisis (Rodriguez, 2007) . In stock forecasting, Almeida and Czado (2012) employ a stochastic copula autoregressive model to model DJI and Nasdaq, and the dependence at the time is modelled by a real-valued latent variable, which corresponds to the Fisher transformation of Kendall's τ. Li and Kang (2018) use a covariate-dependent copula framework to forecast the time varying dependence that improves both the probabilistic forecasting performance and the forecasting interpretability. Liquidity risk is another focus in finance. Weiß and Supper (2013) forecast three types of liquidity-adjusted intraday Value-at-Risk (L-IVaR) with a vine copula structure. The liquidity-adjusted intraday VaR is based on simulated portfolio values, and the results are compared with the realised portfolio profits and losses. In macroeconomic forecasting, most existing reduced-form models for multivariate time series produce symmetric forecast densities. Gaussian copulas with skew Student's-t margins depict asymmetries in the predictive distributions of GDP growth and inflation (Smith and Vahey, 2016) . Real-time macroeconomic variables are forecasted with heteroscedastic inversion copulas (Smith and Maneesoonthorn, 2018 ) that allow for asymmetry in the density forecasts, and both serial and cross-sectional dependence could be captured by the copula function (Loaiza-Maya and Smith, 2020). Copulas are also widely used to detect and forecast default correlation, which is a random variable called time-until-default to denote the survival time of each defaultable entity or financial instrument (Li, 2000) . Then copulas are used in modelling the dependent defaults (Li, 2000) , forecasting credit risk (Bielecki and Rutkowski, 2013) , and credit derivatives market forecasting (Schönbucher, 2003) . A much large volume of literature is available for this specific area. See the aforementioned references therein. For particular applications in credit default swap (CDS) and default risk forecasting see Oh and Patton (2018) and Li and He (2019) respectively. In energy economics, Aloui et al. (2013) employ the time-varying copula approach, where the marginal models are from ARMA(p,q)-GARCH(1,1) to investigate the conditional dependence between the Brent crude oil price and stock markets in the Central and Eastern European transition economies. Bessa et al. (2012) propose a time-adaptive quantile-copula where the copula density is estimated with a kernel density forecast method. The method is applied to wind power probabilistic forecasting and shows its advantages for both system operators and wind power producers. Vine copula models are also used to forecast wind power farms' uncertainty in power system operation scheduling. Wang et al. (2017) shows vine copulas have advantages of providing reliable and sharp forecast intervals, especially in the case with limited observations available. Neural Networks (NNs) are capable of successfully modelling non-stationary and non-linear series. This property has made them one of the most popular (if not the most) non-linear specification used by practitioners and academics in Finance. For example, 89% of European banks use NNs to their operations (European Banking Federation, 2019) while 25.4% of the NNs applications in total is in Finance (Wong et al., 1995) . The first applications of NNs in Finance and currently the most widespread, is in financial trading. In the mid-80s when computational power became cheaper and more accessible, hedge fund managers started to experiment with NNs in trading. Their initial success led to even more practitioners to apply NNs and nowadays 67% of hedge fund managers use NNs to generate trading ideas (BarclayHedge, 2018). A broad measure of the success of NNs in financial trading is provided by the Eurekahedge AI Hedge Fund Index 110 where it is noteworthy the 13.02% annualised return of the selected AI hedge funds over the last 10 years. In academia, financial trading with NNs is the focus of numerous papers. Notable applications of NNs in trading financial series were provided by Kaastra and Boyd (1996) , Tenti (1996) , Panda and Narasimhan (2007) , Zhang and Ming (2008) , and Dunis et al. (2010) . The aim of these studies is to forecast the sign or the return of financial trading series and based on these forecasts to generate profitable trading strategies. A process that imitates how practitioners apply NNs in financial trading. The second major field of applications of NNs in Finance is in derivatives pricing and financial risk management. The growth of the financial industry and the provided financial services have made NNs and other machine learning algorithms a necessity for tasks such as fraud detection, information extraction and credit risk assessment (Buchanan, 2019) . In derivatives pricing, NNs try to fill the limitations of the Black-Scholes model and are being used in options pricing and hedging. In academia notable applications of NNs in risk management are provided by Locarek-Junge and Prinzler (1998) and Liu (2005) and in derivatives by Bennell and Sutcliffe (2004) and Psaradellis and Sermpinis (2016) . As discussed before, financial series due to their non-linear nature and their wide applications in prac-  Investment style or factor portfolios are constructed from constituent securities on the basis of a variety of a-priori observable characteristics, thought to affect future returns. For example a 'Momentum' portfolio might be constructed with positive ('long') exposures to stocks with positive trailing 12-month returns, and negative ('short') exposure to stocks with negative trailing 12-month returns (for full background and context, see, for example Bernstein, 1995; Haugen, 2010) . 112 Explanations as to why such characteristics seem to predict returns fall in to two main camps: firstly that the returns represent a risk premium, earned by the investor in return for taking on some kind of (undiversifiable) risk, and secondly that such returns are the result of behavioural biases on the part of investors. In practice, both explanations are likely to drive style returns to a greater or lesser extent. Several such strategies have generated reasonably consistent positive risk-adjusted returns over many decades, but as with many financial return series, return volatility is large relative to the mean, and there can be periods of months or even years when returns deviate significantly from their long-run averages. The idea of timing exposure to styles is therefore at least superficially attractive, although the feasibility of doing so is a matter of some debate (Arnott et al., 2016; Asness, 2016; Bender et al., 2018) . Overconfidence in timing ability has a direct cost in terms of trading frictions and opportunity cost in terms of potential expected returns and diversification forgone. Linnainmaa (2020) explore the relationship between momentum in factor portfolios and momentum in underlying stock returns. As with valuation spreads mentioned below, there is a risk that using momentum signals to time exposure to momentum factor portfolios risks unwittingly compounding exposure. A related strand of research relates (own) factor volatility to future returns, in particular for momentum factors (Barroso, 2015; Daniel and Moskowitz, 2016 Bernstein (1995) and references therein). Style returns exhibit distinctly non-normal distributions. On a univariate basis, most styles display returns which are highly negatively skewed and demonstrate significant kurtosis. The long-run low correlation between investment styles is often put forward as a benefit of style-based strategies, but more careful analysis reveals that non-normality extends to the co-movements of investment style returns; factors exhibit significant tail dependence. Christoffersen and Langlois (2013) explores this issue, also giving details of the skew and kurtosis of weekly style returns. These features of the data mean that focusing solely on forecasting the mean may not be sufficient, and building distributional forecasts becomes important for proper risk management. Jondeau (2007) writes extensively on modelling non-gaussian distributions. Theory and intuition suggest a plethora of potentially relevant predictors of stock returns -the era of big data further increases the data available for forecasting returns. When forecasting with large numbers of predictors, conventional ordinary least squares (OLS) estimation is highly susceptible to overfitting, 113 This subsection was written by David E. Rapach. which is exacerbated by the substantial noise in stock return data (reflecting the intrinsically large unpredictable component in returns). Over the last decade or so, researchers have explored methods for forecasting returns with large numbers of predictors. Principal component regression extracts the first few principal components (or factors) from the set of predictors; the factors then serve as predictors in a low-dimensional predictive regression, which is estimated via OLS. Intuitively, the factors combine the information in the individual predictors to reduce the dimension of the regression, which helps to guard against overfitting. Ludvigson and Ng (2007) find that a few factors extracted from hundreds of macroeconomic and financial variables improve out-of-sample forecasts of the US market return. Kelly and Pruitt (2013) and Huang et al. (2015b) use partial least squares (Wold, 1966) to construct target-relevant factors from a cross section of valuation ratios and a variety of sentiment measures, respectively, to improve market return forecasts. Since Bates and Granger (1969) , it has been known that combinations of individual forecasts often perform better than the individual forecasts themselves (Timmermann, 2006) . Rapach et al. (2010) show that forecast combination can significantly improve out-of-sample market return forecasts. They first construct return forecasts via individual univariate predictive regressions based on numerous popular predictors from the literature (Goyal and Welch, 2008) . They then compute a simple combination forecast by taking the average of the individual forecasts. Rapach et al. (2013) demonstrate that forecast combination exerts a strong shrinkage effect, thereby helping to guard against overfitting. An emerging literature uses machine-learning techniques to construct forecasts of stock returns based on large sets of predictors. In an investigation of lead-lag relationships among developed equity markets, Rapach et al. (2013) appear to be the first to employ machine-learning tools to predict market returns. They use the elastic net (ENet, Zou and Hastie, 2005) , a generalisation of the popular least absolute shrinkage and selection operator (LASSO, Tibshirani, 1996) . Incorporating insights from Diebold and Shin (2019), Han et al. (2020) use the LASSO to form combination forecasts of cross-sectional stock returns based on a large number of firm characteristics from the cross-sectional literature (e.g., Harvey et al., 2016; McLean and Pontiff, 2016; Hou et al., 2020) , extending the conventional OLS approach of Haugen and Baker (1996) , Lewellen (2015) , and Green et al. forests (Breiman, 2001) , and artificial neural networks -that allow for nonlinear predictive relationships. Time series data on financial asset returns have special features. Returns themselves are hard to forecast, while it seems that volatility of returns can be predicted. Empirical distributions of asset returns show occasional clusters of large positive and large negative returns. Large negative returns, that is, crashes seem to occur more frequently than large positive returns. Forecasting upcoming increases or decreases in volatility can be achieved by using variants of the Autoregressive Conditional Heteroskedasticity (ARCH) model (Engle, 1982; Bollerslev, 1986) or realized volatility models (Taylor, 1986) . These models take (functions of) past volatility and past returns as volatility predictors, although also other explanatory variables can be incorporated in the regression. An important challenge that remains is to predict crashes. Sornette (2003) summarises potential causes for crashes and these are computer trading, increased trading in derivatives, illiquidity, trade and budget deficits, and especially, herding behaviour of investors. Yet, forecasting the exact timing of crashes may seem impossible, but on the other hand, it may be possible to forecast the probability that a crash may occur within a foreseeable future. Given the herding behaviour, any model to use for prediction should include some self-exciting behaviour. For that purpose, Aït-Sahalia et al. (2015) propose mutually exciting jump processes, where jumps (read: crashes as negative jumps) can excite new jumps, also across assets or markets (see also Chavez-Demoulin et al., 2005) . Another successful approach is the Autoregressive Conditional Duration (ACD) model Russell, 1997, 1998) , which refers to a time series model for durations between (negative) events. An alternative view on returns' volatility and the potential occurrence of crashes draws upon the earthquake literature (Ogata, 1978 (Ogata, , 1988 . The idea is that tensions in and across tectonic plates build up, until an eruption, and after that, tension starts to build up again until the next eruption. By modelling the tension-building-up process using so-called Hawkes processes (Hawkes, 1971 (Hawkes, , 2018 Hawkes and Oakes, 1974; Ozaki, 1979) , one can exploit the similarities between earthquakes and financial crashes. Gresnigt et al. (2015) take Hawkes processes to daily S&P 500 data and show that it is possible to create reliable probability predictions of a crash occurrence within the next five days. Gresnigt et al. (2017a,b) further develop a specification strategy for any type of asset returns, and document that there are spillovers across assets and markets. Given investor behaviour, past crashes can ignite future crashes. Hawkes processes are particularly useful to describe this feature and can usefully be implemented to predict the probability of nearby crashes. By the way, these processes can also be useful to predict social conflicts, as also there one may discern earthquake-like patterns. van den Hengel and Franses (2020) document their forecasting power for social conflicts in Africa. In Europe, buildings account for 40% of total energy consumed and 36% of total CO 2 emissions (Patti et al., 2016) . Given that energy consumption of buildings is expected to increase in the coming years, forecasting their electricity consumption becomes critical for improving energy management and planning. The main challenge in energy consumption forecasting is that building energy systems are complex in nature, with their behaviour depending on various factors related to the type (e.g., residential, office, entertainment, business, and industrial) and the end-uses (e.g., heating, cooling, hot water, and lighting) of the building, its construction, its occupancy, the occupants' behaviour and schedule, the efficiency of the installed equipment, and the weather conditions (Zhao and Magoulès, 2012) . Special events, holidays, and calendar effects can also affect the behaviour of the systems and further complicate the consumption patterns, especially when forecasting at hourly or daily level. As a result, producing accurate forecasts typically requires developing customised, building-specific methods. To deal with this task, the literature focuses on three main classes of forecasting methods, namely engineering, statistical, and ML (Mat Daut et al., 2017) . Engineering methods (typically utilised through software tools such as DOE-2, EnergyPlus, BLAST, and ESP-r) build on physical models that forecast consumption through detailed equations which account for the particularities of the building (Al-Homoud, 2001; Zhao and Magoulès, 2012) . Although engineering methods can provide accurate forecasts, they require high level of expertise and computational resources, do not easily adapt to pattern changes, and depend on detailed information which may be difficult to collect or even assume (Foucquier et al., 2013) . Statistical models (mainly linear regression, ARIMA, ARIMAX, and exponential smoothing) are fast to compute, require less information as input, can easily adapt to pattern changes, and be effectively adjusted for different types of buildings (Deb et al., 2017 ). Yet, statistical methods prescribe the behaviour of the systems, while also ignoring building characteristics, thus often leading to inaccurate results. Finally, ML methods (mainly neural networks, support vector machines, decision trees and fussy models) are data-driven and more generic, having also the capacity to account for multiple non-linear dependencies between the energy consumed and the factors influencing its value (Ahmad et al., 2014) . However, data, computational, and expertise limitations may become relevant (Makridakis et al., 2020c) . Drawing from the above, the literature has been inconclusive about which class of methods is the most appropriate, with the conclusions drawn being subject to the examined building type, dataset used, forecasting horizon considered, and data frequency at which the forecasts are produced . To mitigate this problem, combinations of methods and hybrids have been proposed, reporting encouraging results (Zhao and Magoulès, 2012; Mohandes et al., 2019) . Other practical issues refer to data pre-processing. Energy consumption data is typically collected at high frequencies through smart meters and therefore display noise and missing or extreme values due to monitoring issues. As a result, verifying the quality of the input data through diagnostics and data cleansing techniques, as well as optimising the selected time frames, are important for improving forecasting performance (Bourdeau et al., 2019) . Similarly, it is critical to select appropriate regressor variables which are of high quality and possible to accurately predict to assist energy consumption forecasting. Finally, it must be carefully decided whether the bottom-up, the top-down or a combination method will be used for producing forecasts at both building and end-use level (Kuster et al., 2017; Spiliotis et al., 2020c) . Building energy optimisation is an important topic nowadays as it is expected to play a significant role on our way to a carbon-free future. An example for a typical application in this space is optimisation of the heating, ventilation, and air conditioning (HVAC) system. The goal is to minimise its energy use under the constraints of maintaining certain comfort levels in the building. Though this is predominantly an optimisation exercise, forecasting comes in at different points of the system as input into the optimisation, and many problems in this space involve forecasting as a sub-problem, for example: energy consumption forecasting, room occupancy forecasting, inside temperature forecasting, (hyper-local) forecasts of outside temperature, air pressure forecasting for ventilation, and others. For example, Krüger and Givoni (2004) use a linear regression approach to predict inside temperatures in 3 houses in Brazil and optimally select the thermal systems that should be used based on their particularities, and Ruano et al. (2006) (2018b) predict building energy consumption of residential and commercial buildings to improve energy efficiency and achieve energy savings. They use decision tree-based algorithms (random forests, gradient boosted trees), and neural networks for their predictions. Also, recent trends in forecasting such as global forecasting models built across a set of time series , especially using (recurrent) neural networks (Bandara et al., 2020a; Hewamalage et al., 2020) are particularly suitable for this type of processing due to their capabilities to deal with external inputs and cold-start problems. Such capabilities are necessary if there are different regimes in the simulations under which to predict, an example of such a system for HVAC optimisation is presented by Godahewa et al. (2020) . More generally, many challenges in the space of building energy optimisation are classical examples of so-called "predict then optimise" problems (Demirovic et al., 2019; Elmachtoub and Grigas, 2017) . Here, different possible scenario predictions are obtained from different assumptions in the form of input parameters. These input parameters are then optimised to achieve a desired predicted outcome. As both prediction and optimisation are difficult problems, they are usually treated separately (Elmachtoub and Grigas, 2017) , though there are now recent works where they are considered together (Elmachtoub and Grigas, 2017; El Balghiti et al., 2019; Demirovic et al., 2019) , and this will certainly be an interesting avenue for future research. Forecasting electricity prices has various challenges that are highlighted in the detailed review paper by Weron (2014) . Even though there are economically well motivated fundamental electricity price models, forecasting models based on evaluating historic price data are the dominating the academic literature. In recent years the focus on probabilistic forecasting grew rapidly, as they are highly relevant for many applications in energy trading and risk management, storage optimization and predictive maintenance, (Ziel and Steinert, 2018; Nowotarski and Weron, 2018) . Electricity price data is highly complex and is influenced by regulation. However, there is electricity trading based on auctions and on continuous trading. Many markets like the US and European markets organize day-ahead auctions for electricity prices, see figure 8 . Thus, we have to predict multivariate time series type data, (Ziel and Weron, 2018) . In contrast, intraday markets usually apply continuous trading to manage short term variations due to changes in forecasts of renewable energy and demand, and outages (Kiesel and Paraschiv, 2017). The key challenge in electricity price forecasting is to address all potential characteristics of the considered market, most notably (some of them visible in figure 8 ): 1. (time-varying) autoregressive effects and (in)stationarity 2. calendar effects (daily, weekly and annual seasonality, holiday effects, clock-change) 3. impacts from external inputs, that partially have to be predicted in advance: load/demand/consumption, power generation (especially from wind and solar), relevant fuel prices (e.g., oil, coal, natural gas, emission allowances), related power market prices (future, balancing and neighboring markets), availabilities of power plants and interconnectors, import/export related data, and weather data 4. (time-varying) volatility and higher moment effects 5. price spikes (positive and negative) In recent years, statistical and machine learning methods gained a lot of attraction in day-ahead electricity price forecasting. Even though the majority of effects is linear there are specific non-linear de- pendencies that can be explored by using non-linear models, especially neural networks (Dudek, 2016; Lago et al., 2018; Ugurlu et al., 2018; Marcjasz et al., 2019) . Of course this comes along with higher computational costs compared to linear models. Fezzi and Mosetti (2020) illustrate that even simple linear models can give highly accurate forecasts, if correctly calibrated. However, there seems to be consensus that forecast combination is appropriate, particularly for models that have different structures or different calibration window length (Gaillard et al., 2016; Mirakyan et al., 2017; Hubicka et al., 2018) . Another increasing stream of electricity price forecasting models does not focus on the electricity price itself, but the sale/supply and purchase/demand curves of the underlying auctions (see figure 8 , but also Ziel and Steinert, 2016; Shah and Lisi, 2020; Mestre et al., 2020) . This sophisticated forecasting problem allows more insights for trading applications and the capturing of price clusters. In forecasting intraday markets the literature just started to grow quickly. As the aforementioned market characteristics get less distinct if information from day-ahead markets is taken into account appropriately. However, intraday prices are usually more volatile and exhibit more stronger price spikes. Thus, probabilistic forecasting is even more relevant (Janke and Steinke, 2019; Narajewski and Ziel, 2020b) . Recent studies showed that European markets are close to weak-form efficiency. Thus naive point forecasting benchmarks perform remarkably well (Oksuz and Ugurlu, 2019; Narajewski and Ziel, 2020a; Marcjasz et al., 2020) . Short-term load forecasting (STLF) horizon spans from 1 to 168 hours ahead . It is regarded as the basis for power system operation. Processes like unit commitment and optimal power flow rely on STLF (Saksornchai et al., 2005; Bo and Li, 2012 prosumers for applications such as strategic bidding, portfolio optimisation and tariff design (Danti and Magnani, 2017; Ahmad et al., 2019) . Due to the importance of STLF, a vast number of researchers have proposed algorithms of different complexity and structure during the last years (Hahn et al., 2009 ). Towards increasing the prediction accuracy of a single algorithm in STLF studies, hybrid models have been developed and tested. The term "hybrid" refers to the synergy of two or more algorithms. For more details, see section 2.6.12, but also the works of Bozkurt et al.  Deregulated electricity markets have been studied in the last two decades focusing on demand and prices. Several stylised facts of prices, which are crucial for predictive purposes, have been identified: seasonality, mean reversion, volatility, and spikes (or jumps). Very high prices on electricity markets are usually observed in connection to a sudden increase in demand during extreme weather events or as a consequence of drops in the supply in case of plant failures. The presence of spikes is explored in several papers which can be divided into two main streams: spike forecasting and prediction of prices under normal regime through robust estimators. Within the first set of papers, the work by Lu et al. (2005) tries to explain the origins of price spikes employing a new composite index and develops a model based on data mining to forecast price spikes taking advantage of the links with a set of explanatory variables. Spikes are often modelled as one regime of nonlinear models for time series. This approach is followed by Mount et al. (2006) and Becker et al. (2008) . The first paper focuses on regime-switching models with parameters driven by time-varying variables. These models prove very effective in predicting price spikes thanks to the strict connection of transition probabilities with current market conditions. The second paper adopts Markov switching models for spikes prediction including demand and weather variables for the estimation of time-varying transition probabilities. Christensen et al. (2009 Christensen et al. ( , 2012 suggest treating and forecasting price spikes through Poisson autoregressive and discrete-time processes, respectively. In the first paper, spikes are modelled to account for their persistence, while in the second a non-linear variant of the autoregressive conditional hazard model is used to exploit the information of previous spikes. Herrera and González (2014) The focus of the paper is not on the prediction of spikes, but on the forecast of ordinary prices using estimators which are slightly affected by the presence of abnormal observations. A similar approach has been followed by Wang et al. (2020a) using an outlier-robust machine learning algorithm. Crude oil, one of the leading energy resources, has contributed to over one-third of the world's energy consumption (Alvarez-Ramirez et al., 2003) . The fluctuations of the crude oil price have a significant impact on industries, governments as well as individuals, with substantial up-and-downs of the crude oil price bringing dramatic uncertainty for the economic and political development (Kaboudan, 2001; Cunado and De Gracia, 2005) . Thus, it is critical to develop reliable methods to accurately forecast crude oil price movement, so as to guard against the crude oil market extreme risks and improve macroeconomic policy responses. However, the crude oil price movement suffers from complex features such as nonlinearity, irregularities, dynamics and high volatility (Kang et al., 2009; Alquist et al., 2013; Herrera et al., 2018) , making the crude oil price forecasting still one of the most challenging forecasting problems. Some prior studies have suggested that the crude oil price movement is inherently unpredictable, and it would be pointless and futile to attempt to forecast future prices, see Miao et al. (2017) for a detailed summary. These agnostics consider the naive no-change forecast as the best available forecast value of future prices. In recent years, however, numerous studies result in forecasts that are more accurate than naive no-change forecasts, making the forecasting activities of crude oil prices promising (Alquist et al., 2013; . Extensive research on crude oil price forecasting has focused predominantly on the econometric models, such as VAR, ARCH-type, ARIMA, and Markov models (see, for example, Mirmirani and Li, 2004; Agnolucci, 2009; Mohammadi and Su, 2010; e Silva et al., 2010) . In the forecasting literature, unit root tests are commonly applied to examine the stationarity of crude oil prices prior to econometric modelling (Silvapulle and Moosa, 1999; Serletis and Rangel-Ruiz, 2004; Rahman and Serletis, 2012) . It is well-documented that crude oil prices are driven by a large set of external components, which are themselves hard to predict, including supply and demand forces, stock market activities, oil-related events (e.g., war, weather conditions), political factors, etc. In this context, researchers have frequently considered structural models, which relate the oil price movements to a set of economic factors. With so many econometric models, is there an optimal one? Recently, de Albuquerquemello et al. (2018) proposed a SETAR model, allowing for predictive regimes changing after a detected threshold, and achieved performance improvements over six widely used econometric models. Despite their high computational efficiency, the econometric models are generally limited in the ability to nonlinear time series modelling. On the other hand, artificial intelligence and machine learning techniques, such as belief networks, support vector machines (SVMs), recurrent neural networks (RNNs), and extreme gradient boosting (XG-Boost), provided powerful solutions to recognise the nonlinear and irregular patterns of the crude oil price movement with high automation (see, for example, Abramson and Finizza, 1991; Xie et al., 2006; Mingming and Jinliang, 2012; Gumus and Kiran, 2017) . However, challenges also exist in these techniques, such as computational cost and overfitting. In addition, a large number of studies have increasingly focused on the hybrid forecasting models based on econometrics models and machine learning techniques (Jammazi and Aloui, 2012; He et al., 2012; Chiroma et al., 2015) , achieving improved performance. Notably, the vast majority of the literature has focused primarily on the deterministic prediction, with much less attention paid to the probabilistic prediction and uncertainty analysis. However, the high volatility of crude oil prices makes probabilistic prediction more crucial to reduce the risk in decision-making (Abramson and Finizza, 1995; Sun et al., 2018) . The widespread adoption of renewable energy technologies (RETs) plays a driving role in the transition to low-carbon energy systems, a key challenge to face climate change and energy security problems. Forecasting the diffusion of RETs is critical for planning a suitable energy agenda and setting achievable targets in terms of electricity generation, although the available time series are often very short and pose difficulties in modelling. Among the approaches employed to predict the spread of RETs, the innovation diffusion framework appears a suitable choice, for its ability to capture in a simple and efficient way the nonlinear, evolutionary nature of these processes. From the seminal work by Marchetti and Nakicenovic (1979) , theorising that innovation dynamics in energy systems are similar to those of other commercial sectors or technologies, a growing body of literature has treated energy technologies with innovation diffusion models, as if they were new products needing to be accepted in a market on the basis of final decisions of individuals and market mechanisms like incentive measures, tariffs and price changes. Examples of these applications are provided in the papers by Guseo et al. (2007) , Guidolin and Mortarino (2010) , Dalla Valle and Furlan (2014) , Guidolin and Guseo (2012) , and Furlan et al. (2016) . The basic rationale for applying this class of models to the energy context is that learning from others' experience, spread of relevant information and information sharing within communities are primary drivers of these processes, giving rise to imitation dynamics well captured by diffusion models, like the Bass model (Bass, 1969) . On the other hand, the adoption of RETs is characterised by several technological, financial and administrative complexities with high initial costs, that may act as barriers to individual adoption. In a review on the application of diffusion models to renewables by Rao and Kishore (2010) , it has been noticed that renewables' typical characteristics such as low load factor, need for energy storage, small size, high upfront costs create a competitive disadvantage, while Meade and Islam (2015b) suggested that renewable technologies are different from other industrial technological innovations because, in the absence of focused support, they are not convenient from a financial point of view. In this sense, policy measures and incentive mechanisms, such as feed-in tariffs, have been used to stimulate the market. In order to capture the real effect of these measures, the Generalised Bass model (GMB) by Bass et al. (1994) has proven to be an essential modelling tool, allowing to explain a substantial lack of pioneering consumers in RETs adoption and the positive effect exerted by incentives in driving the market (see, in particular, Bunea et al., 2020) . The use of the GBM also allows a more accurate forecasting procedure through an efficient description of the nonlinear trajectory, which, in the case of renewables is often far from being smooth. A more recent stream of research has focused on forecasting the diffusion of renewables in a competitive environment, accounting for the dominant role of non-renewable energy technologies. This involves the use of multivariate models. See, for instance, the works by Guidolin and Guseo (2016) , Furlan and Mortarino (2018) , and Guidolin and Alpcan (2019). Wind energy is a leading source of renewable energy, meeting 4.8% of global electricity demand in 2018, more than twice that of solar energy (IEA, Paris, 2020). Kinetic energy in the wind is converted into electrical energy by wind turbines according to a characteristic 'power curve'. Power production is proportion to the cube of the wind speed at low-to-moderate speeds, and above this is constant at the turbine's rated power. At very high or low wind speeds no power is generated. Furthermore, the power curve is influenced by additional factors including air density, icing, and degradation of the turbine's blades. Forecasts of wind energy production are required from minutes to days-ahead to inform the operation of wind farms, participation in energy markets and power systems operations. However, the limited predictability of the weather and the complexity of the power curve make this challenging. For this reason, probabilistic forecasts are increasingly used in practice . Their value for energy trading is clear (Pinson et al., 2007) , but quantifying value for power system operation is extremely complex. Wind power forecasting may be considered a mature technology as many competing commercial offerings exist, but research and development efforts to produce novel and enhanced products are ongoing. Short-term forecasts (hours to days ahead) of wind power production are generally produced by combining numerical weather predictions (NWP) with a model of the wind turbine, farm or even regional power curve, depending on the objective. The power curve may be modelled using physical information, e.g., provided by the turbine manufacturer, in which case it is also necessary to post-process NWP wind speeds to match the same height-above-ground as the turbine's rotor. More accurate forecasts can be produced by learning the NWP-to-energy relationship from historic data when it is available. State-of-the-art methods for producing wind power forecasts leverage large quantities of NWP data to produce a single forecast (Andrade et al., 2017) and detailed information about the target wind farm (Gilbert et al., 2020a) . A number of practical aspects may also need to be considered by users, such as maintenance outages and requirements to reduce output for other reasons, such as noise control or electricity network issues. Very short-term forecast (minutes to a few hours ahead) are also of value, and on these time scales recent observations are the most significant input to forecasting models and more relevant than NWP. Classical time series methods perform well, and those which are able to capture spatial dependency between multiple wind farms are state-of-the-art, notably vector autoregressive models and variants (Cavalcante et al., 2016; Messner and Pinson, 2018) . Care must be taken when implementing these models as wind power time series are bounded by zero and the wind farm's rated power meaning that errors may not be assumed to be normally distributed. The use of transformations is recommended, though the choice of transformation depends on the nature of individual time series (Pinson, 2012) . Research is ongoing in a range of directions including: improving accuracy and reducing uncertainty in short-term forecasting, extending forecast horizons to weeks and months ahead, and improving very short-term forecast with remote sensing and data sharing (Sweeney et al., 2019) . Ocean waves are primarily generated by persistent winds in one direction. The energy thus propagated by the wind is referred to as wave energy flux and follows a linear function of wave height squared and wave period. Wave height is typically measured as significant wave height, the average height of the highest third of the waves. The mean wave period, typically measured in seconds, is the average time between the arrival of consecutive crests, whereas the peak wave period is the wave period at which the highest energy occurs at a specific point. The benefit of wave energy is that it requires significantly less reserve compared to those from wind and solar renewable energy sources . For example, the forecast error at one hour ahead for the simulated wave farms is typically in the range of 5-7%, while the forecast errors for solar and wind are 17 and 22% respectively (Reikard et al., 2011) . Solar power is dominated by diurnal and annual cycles but also exhibits nonlinear variability due to factors such as cloud cover, temperature and precipitation. Wind power is dominated by large ramp events such as irregular transitions between states of high and low power. Wave energy exhibits annual cycles and is generally smoother although there are still some large transitions, particularly during the winter months. In the first few hours of forecasting wave energy, time series models are known to be more accurate than numerical wave prediction. Beyond these forecast horizons, numerical wave prediction models such as SWAN (Simulating WAves Nearshore, Booij et al., 1999) and WAVEWATCH III ® (Tolman, 2008) are widely used. As there is as yet no consensus on the most efficient model for harnessing wave energy, potential wave energy is primarily measured with energy flux, but the wave energy harnessed typically follows non-linear functions of wave height and wave period in the observations of the six different types of wave energy converters (Reikard et al., 2015) . To model the dependencies of wind speed, wave height, wave period and their lags, Reikard et al. (2011) uses linear regressions, which were then converted to forecasts of energy flux. Pinson et al. (2012) uses Reikard et al.'s (2011) regression model and log-normal distribution assumptions to produce probabilistic forecasts. López-Ruiz et al. (2016) model the temporal dependencies of significant wave heights, peak wave periods and mean wave direction using a vector autoregressive model, and used them to produce medium to long term wave energy forecasts. Jeon and Taylor (2016) model the temporal dependencies of significant wave heights and peak wave periods using a bivariate VARMA-GARCH to convert the two probabilistic forecasts into a probabilistic forecast of wave energy flux, finding this approach worked better than either univariate modelling of wave energy flux or bivariate modelling of wave energy flux and wind speed. Taylor and Jeon (2018) produce probabilistic forecasts for wave heights using a bivariate VARMA-GARCH model of wave heights and wind speeds, and using forecasts so as to optimise decision making for scheduling offshore wind farm maintenance vessels dispatched under stochastic uncertainty. On the same subject, Gilbert et al. (2020b) use statistical post-processing of numerical wave predictions to produce probabilistic forecasts of wave heights, wave periods and wave direction and a logistic regression to determine the regime of the variables. They further applied the Gaussian copula to model temporal dependency but this did not improve their probabilistic forecasts of wave heights and periods. Over the past few years, a number of forecasting techniques for PV power systems has been developed and presented in the literature. In general, the quantitative comparison among different forecast techniques is challenging, as the factors influencing the performance are numerous: the historical data, the weather forecast, the temporal horizon and resolution, and the installation conditions. A recent review no significant difference between the two deterministic models, with the three-parameter approach being slightly more accurate. Figure 11 shows the daily value of normalised mean absolute error (NMAE%) for 216 days evaluated by using PHANN and three parameters electric circuit. The PHANN hybrid method achieves the best forecasting results, and only a few days of training can provide accurate forecasts. Dolara et al. (2018) analysed the effect of different approaches in the composition of a training data-set for the day-ahead forecasting of PV power production based on NN. In particular, the influence of different data-set compositions on the forecast outcome has been investigated by increasing the size of the training set size and by varying the lengths of the training and validation sets, in order to assess the most effective training method of this machine learning approach. As a general comment on the reported results, it can be stated that a method that employs the same chronologically consecutive samples for training is best suited when the availability of historical data is limited (for example, in newly deployed PV plant), while training based on randomly mixed samples method, appears to be most effective in the case of a greater data availability. Generally speaking, ensembles composed of independent trials are most effective.  In electrical power systems with renewable energy dependence, the power generators need to be scheduled to supply the system demand (de Queiroz, 2016) . In general, for modeling renewables future behaviour, such as hydro, wind and solar photovoltaics (PV), stochastic scenarios should be included in the scheduling, usually in a dispatch optimisation problem. Due to the complexity and uncertainly associated, this problem is, in general, modeled with time series scenarios and multi-stage stochastic approaches. This subsection aims to emphasise the importance of forecasting with simulation in renewable energy planning, especially in hydroelectric systems. In this context, due to the data spatial and temporal dependence structure, time series models are useful for future scenarios generation. Although the proposal could be forecasting for short-term planning and scheduling, simulation strategies are explored for considering and estimating uncertainty in medium and/or long-term horizons. According to Hipel and McLeod (1994) , stochastic processes of natural phenomena, such as the renewables ones, are, in general, stationary. One of the main features of hydroelectric generation systems is the strong dependence on hydrological regimes. To deal with this task, the literature focuses on two main classes for forecasting/simulation streamflow data: physical and data-driven models . Water resources management for hydropower generation and energy planning is one of the main challenges for decision-makers. At large, the hydrological data are transformed into the so-called affluent natural energy, that is used for scenarios simulation and serve as input for the optimisation algorithms (Oliveira et al., 2015) . The current state-of-the-art models for this proposal are the periodic ones. Hipel and McLeod (1994) (2018) proposed, for wind energy forecasting with spatia-temporal data, a combination of ridge linear quantile regression and Alternating Direction Method of Multipliers (ADMM) that enables each data owner to autonomously solve its forecasting problem, while collaborating with the others to improve forecasting accuracy. However, as demonstrated by Gonçalves et al. (2020a) , the mathematical properties of these algorithms should be carefully analysed in order to avoid privacy breaches (i.e., when a third party recovers the original data without consent). An alternative approach is to design a market (or auction) mechanism for time series or forecasting data where the data owners are willing to sell their private (or confidential) data in exchange for an economic compensation (Agarwal et al., 2019) . The basic concept consists in pricing data as a function of privacy loss, but it can be also pricing data as a function of tangible benefits such as electricity market profit maximization. Gonçalves et al. (2020b) adapted for renewable energy forecasting the model described in Agarwal et al. (2019) , by considering the temporal nature of the data and relating data price with the extra revenue obtained in the electricity market due to forecasting accuracy improvement. The results showed a benefit in terms of higher revenue resulting from the combination of electricity and data markets. With the advent of peer-to-peer energy markets at the domestic consumer level (Parag and Sovacool, 2016) , smart meter data exchange between peers is also expected to increase and enable collaborative forecasting schemes. For this scenario, Yassine et al. (2015) proposed a game theory mechanism where a energy consumer maximizes its reward by sharing consumption data and a data aggregator can this data with a data analyst (which seeks data with the lowest possible price). Finally, promoting data sharing via privacy-preserving or data monetisation can also solve data scarcity problems in some use cases of the energy sector, such as forecasting the condition of electrical grid assets (Fan et al., 2020) . Moreover, combination of heterogeneous data sources (e.g., numerical, textual, categorical) is a challenging and promising avenue of future research in collaborative forecasting (Obst et al., 2019) . First into the Industrial Revolution, the UK is one of the first out: in 2013 its per capita CO 2 emissions dropped below their 1860 level, despite per capita real incomes being around 7-fold higher (Hendry, 2020) . The model for forecasting UK CO 2 emissions was selected from annual data 1860-2011 on CO 2 emissions, coal & oil usage, capital and GDP, their lags and non-linearities. Figures 12(a) to 12(c) show the nonstationary time series with strong upward then downward trends, punctuated by large outliers from world wars, miners strikes plus shifts from legislation and technological change: . Saturation estimation at 0.1% using Autometrics (Doornik, 2018) We formulated a 3-equation simultaneous model of atmospheric CO 2 and Antarctic Temperature and Ice volume over 800,000 years of Ice Ages in 1000-year frequency (Paillard, 2001; Kaufmann and Juselius, 2013) . Driven by non-linear functions of eccentricity, obliquity, and precession (see panels (a), (b), and (c) of figure 13 respectively), the model was selected with saturation estimation. Earth's orbital path is calculable into the future (Croll, 1875 and Milankovitch, 1969) , allowing 100,000 years of multi-step forecasts at endogenous emissions. Humanity has affected climate since 10 thousand years ago (kya: Ruddiman, 2005 ), so we commence forecasts there. Forecasts over −10 to 100 with time series from 400kya in panels (d) to (f) of figure 13 show paths within the ranges of past data ±2.2SE (Pretis and Kaufmann, 2018) . Atmospheric CO 2 already exceeds 400ppm (parts per million), dramatically outside the Ice-Age range (Sundquist and Keeling, 2009 ). Consequently, we conditionally forecast the next 100,000 years, simulating the potential climate for anthropogenic CO 2 (Castle and Hendry, 2020b) noting the 'greenhouse' temperature is proportional to the logarithm of CO 2 (Arrhenius, 1896) . The orbital drivers will continue to 127 This section was written by David F. Hendry. influence all three variables but that relation is switched off in the scenario for 'exogenised' CO 2 . The 110 dynamic forecasts conditional on 400ppm and 560ppm with ±2SE bands are shown in figure 14 , panels (a) and (b) for Ice and Temperature respectively. The resulting global temperature rises inferred from these Antarctic temperatures would be dangerous, at more than 5 • C, with Antarctic temperatures positive for thousands of years (Vaks et al., 2019; Pretis and Kaufmann, 2020) .  The weather has a huge impact on our lives, affecting health, transport, agriculture, energy use, and leisure. Since Bjerknes (1904) introduced hydrodynamics and thermodynamics into meteorology, weather prediction has been based on merging physical principles and observational information. Modern weather forecasting is based on numerical weather prediction (NWP) models that rely on accurate estimates of the current state of the climate system, including ocean, atmosphere and land surface. Uncertainty in these estimates is propagated through the NWP model by running the model for an ensemble of perturbed initial states, creating a weather forecast ensemble (Buizza, 2018; Toth and Buizza, 2019) . One principal concern in NWP modelling is that small-scale phenomena such as clouds and convective precipitation are on too small a scale to be represented directly in the models and must, instead, be represented by approximations known as parameterisations. Current NWP model development aims at improving both the grid resolution and the observational information that enters the models (Bannister et al., 2020; Leuenberger et al., 2020) . However, for fixed computational resources, there is a trade-off between grid resolution and ensemble size, with a larger ensemble generally providing a better estimate of the prediction uncertainty. Recent advances furthermore include machine learning approaches to directly model the small-scale processes, in particular cloud processes (see, for example, Gentine et al., 2018; Rasp et al., 2018) . Despite rapid progress in NWP modelling, the raw ensemble forecasts exhibit systematic errors in both magnitude and spread (Buizza, 2018) . Statistical post-processing is thus routinely used to correct systematic errors in calibration and accuracy before a weather forecast is issued, see Vannitsem et al. (2018) for a recent review. A fundamental challenge here is to preserve physical consistency across space, time and variables (see, for example, Möller et al., 2013; Schefzik et al., 2013; Heinrich et al., 2020) . This is particularly important when the weather forecast is used as input for further prediction modelling, e.g., in hydrometeorology (Hemri et al., 2015; Hemri, 2018) . At time scales beyond two weeks, the weather noise that arises from the growth of the initial uncertainty, becomes large (Royer, 1993) . Sources of long-range predictability are usually associated with the existence of slowly evolving components of the earth system, including the El Niño Southern Oscillation (ENSO), monsoon rains, the Madden Julian Oscillation (MJO), the Indian Ocean dipole, and the North Atlantic Oscillation (NAO), spanning a wide range of time scales from months to decades (Vitart et al., 2012; Hoskins, 2013) . It is expected that, if a forecasting system is capable of reproducing these slowly evolving components, they may also be able to forecast them (Van Schaeybroeck and Vannitsem, 2018) . The next step is then to find relationships between modes of low-frequency variability and the information needed by forecast users such as predictions of surface temperature and precipitation (Roulin and Vannitsem, 2019; Smith et al., 2020) . To preserve human health, European Commission stated in the Directive (2008/50/EC) that member states have to promptly inform the population when the particulate matter (PM) daily mean value exceeds (or is expected to exceed) the threshold of 50µg/m 3 . Therefore, systems have been designed in order to produce forecasts for up to three days in advance using as input the measured value of concentration and meteorological conditions. These systems can be classified in (i) data-driven models (Carnevale et al., 2016; Stadlober et al., 2018; Corani, 2005) , and (ii) deterministic chemical and transport models (Honoré et al., 2007; Manders et al., 2009) . In this section, a brief overview of the application of these systems to the high polluted area of Lombardy region, in Italy, will be presented. Carnevale et al. (2018) compared the results of three different forecasting systems based on neural networks, lazy learning models, and regression trees respectively. A single model has been identified for each monitoring station. In the initial configuration, only the last three PM measurements available were used to produce the forecast. In this configuration, the systems offered reasonable performance, with correlation coefficients ranging from 0.6 (lazy learning method) to 0.75 (neural network). The work also demonstrated that the performance of the ensemble of the three systems was better than the best model for each monitoring station. Starting from the results of this work, a second configuration was implemented, using as input also the wind speed measured in the meteorological monitoring station closest to the measurement point of PM. The researchers observed an improvement in all performance indices, with the median of the correlation for the best model (neural networks) increasing from 0.75 to 0.82 and the RMSE dropping from 15µg/m 3 to 7µg/m 3 . One of the main drawbacks of data-driven models for air quality is that they provide information only in the point where the measurements are available. To overcome this limitation, recent literature has presented mixed deterministic and data-driven approaches (see, for example, Carnevale et al., 2020) which use the data assimilation procedure and offer promising forecasting performance. From a practical point of view, critical issues regarding forecasting air quality include: • Information collection and data access: even if regional authorities have to publicly provide data and information related to air quality and meteorology, the measured data are not usually available in real-time and the interfaces are sometimes not automated; • Data quantity: the amount of information required by air quality forecasting systems is usually large, in particular towards the definition of the training and validation sets; • Non-linear relationships: the phenomenon of accumulation of pollutants in atmosphere is usually affected by strong nonlinearities, which significantly impact the selection of the models and their performance; • Unknown factors: it is a matter of fact that the dynamic of pollutants in atmosphere is affected by a large number of non-measurable variables (such as meteorological variables or the interaction with other non-measurable pollutants), largely affecting the capability of the models to reproduce the state of the atmosphere. In Water Resources and Flood Risk Management, decision makers are frequently confronted with the need of taking the most appropriate decisions not knowing what will occur in the future. To support their decision-making under uncertainty, decision theory (Berger, 1985; Bernardo, 1994; DeGroot, 2004) invokes Bayesian informed decision approaches, which find the most appropriate decision by maximising (or minimising) the expected value of a "utility function", thus requiring its definition, together with the estimation of a "predictive probability" density (Berger, 1985) due to the fact that utility functions are rarely linear or continuous. Consequently, their expected value does not coincide with the value assumed on the predicted "deterministic" expected value. Accordingly, overcoming the classical 18 th century "mechanistic" view by resorting into probabilistic forecasting approaches becomes essential. The failure of decision-making based on deterministic forecasts in the case of Flood Risk Management is easily shown through a simple example. At a river section, the future water level provided by a forecast is uncertain and can be described by a Normal distribution with mean 10 meters and standard deviation of 5 meters. Given a dike elevation of 10.5 meters, damages may be expected as zero if water level falls below the dike elevation and linearly growing when level exceeds it with a factor of 10 6 dollars. If one assumes the expect value of forecast as the deterministic prediction to compute the damage the latter will result equal to zero, while if one correctly integrates the damage function times the predictive density the estimated expected damage will results into 6.59 millions of dollars and educated decisions on alerting or not the population or evacuating or not a flood-prone area can be appropriately taken. Water resources management, and in particular reservoirs management, aim at deriving appropriate operating rules via long term expected benefits maximisation. Nonetheless, during flood events decision makers must decide how much to preventively release from multi-purpose reservoirs in order to reduce dam failure and downstream flooding risks the optimal choice descending from trading-off between loosing future water resource vs the reduction of short term expected losses. This is obtained by setting up an objective function based on the linear combination of long and short term "expected losses", once again based on the available probabilistic forecast. This Bayesian adaptive reservoir management approach incorporating into the decision mechanism the forecasting information 130 This subsection was written by Ezio Todini. described by the short-term predictive probability density, was implemented on the lake Como since 1997 (Todini, 1999 (Todini, , 2017 as an extension of an earlier original idea (Todini, 1991) . This resulted into: • a reduction of over 30% of of the city of Como frequency; • an average reduction of 12% of the water deficit; • an increase of 3% in the electricity production. Lake Como example clearly shows that instead of basing decisions on the deterministic prediction, the use of a Bayesian decision scheme, in which model forecasts describe the predictive probability density, increases the reliability of the management scheme by essentially reducing the probability of wrong decisions (Todini, 2017 (Todini, , 2018 . 3.6. Social good and demographic forecasting 3.6.1. Healthcare 131 There are many decisions that depend on the quality of forecasts in the health care system, from capacity planning to layout decisions to the daily schedules. In general, the role of forecasting in health care is to inform both clinical and non-clinical decisions. While the former concerns decisions related to patients and their treatments (Makridakis et al., 2019) , the latter involves policy/management, and supply chain decisions that support the delivery of high-quality care for patients. A number of studies refer to the use of forecasting methods to inform clinical decision making. These methods are used to screen high risk patients for preventative health care (Chen et al., 2015; van der Mark et al., 2014; Santos et al., 2015; Uematsu et al., 2014) , to predict mental health issues (Shen et al., 2017; Tran et al., 2013) , to assist diagnosis and disease progression (Ghassemi et al., 2015; Pierce et al., 2010; Qiao et al., 2019) , to determine prognosis (Dietzel et al., 2010; Ng et al., 2007) , and to recommend treatments for patients (Kedia and Williams, 2003; Scerri et al., 2006; Shang et al., 2019) . Common forecasting methods to inform clinical decisions include time series, regression, classification tree, neural networks, Markov models and Bayesian networks. These models utilise structured and unstructured data including clinician notes (Austin and Kusumoto, 2016; Labarere et al., 2014) which makes the data pre-processing a crucial part of the forecasting process in clinical health care. One of the aspects of the non-clinical forecasting that has received the most attention in both research and application is the policy and management. Demand forecasting is regularly used in Emergency Departments (Arora et al., 2020; Choudhury and Urena, 2020; Khaldi et al., 2019; Rostami-Tabar and Ziel, 2020) , ambulance services (Al-Azzani et al., 2020; Setzler et al., 2009; Vile et al., 2012; Zhou and Matteson, 2016) and hospitals with several different specialities (McCoy et al., 2018; Ordu et al., 2019; Zhou et al., 2018) to inform operational, tactical and strategic planning. The common methods used for this purpose include classical ARIMA and exponential smoothing methods, regression, singular spectrum analysis, Prophet, Double-Seasonal Holt-Winter, TBATS and Neural Networks. In public health, forecasting can guide policy and planning. Although it has a wider definition, the most attention is given to Epidemic forecasting (see also section 3.6.2). Forecasting is also used in both national and global health care supply chains, not only to ensure the availability of medical products for the population but also to avoid excessive inventory. Additionally, the 131 This section was written by Bahman Rostami-Tabar. lack of accurate demand forecast in a health supply chain may cost lives (Baicker et al., 2012) and has exacerbated risks for suppliers (Levine et al., 2008) . Classical exponential smoothing, ARIMA, regression and Neural Network models have been applied to estimate the drug utilisation and expenditures (Dolgin, 2010; Linnér et al., 2020) , blood demand (Fortsch and Khapalova, 2016) , hospital supplies (Gebicki et al., 2014; Riahi et al., 2013) and demand for global medical items (Amarasinghe et al., 2010; Hecht and Gandhi, 2008; van der Laan et al., 2016) . It is important to note that, while the demand in a health care supply chain has often grouped and hierarchical structures (Mircetica et al., 2020) , this has not been well investigated and needs more attention. Forecasting the evolution of a pandemic, the growth of cases and fatalities for various horizons and levels of granularity, is a complex task with raw and limited data -as each pandemic has unique features with several factors affecting the severity and the contagiousness. Be that as it may, forecasting becomes an paramount task for the countries to prepare and plan their response (Nikolopoulos, 2020) , both in healthcare and the supply chains (Beliën and Forcé, 2012) . Successful forecasting methods for the task include time-series methods, epidemiological and agentbased models, metapopulation models, approaches in metrology (Nsoesie et al., 2013) , machine and deep learning methods . Andersson et al. (2008) used regression models for the prediction of the peak time and volume of cases for a pandemic with evidence from seven outbreaks in Sweden. Yaffee et al. (2011) forecasted the evolution of the Hantavirus epidemic in USA and compared causal and machine-learning methods with time-series methods and found that univariate methods quite successful. Their approach is based on extraction of trends from the data using machine learning. Pinson and Makridakis (2020) organised a debate between Taleb and Ioannidis on forecasting pandemics. Ioannidis et al. (2020) claim that forecasting for COVID-19 has by and large failed. However they give recommendations of how this can be averted. They suggest that the focus should be on predictive distributions and models should be continuously evaluated. Moreover, they emphasise the importance of multiple dimensions of the problem (and its impact). Taleb et al. (2020) discuss the dangers of using naive, empirical approaches for fat-tailed variables and tail risk management. They also reiterate the inefficiency of point forecasts for such phenomena. Finally, Nikolopoulos et al. (2020) focused on forecast-driven planning, predicting the growth of COVID-19 cases and the respective disruptions across the supply chain at country level with data from the USA, India, UK, Germany, and Singapore. Their findings confirmed the excess demand for groceries and electronics, and reduced demand for automotive -but the model also proved that the earlier a lock-down is imposed, the higher the excess demand will be for groceries. Therefore, governments would need to secure high volumes of key products before imposing lock-downs; and, when this is not possible, seriously consider more radical interventions such as rationing. Dengue is a mosquito-borne viral infection. Estimates of World Health Organisation reveals that that about half of the world's population is now at risk for Dengue infection. Aedes aegypti and Aedes albopictus are the principal vector for dengue transmission and is a highly domesticated mosquito. The tropical and sub-tropical areas form an ideal condition for them to be active all year around. Rainfall, temperature, and relative humidity are thought as important factors attributing towards the growth and dispersion of mosquito vectors and potential of dengue outbreaks. Forecasting dengue incidence can help public health officials in many ways: (i) guide effective planning of medical services, (ii) improve prevention and control programs, and (iii) analyse the effectiveness of strategies are to name few. In reviewing the existing literature, two data types have been used to forecast dengue incidence: (i) incidence of laboratory-confirmed dengue cases among the clinically suspected patients in various time frequencies (such as weekly, monthly, quarterly, or yearly), and (ii) web-based data (Google trends, social media) associated with Dengue cases. Dengue incidence data show complex nonlinear dynamics with strong seasonality, nonstationarity (changes in dominant periodic components over time) and, sometimes, intermittent dynamics. SARIMA models have been quite popular in forecasting laboratory-confirmed dengue cases (Gharbi et al., 2011; Martinez and Silva, 2011; Promprou et al., 2006) . Chakraborty et al. (2019) used a hybrid model combining ARIMA and neural network autoregressive (NNAR) to forecast dengue cases. In light of the biological relationships between climate and transmission potential, several studies have used additional covariates such as, rainfall, temperature, wind speed, and humidity to forecasts dengue incidence. Poisson regression model has been widely used to forecast dengue incidence using climatic factors and lagged time between dengue incidence and weather variables (Hii et al., 2012; Koh et al., 2018) . Several researchers looked at the use of Quasi-Poisson and negative binomial regression models to accommodate over dispersion in the counts (Lowe et al., 2011; . Cazelles et al. (2005) used wavelet analysis to explore the dynamic of dengue incidence and wavelet coherence analyses was used to identify time and frequency specific association with climatic variables. de Almeida Marques-Toledo et al. (2017) took a different perspective and look at weekly tweets to forecast Dengue cases. Rangarajan et al. (2019) used Google trend data to forecast Dengue cases. Authors hypothesised that web query search related to dengue disease correlated with the current level of dengue cases and thus may be helpful in forecasting dengue cases. A future direction to research in this field is to explore the use of spatio-temporal hierarchical forecasting Kourentzes and Athanasopoulos, 2019) ; see also section 2.9 on forecasting by aggregation. Actuarial, Demographic and Health studies are some examples where mortality data are commonly used. A valuable source of mortality information is the Human Mortality Database (HMD), a database that provides mortality and population data for several countries or regions. In some situations, the lack of reliable mortality data can be a problem, especially in developing countries, due to delays in registering or miscounting deaths (Checchi and Roberts, 2005) . Analysis of National Causes of Death for Action (ANACONDA) is a valuable tool that assesses the accuracy and completeness of data for mortality and cause of death by checking for potential errors and inconsistencies (Mikkelsen et al., 2020) . The analysis of mortality data is fundamental to public health authorities and policymakers, to make decisions or to evaluate the effectiveness of prevention and response strategies adopted. When facing a new pandemic, mortality surveillance is essential for monitoring the overall impact on public health in terms of disease severity and mortality (Setel et al., 2020; Vestergaard et al., 2020) . A useful metric is excess mortality and is the difference between the observed number of deaths and the expected number of deaths under "normal" conditions (Checchi and Roberts, 2005; Aron and Muellbauer, 2020) . Thus, it can only be estimated with accurate and high-quality data from previous years. Excess mortality has been used to measure the impact of heat events (Matte et al., 2016; Limaye et al., 2018) , pandemic influenza (Nunes et al., 2011; Nielsen et al., 2013) and nowadays COVID-19 (Nogueira et al., 2020; Sinnathamby et al., 2020; Ritchie et al., 2020) , among others. Excess mortality data have been making available by the media publications The Economist, The New York Times and The Financial Times. Moreover, a monitoring system of the weekly excess mortality in Europe has been performed by the EuroMOMO project (Vestergaard et al., 2020) . Over the years, several methodologies were developed for mortality modelling and forecasting (Booth and Tickle, 2008; Janssen, 2018) . Amongst the three forecasting approaches (expectation, extrapolation, and explanation), the majority of the developments were in the field of extrapolative methods. These are considered more objective, easy to use and more likely to obtain better forecast accuracy than the other two approaches (Janssen, 2018). The Lee-Carter mortality methodology (Lee and Carter, 1992 ) and its variants have been one of the most widely used extrapolative methods. Lee-Carter methodology has several advantages: it comprises a simple stochastic model with only one time-varying parameter, it performs relatively well when past trends are linear, and it can forecast a changing age pattern of mortality (Booth and Tickle, 2008) . Extrapolative time series models are also commonly used, such as univariate and multivariate ARIMA modelling. The introduction of stochastic time series models has the advantage of obtaining a forecast probability distribution rather than a deterministic point forecast and, also, enable the determination of forecast intervals (Booth and Tickle, 2008) . Janssen (2018) presents a review of the advances in mortality forecasting and possible challenges for future research. 135 An important use of mortality forecasts for those individuals at age over 60 is in the pension and insurance industries, whose profitability and solvency crucially rely on accurate mortality forecasts to adequately hedge longevity risks (see, e.g., Shang and Haberman, 2020a,b) . Longevity risk is a potential systematic risk attached to the increasing life expectancy of annuitants, which can eventually result in a higher payout ratio than expected (Crawford et al., 2008) . When a person retires, an optimal way of guaranteeing one individual's financial income in retirement is to purchase an annuity (Yaari, 1965 ). An annuity is a financial contract offered by insurers guaranteeing a steady stream of payments for either a temporary or the lifetime of the annuitants in exchange for an initial premium fee. Temporary annuities have grown in popularity in many countries, e.g., Australia and the United States of America. Immediate lifetime annuities, where rates are locked in for life, have been shown to deliver poor value for money (i.e., they may be expensive for the annuitants; see, for example, Cannon and Tonks, 2008, Chapter 6 ). These temporary annuities pay a pre-determined and guaranteed level of income which is higher than the level of income provided by a lifetime annuity for a similar premium. Temporary annuities also offer an alternative to lifetime annuities and allow the annuitants the option of also buying a deferred annuity at a later date. The price of an annuity with a maturity period is a random variable, as it depends on the value of zerocoupon bond price and mortality forecasts. The zero-coupon bond price is a function of interest rate and is comparably more stable than the mortality forecasts for the retirees. In actuarial science and demography, mortality forecasting methods can be grouped into three categories: extrapolation, explanation and expectation (Booth and Tickle, 2008) . The extrapolation approach identifies age patterns and trends in time. The explanation approach captures the correlation between mortality and underlying cause of death. The expectation approach is based on the subjective opinion of experts, who set a long-run mortality target. Methods based on expectation make use of the opinions of experts concerning future mortality or life expectancy with a specified path and speed of progression towards the assumed value (Continuous Mortality Investigation, 2020). The advantage of this approach is that demographic, epidemiological, medical and other relevant information may be incorporated into the forecasts. The disadvantages are that such information is subjective and biased towards experts' opinions, and it only produces scenario-based deterministic forecasts (Ahlburg and Vaupel, 1990; Wong-Fupuy and Haberman, 2004) . Methods based on explanation approach incorporate medical, social, environmental and behavioural factors into mortality modelling. Example include smoking and disease-related mortality models. The benefit of this approach is that mortality change can be understood from changes in related explanatory variables; thus, it is attractive in terms of interpretability (Gutterman and Vanderhoof, 1998) . In the extrapolation approach, many parametric and nonparametric methods have been proposed (see, e.g., Alho and Spencer, 2005; Hyndman and Ullah, 2007; Shang et al., 2011) . Among the parametric methods, the method of Heligman and Pollard (1980) is well-known. Among the nonparametric methods, the Lee-Carter model (Lee and Carter, 1992) , Cairns-Blake-Dowd model (Cairns et al., 2009; Dowd et al., 2010) , and functional data model (Hyndman and Ullah, 2007) , as well as their extensions and generalisations are dominant. Can we predict the occurrence of WW3 in the next 20 years? Is there any trend in the severity of wars? The study of armed conflicts and atrocities, both in terms of frequency over time and the number of casualties, has received quite some attention in the scientific literature and the media (e.g., Cederman, 2003; Friedman, 2015; Hayes, 2002; Norton-Taylor, 2015; Richardson, 1948 Richardson, , 1960 , falling within the broader discussion about violence (Berlinski, 2009; Goldstein, 2011; Spagat et al., 2009) , with the final goal of understanding whether humanity is becoming less belligerent (Pinker, 2011) , or not (Braumoeller, 2019) . Regarding wars and atrocities, the public debate has focused its attention on the so-called Long Peace Theory (Gaddis, 1989) , according to which, after WW2, humanity has experienced the most peaceful period in history, with a decline in the number and in the severity of bloody events. Scholars like Mueller (2009a,b) and Pinker (2011 Pinker ( , 2018 claim that sociological arguments and all statistics suggest we live in better times, while others like Gray (2015a,b) and Mann (2018) maintain that those statistics are often partial and misused, the derived theories weak, and that war and violence are not declining but only being transformed. For Mann, the Long Peace proves to be ad-hoc, as it only deals with Western Europe and North America, neglecting the rest of the world, and the fact that countries like the US have been involved in many conflicts out of their territories after WW2. Recent statistical analyses confirm Gray's and Mann's views: empirical data do not support the idea of a decline in human belligerence (no clear trend appears), and in its severity. Armed conflicts show long inter-arrival times, therefore a relative peace of a few decades means nothing statistically (Cirillo and Taleb, 2016b) . Moreover, the distribution of war casualties is extremely fat-tailed (Clauset, 2018; Clauset and Gleditsch, 2018) , often with a tail exponent ξ > 1 (Cirillo and Taleb, 2016b) , indicating a possibly infinite mean, i.e., a tremendously erratic and unforeseeable phenomenon. An only apparently infinitemean phenomenon though (Cirillo and Taleb, 2019) , because no single war can kill more than the entire world population, therefore a finite upper bound exists, and all moments are necessarily finite, even if difficult to estimate. Extreme value theory (Embrechts et al., 2013) can thus be used to correctly model tail risk and make prudential forecasts (with many caveats like in Scharpf et al., 2014) , while avoiding naive extrapolations (Taleb et al., 2020) . As history teaches (Nye, 1990) , humanity has already experienced periods of relative regional peace, like the famous Paces Romana and Sinica. The present Pax Americana is not enough to claim that we are structurally living in a more peaceful era. The Long Peace risks to be another apophenia, another example of Texan sharpshooter fallacy (Carroll, 2003) . Similar mistakes have been made in the past. Buckle (1858) wrote: "that [war] is, in the progress of society, steadily declining, must be evident, even to the most hasty reader of European history. If we compare one country with another, we shall find that for a very long period wars have been becoming less frequent; and now so clearly is the movement marked, that, until the late commencement of hostilities, we had remained at peace for nearly forty years: a circumstance unparalleled [...] in the affairs of the world". Sadly, Buckle was victim of the illusion coming from the Pax Britannica (Johnston, 2008) : the century following his prose turned out to be the most murderous in human history. Forecasting in businesses is a complicated procedure, especially when predicting numerous, diverse series, dealing with unstructured data of multiple sources, and incorporating human judgment (Lim and O'Connor, 1996a) . In this respect, since the early 80's, various Forecasting Support Systems (FSSs) have been developed to facilitate forecasting and support decision making (Kusters et al., 2006) . Rycroft (1993) provides an early comparative review of such systems, while many studies strongly support their utilisation over other forecasting alternatives (Tashman and Leach, 1991; Sanders and Manrodt, 2003) . In a typical use-case scenario, the FSSs will retrieve the data required for producing the forecasts, will provide some visualisations and summary statistics to the user, allow for data pre-processing, and then produce forecasts that may be adjusted according to the preferences of the user. However, according to Ord and Fildes (2013) , effective FSS should be able to produce forecasts by combining relevant information, analytical models, judgment, visualisations, and feedback. To that end, FSSs must (i) elaborate accurate, efficient, and automatic statistical forecasting methods, (ii) enable users to effectively incorporate their judgment, (iii) allow the users to track and interact with the whole forecasting procedure, and (iv) be easily customised based on the context of the company. Indeed, nowadays, most off-the-self solutions, such as SAP, SAS, JDEdwards, and ForecastPro, offer a variety of both standard and advanced statistical forecasting methods, as well as data pre-processing and performance evaluation algorithms. On the other hand, many of them still struggle to incorporate state-of-the-art methods that can further improve forecasting accuracy, such as automatic model selection algorithms and temporal aggregation (Petropoulos, 2015) , thus limiting the options of the users. Similarly, although many FSSs support judgmental forecasts and judgmental adjustments of statistical forecasts, this is not done as suggested by the literature, i.e., in a guided way under a well-organised framework. As a result, the capabilities of the users are restrained and methods that could be used to mitigate biases, overshooting, anchoring, and unreasonable or insignificant changes that do not rationalise the time wasted, are largely ignored (Fildes and Goodwin, 2013; Fildes et al., 2006) . Other practical issues of FSSs are related with their engine and interfaces which are typically de- signed so that they are generic and capable to serve different companies and organisations of diverse needs (Kusters et al., 2006) . From a developing and economic perspective, this is a reasonable choice. However, the lack of flexibility and customisability can lead to interfaces with needless options, models, tools, and features that may confuse inexperienced users and undermine their performance (Fildes et al., 2006) . Thus, simple, yet exhaustive interfaces should be designed in the future to better serve the needs of each company and fit its particular requirements (Spiliotis et al., 2015) . Ideally, the interfaces should be adapted to the strengths and weaknesses of the user, providing useful feedback when possible . Finally, web-based FSSs could replace windows-based ones that are locally installed and therefore of limited accessibility, availability, and compatibility (Asimakopoulos and Dix, 2013) . Cloud computing and web-services could be exploited in that direction. One of the central promises in cloud computing is that of elasticity. Customers of cloud computing services can add compute resources in real-time to meet and satisfy increasing demand and, when demand for a cloud-hosted application goes down, it is possible for cloud computing customers to down-scale. The benefit of the latter is particularly economically interesting during the current pandemic. Popular recent cloud computing offerings take this elasticity concept one step further. They abstract away the computational resources completely from developers, so that developers can build serverless applications. In order for this to work, the cloud provider handles the addition and removal of compute resources "behind the scenes". To keep the promise of elasticity, a cloud provider must address a number of forecasting problems at varying scales along the operational, tactical and strategic problem dimensions (Januschowski and Kolassa, 2019) . As an example for a strategic forecasting problems: where should data centres be placed? In what region of a country and in what geographic region? As an example for tactical forecasting problems, these must take into account energy prices (see Section 3.4.3) and also, classic supply chain problems (Larson et al., 2001) . After all, physical servers and data centres are what enables the cloud and these must be ordered and have a lead-time. The careful incorporation of life cycles of compute types is important (e.g., both the popularity of certain machine types and the duration of a hard disk). Analogous to the retail sector, cloud resource providers have tactical cold-start forecasting problems. For example, while GPU or TPU instances are still relatively recent but already well estabilished, the demand for quantum computing is still to be decided. In the class of operational forecasting problems, cloud provider can choose to address short-term resource forecasting problems for applications such as adding resources to applications predictively and make this available to customers (Barr, 2018) . The forecasting of the customer's spend for cloud computing is another example. For serverless infrastructure, a number of servers is often maintained in a ready state (Gias and Casale, 2020) and the forecasting of the size of this 'warmpool' is another example. We note that cloud computing customers have forecasting problems that mirror the forecasting challenges of the cloud providers. Interestingly, forecasting itself has become a software service that cloud computing companies offer (Januschowski et al., 2018a; Poccia, 2019; Liberty et al., 2020) Many challenges in this application area are not unique to cloud computing. Cold start problems exist elsewhere for example. What potentially stands out in cloud computing forecasting problems may be the scale (e.g., there are a lot of physical servers available), the demands on the response time and granularity of a forecast and the degree of automation. Consider the operational forecasting problem of predictive scaling. Unlike in retail demand forecasting, no human operator will be able to control this and response times to forecasts are in seconds. It will be interesting to see whether approaches based on reinforcement learning (Gamble and Gao, 2018) can partially replace the need to have forecasting models (Januschowski et al., 2018b) . Surveys of forecasting practice (De Baets, 2019) have shown that of use of pure judgmental forecasting by practitioners has become less common. About 40 years ago, Sparkes and McHugh (1984) found that company action was more likely to be influenced by judgment forecasts than by any other type of forecast. In contrast, Fildes and Petropoulos (2015) found that nowadays only 15.6% of forecasts in the companies that they surveyed were made by judgment alone. The majority of forecasts (55.6%) were made using a combination of statistical and judgmental methods. This begs the question of whether judgmental forecasting is still relevant and how wise it is to rely on this type of forecasting? Answers here depend on the type of information on which the judgmental forecasts are based (Harvey, 2007) . Recently, a major study on geo-political forecasts based solely on information held in the forecasters' memory has been summarised by Tetlock and Gardner (2015) . This showed clear individual differences: a small proportion of participants ('superforecasters') were particularly good at making probabilistic forecasts about geo-political matters. Furthermore, people could be trained to make better probabilistic forecasts of this type. In contrast, people have difficulty making cross-series forecasts. They have difficulty learning the correlation between variables and using it to make their forecasts (Harvey et al., 1994; Lim and O'Connor, 1996a,b) . Additionally, they appear to take account of the noise as well as the pattern when learning the relation between variables; hence, when later using one of the variables to forecast the other, they add noise to their forecasts (Gray et al., 1965) . Judgmental extrapolation from a single time series is subject to various effects. First, people are influenced by optimism. For example, they over-forecast time series labelled as 'profits' but under-forecast the same series labelled as 'losses' (Harvey and Reimers, 2013) . Second, they add noise to their forecasts so that a sequence of forecasts looks similar to ('represents') the data series (Harvey, 1995) . Third, they damp trends in the data (Eggleton, 1982; Harvey and Reimers, 2013; Lawrence and Makridakis, 1989) . Fourth, forecasts from un-trended independent series do not lie on the series mean but between the last data point and the mean; this is what we would expect if people perceived a positive autocorrelation in the series (Reimers and Harvey, 2011) . These last two effects can be explained in terms of the under-adjustment that characterises use of anchor-and-adjust heuristics: forecasters anchor on the last data point and adjust towards the trend line or mean -but do so insufficiently. However, in practice, this under-adjustment may be appropriate because real linear trends do become damped and real series are more likely to contain a modest autocorrelation than be independent . We should therefore be reluctant to characterise these last two effects as biases. In summary, the literature suggests that there are good reasons for practitioners to prefer statistical or combined statistical and judgmental approaches to pure judgmental forecasting when past values of the variable to be forecast (or past values of variables correlated with the variable to be forecast) are available. When no such information is available, pure judgmental forecasting may provide the only option. In this situation, selection and training of forecasters would be sensible. Judgmental adjustments give practitioners a quick and convenient way to incorporate their insights, experience and additional information they possess into a set of baseline forecasts that have been externally produced. Their effectiveness has made them a highly cherished tool among forecasting practitioners (Arvan et al., 2019; Eksoz et al., 2019; Lawrence et al., 2006; Petropoulos et al., 2016) . Recent research based on business practice and consultancy work have revealed intriguing findings about the way that practitioners make judgmental adjustments. Fildes et al. (2009) examined the judgmental adjustment applications in four large supply-chain companies and found evidence that the adjustments in a 'negative' direction improved the accuracy more than the adjustments in a 'positive' direction. Analogous findings were observed by Franses and Legerstee (2009b) who examined a large set of forecasts produced by a multinational pharmaceutical company. This effect may be attributable to wishful thinking or optimism that may underlie positive adjustments. The adjustments 'larger' in magnitude were also more beneficial in terms of the final forecast accuracy than 'smaller' adjustments (Fildes et al., 2009) . This may simply be because smaller adjustments are merely a sign of tweaking the numbers, but large adjustments are conducted when there is a highly valid reason to make them. Syntetos et al. (2009) also found supporting evidence for these tendencies in the case of forecasting products with intermittent demand. The positive adjustments were less accurate than the negative ones and smaller adjustments ended up producing worse performance than larger ones. Given the undisputable existence of judgmental adjustments in forecasting practice what are the reasons that practitioners report for introducing their adjustments?Önkal and Gönül (2005) conducted a series of interviews and a survey on forecasting practitioners to explore these reasons. The main reasons conveyed are (i) to incorporate the practitioners' intuition and experience about the predictions generated externally, (ii) to accommodate sporadic events and exceptional occasions, (iii) to integrate confidential/insider information that may have not been captured in the forecasts, (iv) to hold responsibility and to gain control of the forecasting process, (v) to incorporate the expectations and viewpoints of the practitioners, and (vi) to compensate for various judgmental biases that are believed to exist in the predictions. These studies also revealed that forecasting practitioners are very fond of judgmental adjustments and perceive them as a prominent way of 'completing' and 'owning' the predictions that are generated by others. Conversely, what are the primary reasons for practitioners preferring not to adjust? Önkal and Gönül (2005) and Gönül et al. (2009) report that these occasions are (i) when the practitioners are adequately informed and knowledgeable about the forecasting method(s) that are used to generate the baseline forecasts, (ii) when there are accompanying explanations and convincing communications that provide the rationale behind forecast method selection, (iii) when baseline predictions are supplemented by additional supportive materials such as scenarios and alternative forecasts, (iv) when the forecasting source is believed to be trustworthy and reliable, and (v) when organisational policy or culture prohibits judgmental adjustments. In these circumstances, the baseline forecasts are accepted more by the practitioners and their adjustments tend to be less frequent. Despite the plethora of recent research on judgmental adjustments, there are still many mechanisms to understand and many avenues to explore with respect to this popular intervention tool that serves forecasting practitioners. Forecasting practice relies on Forecast Support Systems (FSS) that combine judgment and statistics, attempting to facilitate improved forecasting accuracy (Fildes et al., 2006) . But how can we design FSS for the successful integration of judgment and statistics? The perceived quality and accessibility of a FSS can be influenced by its design. More on this can be found in the literature on the Technology Acceptance Model (Davis et al., 1989) and decision making (for instance by means of framing, visual presentation or nudging; e.g., Gigerenzer, 1996; Kahneman and Tversky, 1996; Payne, 1982; Thaler and Sunstein, 2009) . A number of studies have investigated the design aspects of FSS, with varying success. One of the more straightforward approaches is to change the look and feel of the FSS as well as its presentation style. Harvey and Bolger (1996) found that trends for instance were more easily discernible when the data was displayed graphically rather than tabular. Additionally, simple variations in presentation such as line graphs versus point graphs can alter accuracy (Theocharis et al., 2018) . One could also tweak the functionalities of the FSS. Goodwin (2000b) investigated three ways of improving judgmental adjustment via changes in the FSS. The first change was to set 'no adjusting' as default in the program. This would make 'no change' the default and 'change' the more effortful option. The second change is programming the FSS so that it would ask for the adjustment size, if the forecaster indicated a willingness to adjust. A third implementation was the addition of an 'explanation' feature: when the forecaster wanted to adjust the forecast, they were explicitly asked to document the reason for the change. The default option and the explanation feature were successful in heightening the acceptance of the statistical forecast and thus improving the forecast accuracy. Asking for the size of the adjustment was not. Other ways of influencing forecaster's behaviour with the FSS is to provide guidance or restriction (Goodwin et al., 2011) . They set up an experiment such that one group received 'guidance' in the form of information, and the other group was 'restricted' in what they could do. The latter group could, for instance, not make small adjustments, which have previously been found to be more damaging than they are helpful (Fildes et al., 2009) . However, the researchers found that neither restrictiveness nor guidance was successful in improving accuracy, and both were met with resistance by the forecasters. While the previous works focused on voluntary integration, Goodwin (2000a) also published a study on the mechanical integration. In this case, the FSS processes the changes. This could be done via psychological bootstrapping, combining different inputs automatically with equal or varying weights, or automatic correction for bias by the FSS (Goodwin, 2000a (Goodwin, , 2002 . A way of automatically combining judgment and statistics was investigated by Baecke et al. (2017) , who compared normal judgmental adjustment with what they term "integrative judgment". In brief, integrative judgment takes the judgmental information into account as a predictive variable in the forecasting model and generates a new forecast. The advantage of this method is that forecasters still have their input into the forecasting process and the resistance found in the study by Goodwin et al. (2011) should not thus occur. Whatever the solution will be, the FSS will have to improve forecast accuracy, but in addition, it will need to be easy to use, understandable, and acceptable (Fildes et al., 2006) . Evaluating forecasting capabilities can be a difficult task. One prominent way to evaluate an expert's forecast is to score the forecast once the realisation of the uncertainty is known. Scoring forecasts using the outcome's realisations over multiple forecasts offers insights into an individual's expertise. Experts can also use scoring information to identify ways to improve future forecasts. In addition, scoring rules and 142 This subsection was written by Yael Grushka-Cockayne. evaluation measures can be designed to match decision-making problems, incentivising forecasts that are most useful in a specific situation (Winkler et al., 2019) . Scoring rules were first suggested for evaluate meteorological forecasts in work by Brier (1950) . Scoring rules have since been used in a wide variety of settings, such as business and other applications. When forecasting a discrete uncertainty with only two possible outcomes (e.g., a loan with be defaulted on or not, a customer will click on an ad or not), the Brier score assigns a score of −(1 − p) 2 , where p is the probability forecast reported that the event will occurs. The greater the probability reported for an event that occurs, the higher the score the forecast receives. Over multiple forecasts, better forecasters will tend to have higher average Brier scores. For discrete events with more than two outcomes, a logarithmic scoring rule can be used. The scoring rules are attractive to managers in practice since they are considered proper. Proper scoring rules incentivise honest forecasts from the experts, even prior to knowing the realisation of an uncertainty, since ex ante the expected score is maximised only when reported probabilities equals true beliefs (Winkler et al., 1996; O'Hagan et al., 2006; Bickel, 2007; Merkle and Steyvers, 2013) . Examples of a scoring rule that is not proper yet still commonly used are the linear score, which simply equals the reported probability or density for the actual outcome, or the skill score, which is the percentage improvement of the Brier score for the forecast relative to the Brier score of some base line naive forecast (Winkler et al., 2019) . The use of prediction markets as a mechanism for collecting forecasting have been proposed as practical and useful for aligning forecasting activities with business goals. Chen et al. (2005) found that prediction markets can generate highly accurate prediction. Bassamboo et al. (2015) demonstrate the use of prediction markets in forecasting quantities important to operational decisions, such as sales forecasts, price commodity forecasts, or product features. Regardless of how much effort is poured into training forecasters and developing elaborate forecast support systems, decision-makers will either modify or discard the predictions if they do not trust them. Hence, trust is essential for forecasts to be actually used in making decisions (Alvarado-Valencia and Barrero, 2014; . Given that trust appears to be the most important attribute that promotes a forecast, what does it mean to practitioners? Past work suggests that trusting a forecast is often equated with trusting the forecaster, their expertise and skills so that predictions could be used without adjustment to make decisions (Önkal et al., 2019) . It is argued that trust entails relying on credible forecasters that make the best use of available information while using correctly applied methods and realistic assumptions with no hidden agendas (Gönül et al., 2012) . Research suggests that trust is not only about trusting forecaster's competence; users also need to be convinced that no manipulations are made for personal gains and/or to mislead decisions (Twyman et al., 2008) . Surveys with practitioners show that key determinants of trust revolve around (i) forecast support features and tools (e.g., graphical illustrations, rationale for forecasts), (ii) forecaster competence/credibility, (Taylor and Thomas, 1982) , (iv) users to be supported with forecasting training (Merrick et al., 2006) , (v) providing explanations/rationale behind forecasts (Gönül et al., 2006; Önkal et al., 2008) Trust must be earned and deserved (Maister et al., 2012) and is based on building a relationship that benefits both the providers and users of forecasts. Take-aways for those who make forecasts and those who use them converge around clarity of communication as well as perceptions of competence and integrity. Key challenges for forecasters are to successfully engage with users throughout the forecasting process (rather than relying on a forecast statement at the end) and to convince them of their objectivity and expertise. In parallel, forecast users face challenges in openly communicating their expectations from forecasts , as well as their needs for explanations and other informational addendum to gauge the uncertainties surrounding the forecasts. Organisational challenges include investing in forecast management and designing resilient systems for collaborative forecasting. Communicating forecast uncertainty is a critical issue in forecasting practice. Effective communication allows forecasters to influence end-users to respond appropriately to forecasted uncertainties. Some frameworks for effective communication have been proposed by decomposing the communication process into its elements: the communicator, object of uncertainty, format of expression, audience, and its effect (National Research Council, 2006; van der Bles et al., 2019) . Forecasters have long studied part of this problem focusing mostly in the manner by which we express forecast uncertainties. Gneiting and Katzfuss (2014) provides a review of recent probabilistic forecasting methods. Forecasting practice however revealed that numeracy skills and cognitive load can often inhibit end-users from correctly interpreting these uncertainties Raftery, 2016) . Attempts to improve understanding through the use of less technical vocabulary also creates new challenges. Research in psychology show that wording and verbal representation play important roles in disseminating uncertainty . Generally forecasters are found to be consistent in their use of terminology, but forecast end-users often have inconsistent interpretation of these terms even those commonly used (Budescu and Wallsten, 1985; Clark, 1990; Ülkümen et al., 2016) . Pretesting verbal expressions and avoiding commonly misinterpreted terms are some easy ways to significantly reduce biases and improve comprehension. Visualisations can also be powerful in communicating uncertainty. Johnson and Slovic (1995) and Spiegelhalter et al. (2011) propose several suggestions for effective communication (e.g., multiple-format use, avoiding framing bias, and acknowledging limitations), but also recognise the limited amount of existing empirical evidence. Some domain-specific studies do exist. For example, Riveiro et al. (2014) showed uncertainty visualisation helped forecast comprehension in a homeland security context. With respect to the forecaster and her audience, issues such as competence, trust, respect, and optimism have been recently examined as a means to improve uncertainty communication. Fiske and Dupree (2014) discusses how forecast recipients often infer apparent intent and competence from the uncertainty provided and use these to judge trust and respect. This suggests that the amount of uncertainty information provided should be audience dependent (Politi et al., 2007; Han et al., 2009 When dealing with the public, experts assert that communicating uncertainty helps users understand forecasts better and avoid a false sense of certainty (Morss et al., 2008) . Research however shows that hesitation to include forecast uncertainty exists among experts because it provides an opening for criticism and the possibility of misinterpration by the public (Fischhoff, 2012) . This is more challenging when the public has prior beliefs on a topic or trust has not been established. Uncertainty can be used by individuals to reinforce a motivated-reasoning bias that allows them to "see what they want to see" (Dieckmann et al., 2017 As seen throughout 2020, (leisure) tourism demand is very sensitive to external shocks such as natural and human-made disasters, making tourism products and services extremely perishable (Frechtling, 2001) . As the majority of business decisions in the tourism industry require reliable demand forecasts (Song et al., 2008) , improving their accuracy has continuously been on the agenda of tourism researchers and practi-tioners alike. Depending on data availability, as well as on geographical aggregation level, tourism demand is typically measured in terms of arrivals, bed-nights, visitors, exports receipts, import expenditures, etc. Since there are no specific tourism demand forecast models, standard univariate and multivariate statistical models, including common aggregation and combination techniques, etc., have been used in quantitative tourism demand forecasting (see, for example, Song et al., 2019; Jiao and Chen, 2019 , for recent reviews). Machine learning and other artificial intelligence methods, as well as hybrids of statistical and machine learning models, have recently been employed more frequently. Traditionally, typical micro-economic demand drivers (own price, competitors' prices, and income) and some more tourism-specific demand drivers (source-market population, marketing expenditures, consumer tastes, habit persistence, and dummy variables capturing one-off events or qualitative characteristics) have been employed as predictors in tourism demand forecasting (Song et al., 2008) . One caveat of some of these economic demand drivers is their publication lag and their low frequency, for instance, when real GDP (per capita) is employed as a proxy for travellers' income. The use of leading indicators, such as industrial production as a leading indicator for real GDP, has been proposed for short-term tourism demand forecasting and nowcasting (Chatziantoniou et al., 2016) . During the past couple of years, web-based leading indicators have also been employed in tourism demand forecasting and have, in general, shown improvement in terms of forecast accuracy. However, this has not happened in each and every case, thereby confirming the traded wisdom that there is no single best tourism demand forecasting approach . Examples of those web-based leading indicators include Google Trends indices (Bangwayo-Skeete and Skeete, 2015), Google Analytics indicators (Gunter andÖnder, 2016) , as well as Facebook 'likes' (Gunter et al., 2019) . The reason why these expressions of interaction of users with the Internet have proven worthwhile as predictors in a large number of cases is that it is sensible to assume potential travellers gather information about their destination of interest prior to the actual trip, with the Internet being characterised by comparably low search costs, ergo allowing potential travellers to forage information (Pirolli and Card, 1999) with only little effort (Zipf, 2016) . A forecaster should include this information in their own set of relevant information at the forecast origin (Lütkepohl, 2005) , if taking it into account results in an improved forecast accuracy, with web-based leading indicators thus effectively Granger-causing (Granger, 1969) actual tourism demand. Particularly during the first half of the COVID-19 pandemic, which was characterised by global travel restrictions and tourism businesses being locked down, scenario forecasting and other forms of hybrid and judgmental forecasting played an important role, thereby highlighting an important limitation of quantitative tourism demand forecasting as currently practised. Based on the rapid development of information technology and artificial intelligence, Li and Jiao (2020) , however, envisage a "super-smart tourism forecasting system" (Li and Jiao, 2020, p. 264) for the upcoming 75 years of tourism demand forecasting. According to these authors, this system will be able to automatically produce forecasts at the micro level (i.e., for the individual traveller and tourism business) in real time while drawing on a multitude of data sources and integrating multiple (self-developing) forecast models. Airports have long invested in forecasting arrivals and departures of aircrafts. These forecasts are important in measuring airspace and airport congestions, designing flight schedules, and planning for the assignment of stands and gates (Barnhart and Cohn, 2004 Passenger-centric forecasting problems have received some attention in the literature, particularly in the past decade. Wei and Hansen (2006) build an aggregate demand model for air passenger traffic in a hub-and-spoke network. The model is a log-linear regression that uses airline service variables such as aircraft size and flight distance as predictors. Barnhart et al. (2014) develop a multinomial logit regression model, designed to predict delays of US domestic passengers. Their study also uses data from the US Department of Transportation (Bureau of Transportation Statistics, 2020). Guo et al. (2020) recently develop a predictive system that generates distributional forecasts of connection times for transfer passengers at an airport, as well as passenger flows at the immigration and security areas. Their approach is based on the application of regression trees combined with copula-based simulations. This predictive system has been implemented at Heathrow airport since 2017. With an increasing amount of available data that is associated with activities in the aviation industry, predictive analyses and forecasting methods face new challenges as well as opportunities, especially in regard to updating forecasts in real time. The predictive system developed by Guo et al. (2020) is able to generate accurate forecasts using real-time flight and passenger information on a rolling basis. The parameters of their model, however, do not update over time. Therefore, a key challenge in this area is for future studies to identify an efficient way to dynamically update model parameters in real time. Traffic flow forecasting is an important task for traffic management bodies to reduce traffic congestion, perform planning and allocation tasks, as well as for travelling individuals to plan their trips. Traffic flow is complex spatial and time-series data exhibiting multiple seasonalities and affected by spatial exogenous influences such as social and economic activities and events, various government regulations, planned road works, weather, traffic accidents, etc. (Polson and Sokolov, 2017) . Methods to solve traffic flow forecasting problems vaguely fall into three categories. The first uses parametric statistical methods such as ARIMA, seasonal ARIMA, space-time ARIMA, Kalman filters, etc. (see, for example, Whittaker et al., 1997; Vlahogianni et al., 2004; Kamarianakis and Prastacos, 2005; Vlahogianni et al., 2014) . The second set of approaches uses purely of neural networks (Mena-Oreja and Gozalvez, 2020). The third group of methods uses various machine learning, statistical non-parametric techniques or mixture of them (see, for example, Hong, 2011; Zhang et al., 2016 Zhang et al., , 2017 . Although neural networks are probably the most promising technique for traffic flow forecasting (see, for example, Polson and Sokolov, 2017; Do et al., 2019) , statistical techniques, such as Seasonal-Trend decomposition based on Regression (STR), can outperform when little data is available or they can be used for imputation, de-noising, and other pre-processing before feeding data into neural networks which often become less powerful when working with missing or very noisy data. figure 15 and the magnified forecast and the forecasting errors are on figure 16. Forecasting of inbound call arrivals for call centres supports a number of key decisions primarily around staffing (Aksin et al., 2007) . This typically involves matching staffing level requirements to service demand as summarized in Figure 17 . To achieve service level objectives, an understanding of the call load is required in terms of the call arrivals (Gans et al., 2003) . As such, forecasting of future call volume or call arrival rates is an important part of call centre management. There are several properties to call arrival data. Depending on the level of aggregation and the frequency with which data is collected, e.g., hourly, call arrival data may exhibit intraday (within-day), in- traweek, and intrayear multiple seasonal patterns (Avramidis et al., 2004; Brown et al., 2005b) . In addition, arrival data may also exhibit interday and intraday dependencies, with different time periods within the same day, or across days within the same week, showing strong levels of autocorrelation (Tanir and Booth, 1999; Brown et al., 2005b; Shen and Huang, 2005) . Call arrivals may also be heteroscedastic with variance at least proportional to arrival counts (Taylor, 2008) , and overdispersed under a Poisson assumption having variance per time period typically much larger than its expected value (Jongbloed and Koole, 2001; Avramidis et al., 2004; Steckley et al., 2005) . These properties have implications for various approaches to modelling and forecasting call arrivals. The first family of methods are time series methods requiring no distributional assumptions. Early studies employed auto regressive moving average (ARMA) models (Andrews and Cunningham, 1995; Tandberg et al., 1995; Xu, 1999; Antipov and Meade, 2002) , exponential smoothing (Bianchi et al., 1993 (Bianchi et al., , 1998 , fast Fourier transforms (Lewis et al., 2003) , and regression (Tych et al., 2002) . The first meth- ods capable of capturing multiple seasonality were evaluated by Taylor (2008) and included double seasonal exponential smoothing (Taylor, 2003b) and multiplicative double seasonal ARMA (SARMA). Since then several advanced time series methods have been developed and evaluated (Taylor, 2010; De Livera et al., 2011; Taylor and Snyder, 2012) , including artificial neural networks (Millán-Ruiz and Hidalgo, 2013; Pacheco et al., 2009; and models for density forecasting (Taylor, 2012) . Another family of models relies on the assumption of a time-inhomogeneous Poisson process adopting fixed (Jongbloed and Koole, 2001; Brown et al., 2005b; Shen and Huang, 2008a; Taylor, 2012) and mixed modelling (Avramidis et al., 2004; Aldor-Noiman et al., 2009; Ibrahim and L'Ecuyer, 2013) approaches to account for the overdispersed nature of the data, and in some cases, interday and intraday dependence. The works by Weinberg et al. (2007) and Soyer and Tarimcilar (2008) model call volumes from a Bayesian point of view. Other Bayesian inspired approaches have been adopted mainly for estimating various model parameters, but also allowing for intraday updates of forecasts (Landon et al., 2010; Aktekin and Soyer, 2011) . A further class of approach addresses the dimensionality challenge related to high frequency call data using Singular Value Decomposition (SVD). Shen and Huang (2005) and Shen and Huang (2008a) use the same technique to achieve dimensionality reduction of arrival data, and to create a forecasting model that provides both interday forecasts of call volume, and an intraday updating mechanism. Several further studies have extended the basic SVD approach to realise further modelling innovations, for example, to forecast call arrival rate profiles and generate smooth arrival rate curves (Shen et al., 2007; Shen and Huang, 2008b; Shen, 2009) . A more comprehensive coverage of different forecasting approaches for call arrival rate and volume can be found in a recent review paper by Ibrahim et al. (2016) . With the exception of weather forecasts, there are few forecasts which have as much public exposure as election forecasts. They are frequently published by mass media, with their number and disclosure reaching a frenzy as the Election Day approaches. This explains the significant amount of methods, approaches and procedures proposed and the paramount role these forecasts play in shaping people's confidence in (soft/social) methods of forecasting. The problem escalates because, regardless whether the goal of the election forecast is an attempt to ascertain the winner in two-choice elections (e.g., a referendum or a Presidential election) or to reach estimates within the margins of error in Parliamentary systems, the knowledge of the forecasts influences electors' choices (Pavía et al., 2019) . Election forecasts not only affect voters but also political parties, campaign organizations and (international) investors, who are also watchful of their evolution. Scientific approaches to election forecasting include polls, information (stock) markets and statistical models. They can also be sorted by when they are performed; and new methods, such as social media surveillance, are also emerging (Huberty, 2015; Ceron et al., 2016) . Probabilistic (representative) polls are the most commonly used instrument to gauge public opinions. The progressive higher impact of non-sampling errors (coverage issues, non-response bias, measurement error: Biemer, 2010) is, however, severely testing this approach. Despite this, as Kennedy et al. (2017) show in a recent study covering 86 countries and more than 500 elections, polls are still powerful and robust predictors of election outcomes after adjustments (see, also, Jennings et al., 2020) . The increasing need of post-sampling adjustments of probabilistic samples has led to a resurgence of interest in non-probabilistic polls (Pavía and Larraz, 2012; Wang et al., 2015b; Elliott and Valliant, 2017) , abandoned in favour of probabilistic sampling in 1936, when Gallup forecasted Roosevelt's triumph over Landon using a small representative sample despite Literacy Digest failing to do so with a sample of near 2.5 million responses (Squire, 1988) . A person knows far more than just her/his voting intention (Rothschild, 2009 ) and when s/he makes a bet, the rationality of her/his prediction is reinforced because s/he wants to win. Expectation polls try to exloit the first issue (Graefe, 2014) , while prediction markets, as efficient aggregators of information, exploit both these issues to yield election forecasts. Several studies have proven the performance of these approaches (Wolfers and Zitzewitz, 2004; Berg et al., 2008; Erikson and Wlezien, 2012; Williams and Reade, 2016) , even studying their links with opinion polls . Practice has also developed econometric models (Fair, 1978) that exploit structural information available months before the election (e.g., the evolution of the economy or the incumbent popularity). Lewis-Beck has had great success in publishing dozens of papers using this approach (see, e.g., Lewis-Beck, 2005) . Special mention also goes to Election-Day forecasting strategies, which have been systematically commissioned since the 1950s (Mitofsky, 1991) . Exit (and entrance) polls (Pavía, 2010; Klofstad and Bishin, 2012) , quick-counts (Pavía-Miralles and Larraz-Iribas, 2008) , and statistical models (Moshman, 1964; Bernardo, 1984; Pavía-Miralles, 2005) have been used to anticipate outcomes on Election Day. Some of these strategies (mainly random quick-counts) can be also employed as auditing tools to disclose manipulation and fraud in weak democracies (Scheuren and Alvey, 2008) . Forecasting is inherent to sport. First in this section, we look at forecast competitions in sport, and following this we consider the role forecasts play in sporting outcomes. Forecast competitions are common; see section 3.8.13. Sport provides a range of forecast competitions, perhaps most notably the competition between bookmakers and their customers -betting. A bet is a contingent contract, a contract whose payout is conditional on specified future events occurring. Bets occur fundamentally because two agents disagree about the likelihood of that event occurring, and hence it is a forecast. Bookmakers have been extensively analysed as forecasters; Forrest et al. (2005) evaluated biases in the forecasts implied by bookmaker odds over a period where the betting industry became more competitive, and found that relative to expert forecasts, bookmaker forecasts improved. With the internet age, prediction markets have emerged, financial exchanges where willing participants can buy and sell contingent contracts. In theory, such decentralised market structures ought to provide the most efficient prices and hence efficient forecasts (Nordhaus, 1987) . A range of papers have tested this in the sporting context (Gil and Levitt, 2007; Croxson and Reade, 2014 Sport is a spectacle, and its commercial success is conditioned on this fact. Hundreds of millions of people globally watch events like the Olympics and the FIFA World Cup -but such interest is conditioned on anticipation, a forecast that something interesting will happen. A superstar is going to be performing, the match will be a close encounter, or it will matter a lot for a bigger outcome (the championship, say). These are the central tenets of sport economics back to Neale (1964) and Rottenberg (1956) , most fundamentally the 'uncertainty of outcome hypothesis'. Cities and countries bid to host large events like the World Cup based on forecasts regarding the impact of hosting such events. Forecasts that are often inflated for political reasons (Baade and Matheson, 2016) . Equally, franchise-based sports like many North American sports attract forecasts regarding the impact of a team locating in a city, usually resulting in public subsidies for the construction of venues for teams to play at (Coates and Humphreys, 1999) . Governments invest in sporting development, primarily to achieve better performances at global events, most notably the Olympics (Bernard and Busse, 2004) . Many sporting events themselves rely on forecasts to function; high jumpers predict what height they will be able to jump over, and free diving contestants must state the depth they will dive to. Less formally, teams will set themselves goals: to win matches, to win competitions, to avoid the 'wooden spoon'. Here, forecast outcomes are influenced by the teams, and competitors, taking part in competitions and, as such, are perhaps less commonly thought of as genuine forecasts. Megaprojects are significant activities characterised by a multi-organisation structure, which produces highly visible infrastructure or asset with very crucial social impacts (Aaltonen, 2011) . Megaprojects are complex, require huge capital investment, several stakeholders are identified and, usually a vast number of communities and the public are the receivers of the project's benefits. There is a need megaprojects especially those that deliver social and economic goods and create economic growth (Flyvbjerg et al., 2003) . Typical features of megaprojects include some or all the following: (i) delivering a substantial piece of physical infrastructure with a life expectancy that spans across decades, (ii) main contractor or group of contractors are privately owned and financed, and (iii) the contractor could retain an ownership stake in the project and the client is often a government or public sector organisation (Sanderson, 2012) . However, megaprojects are heavily laced with extreme human and technical complexities making their delivery and implementation difficult and often unsuccessful (The RFE Working Group Report, 2015; Merrow et al., 1988) . This is largely due to the challenge of managing megaprojects including extreme complexity, increased risk, tight budget and deadlines, lofty ideals (Fiori and Kovaka, 2005) . Due to the possibility and consequences of megaproject failure (Mišić and Radujković, 2015) , forecasting the outcomes of megaprojects is becoming of growing importance. In particular, it is crucial to identify and assess the risks and uncertainties as well as other factors that contribute to disappointing outcomes of megaprojects in order to mitigate them (Flyvbjerg et al., 2003; Miller and Lessard, 2007) . Literature review in forecasting in megaprojects are scarce. However, there are a few themes that have emerged in the extant literature as characteristics of megaprojects that should be skilfully managed to provide a guideline for the successful planning and construction of megaprojects (Fiori and Kovaka, 2005; Flyvbjerg, 2007; Sanderson, 2012) . Turner and Zolin (2012) even claim that we cannot even properly define what success is. They argue that we need to reliable scales in order to predict multiple perspectives by multiple stakeholders over multiple time frames -so definitely a very difficult long term problem. This could be done via a set of leading performance indicators that will enable managers of Megaprojects to forecast during project execution how various stakeholders will perceive success months or even years into the operation. At the very early stages of a project's lifecycle, a number of decisions must been taken and are of a great importance for the performance and successful deliverables/outcomes. Flyvbjerg (2007) stress the importance of the front-end considerations particularly for Megaprojects Failure to account for unforeseen events frequently lead to cost overruns. Litsiou et al. (2019) suggest that forecasting the success of megaprojects is particularly a challenging and critical task due to the characteristics of such projects. Megaproject stakeholders typically implement impact assessments and/or cost benefit Analysis tools (Litsiou et al., 2019) . As Makridakis et al. (2010) suggested, judgemental forecasting is suitable where quantitative data is limited, and the level of uncertainty is very high; elements that we find in megaprojects. By comparing the performance of three judgemental methods, unaided judgement, semi-structured analogies (sSA), and interaction groups (IG), used by a group of 69 semi-experts, Litsiou et al. (2019) found that, the use of sSA outperforms unaided judgment in forecasting performance. The difference is amplified further when pooling of analogies through IG is introduced. Based on IMS-Health quarterly number of cimetidine and ranitidine packages sold in Italy, the CRCD model was tested to evaluate a diachronic competition that produced substitution. Cimetidine is a histamine antagonist that inhibits the production of stomach acid and was introduced by Smith, Kline & French in 1976. Ranitidine is an alternative active principle introduced by Glaxo in 1981 and was found to have far-improved tolerability and a longer-lasting action. The main effect in delayed competition is that the first compound spread fast but was suddenly outperformed by the new one principle that modified its stand-alone regime. Guseo and Mortarino (2012) give some statistical comparisons with the restricted model Krishnan-Bass-Kummar Diachronic model (KBKD) by Krishnan et al. (2000) . Previous results were improved with the UCRCD model in Guseo and Mortarino (2014) by considering a decomposition of word-of-mouth (WOM) effects in two parts: within-brand and cross-brand contributions. The new active compound exploited a large cross-brand WOM and a positive within-brand effect. After the start of competition, cimetidine experienced a negative WOM effect from its own adopters and benefited from the increase of the category's market potential driven by the antagonist. Forecasting is more realistic with the UCRCD approach. Restricted and unrestricted UCRCD models were applied in Germany by Guidolin and Guseo (2016) to the competition between nuclear power technologies and renewable energy technologies (wind and solar) in electricity production. Due to the 'Energiewende' policy started around 2000, the substitution effect, induced by competition, is confirmed by the electricity production data provided by BP 153 . An advance is proposed in Furlan et al. (2020) with three competitors and exogenous control functions obtaining direct inferences that provide a deeper analysis and forecasting improvements in energy transition context. petition models suggested a more modulated access to the residual carrying capacity. The Lotka-Volterra with churn model (LVch) by Guidolin and Guseo (2015) represents 'churn effects' preserving within and cross-brand effects in a synchronic context. LVch includes unrestricted UCRCD model. An application of LVch model is discussed with reference to the competition/substitution between compact cassettes and compact discs for pre-recorded music in the US market. Obtained results of LVch out-152 This subsection was written by Renato Guseo. 153 https://www.bp.com/en/global/corporate/energy-economics/statistical-review-of-world-energy.html (Accessed: 2020-09-01) perform restricted and unrestricted UCRCD analyses. In this context the residual market is not perfectly accessible to both competitors and this fact, combined with WOM components, allows for better interpretation and forecasting. A special application of the LVch model, Lotka-Volterra with asymmetric churn (LVac), is proposed in Guidolin and Guseo (2020) . It is based on a statistically oriented reduction: The late entrant behaves as a standard Bass (1969) model that modifies the dynamics and the evolution of the first entrant in a partially overlapped market. The case study is offered by a special form of competition where the iPhone produced an inverse cannibalisation of the iPad. The former suffered a local negative interaction with some benefits: A long-lasting life cycle and a larger market size induced by the iPad. A limitation in models for diachronic competition relate to high number of rivals, implying complex parametric representations with respect to the observed information. A second limitation, but also an opportunity, is the conditional nature of forecasting if the processes partially depend upon exogenous control functions (new policy regulations, new radical innovations, regular and promotional prices, etc.). These tools may be used to simulate the effect of strategic interventions, but a lack of knowledge of such future policies may affect prediction. Safety and security are among the most pressing and complex tasks of modern time as challenges such as extreme environmental conditions, system faults and malfunctions, fraud activities and disease outbreaks can cause significant harm to valuable lives and assets if not detected and treated quickly (Ben-Gal, 2005) . The problem of anomaly detection in environmental sensors are twofold: (i) identifying data issues that make the data unreliable and untrustworthy (Hand, 2009; Sobhani et al., 2020) , and (ii) identifying real unexpected events such as extreme weather conditions, water contamination. Early detection of anomalies in data will limit the use of corrupted data in subsequent forecasting applications such as water-quality monitoring and early warning systems (Storey et al., 2011) , predicting algal bloom outbreaks leading to fish kill events and potential human health impacts, forecasting water level and currents (Archer et al., 2003; Glasgow et al., 2004) . A winning team of the Global Energy Forecasting Competition 2014 (GEFCom2014) also incorporated a model-based approach for anomaly detection and data cleansing (Xie and Hong, 2016) . Talagala et al. (2019) combine data features and an approach based on extreme value theory to identify anomalies in water quality data. Hong et al. (2010) also discussed the data quality issues associated with the historical weather data and their impact on short term load forecasting. With rapid advances in data collection technology, cybersecurity has become a major concern in the modern world (Singer and Friedman, 2014) . Among various cybersecurity issues, data integrity attacks, where unauthorised parties access protected or confidential data and inject misinformation, are of great interest to the forecasting community as data quality has a direct impact on forecast accuracy. These misinformation can cause over-forecasts that demand unnecessary expenses for the upgrade and maintenance of infrastructure (Luo et al., 2018a) . According to Sobhani et al. (2020) , a 1% improvement in forecast accuracy can help a large power company to save millions of dollars. Luo et al. (2018a) and Luo et al. (2018b) address the problem of load forecasting under data integrity attacks using model based approaches. In intrusion detection systems, on the other hand, anomalies in system events might indicate a possible breach of security (Warrender et al., 1999) . The feature based approach proposed by Talagala et al. (2020b) has the ability to deal with such data challenges. Efficient forecasting models for early detection of disease outbreaks are a must for modern surveillance. Outbreaks of interest include bioterrorist-attacks, laboratory releases, naturally occurring epidemics such as the coronavirus pandemic and the avian influenza outbreak. Analysis of the surveillance data in a timely manner is essential for rapid epidemiological response and thereby reduce morbidity and mortality (Lotze and Shmueli, 2009 ). However, early detection of disease outbreaks are challenging as different diseases cause different signals and patterns and they are clearly visible mostly during the later stage of an epidemic (Wagner et al., 2001) . According to Lotze and Shmueli (2009) , most existing temporal detection methods for bio-surveillance use forecasting component followed by a residual analysis as in (Hutwagner et al., 2003; Lombardo et al., 2004) . The forecasting of agricultural time series falls under the broader group of forecasting commodities, of which agricultural and related products are a critical subset. While there has been considerable work in the econometrics and forecasting literature on common factor models in general there is surprisingly little work so far on the application of such models for commodities and agricultural time series -and this is so given that there is considerable literature in the linkage between energy and commodities, including agricultural products, their prices and futures prices, their returns and volatilities. Furthermore, a significant number of papers is fairly recent which indicates that there are many open avenues of future research on these topics, and in particular for applied forecasting. The literature on the latter connection can consider many different aspects in modelling as we illustrate below. We can identify two literature strands, a much larger one on the various connections of energy with commodities and the agricultural sector (and in this strand we include forecasting agricultural series) and a smaller one that explores the issue of common factors. An early reference of the impact of energy on the agricultural sector is Tewari (1990) and then after a decade we find Gohin and Chantret (2010) There is clearly room for a number of applications in the context of this recent research, such along the lines of further identifying and then using common factors in constructing forecasting models, exploring the impact of the COVID-19 crisis in agricultural production or that of climate changes on agricultural prices. Espresso coffee is among the most popular beverages, and its quality is one of the most discussed and investigated issues. Besides human-expert panels, electronic noses, and chemical techniques, forecasting the quality of an espresso by means of data-driven approaches, such as association rule mining, is an emerging research topic . In this section, a forecasting model of the espresso quality is presented as the result of a data-driven analysis applied to a real-world data set of espresso brewing by professional coffee-making machines. Coffee ground size, coffee ground amount, and water pressure have been selected among the most influential external variables. The ground-truth quality evaluation has been performed for each shot of coffee based on three well-known quality variables selected by domain experts and measured by specific sensors: the extraction time, the average flow rate, and the espresso volume. An exhaustive set of more than a thousand coffees has been produced to train a model able to forecast the effect of non-optimal values on the espresso quality. For each external variable, different categorical values are considered: ground size can be coarse, optimal, or fine; ground amount can be high, optimal, or low; brewing water pressure can be high, optimal, or low. The experimental setting of categorical variables allows us to apply association rule mining, a powerful data-driven exhaustive and explainable approach (Han et al., 2011; Tan et al., 2005) , successfully exploited in different application contexts (Acquaviva et al., 2015; Di Corso et al., 2018) . Association rule mining parameters minimum support and minimum confidence are set to very low values, i.e., 0.5% and 10% respectively, so that all possible correlations are extracted. Then, a sorting is performed so that the most interesting rules are presented and included in the forecasting model: the lift and the confidence metrics have been used to this aim. Among the most interesting results, we highlight the following. If the water pressure is low, the amount of coffee ground is too high, and the grinding is fine, then we can forecast a low-quality coffee due to excessive percolation time, with a confidence near to 100%. If the amount of coffee ground is low, the ground is coarse, and the pressure is high, then we can forecast a low-quality coffee due to excessive flow rate. Furthermore, the coarseness of coffee ground generates an excessive flow rate forecast, despite the optimal values of dosage and pressure, with a very high confidence (near to 100%). In most of the cases (83%), the espresso volume is in the optimal-quality range. Espresso machines are in fact designed and calibrated to extract a pre-determined amount of coffee. Hence, the data-driven forecasting model almost never provides an espresso-volume quality warning. In most of the cases, the espresso machines produce an adequate amount of coffee even in non-optimal conditions. It is therefore extremely important the control of the other quality indexes (percolation time and flow rate) to forecast the coffee quality, since they strongly impact on the final flavour and body of the espresso despite the optimal volume. The most fascinating aspect of S curve fitting is the ability to predict from early measurements the final ceiling. This very fact, however, constitutes also the fundamental weakness and the major criticism of these forecasts because logistic fits on early data can often accommodate very different values for the final ceiling. Debecker and Modis (1994) have carried out an extensive simulation study aiming to quantify the uncertainties on the parameters determined by logistic fits. The study was based on 35,000 S curve fits on simulated data, smeared by random noise and covering a variety of conditions. The fits were carried out via a χ 2 minimisation technique. The study produced lookup tables and graphs for determining the uncertainties expected on the three parameters of the logistic function. A frequent point of misunderstanding and confusion is whether a forecaster should fit an S curve to the cumulative number or to the number per unit of time. Here the forecaster must exercise wise judgment. What is the "species" and what is the niche that is being filled? To the frustration of business people there is no universal answer. When forecasting the sales of a new product it is often clear that one should fit the cumulative sales because the product's market niche is expected to eventually fill up. But if we are dealing with something that is going to stay with us for a long time (e.g., the Internet or a smoking habit), then one should not fit cumulative numbers. At times this distinction may not be so obvious. For example at the appearance of COVID-19 many people (often amateurs) began fitting S curves to the cumulative number of infections. Some of them were rewarded because indeed the diffusion of the virus in some countries behaved accordingly (Debecker and Modis, 2020) . But many were frustrated and tried to "fix" the logistic equation by introducing more parameters, or simply gave up on trying to use logistics with COVID-19. And yet, there are special situations (e.g., the US), which can be illuminated by logistic fits but on the daily number of infections, not on the cumulative number. As of August 1, 2020, leaving out the three eastern states that had gotten things under control, the rest of the US displayed two classic S curve steps followed by plateaus (see figure 18 ). The two plateaus reflect the number of infections that American society was willing to tolerate at the time, as the price to pay for not applying measures to restrict the virus diffusion. No matter what fitting program one uses, the fitted S curve will flatten toward a ceiling as early and as low as it is allowed by the constraints of the procedure. As a consequence fitting programs may yield logistic fits that are often biased toward a low ceiling. Bigger uncertainties on the data points accentuate this bias by permitting larger margins for the determination of the S curve parameters. To compensate for this bias the user must explore several fits with different weights on the data points during the calculation of the χ 2 . He or she should then keep the answer that gives the highest ceiling for the S curve (most often obtained by weighting more heavily the recent historical data points). Of course, this must be done with good justification; here again the forecaster must exercise wise judgment.  Forecasting competitions provide a "playground" for academics, data scientists, students, practitioners, and software developers to compare the forecasting performance of their methods and approaches against others. Organisers of forecasting competitions test the performance of the participants' submissions against some hidden data, usually the last window of observations for each series. The benefits from forecasting competitions are multifold. Forecasting competitions (i) motivate the development of innovative forecasting solutions, (ii) provide a deeper understanding of the conditions that some methods work and others fail, (iii) promote knowledge dissemination, (iv) provide a much-needed, explicit link between theory and practice, and (v) leave as a legacy usable and well-defined data sets. Participation in forecasting competitions is sometimes incentivised by monetary prizes. However, the stakes are usually much higher, including reputational benefits. The most famous forecasting competitions are the ones organised by Spyros Makridakis. Initially, the research question focused on the relative performance of simple versus complex forecast. M and M3 competitions (Makridakis et al., 1982; Makridakis and Hibon, 2000) empirically showed that simple methods (such as exponential smoothing) are equally good compared to other more complex methods and models (such as ARIMA and neural networks) in point-forecast accuracy -if not better. Moreover, the early Makridakis competitions showed the importance of forecast combinations in increasing predictive accuracy. For example, the winner of the M3 competition was the Theta method, a simple statistical method that involved the combination of linear regression and simple exponential smoothing forecasts (Assimakopoulos and Nikolopoulos, 2000) . The M4 competition (Makridakis et al., 2020c) challenged researchers and practitioners alike with a task of producing point forecasts and prediction intervals for 100 thousand time series of varied frequencies. This time, the main hypothesis focused on the ability of machine learning and neural network approaches in the task of time series forecasting. Machine learning approaches that focused on each series independently performed poorly against statistical benchmarks, such as Theta, Damped exponential smoothing or simple averages of exponential smoothing models. However, the best two performing submissions in the M4 competition (Smyl, 2020; Montero-Manso et al., 2020) used neural network and machine learning algorithms towards utilising cross-learning. So, the main learning outcome from the M4 competition is that, if utilised properly, machine learning can increase the forecasting performance. Similarly to previous competitions, M4 demonstrated again the usefulness of combining across forecasts, with five out of the top six submissions offering a different implementation of forecast combinations. Several other forecasting competitions focused on specific contexts and applications. For example, M2 competition (Makridakis et al., 1993) suggested that the benefits from additional information (domain expertise) are limited. The tourism forecasting competition also showed that exogenous variables do not add value, while naive forecasts perform very well on a yearly frequency. The NN3 competition confirmed the superior performance of statistical methods, but noted that neural network approaches are closing the distance. Tao Hong's series of energy competitions (Hong et al., 2014 demonstrated best practices for load, price, solar, and wind forecasting, with extensions to probabilistic and hierarchical forecasts. Finally, many companies have hosted forecasting challenges through the Kaggle platform. Bojer and Meldgaard (2020) reviewed the Kaggle competitions over the last five years, and concluded that access to hierarchical information, cross-learning, feature engineering, and combinations (ensembles) can lead to increased forecasting performance, outperforming traditional statistical methods. These insights can be a forerunner to the results of the M5 competition. 3.9. The future of forecasting practice 159 Plus ça change, plus c'est la même chose. Jean-Baptiste Karr (1849) It would be a more straightforward task to make predictions about the future of forecasting practice if we had a better grasp of the present state of forecasting practice. For that matter, we lack even a common definition of forecasting practice. In a recent article, Makridakis et al. (2020a) lamented the failure of truly notable advances in forecasting methodologies, systems, and processes during the past decades to convince many businesses to adopt systematic forecasting procedures, leaving a wide swath of commerce under the guidance of ad hoc judgment and intuition. At the other extreme, we see companies with implementations that combine state-of-the-art methodology with sophisticated accommodations of computing time and costs, as well as consideration of the requirements and capabilities of a diverse group of stakeholders (Yelland et al., 2019) . So, it is not hyperbole to state that business forecasting practices are all over the place. What surely is hyperbole, however, are the ubiquitous claims of software providers about their products accurately forecasting sales, reducing costs, integrating functions, and elevating the bottom line Sorensen, 2020) . For this section, we grilled a dozen practitioners and thought leaders ("the Group") about developments playing out in the next decade of forecasting practice, and have categorised their responses: • Nature of forecasting challenges; • Changes in the forecasting toolbox; • Evolution in forecasting processes such as integration of planning functions; • Expectations of forecasters; and • Scepticism about real change. Forecasting Challenges: Focusing on operations, the Group sees demand forecasting becoming ever more difficult due to product/channel proliferation, shorter lead times, shorter product histories, and spikes in major disruptions. • Operational forecasts will have shorter forecast horizons to increase strategic agility required by business to compete, sustain, and survive. • New models will need to incorporate supply-chain disruption. Demand chains will need to be restarted, shortening historical data sets and making traditional models less viable due to limited history. • Lead times will decrease as companies see the problems in having distant suppliers. Longer lead times make accurate forecasting more difficult. Forecasting Tool Box: Unsurprisingly, this category received most of the Group's attention. All predict greater reliance on AI/ML for automating supply-and-demand planning tasks and for reconciling discrepancies in hierarchical forecasting. Longer-horizon causal forecasting models will be facilitated by big data, social media, and algorithmic improvements by quantum computing. Post-COVID, we will see a greater focus on risk management/mitigation. The Cloud will end the era of desktop solutions. • Quantum computers will improve algorithms used in areas like financial forecasting (e.g., Monte Carlo simulations), and will change our thinking about forecasting and uncertainty. • Although social media is a tool for "what's trending now", new models will be developed to use social-media data to predict longer-term behaviour. Step aside Brown (exponential smoothing) and Bass (diffusion). • Greater automation of routine tasks (data loading, scrubbing, forecast generation and tuning, etc.) through AI/ML-powered workflow, configurable limits, and active alerts. More black box under the hood, but more clarity on the dashboard. • Greater focus on risk management/mitigation through what-if scenarios, simulations, and probabilistic forecasting. Forecasting Processes and Functional Integration: Systems will become more integrated, promoting greater collaboration across functional areas and coordination between forecast teams and those who rely upon them. Achieving supply-chain resilience will become as important as production efficiency, and new technology such as Alert and Root Cause Analysis systems will mitigate disruptions. • S&OP will expand from its home in operations to more fully integrate with other functions such as finance and performance management, especially in larger multinationals. • The pandemic has forced firms to consider upping supply-chain resilience. Firms are building in capacity, inventory, redundancy into operations-somewhat antithetical to the efficiency plays that forecasting brings to the table. • Forecasting will be more closely tied to Alert and Root Cause Analysis systems, which identify breakdowns in processes/systems contributing to adverse events, and prevent their recurrence. Expectations of Forecasters: Agreement was universal that the forecaster's job description will broaden and become more demanding, but that technology will allow some redirection of effort from producing forecasts to communicating forecasting insights. • The interest around disease models increases our awareness of the strengths and weaknesses of mathematical models. Forecasters may need to become more measured in their claims, or do more to resist their models being exploited. • We will see a transformation from demand planner to demand analyst, requiring additional skill sets including advanced decision making, data and risk analysis, communication, and negotiation. • Professional forecasters will be rare except in companies where this expertise is valued. Fewer students are now educated or interested in statistical modelling, and time is not generally available for training. • Forecasters will learn the same lesson as optimisation folks in the 1990s and 2000s: the importance of understanding the application area-community intelligence. Scepticism: Many were sceptical about the current enthusiasm for AI/ML methods; disappointed about the slow adoption of promising new methods into software systems and, in turn, by companies that use these systems; and pessimistic about the respect given to and influence of forecasters in the company's decision making. • While AI/ML are important additions to the forecaster's toolbox, they will not automatically solve forecasting issues. Problems include data hunger, capacity brittleness, dubious input data, fickle trust by users (Kolassa, 2020c) , and model bias. • Practices in the next decade will look very similar to the present. Not that much has changed in the last decade, and academic developments are slow to be translated into practice. • Politics, gaming, and the low priority given to forecasting are the prime drivers of practice, thus limiting interest in adopting new methodologies. • None of the topical items (AI/ML, big data, demand sensing, new forecasting applications) will have much of an impact on forecasting practice. Forecasting departments hop from one trend to the other without making much progress towards better forecasting accuracy. • Software companies will struggle, despite good offerings. Most companies do not want to invest in excellent forecasting engines; whatever came with their ERP system is "good enough". • Forecasting will continue to suffer from neglect by higher levels of management, particularly when forecasts are inconveniently contrary to the messages management hopes to convey. Mr. Buffett said his advice for the cash left to his wife was that 10 per cent should go to short-term government bonds and 90 per cent into a very low-cost S&P 500 index fund. In this paper, there are more than 140 sections and subsections written by 74 of some of the bestknown forecasting researchers and practitioners in the world, making it into a selective, encyclopedic piece covering, into a single source, a great deal of the available knowledge about the theory and practice of forecasting. But some people argue if there is any value in attempting to predict the future and if forecasting is any different than fortune telling, given the large numbers of mistaken forecasts made in the past, including our inability to predict the progression of COVID-19 and its economic and human consequences? What is, therefore, the usefulness of a paper like the present one when crystal balling is not possible, and uncertainty reigns? It is the aim of this concluding article to set the record straight, explaining the benefits and practical value of forecasting while reporting its limitations too. The Myriad of Forecasts: All planning and the great majority of decisions we make require forecasting. Deciding what time to get up in the morning, not to be late for work implies a correct prediction of the commuting time to go to the office. Determining what to study is another decision requiring elaborate predictions about the demand for future jobs decades away. In the business world, firms must decide/forecast how many products to manufacture, the price they should be sold, how much money to spend on advertising and promotion, how much and in what type of new technologies to invests and a plethora of other future-oriented decisions requiring both predictions and assessing their inevitable uncertainty. Whether we like it or not, we have no choice but making these forecasts to benefit as much as possible from their value, knowing perfectly well that all predictions are uncertain and many turn out to be wrong. The Pervasiveness of Uncertainty: Apart from some areas of hard sciences, all other forecasts are uncertain and must be accompanied with a measure of its magnitude, expressed as a prediction interval, or as a probability distribution around the most likely forecast. Although the value and usage of forecasts is clear, that of uncertainty is not. Worse, it becomes an unwelcome source of anxiety whose usefulness is misunderstood. Executives want to know the exact sales of their firm for next month to set up their production schedule. Instead, they are given prediction intervals (PIs) around such forecast and told that most of the time, sales will be within this interval, assuming that the fluctuations follow some distributional assumptions. They argue that forecasting must decrease, not amplify, uncertainty and that the PIs are too wide and 'uninformative' to be used for making practical business decisions. The trouble is that these PIs are based on past fluctuations and present the best estimation of future uncertainty, even if they seem too wide. Worse, empirical research has shown that they are actually too narrow, underestimating uncertainty often considerably. Assessing Uncertainty and Dealing with its Implied Risks: Uncertainty entails risks, requiring action to minimise their negative consequences. There are two kinds of uncertainty that can be illustrated by the commuting example. The first relates to fluctuations in the commuting time under normal driving conditions when there are no serious accidents, road works or major snowstorms. Such fluctuations are small and can be captured by a normal curve that allows to balance the risk of arriving earlier or later than the desired time. In the opposite case, uncertainty is fat-tailed and hard to estimate, as delays can be substantial depending upon the seriousness of the accident or that of the snowstorm while the risk of being early to work is smaller than being late. Moreover, such risk is substantially different when going to the airport to catch a flight, requiring starting much earlier than the average time it takes to go to the airport to minimise the risk of missing the flight. More Accurate Ways of Forecasting and Assessing Uncertainty: Extensive empirical research, including forecasting competitions, has shown that systematic approaches improve the accuracy of forecasting and the correct assessment of uncertainty resulting in substantial benefits when compared to ad-hoc judgmental alternatives . The biggest advantage of such approaches is their ability to identify and estimate, in a mathematically optimal manner, past patterns and relationships that are subsequently extrapolated to predict their continuation, avoiding the over optimism and wishful thinking associated with judgmental approaches. At the same time, it must be clear that the accuracy of the forecasts and the correctness of uncertainty will depend on the established patterns/relationship not changing much during the forecasting horizon. Using Benchmarks to Evaluate the Value of Forecasting: The accuracy of the forecasts and the correct assessment of uncertainty must be judged not on their own but in comparison to some simple, readily available benchmarks. In stock market forecasts, for instance, the accuracy of predictions is compared to that of today's price used as the forecast for future periods. Empirical comparisons have shown that such a benchmark beats the great majority of professional forecasters, hence Buffet's advice in the epigram for his wife to invest in a low-cost index fund that selects stocks randomly. In weather forecasting, meteorologists are judged by the improvement of their forecasts over the naive prediction that tomorrow's weather will be the same as today. Concluding remark: Accepting the advantages and limitations of systematic forecasting methods and most importantly avoiding any exaggerated expectations of what it can achieve is critical. Such methods do not possess any prophetic powers, they simply extrapolate established patterns and relationships to predict the future and assess its uncertainty. Their biggest advantage is their objectivity and ability for optimal extrapolation. Their biggest disadvantages are: (i) the patterns and the relationships must remain fairly constant during the forecasting phase for the forecasts to be accurate, and (ii) uncertainty must not be fat-tailed so it can be measured quantitatively. The views expressed in this paper are those of the authors and do not necessarily reflect the views of their affiliated institutions and organisations.  
