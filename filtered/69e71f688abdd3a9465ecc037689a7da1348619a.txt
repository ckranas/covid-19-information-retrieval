69e71f688abdd3a9465ecc037689a7da1348619a
Accepted abstracts for 39th International Symposium on Intensive Care and Emergency Medicine P001 Prognostic value of a genetic polymorphism of AQP5 in sepsis depends on a source of infection
V  Pisarev A  Chumachenko I  Tyurin R  Cherpakov A  Tutelyan 

Negovsky Institute of General Reanimatology Negovsky Institute of General Reanimatology 
Introduction: Increasing evidence supports a central role for "immunosuppression" in sepsis. It is necessary to develop biomarkers of immune dysfunction that could help to identify patients at risk of poor outcomes [1] . The decreased expression of human leucocyte antigen (HLA)-DRA is proposed as a major feature of immunodepression and its persistent decrease is associated with mortality in sepsis [2] . In a previous study, we evidenced that FCER1A (Fc Fragment Of IgE Receptor Ia) is the gene showing the lowest expression levels of the entire transcriptome in sepsis [3] . Here we studied the association between FCER1A expression and mortality in infected surgical patients. Methods: FCER1A and HLA-DRA expression levels were quantified by droplet digital PCR in blood of 257 infected surgical patients. 26 patients died within 28 days (10.11%). Spearman test was used to evaluate the association between gene expression and the Sequential Organ Failure Assessment (SOFA) score. Areas under Receiver Operating Curves (AUROC) were used to determine the gene expression cut-off values predicting mortality. Kaplan-Meier survival curves were obtained and differences in survival between groups were evaluated using the Log rank test. Cox regression was employed to assess mortality risk at 28 days. Results: Gene expression levels of FCER1A and HLA-DRA correlated inversely with patients' severity (r: -0.5 p<0.001; r: -0.3, p<0.001 respectively). Both genes showed significant AUROCs to predict survival, but FCER1A showed the best accuracy (Fig. 1) . Patients with Introduction: Severe pulmonary and renal conditions such as acute respiratory distress syndrome (ARDS), respiratory failure, and deterioration in kidney function often occur in patients with nosocomial pneumonia (NP). The emergence and course of infection is genetically determined, hence host genetic landscape may influence an ability to resist infection. Methods: Variants for genotyping were selected using the PheWAS Catalog which presents genotypic data for 13835 Caucasian patients, 1358 phenotypes and 3144 single nucleotide polymorphisms (SNPs) with P < 0.05 [1] . SNPs with the lowest P-values for phenotypes with both, respiratory and renal manifestations were selected: intergenic variants rs7130588 and rs4980785, rs347344 (EDIL3) and rs2470893 (CYP1A1). CYP1A1 gene was associated with pneumonia and ARDS in our previous investigations, so we included in our analysis three sites of CYP1A1 gene (rs2606345, rs4646903 and rs1048943) studied on a smaller sample. Genotyping was performed on 7 sites for a sample Results: Allele rs2606345-G of the CYP1A1 gene was protective against ARDS and an increase in creatinine level (Fig. 1) . The rs7130588-G allele was associated with lung complications and with the development of severe respiratory insufficiency (Fig. 2) . Conclusions: The SNPs rs2606345 and rs7130588 can influence the aggravation of pulmonary and renal symptoms through genetically mediated response to infection. Introduction: An uncontrolled inflammatory response plays a major role in the sepsis related organ dysfunction. Mesenchymal stem cells(MSCs) can improve survival of sepsis experimental models by modulating the inflammatory response. Macrophages have been considered as important immune effector cells and their polarization imbalance aggravates the disordered inflammation reaction. The project aims to identify the effects of MSCs on macrophages polarization against dysregulated inflammatory response. Methods: RAW264.7 cells were plated in the lower chambers of transwell system in the presence or absence of Lipopolysaccharide (LPS). Then, MSCs were seeded in the upper chambers and incubation for different time. Finally, transforming growth factor beta (TGFβ) receptor (TGF-βR) inhibitor was added in transwell system. The phenotype of RAW264.7 cells were analyzed by flow cytometry, the levels of inflammatory cytokines were detected by Enzyme-linked immunosorbent assay (ELISA). Results: Our data showed that LPS increased the level of interleukin (IL)-6 in RAW264.7 cells (p<0.001) (Fig. 1 ). In line with IL-6 expression, LPS induced the expression of M1 macrophage (p<0.001). Moreover, LPS stimulated RAW264.7 cells co-culture with MSCs in transwell system, MSCs inhibited the expression of IL-6 and M1 macrophages, while increased M2 macrophages (p<0.001). Compared with LPS group, the concentration of TGF-Β was obviously increased in MSCs treatment groups (p<0.001), furthermore, there were no significantly difference between MSCs directed and indicted groups. More significantly, TGF-βR inhibitor abolished the impact of MSCs on LPS stimulated RAW264.7 cells (p<0.001) (Fig. 2) . Conclusions: MSCs polarized M1 macrophages into M2 macrophages and decreased pro-inflammatory cytokine levels by paracrining TGF-β. Introduction: Sepsis is dysregulated response to an infection, which can lead to progressive microcirculatory dysfunction, release of reactive oxygen intermediates (ROI) and life-threatening organ dysfunction. Our aim was to investigate the relationship between organ damage -characterized by the Sequential Organ Failure Assessment (SOFA) scores, microcirculatory failure and ROI production, in a large animal model of experimental sepsis. Methods: Fecal peritonitis was induced in anesthetized minipigs (n=28; 0.5g/kg autfeces containing 5-9 x10 6 CFU bacteria i.p.), control animals (n=9) received sterile saline i.p. Invasive hemodynamic monitoring and blood gas analyses were performed between 16-24 hrs, the signs for failure of circulatory, respiratory and urinary systems were evaluated in accordance with the SOFA score. The microcirculatory perfusion rate in the sublingual region was measured by orthogonal polarization spectral imaging technique (Cytoscan A/R). The leukocyte-origin ROI production was determined by lucigenine (mostly O 2 -. ) and luminol-based (H 2 O 2 ) chemiluminescence methods. Results: Between 16-24 hrs after induction the SOFA score indicated moderate organ failure in 19 animals (M: 1.9; 25p: 1.5, 75p: 2.9) and the change was statistically significantly higher in 9 pigs, suggesting severe organ dysfunction (M: 4.1; 25p: 3.5, 75p: 5.2). The microcirculation was significantly deteriorated in all cases, independently of SOFA score data. The H 2 O 2 production was significantly lower in septic animals as compared to controls, while the lucigenine enhanced ROI production correlated with the SOFA score-indicated moderate and severe organ dysfunction. Conclusions: Sublingual microcirculatory parameters are not correlating with the severity of SOFA score-indicated organ dysfunction in abdominal sepsis. The measurement of ROI production of the whole blood seems to be better biomarker for the detection of the progression of events from moderate to severe organ damages. Introduction: The purpose of this study was to characterize differences in sepsis management in patients with and without left ventricular (LV) dysfunction. Septic patients with LV dysfunction have higher mortality, and limited guidance exists for sepsis management of patients with LV dysfunction. The possibility exists that the cornerstones of sepsis management may contribute to these poor outcomes. Methods: A retrospective chart review was conducted from May 2016 -January 2018 at two centers. Adult patients who had a diagnosis of sepsis, were treated with vasopressors for > 3 hours, and had an echocardiogram within 12 months were included. Patients were divided into two groups: reduced ejection fraction (EF) of < 40% and preserved EF defined as EF ≥40%. Information about patient outcomes and sepsis management were collected. The primary outcome was the need for mechanical ventilation (MV). Categorical and continuous data were analyzed using the Chi-Squared and Mann-Whitney U tests, respectively. The IRB has approved this project. Results: A total of 37 patients with EF < 40% and 42 patients with EF ≥40% were included. No significant differences in fluid management, vasoactive agent maximum rate or duration, or steroid use were observed. Net fluid balance between low and preserved EF was positive 4.6 liters vs. 5.1 liters (p = 0.814), respectively. The number of patients that needed MV was higher in the low EF cohort (86% vs. 57%, p = 0.004), and this cohort had fewer MV-free days (20, IQR 0-25 vs. 24 (IQR 0 -28), p=0.064. Conclusions: No significant differences were observed with regard to sepsis management, reflecting current guidelines. The significantly increased need for MV is a provocative result. A potential mechanism is the inability of a patient with reduced LV dysfunction to maintain appropriate cardiac and respiratory function in the face of fluid overload. Prospective analysis of the role of fluid balance in septic patients with LV dysfunction is warranted. Introduction: The relationship between myocardial injury and systemic inflammation in sepsis response is not well understood [1] . It´s proposed to evaluate the association between myocardial injury biomarkers, high-sensitive troponin T (hs-cTnT) and N-terminal pro-brain natriuretic peptide (NT-ProBNP), with inflammatory mediators (IL-6, IL-1Β , IL-8, IL-10, IL-12 / IL-23p40, IL17A, IL-21 and TNF-α ) and biomarkers, C protein reactive (CPR) and procalcitonin (PCT), in septic patients Methods: This was a prospective cohort study performed in three intensive care units, from September 2007 to September 2010 enrolling patients with sepsis (infection associated with organ dysfunction), and septic shock (hypotension refractory by fluids infusion requiring vasopressor). Blood samples were collected up to 48h after the development of first organ dysfunction (D0) and on the 7th day after inclusion in the study (D7) Results: Ninety-five patients were enrolled, with median age 64 years (interquatile?48-78), APACHE II: median 19 (14-22), SOFA: median 8 (5-10); 24.2% were admitted in ICU with sepsis and 75.8% with septic shock. Hospital mortality was 34.7%. In D0, NT-ProBNP correlated with IL-8 (r = 0.495, p <0.001) and IL-10 (r = 0.471, p <0.001). In D7, hs-cTnT and NT-ProBNP correlated with PCT (r = 0.446, p < 0.001 and r = 0.495, p < 0.001; respectively). NT-ProBNP D0 was higher in nonsurvivors than in survivors on mortality in seventh day (p = 0.029) and in-hospital mortality (p = 0.030). hs-cTnT D7 (p = 0.030) and NT-ProBNP D7 (p <0.001) were significantly higher in non-survivors on in-hospital mortality. NT-ProBNP D7 (OR 9.28; IC95% 2.05-41.94, p=0,004) and hs-cTnT D7 (OR 10,93; IC95% 2.139 -55.795, p=0,04) were independently associated with in-hospital mortality Conclusions: NT-ProBNP plasma levels at D0 correlated with IL-8 and IL-10, and both NT-ProBNP and hs-cTnT at D7 correlated with PCT. In addition, NT-ProBNP has been shown to be an important predictor of mortality Introduction: Heparin-binding protein (HBP) acts proinflammatory on immune cells and induces vascular leakage through cytoskeletal rearrangement and cell contraction in the endothelium and is a promising novel prognostic biomarker in sepsis and septic shock. However, studies on repeated measures of HBP are lacking. Our objective was to describe the kinetics of plasma HBP during septic shock and correlate it to hemodynamic parameters. Methods: We included patients with septic shock (sepsis-3) on admission to Helsingborg hospital's intensive care unit (ICU) during September 2016 to February 2018. Patients were sampled from ICU admission and every 4 hours for 72 hours or until death or ICU discharge. The plasma samples were analyzed for HBP and converted using the natural log (lnHBP) for normality. lnHBP was then evaluated against mean arterial pressure (MAP) as primary analysis and against systemic vascular resistance index (SVRI) as a secondary analysis, using mixed-effects linear regression models, treating patient id as a random intercept and adjusting for hemodynamic parameters. Results: A total of 22 patients were included with median age 67 years, 9 females (41%), 7 surgical admissions (32%), median SOFA-score 12 points on day one and 6 deaths from all causes within 90 days (27%). Plasma HBP ranged from 0 to 932 ng/ml with a median of 47 ng/ml (lnHBP range 1.6 to 6.8, median: 3.9 ). An increase lnHBP was significantly associated with a decrease in MAP (Coef. -2.58 mmHg, 95% CI: -0.62 to -4.55, p=0.010, n=22), when adjusting for heart rate (HR), noradrenaline (NA), vasopressin (VP), dobutamine (DBT) and levosimendan (LS). In a secondary subgroup analysis, an increase in lnHBP was also significantly associated with a decrease in SVRI (Coef. -94.2 dyne*s*cm-5*m-2, 95% CI: -1.3 to -187.1, p=0.047, n=13), when adjusting for MAP, HR, NA, VP, DBT, LS and cardiac index. Conclusions: Repeated measures of plasma HBP during septic shock were correlated with important hemodynamic parameters in this small pilot study. Introduction: Mid-regional pro-Adrenomedullin (MR-proADM) comes from the synthesis of the hormone adrenomedullin (ADM), which is overexpressed during inflammation and progression from sepsis to septic shock. Thus, MR-proADM can be a useful biomarker for the clinical management of septic patients [1] . The aim of our study was to understand the ability of MR-proADM to predict 30-day (30-d) mortality and to find a correlation between MR-proADM and Sequential Organ Failure Assessment (SOFA) score in the first 24 hours from Intensive Care Unit (ICU) admission. Methods: We evaluated 28 consecutive septic shock patients according to 2016 Sepsis III definitions. Clinical data from the medical records included demographics, comorbidities, laboratories, microbiology and biomarker levels. Whole blood samples for biomarker profiling were collected at 24, 72 and 120 hours from ICU admission. MR-proADM measurement was detected in EDTA plasma using a sandwich immunoassay by TRACE® (Time Resolved Amplified Cryptate Emission) technology (Kryptor Thermo Fischer Scientific BRAHMS). Results: Overall 30-d mortality rate was 50.0%. MR-proADM [odds ratio (OR) = 1.195], SOFA score (OR = 2.174) and Lactate (Lac) levels (OR = 1.956) in the first 24 hours were associated with 30-d mortality in univariate logistic analysis (P value < 0.05, Table 1 ). 30-d mortality rate was not associated with procalcitonin (PCT) levels (OR = 1.002). Further linear regression analysis showed significant correlation between MR-proADM and SOFA score at 24 hours from ICU admission (P value<0.001, Fig. 1 , Table 2 ). Conclusions: MR-proADM demonstrated superior accuracy to predict 30-d mortality compared to PCT levels and is directly linked to SOFA score at 24 hours from admission. MR-proADM may aid early identification of poor prognosis septic patients who could benefit a more intensive management. Introduction: Study of the expression of cell free DNA (cfDNA) in the search for new biomarkers for infection, sepsis and septic shock. Methods: The population studied was all patients included in the sepsis protocol from March 2017 to January 2018, hospitalized patients of a federal public hospital. Plasma samples were collected for quantification of cfDNA, which after centrifugation were stored at -80°C and then thawed and analyzed by fluorescence using a Varioskan Flash fluorometer). CfDNA values were expressed as ng/mL. The patients were divided into 2 groups: Infection and sepsis/septic shock. We analyzed mortality, Sequential Organ Failure Assessment Score (SOFA score), qSOFA (quick SOFA), comorbidities, cfDNA and laboratory parameters of 111 patients. Results: Among the 111 patients, 28% were classified as infection and 72% sepsis/septic shock. Overall lethality was 33%, infection 9.7%, and sepsis/septic shock 42.5% (p<0.001). The mean of cfDNA, SOFA and lactate was higher according to the classification of infection and sepsis/septic shock: CfDNA (159.4±117.3 and 282.7±358.6, p=0.006), SOFA (1.9±2.1 and 6.6±4.3, p<0.001), QSOFA (positive in 25% and 75%, lactate (1.6±0.8 and 3.8±3.5, p<0.001). We analyzed leukocytes, creatinine, CRP (C reactive protein), INR (International Normalized Ratio), as predictors of severity and only CRP showed no association with disease severity (P=0.84). Levels of cfDNA and qSOFA showed worse prognostic utility as a predictor of sepsis / septic shock when compared to lactate and SOFA: OR 1.00 (95% CI 0.41-2.45), p=0.98 for cfDNA, OR 2.4 (95% CI 1.37-4.21), p=0.002 for SOFA and OR 2.00 (95% CI 0.94-4.28), p=0.072 for lactate. Negelkerke R Square was 0,633 for cfDNA. In addition, area under the curve for cfDNA mortality was 0.60 (95% CI 0.46-0.73) and SOFA 0.81 CI 95% 0.19-0.91). Conclusions: Our study suggests that cfDNA and qSOFA have worse prognostic accuracy when compared to lactate and SOFA, variables already used in clinical practice and easily measured. Introduction: The aim of this study is to develop a "molecular equivalent" to Sequential Organ Failure Assessment (SOFA) score, which could identify organ failure in an easier, faster and more objective manner, based on the evaluation of Lipocalin-2 (LCN2/NGAL) expression levels by using droplet digital PCR (ddPCR). Sepsis has been classically defined as the exuberant, harmful, pro-inflammatory response to infection. This concept is changing [1] and the presence of a life-threatening organ dysfunction caused by a dysregulated host response to infection is now considered a central event in the pathogenesis of sepsis [2] . Methods: LCN2 expression levels were quantified by ddPCR in blood of a total of 257 surgical patients with a diagnosis of infection. Spearman analysis was used to evaluate if LCN2 correlated in a significant manner with SOFA score. Area under the receiver operating curve (AUROC) analysis and multivariate regression analysis were employed to test the ability of LCN2 to identify organ failure and mortality risk. Results: Spearman analysis showed that there was a positive, significant correlation between LCN2 expression levels and SOFA score (Fig. 1) . AUROCs analysis showed that LCN2 presents a good diagnostic accuracy to detect organ failure and mortality risk (Fig 2) . In the multivariate regression analysis, patients showing LCN2 expression levels over the Optimal Operating Points (OOPs) identified in the AUROCs showed a higher risk of developing organ failure (Table 1) and a higher mortality risk (Table 2) . Conclusions: Quantifying LCN2 expression levels by ddPCR is a promising approach to improve organ failure detection and mortality risk in surgical patients with infection. Introduction: Sepsis is an inflammatory state due to an exacerbated immune response against infection. In cancer patients, sepsis presents a 10-fold higher mortality than in general population and leads to longer intensive care unit (ICU) and hospital lengths of stay. It has been shown that reduced levels of circulating immunoglobulins (Ig) might be a surrogate marker of unfavorable outcome in sepsis [1] . The aim of this study was to evaluate the association between Ig levels in plasma and 60-day mortality rate in cancer patients with septic shock. Methods: From December 2017 to November 2018, we conducted a prospective study in the intensive care unit (ICU) of Cancer Institute of State of Sao Paulo, an 84-bed ICU linked to University of Sao Paulo. Patients ≥18 years old with cancer and septic shock were enrolled. Descriptive statistics were computed for demographic and outcome variables. Laboratory data and Ig levels were collected at ICU admission and at days 1, 2 and 3. A multivariate analysis was performed to evaluate predictors of 60-day mortality. Results: A total of 190 patients were included in the study. The 30-day and 60-day mortality were 40.5% and 45.3%, respectively. No significant differences in IgM and IgG levels were observed between survivors and non-survivors. In both groups, the median IgM levels were low and the median IgG levels were normal. In the multivariate analysis for 60-day mortality, a favorable status performance measured by the Eastern Cooperative Oncology Group (ECOG) was associated with better survival; metastatic disease, higher Sequential Organ Failure Assessment (SOFA) score at admission and higher levels of initial lactate were associated with increased mortality. Conclusions: Low levels of serum endogenous immunoglobulins are not predictors of 60-day mortality in cancer patients with septic shock. Introduction: Cytovale has developed a rapid biophysical assay of the host immune response which can serve as a rapid and reliable indicator of sepsis. Neutrophils and monocytes undergo characteristic structural and morphologic changes in response to infection. One type of response is the generation of neutrophil extracellular traps (NETs), these have been proposed as potential mediators for widespread tissue damage. During NETosis there is a fundamental reorganization of a cell's chromatin structurea signal that we have shown is sensitively measured by the Cytovale cytometer. We hypothesized that quantification of plasticity (deformability) of leukocytes in the peripheral blood provides an early indicator of sepsis. The Cytovale assay uses microfluidic cytometry to measure the plasticity of up to 100,000 white blood cells from EDTA-anticoagulated, peripherally-collected whole blood and provides a result in 5 minutes. Methods: In two prospective studies conducted in two academic medical centers in Baton Rouge, LA, the Cytovale test was performed on peripheral blood samples obtained from 500 patients who presented to the emergency department with signs or symptoms suggestive of infection. The two studies included high acuity patients (400 patient study) and low acuity patients (100 patient study). An adjudicated reference diagnosis of sepsis or no sepsis was established for each subject, using consensus definitions, by review of the complete medical records. Results: The Receiver Operator Curve (ROC) performance of the Cytovale assay for both studies demonstrated an Area Under the Curve (AUC) greater than 0.85 (Fig. 1) . Conclusions: Measurement of neutrophil and monocyte plasticity by a novel assay provides an accurate and rapid indication of sepsis in patients who present to an emergency room with signs or symptoms of infection. Plasma hepatocyte growth factor in sepsis and its association with mortality: a prospective observational study Introduction: Sepsis and septic shock are commonly associated with endothelial cell injury. Hepatocyte growth factor (HGF) is a multifunctional protein involved in endothelial cell injury and plays a pivotal role in sepsis. This study assesses its correlation with relevant endothelial cell injury parameters and prognostic value in patients with sepsis. Methods: A prospective, observational cohort study was conducted in patients with sepsis admitted to the department of critical care medicine at the Zhongda Hospital from November 2017 to March 2018. The plasma HGF level was collected on the first 24h after admission (day 1) and day 3, then was measured by enzyme-linked immunosorbent assay. The primary endpoint was defined as all-cause 28-day mortality. Furthermore, we analyzed the correlation of HGF with relevant endothelial cell injury markers. Results: Eighty-six patients admitted with sepsis were included. HGF levels of non-survivors were elevated upon day 1 (1940.62 ± 74.66pg/mL vs. 1635.61 ± 47.49pg/mL; p = 0.002) and day 3 (1824.82 ± 137.52pg/mL vs. 1309.77 ± 83.49pg/mL; p = 0.001) compared with that in survivors, and showed a strong correlation with von Willebrand factor (r = 0.45, p <0.0001), lactate (r = 0.35, p = 0.0011), pulmonary vascular permeability index (r = 0.38, p = 0.0241), first 24 h fluid administration (r = 0.38, p <0.0001) and sequential organ failure assessment score (r = 0.40, p = 0.0001) (Fig. 1) . Plasma levels were able to discriminate prognostic significantly on day 1(AUC: 0.72, 95%CI: 0.60-0.84) and day 3 (AUC: 0.77, 95%CI: 0.63-0.91) (Fig. 2) . Conclusions: HGF levels are associated with sepsis and are correlated with established markers of endothelial cell injury. Elevated HGF level in sepsis patients is a predictor of mortality. Methods: Adult patients with septic shock by the Sepsis-3 classification due to lung infection or primary bacteremia or acute cholangitis are screened using two consecutive measurements of ferritin and of HLA-DR/CD14 co-expression for MALS (ferritin above 4,420 ng/ml) or immunosuppression (HLA-DR/CD14 less than 30%) and randomized into immunotherapy with either anakinra (targeting MALS) or recombinant IFNγ (targeting immunosuppression) and into placebo treatment. Main exclusion criteria are primary and secondary immunodeficiencies and solid and hematologic malignancies. Results: 101 patients have been screened so far. Most common infections are community-acquired pneumonia (41.6%), hospitalacquired pneumonia (26.7%) and primary bacteremia (12.9%). Mean +/-SD SOFA score is 12.6 +/-2.9 and Charlson's comorbidity index 5.21 +/-2.44; 25 patients have MALS (24.8%); two immunosuppression (2%); the majority remain unclassified for immune state. Conclusions: Current screening suggests greater frequency of MALS than recognized so far in a setting of septic shock due to lung infection or primary bacteremia or acute cholangitis. Development of an algorithm to predict mortality in patients with sepsis and coagulopathy D Hoppensteadt 1 , A Walborn 2 , M Rondina 3 , J Fareed 1 study was to develop an equation incorporating biomarker levels at ICU admission to predict mortality in patients with sepsis, to test the hypothesis that using a combination of biomarkers of multiple systems would improve predictive value. Methods: Plasma samples were collected from 103 patients with sepsis at the time of ICU admission. Biomarker levels were measured using commercially available, ELISA methods. Clinical data, including the ISTH DIC score, SOFA score, and APACHE II score were also collected. 28-day mortality was used as the primary endpoint. Stepwise linear regression modeling was performed to generate a predictive equation for mortality. Results: Differences in biomarker levels between survivors were quantified and using the Mann-Whitney test and the area under the receiver operating curve (AUC) was used to describe predictive ability. Significant differences (p<0.05) were observed between survivors and non-survivors for PAI-1 (AUC=0.70), procalcitonin (AUC=0.77), HMGB-1 (AUC=0.67), IL-6 (AUC=0.70), IL-8 (AUC=0.70), protein C (AUC=0.71), Angiopoietin-2 (AUC=0.76), endocan (AUC=0.58), and platelet factor 4 (AUC=0.70). A predictive equation for mortality was generated using stepwise linear regression modeling. This model incorporated procalcitonin, VEGF, the IL-6:IL-10 ratio, endocan, and PF4, and demonstrated a better predictive value for patient outcome than any individual biomarker (AUC=0.87). Conclusions: The use of a mathematical modeling approach resulted in the development of a predictive equation for sepsis-associated mortality with performance than any individual biomarker or clinical scoring system. Furthermore, this equation incorporated biomarkers representative of multiple physiological systems that are involved in the pathogenesis of sepsis. The effects of biomarker clearances as markers of improvement of severity in abdominal septic shock during blood purification T Taniguchi 1 , K Sato 2 , M Okajima 2 Introduction: Sepsis associated coagulopathy (SAC) is commonly seen in patients which leads to dysfunctional hemostasis. The purpose of this study is to determine the thrombin generation potential of baseline blood samples obtained from SAC patients and demonstrate their relevance to thrombin generation markers. Methods: Baseline citrated blood samples were prospectively collected from 49 patients with SAC at the University of Utah clinic. Citrated normal controls (n=50) were obtained from George King Biomedical (Overland Park, KS). Thrombin generation studies were carried out using a flourogenic substrate method. TAT and F1.2 were measured using ELISA methods (Seimens, Indianapolis, IN). Functional antithrombin levels were measured using a chromogenic substrate method. Results: The peak thrombin levels were lower (82 ± 40nM) in the DIC patients in comparison to higher levels observed in the normal plasma (133 ± 10nM). The AUC was lower (561 ± 280) in the DIC group in comparison to the normals (624 ± 18). The DIC group showed much longer lag time (4.1 ± 2.1) in comparison to the normal group (2.1 ± 2.2). Wide variations in the results were observed in these parameters in the DIC group. The F1.2 levels in the DIC group were much higher (570±48 pmol) in comparison to the normal (210 ± 25 pmol). The TAT levels also increased in the DIC group (27.9 ± 5.1 ng/ml) in comparison to the normal (2.8 ± 0.8 ng/ml). The functional antithrombin levels were decreased in the DIC group (64 ± 11%). Conclusions: These results validate that thrombin generation such as F1.2 and TAT are elevated in patients with DIC. However thrombin generation parameters are significantly decreased in this group in comparison to normals. This may be due to the consumption of prothrombin due to the activation of the coagulation system. The decreased functional AT levels observed in the DIC group are due to the formation of the complex between generated thrombin and antithrombin. Introduction: Sepsis-associated disseminated intravascular coagulation (DIC) is a complex clinical scenario involving derangement of many processes, including hemostasis. Assessment of markers including inflammation, endothelial function, and endogenous anticoagulants may provide insight into DIC pathophysiology and lead to improved methods for assessment of patient condition and response to treatment. Methods: Citrated plasma samples were collected from 102 patients with sepsis and suspected DIC at ICU admission and on days 4 and 8. DIC score was determined using the ISTH scoring algorithm (e.g. platelet count, PT/INR, fibrinogen and D-Dimer). CD 40 Ligand (CD40L), Plasminogen inhibitor 1 (PAI-1), nucleosomes, Procalcitonin (PCT), Microparticle tissue factor (MP-TF) and Prothrombin 1.2 (F1. 2) were measured using commercially available ELISA kits. Protein C activity was measured using a clot-based assay. Interleukin 6 (IL-6), Interleukin 8 (IL-8), Interleukin 10 (IL-10), Tumor necrosis factor alpha (TNFα), and Monocyte chemoattractant protein (MCP-1) were measured using biochip technology. Results: Significant differences in levels of Protein C (p=0.009), PCT (p=0.0005), IL-6 (p=0.019), IL-8 (p=0.0149), PAI-1 (p=0.015), were observed between survivors and non-survivors. Significant variation of Protein C (p=0.002), nucleosomes (p=0.05), PCT (p<0.0001), IL-6 (p=0.001), IL-8 (p=0.003), IL-10 (p=0.011), TNFα (p=0.021) and MCP-1 (p=0.021) were observed based on severity of DIC score. Conclusions: Markers from multiple systems perturbed in DIC were associated with mortality, suggesting that while these systems may not be routinely evaluated in the normal course of patient care, dysfunction of these systems contributes significantly to mortality. In addition, numerous inflammatory cytokines showed an association with DIC score. This suggests that the measurement of additional markers in sepsis-associated DIC may be of value in the prediction of mortality and may be helpful in guiding treatment for these patients. Introduction: The Endotoxin Activity Assay (EAA) is a rapid immunodiagnostic test based on chemiluminescence. It was approved by the FDA in 2003 as a diagnostic reagent for risk assessment of severe sepsis in the ICU. Ascertaining endotoxin levels in the bloodstream is important in targeting patients and determining the appropriate timing for initiation of treatment. It has high sensitivity and specificity for endotoxin, and is considered to be useful in predicting clinical symptoms and determining prognosis. The usefulness of the EAA has yet to be fully clarified. Methods: A total of 142 patients admitted to the ICU between January 2014 and June 2018 with suspected sepsis or sepsis were enrolled. The EAA was conducted within 24 hr after admission. Patient characteristics were determined, together with levels of IL-6, procalcitonin, presepsin, and PaO2/FiO2. Thereafter, the patients were classified into 5 groups depending on their EAA value: 1) < 0.2; 2) from ≤ 0.2 to < 0.4; 3) from ≤ 0.4 to < 0.6; 4) from ≤ 0.6 to < 0.9; and 5) ≤0.9). The transition of various markers was also examined. The Spearman rank correlation, Wilcoxon rank sum test, and a nonrepeated ANOVA were used for the statistical analysis. A P-value of < 0.05 was considered statistically significant. The EAA values showed a positive correlation with both the APACHE II (r=0.48) and SOFA scores (r=0.56)(P<0.01), although that with the latter was stronger. A significant correlation was also observed with levels of procalcitonin (r=0.45) and presepsin (r=0.51 Early diagnosis is important to allow early intervention. The current clinical methods are insufficient for early detection. We hypothesized that intraperitoneal microdialysis allows detection of peritonitis prior to changes in standard clinical parameters in a pig model. Methods: Bacterial peritonitis was induced in 5 pigs by bowel perforation and intraperitoneal fecal instillation, one pig underwent sham surgery. Intraperitoneal microdialysis catheters were placed in each abdominal quadrant. The observation time was 10 hours. Results: In peritonitis pigs the intraperitoneal lactate increased during the first two hours and remained elevated throughout the observation time (Table 1) , whereas the arterial lactate remained within reference range (<1.6 mM). Intraperitoneal glucose decreased significantly. Hemodynamics were hardly influenced during the first two hours, and decreased thereafter. Sham surgery did not influence in any of the parameters. Conclusions: A rapid and pronounced increase in intraperitoneal lactate and decrease in intraperitoneal glucose was observed after instillation of intraabdominal feces. Systemic lactate increase was absent, and the hemodynamic response was delayed. Postoperative intraperitoneal microdialysis is applicable in detecting peritonitis earlier than standard clinical monitoring and should be evaluated in a clinical study in order to explore if early intervention based on MD data will reduce ICU length of stay, morbidity and mortality. Introduction: Procalcitonin (PCT) is a serum biomarker suggested by the Surviving Sepsis Campaign to aid in determination of the appropriate duration of therapy in septic patients. Trauma patients have a high prevalence of septic complications, often difficult to distinguish from inflammatory response. PCT values typically declined after 72h from trauma and increased only during secondary systemic bacterial infections. The aims of the study are to evaluate reliability and usefulness of PCT serum concentration in trauma. Methods: We retrospectively analyzed data from 40 trauma patients admitted to ICU at Bufalini Hospital -Cesena, from July 2017 to August 2018. We collected data about antimicrobial therapy, Injury severity score (ISS), first arterial Lactate in emergency room, SOFA score and Sepsis severity. Plasma PCT concentration was measured using an automate analyzer (Modular E-Brahms) on 1st day of antimicrobial therapy and every 48h hours. Antimicrobial therapy was stopped according to a local protocol; however medical judgment was considered the overriding point for therapeutic decision. Results: Median ISS of patients was 33.5, inter quartile range (IQR) 13.5. PCT mean concentration at the starting of antimicrobial treatment was 13.07 μg/L (d.s 40.1), median 0.72 (IQR 10.13). No significative correlation (Spearman´s Rho Test) was found between PCT at day 1 of antimicrobial therapy and ISS (Rho -0.186), between first arterial Lactate in ER and PCT (Rho 0.158). Daily course of PCT was not related to distance from trauma (Rho -0.116). In 21 of 40 patients (52.2 %) PCT measurement led physician to save days of antimicrobial therapy compared with standard clinical practice. We couldn´t find any cut off value. Conclusions: Our experience suggests that PCT could help physician to optimize duration of antimicrobial therapy in trauma patients. No standard approach can be recommended at present. Introduction: Long duration of antimicrobial treatment may predispose to colonization and subsequent infections by multidrugresistant organisms (MDRO) and Clostridium difficile. PROGRESS (ClinicalTrials.gov registration NCT03333304) is an on-going trial aiming to use PCT for the restraining of this calamity. Methods: Adult patients with sepsis by the Sepsis-3 classification and any of five infections (pneumonia community-acquired; hospital-acquired or ventilator-associated; acute pyelonephritis; primary bacteremia) are randomized to PCT-guided treatment or standard of care (SOC) treatment. In the PCT arm antibiotics are discontinued when PCT on or after day 5 is decreased by more than 80% of the baseline or remains below 0.5 ng/ml; in the SOC arm antibiotics are discontinued at the discretion of the attending physician. Patients are followed for six months. Primary endpoint is the rate of infections by MDRO and/or C.difficile or death. Serial stool samples are cultured for MDRO and screened for glutamate dehydrogenase antigen and toxins of C.difficile. Results: 201 patients have been enrolled so far. Mean ± SD SOFA score is 4.4 ± 2.4. Most common diagnoses are community-acquired The PROGRESS trial is the first trial assessing the probable benefit from PCT guidance to reduce ecological sequelae from long-term antibiotic exposure. Analysis of baseline patient characteristics indicates that PROGRESS is a real-world trial so that results can have major clinical impact. Prospective multi-site validation of 11-gene host response signature for influenza diagnosis S Thair 1 , S Schaffert 1 , M Shojaei 2 , T Sweeney 3 There are no blood-based diagnostics able to identify influenza infection and distinguish it from other infections. We have previously described a blood-based 11-gene influenza meta-signature (IMS) score to differentiate influenza from bacterial and other viral respiratory infections. Methods: We prospectively validated the IMS in a multi-site validation study by recruiting 654 individuals (608 patients with suspected influenza, 46 healthy controls) in 10 community or hospital clinics across Australia. We assayed the IMS and 15 genes from viral genome of 3 influenza strains to generate the Blood Flu Score (BFS) as a measure of viremia using Nanostring from whole blood RNA. Results: Using clinically determined phenotypes, the IMS score distinguished patients with influenza from healthy (AUC=0.95), non-infected (AUC=0.83), bacterial (AUC=0.88), other viruses (AUC=0.77) ( Figure 1A) . Interestingly, probes of BFS were found in all phenotypic groups (non-infected, bacterial, and other viral infections) to varying degrees, and positively correlate with the IMS score (r=0.53). IMS AUROCs improve when the BFS is used to inform the phenotypic groups: healthy (AUC=0.92), non-infected (AUC=0.90), bacterial (AUC=0.93), other viruses (AUC=0.88) ( Figure  1B ). Patients who were clinically influenza negative but had a high IMS and BFS were admitted less often, yet had~4-fold higher mortality than those who were clinically influenza negative with low IMS and no BFS (Table 1) . Conclusions: Collectively, our prospective multi-center validation of the IMS demonstrates its potential in diagnosis of influenza infections. Introduction: Previous findings of our group suggest that patients with Gram-negative hospital-acquired severe sepsis have better prognosis when sepsis is developing after recent multiple trauma through stimulation of favorable interleukin (IL)-10 responses [1] . Under a similar rationale, we investigated if preceding osteomyelitis may affect experimental osteomyelitis. Methods: Sham or experimental osteomyelitis was induced in 32 male New Zealand white rabbits after drilling a hole at the upper metaphysis of the left tibia and implementing diluent or 5log10 of Staphylococcus aureus using foreign body. After three weeks, the foreign body was removed and experimental pyelonephritis or sham surgery was induced after ligation of the right pelvo-ureteral junction and instillation of 6log10 of Escherichia coli in the renal pelvis. Survival was recorded and circulating mononuclear cells were isolated and stimulated for the production of tumour necrosis factor-alpha (TNFa) and IL-10. At death or sacrifice, tissue outgrowth and myeloperoxidase (MPO) were measured. Results: Four sham-operated rabbits (S), 16 rabbits subject to sham surgery and then pyelonephritis (SP) and 12 rabbits subject to osteomyelitis and then pyelonephritis (OP) were studied. Survival after 14 days of group SP was 56.3% and of group OP 100% (log-rank 6.59; p: 0.010). Lab findings are shown in Figure 1 . Il-10 production was blunted. Negative correlation between E. coli outgrowth and tissue MPO was found at the right kidney of the OP group (rs: -0.767, p: 0.016) but not of the SP group (rs: -0.318, p: 0.340). Conclusions: Preceding staphylococcal osteomyelitis provides survival benefit to subsequent experimental osteomyelitis through downregulation of innate immune responses leading to efficient phagocytosis. Introduction: Activation of neutrophils is a mandatory step and a sensitive marker of a systemic inflammatory response syndrome (SIRS) which is closely related to development of multiple organ failure. The search for drugs that can prevent SIRS and reduce mortality in critically ill patients remains significant. The aim of this study was to study the anti-inflammatory effect of the synthetic analogue of leu-enkephalin (Dalargin) on human neutrophils. Methods: The study was conducted on isolated from the blood of healthy donors neutrophils. Their activation was assessed by fluorescent antibodies to markers of degranulation CD11b and CD66b (SD11b-FITC and CD66b-AlexaFluor647 (BD Biosciences, USA). As inductors of inflammation lipopolysaccharide (LPS) and the peptide formyl Met-Leu-Pro (fMLP) were used. 100mkM fMLP and dalargin in concentrations of 50 and 100 μ g / ml were added to neutrophils at a concentration of 4 ppm / ml and incubated for 30 min at 37°C; then antibodies were added and incubated for 30 min on ice; then fluorescence was assessed by flow cyto flow meter Beckman-Coulter FC 500. Non-parametric criteria were used; data were presented as a median and 25%-75% interquartile intervals. The statistical significance was estimated using Mann-Whitney test. The difference was considered statistically significant at P<0.05 Results: Synthetic analogue of leu-enkephalin in various concentrations has an anti-inflammatory effect on both intact and preactivated with bacterial components neutrophils, reducing their activation and degranulation in a dose-dependent manner (Figs. 1, 2) . Conclusions: Synthetic analogue of leu-enkephalin prevents neutrophil activation by bacterial compounds. This has a potential of translation into clinical practice for sepsis treatment. Introduction: The endothelin system plays important roles in circulatory regulation through vasoconstrictor ET-A and ET-B2 receptors and vasodilator ET-B1 receptors (ETAr; ETBr, respectively). Tissue hypoxia during the progression of sepsis is associated with microcirculatory and mitochondrial disturbances. Our aim was to investigate the possible influence of ETAr antagonist, ETBr agonist or combined treatments on oxygen dynamics, microcirculatory and mitochondrial respiration parameters in experimental sepsis. Methods: Male Sprague-Dawley rats (n=8/group) were subjected to faecal peritonitis (0.6 g/kg faeces ip) or sham-operation. Septic animals were treated with sterile saline solution, or received the ETAr antagonist ETR-p1/fl peptide (100 nmol/kg iv), ETBr agonist IRL-1620 (0.55 nmol/kg iv) or same doses as combination therapy, 22 hr after sepsis induction. Invasive hemodynamic monitoring and blood gas analyses were performed during a 90-min observational window. Introduction: Sepsis often induces immunosuppression, which is associated with high mortality rates. Nivolumab is a human IgG-4 antibody directed against the programmed cell death 1 (PD-1) immunecheckpoint inhibitor, which disrupts PD-1-mediated signaling and restores antitumor immunity. Nivolumab is an approved anti-cancer drug that may have the potential to improve sepsis-induced immunosuppression. Methods: This multicenter, open-label study investigated the safety, pharmacokinetics and pharmacodynamics of a single intravenous infusion of 480 or 960 mg nivolumab in Japanese patients with immunosuppressive sepsis (lymphocytes ≤ 1100 /μL). The dosing of nivolumab was set using the predicted steady state concentration of nivolumab at 3 mg/kg every 2 weeks (Q2W), which was the approved dosage for cancer patients at the time of planning. Results: Five and eight patients were assigned to the 480 and 960 mg groups, respectively. The mean (standard deviation) peak serum drug concentration in the 480 mg group was comparable to the predicted median concentration (90% PI [prediction (Figures 1 and 2 ). Adverse events (AEs) were observed in four patients in each group. Drug related-AEs were observed in only one patient in the 480 mg group (Table 2) . No deaths related to nivolumab occurred. Conclusions: A single dose of 960 mg nivolumab appeared to be well tolerated and sufficient to maintain nivolumab blood concentration in patients with sepsis. Results suggest both 480 and 960 mg nivolumab therapy could improve relevant immune indices. Introduction: The systemic inflammatory response syndrome (SIRS) accompanies tissue trauma and infection and, when severe or dysregulated, contributes to multiple organ failure and critical illness. Observational studies in man and animal have shown that low-dose acetyl-salicylic acid promotes resolution of inflammation and might attenuate excessive inflammation by increasing the synthesis of specialised pro-resolving lipid mediators (SPMs). Methods: We randomly assigned patients with SIRS who were expected to stay in ICU for more than 48 hours to receive enteral aspirin (200 mg per day) or placebo for 7 days or until death or discharge from the ICU, whichever came first. The primary outcome was IL-6 serum concentration at 48h after randomisation. The secondary outcomes included safety and feasibility outcomes. In one center, additional blood samples were taken during the first three days for exploratory analysis of SPMs using reversed-phase highperformance liquid chromatography -tandem mass spectrometry (RP-HPLC-MS/MS). Results: From March 2015 through December 2017 a total of 48 patients across four general ICUs in Australia underwent randomization (Table 1) . Compared to placebo patients, IL-6 serum concentration after 48h in aspirin-treated patients was not significantly lower (40 [16-166] pg/ml vs 44 [7.4-85] pg/ml; p=0.66). There were no significant differences for control vs. aspirin-treated patients in the change of pro-resolving/anti-inflammatory lipids between the time points (Figure 1, 2 ). There were no between-group differences with respect to ICU or hospital mortality, number of bleeding episodes or requirements for red cell transfusions (Table 2) . Conclusions: In patients admitted to the ICU with SIRS, low-dose aspirin did not result in a decreased concentration of inflammatory biomarkers compared with placebo. Introduction: Sepsis is associated with excessive ROS production, NF-kB, iNOS and inflammatory mediators overexpression. Vitamin C is a cellular antioxidant, it increases eNOS and decreases NF-kB; it has several immune-enhancing effects and is crucial for endogenous vasopressors synthesis. Vitamin C reserves in sepsis are often as poor as in scurvy [1] . In recent studies, intravenous high Vitamin C dose seems to reduce organ failure and improve outcome in septic shock. Methods: We treated all septic shock patients admitted to our ICU in 7 months (from 3/2018 to 9/2018) with intravenous Vitamin C 1.5g/ 6h and Thiamine 200 mg/12h (for its synergistic effects) [2] as adjunctive therapy for 4 consecutive days and we compared data to septic shock patients admitted in the previous 7 months period. We enrolled 24 patients: 13 received Vitamins supplementation, 11 standard of care. We analysed 28-days mortality, SOFA at 48 and 96 hours, PCT variation from baseline in first 5 days, vasoactive therapy length and DAF 28 (Days Alive and Free from vasopressors, mechanical ventilation and RRT in 28 days follow up). Patients with end stage kidney disease were ruled out. We analysed data with Mann-Whitney and Wilcoxon tests. Results: Vit C group showed lower 28-days mortality (23% vs 54.5%: NS); SOFA improvement at 48 (-2.4±1.5 vs -0.9±1.9: p=0.01) and 96 hours (-4.2±2 vs -1.9±2: p<0.01) was higher in Vit C group; Vit C patients had faster PCT reduction without statistical significance. Mean vasoactive therapy length was quite similar. DAF was 16.5 (±9.1) days in Vit C group and 9.3 (±8.9) in controls (p=0.03). 3 control patients needed RRT, none in Vit C group. Conclusions: Despite small study size, we found that Vit C has positive effects on survival and improves SOFA score (Fig.1) and DAF (Fig.2 ) in septic shock. No Vit C patient developed oxalate nephropathy nor worsened renal function. Introduction: Toxin-producing gram-positive organisms cause some of the most severe forms of septic shock [1, 2] . Adjunctive therapies such as intravenous immunoglobulins (IVIG) have been proposed for these patients [3, 4] . However, at patient presentation, the presence of a toxin-producing organism is most often unknown. Methods: We reviewed the use of IVIG in our patients requiring extracorporeal membrane oxygenation (ECMO) in a 2-year period between February 2016 and March 2018. Results: In 44% (15/34) of the patients that received IVIG for presumed toxin-mediated shock, group A Streptococcus or Panton-Valentine leukocidin producing S. aureus was isolated, but the clinical characteristics of these 15 patients were not significantly different from the ones with other final diagnoses, except for a predisposing influenza infection and the presence of an often very high procalcitonin level. These 34 patients were extremely unwell at presentation with a SOFA score of 15 ± 3, high lactate levels (8.6 ± 5.8 mmol/L) and need for vasopressors (equivalent norepinephrine dose of 0.93 ± 0.62 μ g/kg/min). They had very high inflammatory parameters with a procalcitonin ≥ 100 ng/mL in more than half of patients (18/34). IVIG use in these patients was generally safe, with only 1 possible transfusion reaction. The mortality of 35% (12/34) was lower than predicted based on the SOFA scores. Conclusions: IVIG administration can be considered in a selected group of patients presenting with acute and very severe septic shock, as part of a multimodal approach [5] . Introduction: Extra corporeal treatments are used in septic patients to decrease the inflammatory mediators, but definitive conclusions are lacking . More over in many studies the effect of AKI isn't evaluated and this may be an important bias. . The aim of this study is to evaluate in septic patients with AKI: 1the effect of the adsorbing membrane Oxiris on the immunological response 2-the different response in survivors and non survivors Methods: From our local data base we analyzed retrospectively 50 septic shock patients with AKI (KDIGO classification) submitted to CRRT with the adsorbing membrane oXiris (Baxter, USA ) . At basal time ( T0 ) and at the end of the treatment ( T1 ) we evaluated the following variables: IL 6 IL 10 Procalcitonin Endotoxin (EAA). All data are expressed as mean ±SD or median and IQR. Student T test or Mann-Whitney was used to compare values changes. P < 0.05 was considered statistically significant. Results: Thirty patients with sepsis /septic shock and AKI were enrolled in this study. 10 patients had AKI 3, 15 patients AKI 2, 5 patients AKI 1. The duration of treatment was 38± 10 hours. 20 patients had citrate as anticoagulation and 10 heparine continous ev. At Table 1 are shown the main results of this study in all the patients. Survivors vs non survivors had a significant decrease of IL 6, Procalcitonin and EAA. Conclusions: Data of this study confirm on clinical ground previous study "in vitro" [1] that the adsorbing membrane oXiris has important immunological effect during septic shock with AKI. This must be confirmed in a RCT. Introduction: Sepsis is common and often fatal, representing a major public health problem. Hemoadsorption (CytoSorb) therapy aims to reduce cytokines and stabilise the overall immune response in septic shock patients. Methods: A prospective, multi-centre, investigator initiated study to evaluate Hemoadsorption (CytoSorb) Therapy in septic shock patients admitted to a tertiary ICU's in India during 2016 to 2018. All centres followed a common protocol and received ethics committee approval. Results: A total of 45 patients were administered CytoSorb in addition to standard of care. A total of 26 patients (62%) survived out of 45 patients. Among survival group, 17 patients (71%) were administered CytoSorb within 48 hours of ICU admission resulting in significant reduction in Sepsis Scores, APACHE II (24.42 vs 19.33) and SOFA (12.46 vs 8.71) post CytoSorb therapy. Also there was reduction in inflammatory markers like Cytokines IL6 in most of the patients. All patients in survivor group showed a significant improvement in MAP (69.2 vs 76.8) and reduction in vasopressors (Epinephrine 0.3 to 0.03 mcg/kg/min, Nor-Epinephrine 0.34 to 0.04 mcg/kg/min) after CytoSorb therapy. No device related adverse effect was observed in any of the patients. Among the non-survivor group, (16 patients, 38%) we observed that CytoSorb was administered after 48 hours of ICU admission. Although a few patients showed improvement in SOFA score, majority did not show a significant improvement with MAP (68.05 vs 60.4 mm of Hg) and required increased demand in Vasopressors. Conclusions: In this multi-centered prospective IIS study, we could observe clinical benefits of Hemoadsorption (CytoSorb) therapy in Septic shock patients if the therapy was initiated early. Larger randomised study are required to establish the above clinical benefits in larger patient population. A single centre experience with hemoadsorption (CytoSorb) in varied causes of sepsis and MODS Y Mehta 1 , C Mehta 1 , A Kumar 1 , J George 2 , A Gupta 3 , S Nanda 1 , G Kochar 1 , A Raizada 3 Introduction: Sepsis and the multiorgan failure is a leading cause of mortality in the intensive care unit. Promising new therapies continue to be investigated for the management of septic shock. We tried to evaluate a novel Hemoadsorption therapy (CytoSorb) through a retrospective evaluation of patient's data in our centre. We used it as an adjuvant therapy in our patients with Sepsis due to varied causes. Methods: We retrospectively analysed data of 100 Introduction: Septic shock is a life-threatening multiple organ dysfunction that has high morbidity and mortality in critically ill patients, due to a dysregulated host response to infection. The aim of this study was to evaluate the efficacy of therapeutic cytokine removal (CytoSorb®) in the management of patients with septic shock. Methods: We retrospectively analyzed patients admitted to ICU with septic shock between June 2015 and November 2017. Patients included in the study were diagnosed according to The Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3), received maximal supportive care including continuous veno-venous hemodiafiltration (CVVHDF) for acute kidney injury and Cytosorb® haemoadsorption column was added to return limb of the CVVHDF circuit. Demographic data, procalcitonin and leukocyte levels before and after therapeutic cytokine removal and duration of Cytosorb® haemoadsorption column application and APACHE II scores were recorded. Results: The mean age of 48 patients included in the study was 54 ±16.4 years (74% male) and the mean body mass index was 23.1 ±5.9. The mean APACHE II score was 21.5 with an expected and actual mortality rates of 40% and 25%, respectively. 18% of the patients were admitted with sepsis and 82% of them with septic shock. 37.5% (n=18) of the cases were solid organ transplant recipients. CVVHDF was applied in all patients during therapeutic cytokine removal. Treatment was combined with ECMO in 8 patients. While the mean duration of CVVHDF was 102.1 hours, the duration of Cytosorb® haemoadsorption column application was 24.1±13.4 hours. Procalcitonin (16.5 ± 25ng/ml vs 25±34ng/ml) and leucocyte levels (14048±10490/ mm3 vs 9278±8693 mm3) after therapeutic cytokine removal were found significantly lower than the pretreatment values (respectively p=0.0013, p=0.006). Conclusions: Therapeutic cytokine removal applied with CVVHDF in septic shock patients have positive contributions to biochemical parameters and provide survival advantage. Introduction: Recent studies have focused on demonstrating the potential benefits of immunomodulation in the management of septic patients. The aim of our study was to assess the effects of a hemoadsorption column (CytoSorb®) in critical ill septic patients. Methods: After ethical approval was obtained, we prospectively included 39 patients admitted to the general ICU of Fundeni Clinical Institute. Three consecutive sessions of renal replacement therapy (continuous venovenous hemodiafiltration) in combination with CytoSorb® were applied after ICU admission. Clinical (heart rate, arterial pressure, temperature, Glasgow coma scale) and paraclinical data (PaO2, serum bilirubin and creatinine, platelet count, white blood cell count, pH, C-reactive protein and procalcitonine), vasopressor support and need for mechanical ventilation were recorded before and after the three sessions. Results: The mean age in the study group was 57±15 years. Median number of organ dysfunction at the time of ICU admission was 4 [1] [2] [3] [4] [5] and the mean SOFA score was 10.0±3.7. The use of CytoSorb® was associated with a non-significant increase in PaO2/FiO2 ratio from 216±80 to 248±94 (p=0,278) and creatinine levels from 1.8±1.3 to 1.6 ±1.2 mg/dL (p=0.685). Although we observed a non-significant increase in C-reactive protein levels from 136±66 mg/L to 144±72 mg/ L (p=0.577), we noted a significant decrease in procalcitonine levels from a median of 20.0 [2.2, 100 .0] ng/dL to a median of 9.1 [0.7, 55.2] ng/dL (p=0.041). A significant decrease in platelet count was also noted from 135384±99263 /mm3 to 78470±61624 /mm3 (p=0.002). Mean SOFA score decreased non-significantly from 10.0±3.7 to 9.1 ±4.1 (p=0.533). Conclusions: The use of CytoSorb was associated with a slight nonsignificant improvement in organ function and a decrease of procalcitonine levels. Thrombocytopenia remains one of the most important complications of renal replacement therapy. Introduction: Circulating cell-free neutrophil extracellular traps (NETs) would induce a microcirculatory disturbance of sepsis. The removal of NETs remnants from the circulation could reduce NETs-dependent tissue injury. To address this issue, we evaluated the effect of hemoperfusion with a polymyxin B cartridge (PMX-DHP; Toray, Japan), which was originally developed for the treatment in patients with Gram-negative bacterial infection, on circulating cell-free NETs in patients with septic shock and in phorbol myristate acetate (PMA)-stimulated neutrophils obtained from healthy volunteer. Methods: Ex vivo closed loop hemoperfusion was performed through a circuit formed by connecting the small PMX module to a tube and a peristalsis pump. Whole blood from healthy volunteers incubated with or without PMA or from septic shock patients were applied to circuit and perfused. Blood was collected at 0, 1 and 2 hr after perfusion. Circulating cell-free NETs were assessed by myeloperoxidase (MPO)-, neutrophil elastase (NE)-, and cell free (cf)-DNA. Results: Plasma MPO-DNA, NE-DNA and cf-DNA levels were significantly increased at 2 hr after PMA stimulation when compared with plasma levels without PMA. When either blood from septic shock patients or PMA-stimulated neutrophils obtained from volunteers were applied to circuit, circulating MPO-DNA, NE-DNA and cf-DNA were significantly reduced in perfusion with PMX filter than in perfusion without PMX filter at times 1 and 2 hr. Conclusions: In the ex vivo experiments, MPO-DNA, NE-DNA and cf-DNA were found to decrease after ex vivo perfusion through PMX filters. Selective removal of circulating components of NETs may improve the remote organ damage in patients with septic shock. A retrospective study of septic shock patients who were treated with direct hemoperfusion with polymyxin B-immobilized fibers based on the levels of endotoxin activity assay S Sekine, H Imaizumi, I Saiki, A Okita, H Uchino Tokyo Medical University, anesthesiology/ICU, Tokyo, Japan Critical Care 2019, 23(Suppl 2):P047 Introduction: The purpose of this study was to evaluate the outcomes for septic shock patients with direct hemoperfusion with polymyxin B-immobilized fibers (PMX-DHP) and endotoxin activity assay (EAA). Methods: According to the levels of EAA, 41patients were classified for three groups (low group (GL); EAA <0.4, intermediate group (GM); EAA >0.4 or EAA <0.6, high group (GH); EAA >0.6). In order to evaluate the severity of illness, acute physiology and chronic health eva-luationII (APACHE II) score, the sequential organ failure assessment (SOFA) score, catecholamine index (CAI) were recorded. And the presence of PMX-DHP treatments were also recorded. Blood samples were obtained to measure EAA levels, inflammatory markers (procalcitonin (PCT), C-reactive protein (CRP), and white blood cell count (WBC)), serum lactate level as an indicator of tissue hypoxia, and for blood culture. APACHE II score, SOFA score, CAI, inflammatory markers, serum lactate levels (Lac) and blood culture results were examined for diagnosis of septic shock and prognosis of 30-days mortality. Each values were also compared to EAA levels. Results: 41 septic shock patients were included (GL/ GM/ GH: 12/ 13/ 16). In GH, APACHE II and SOFA score was significantly higher than that in GL (p< 0.05). EAA levels were significantly increased in gramnegative bacteremia patients compared to the patients with grampositive bacteremia or fungemia. There was no relationship between EAA levels and other inflammation markers, CAI, and Lac. In GM, 30days mortality in patient with PMX-DHP treatments was lower than that of without PMX-DHP treatments (0.14 (1/7) vs 0.5 (3/6), p=0.27). In GH, 30-days mortality in patient with PMX-DHP treatments was same as that of without PMX-DHP treatments (0.5 (3/6) vs 0.5 (5/10), p=1.0). Conclusions: These results of this study suggest PMX-DHP treatment may improve the outcome of septic shock patients with intermediate EAA levels. Introduction: Numerous inconclusive randomized clinical trials (RCTs) in sepsis in the past years suggest a need to re-think trial design to improve resource allocation and facilitate policy adoption decisions. The INCLASS study (clinicaltrials.gov NCT: 03345992) is an ongoing RCT evaluating clarithromycin as an immune modulator in high-risk septic patients with clinical and cost-effectiveness outcomes. We aim to compare the original one-shot trial with an alternative sequential design that balances trial costs and value of information. Methods: Adult patients with sepsis, respiratory failure and total SOFA score of at least 7, are randomized to receive intravenous clarithromycin or placebo adjunctive to standard-of-care therapy. For the cost-effectiveness study, efficacy is measured in Quality-Adjusted Life Years (QALYs) by EQ-5D-3L questionnaire at 90 days. The endpoint is the Incremental Net Monetary Benefit (INMB) of clarithromycin compared to placebo, defined as WTP x (Increment in QALY) -(increment in costs), where WTP is willingness to pay per QALY gained. Fixed and variable costs of trial execution (including administrative, insurance, supplies, tests) are calculated; hospitalization cost is extracted from patient records; medical care beyond day 28 is recorded; cost of adoption in the general population is estimated. Previous data from RCTs using clarithromycin are used to form a prior belief about the INMB. Known incidence of sepsis with respiratory failure allows estimation of the population to benefit from trial decision. A Bayesian model is used to determine the sequential design that maximizes trial value. Results: We will compare the performance of the sequential trial design with the one-shot design of INCLASS trial in terms of sample size, cost, social-welfare, and probability of correctly identifying the best treatment. Conclusions: In this protocol we validate a Bayesian model for sequential clinical trials and assess the benefits for the patient population and health care system. The effect on the outcome of critically ill patients with catecholamine resistant septic shock and acute renal failure through implementation of adsorption therapy G Schittek 1 Introduction: CytoSorb-Adsorption has been described as an effective way for hemodynamic stabilisation in septic shock [1] . Aim of this study was to examine whether the adsorption-therapy could influence patient-outcome with catecholamine resistant septic shock (CRSS) and acute renal failure(ARV). Furhtermore we tried to identify clinical constellations that would predict an effective use of adsorbers [ 7, 34] . Initial IL-6 in patients with catecholamine-reduction through adsorption was non-significantly different to those with no reduction (2376 ng/l [838, 5000] vs. 5000 ng/l [3488, 5000]). Mortality did not differ significantly between the groups (71% vs 79%). Length of intensive care unit stay (LOS) did differ significantly (13 days [4, 24] vs 22 days [19, 29] ). Conclusions: IL-6 can be reduced with adsorption. Patients with catecholamine-reduction did not differ in regard to their initial IL-6. LOS was shorter for patients treated with adsorption. According to our experience adsorption can be taken into consideration when CRSS is beginning. Introduction: In our Intensive Care Unit (ICU), we have already started expanded application to the contact precautions. Applied patients are; 1) Emergency admission, 2) Patients who had already had bacteria* that are required to contact precautions, 3) Scheduled surgical patients with prolonged ICU stay, although we have not yet decided the started period of expanded application exactly. *Detected Bacteria(DB);MRSA, CD, MDRP, ESBL, Pseudomonas A, PISP, PRSP, VRSA. The aim of this study was to determine the adequate starting period of expanded application to the contact precautions in the scheduled surgical patients in the mixed ICU. Methods: We performed retrospective observational study on 4221 patients who were admitted to our ICU after planed surgery from May 2013 to Dec. 2017. We detected the patients who acquired BD newly and investigated the relation to the length of ICU stay. The relationship between detection rate and categorized date was also analyzed using logistic regression adjusted for age, gender, APACHE2, and SOFA score. Using Youden´s index and ROC curve, we also calculated cutoff point of the duration of ICU stay related to detection rate. Finally, we made the logistic regression model of each cutoff day(day1 to 7) and compared Odds Ratio(OR) and AUC of each models using stata. Results: Category day 2 or more, especially day 4 or more had significantly higher detection rate of DB compared to day 1 ( Results: PaO2/FiO2 was lower than 240 mmHg in 171 (67%) patients. Compared to patients in group 2, patients in group 1 were less severely ill at admission but presented a higher SOFA and CPIS score and a greater incidence of ARDS and shock at pneumonia onset (Fig 1) . 117 (69%) patients in group 1 had a microbiological diagnosis of pneumonia, compared to 71 patients (85%) in group 2 (p=0.007). PaO2/FIO2 ≤240 mmHg was associated with less probability of having microbiological diagnosis of pneumonia (OR 0.40, 95% CI 0.21 to 0.79, p=0.008). When adjusted for other variables significantly associated with positive microbiology, PaO2/FIO2 ≤240 mmHg remained significantly associated with less probability of a microbiological diagnosis (adjusted OR 0.34, 95% CI 0.12 to 0.94, p=0.038). Hospital mortality was significantly higher in patients in group 1 compared to group 2 (42% vs 29%, p=0.044). However, no difference was found in non-response to treatment, ICU and hospital stay, ICU mortality (Table 1) and 90-days survival (Fig 2) . Conclusions: A significant higher number of patients with VAP didn't have a definitive etiological diagnosis when using the proposed threshold criteria of PaO2/FIO2 ≤240 mmHg. PaO2/FiO2 ratio does not seem a good predictor of etiology in patients with VAP. Introduction: Immunological dysfunction is common in critically ill patients but the optimal method to measure it and its clinical significance are unknown. Levels of tumor necrosis factor alpha (TNF-α) after ex-vivo whole blood stimulation with lipopolysaccharide has been proposed as a possible method to quantitate immunological function. We hypothesized that patients with a lower post-stimulation TNF-α level would have increased rates of nosocomial infections (NIs) and worse clinical outcomes. Methods: A secondary analysis of a phase 2 randomized, multicentre, double-blinded placebo controlled trial [1] . There were no differences in allocation groups; all the patients were analyzed as one cohort. On enrolment, whole blood was incubated with LPS ex-vivo and TNF-α level was measured. Patients were grouped in tertiles according to delta and peak TNF-α level. The primary outcome was the development of NIs; secondary outcomes included 90-day mortality. Results: Data was available for 201 patients. Baseline characteristics and outcomes are reported in Tables 1 and 2 . Patients in the highest tertile for post LPS stimulation delta TNF-α compared to the lowest tertile were younger, had a lower acuity of illness and had lower baseline TNF-α. When grouped according to peak post-stimulation TNF-α levels, patients in the highest tertile had higher serum TNF-α at baseline. Both comparisons showed no difference between NIs and clinical outcomes between tertiles. In multi-variate analysis peak or delta TNF-α were not associated with the occurrence of NIs. Conclusions: Admission ex-vivo stimulated TNF-a level is not associated with the occurrence of NIs or clinical outcomes. Further study is required to evaluate the ability of this assay to quantify immune function over the course of critical illness. Results: Sanitary and epidemiological examination revealed the connection between infection and intravenous infusion of dexamethasone performed concurrently with chemotherapy. In 5 patients fever with chills and hypertension developed within 2 hours after infusion of the infected drug; empirical intravenous antibiotic therapy started immediately after collecting blood culture. In 6 patients fever appeared after 2-4 days outpatiently, so they received antibiotics per os. All these patients had permanent vascular access, and BSI was detected either the next chemotherapy course when fever reappeared (3 pts) while using vascular access, or as a result of a specific examination (3 pts). In all cases empirical antibiotic therapy started on the first day of fever, drug correction was performed in 6 patients according to results of bacteriological research. Septic shock developed in 1 patient, pneumonia in 3 patients. Permanent vascular access was preserved only in 1 case. All patients were cured and continued to receive antitumor treatment. Conclusions: Detection of more than 1 case of B. cenocepacia BSI should be the reason for sanitary and epidemiological examination. A favorable outcome of BSI treatment is associated with the early start of antibiotic therapy and its correction after microbiological examination. Emerging Conclusions: Implementation of ASP in hospital allows to decrease incidence of ESKAPE-bacteremia and candidemia, which may lead to improved clinical outcomes in ICU's patients (Fig 1) . Association of multi-drug resistant (MDR), extended-drug resistant (XDR) and pan-drug resistant (PDR) gram negative bacteria and mortality in an intensive care unit(icu) S Chatterjee 1 , S Sinha 1 , A Bhakta 2 , T Bera 3 , T Chatterjee 3 , S Introduction: Colistin-resistant Klebsiella pneumoniae (CR-KP) is increasingly reported around the world. It is worrying to note emergence of resistance to last line of defence against MDR gram negative infections in regions endemic to Carbapenem resistance. We report the first outbreak of CR-KP co-producing carbapenemases in an adult Intensive Care Unit (ICU) from South India. Methods: Retrospective analysis of all patients with Carbapenem resistant Klebsiella pneumoniae blood stream infection (BSI) was done between January 2017 and December 2017. Microbiological and clinical variables along with outcomes were analysed. Results: Seven patients had CR-KP with no prior exposure to Colistin. All seven were modified Hodge test (MHT) negative making probability of blaKPC unlikely. In resource limited setting, analysis beyond MHT could only be performed for CR-KP samples. 2/7 samples belonging to CR-KP isolates produced the blaNDM-1 whilst 5/7 CR-KP isolates did not produce either blaKPC or blaNDM carbapenemases prompting hypothesis of blaOXA-48 or blaVIM as the causative factor. Compared to Carbapenem resistance only group, CR-KP group had higher APACHE II, ICU length of stay and mechanical ventilation duration. 28 day mortality was noted to be 56.3% for Carbapenem resistant and 43% for CR-KP groups. Aggressive infection control measures were undertaken with successful containment of CR-KP strains along with reduction in overall BSI. Conclusions: Infection control measures form the backbone of patient care in centres showing endemicity for Carbapenem resistant Klebsiella to prevent Colistin resistance and also to reduce occurrence of overall blood stream infections. Rapid diagnosis of carbapenem resistance: experience of a tertiary care cancer center with multiplex PCR S Mukherjee Tata Medical Center, Critical Care Medicine, Kolkata, India Critical Care 2019, 23(Suppl 2):P074 Introduction: Sepsis due to carbapenem resistant organisms has high mortality; inappropriate empirical antibiotic is one of the main causes of this poor outcome. On the contrary, "Too much" broad spectrum empiric antibiotics will increase drug resistance, even in community, because of selection pressure. So, early diagnosis of resistance pattern (carbapenemase genes) is crucial. Aim of this study is to compare rapid diagnostic test like polymerase chain reaction (PCR) with conventional culture sensitivity (C/S) to identify carbapenem resistance. Methods: This is a prospective observational study done in Tata Medical Center, Kolkata, India. Real time multiplex PCR technique has been developed "in house" in our microbiology lab and can identify NDM, NDM1, KPC, OXA -23, OXA -48, OXA -58 & VIM carbapenemase genes. Blood cultures were sent as per clinical & laboratory diagnosis of sepsis in ICU patients. Culture positive samples had been used for conventional C/S by VITEK 2 system along with PCR study to identify carbapenemase genes. Result of PCR technique was been compared with conventional C/S method. Results: Multiplex PCR results were available within 3-4 hours of positive blood culture compared to conventional C/S method that takes 2 -3 days. Among 392 positive blood cultures, 146 samples were positive for carbapenemase genes. Most common gene identified was OXA -48 (43%), followed by NDM1 (25%). Our PCR technique has very high sensitivity, specificity, positive & negative predictive value (99.35%, 94.65%, 93.1% & 99.43% respectively) while comparing with final C/S report by VITEK 2 system (Table 1) . There was only one false negative diagnosis for carbapenem resistance. Conclusions: Real time multiplex PCR for carbapenemase gene can be helpful for early diagnosis of carbapenem resistance and can help us to choose / modify antibiotics or to use 'targeted therapy'. It is more practical to "rule -in" infection rather than "rule -out" by this technique. Carbapenemase producing enterobacteriaceae colonization in an ICU: risk factors and clinical outcomes M Miranda, JP Baptista, J Janeiro, P Martins Centro Hospitalar e Universitário de Coimbra, Intensive Care Unit, Coimbra, Portugal Critical Care 2019, 23(Suppl 2):P075 Introduction: Carbapenemase-producing Enterobacteriaceae (CPE) colonization has been increasingly reported in Intensive Care Units (ICUs) since their first identification more than 20 years ago. Colonization with CPE seems to constitute a risk factor for mortality. The aim of our study was to identify associated risk factors and clinical outcomes among patients with fecal colonization by CPE admitted to a Portuguese tertiary hospital ICU. Methods: A 2-year retrospective study was performed in patients with previous unknown CPE status (colonization or infection), admitted to our ICU. Rectal swabs were performed and analyzed using real-time polymerase chain reaction testing. Clinical records were reviewed to obtain demographic and clinical data. Results: Of 903 patients admitted, 38 (4.2%) harbored CPE, 15 (39.5%) were colonized at admission and 23 (60.5%) acquired CPE colonization during ICU stay. The most frequent carbapenemase genes detected were KPC (87.5%) and VIM (12.5%). CPE carriers had high rates of hospitalization (previous or ongoing), invasive procedures (mainly intraabdominal surgery), malignancy (hematopoietic or solid tumor), Introduction: Gram-negative pathogens-particularly Pseudomonas aeruginosa and Enterobacteriaceae-predominate in nosocomial pneumonia (NP) and cIAI both. These infections are becoming difficult to treat with available treatment options due to growing antimicrobial resistance in India. Ceftazidimeavibactam has in-vitro activity against Gram-negative organisms producing class A, class C and some class D beta-lactamases. We carried out a qualitative analysis to assess the safety and efficacy outcomes of the Indian population cohorts involved in the RE-PROVE and RECLAIM trials. Methods: In line with the global REPROVE protocol, Indian patients enrolled in the study with NP, were randomly assigned (1:1) to 2000 mg ceftazidime and 500 mg avibactam or 1000 mg meropenem. In the RECLAIM study, Indian patients with a diagnosis of cIAI were enrolled in the study and were randomly assigned (1:1) to receive either ceftazidime-avibactam (2000 mg of ceftazidime and 500 mg of avibactam) followed by metronidazole (500 mg); or meropenem (1000 mg). The primary efficacy outcome measure in the REPROVE and RECLAIM studies was clinical cure rate of CAZ-AVI compared with that of meropenem at TOC (test-of-cure) visit in pre-defined analysis sets. In both studies, Non-inferiority was concluded if the lower limit of the twosided 95% CI for the treatment difference was greater than -12·5% in the primary analysis sets. As the Indian subset study was not statistically powered to detect a difference in the subgroup, we descriptively analysed the efficacy results in the Indian population and compared them with the overall results in the global trial. In addition, the study also analysed the safety of CAZ-AVI in the Indian patients by monitoring the number and severity of adverse events. Introduction: Early administration of effective intravenous antimicrobials is recommended for the management of the patients with sepsis. Although Meropenem (MEPM) is one of the first-line drugs in patients with sepsis because of its broad spectrum, the optimal dose in the critical care settings especially during continuous renal replacement therapy (CRRT) has not been established since therapeutic drug monitoring of MEPM has not been popular. Methods: Eighteen critically ill patients who received CRRT were enrolled in this study. One gram of MEPM was administered over 1 hour, every 12 hours, and blood samples at 1, 2, 6, 9 and 12 hours after administration were collected on day 1, 2 and 5. All samples were stored at -80°C until analysis. The measurement of the blood concentration of MEPM was performed using high performance liquid chromatography with ultraviolet detection (HPLC-UV Introduction: Meningitis is one of the complications of severe traumatic brain injury, and it is often associated with encephalitis (incidence from 1.3-4.8% to 10-20%). The aim of the investigation was to study the dynamics of the concentration of meropenem in serum and cerebrospinal fluid (CSF) with intravenous and intrathecal administration of meropenem. Methods: In eight patients with bacterial meningoencephalitis blood serum and CSF were studied prior to the administration of meropenem and 5-10 min, 1, 2.5 and 5 hrs after it. Antibiotic regimen: 2000 mg of vancomycin (1000 mg BID) and meropenem (2000 mg TID diluted in 100 ml of saline IV + 20 mg BID diluted in 5 ml of saline bolus slowly intrathecally). Meropenem infusion was carried out for 30 minutes, 5 mins after it 5 ml of blood and 1 ml of CSF were sampled. Prior to antibiotics administration blood and CSF were taken for microbiological examination. To determine the concentration of antibiotics iquid chromatography/mass spectrometry was used. The samples were analyzed on an Agilent 1260 Infinity liquid chromatograph coupled to a Sciex QTrap 6500 mass detector (Sciex, US Introduction: The prophylactic use of probiotics has emerged as a promising alternative to current strategies viewing to control nosocomial infections in a critically-ill setting. However, their beneficial role in VAP prevention remains inconclusive. Our aim was to delineate the efficacy of probiotics for both VAP prophylaxis and restriction of ICU-acquired infections in multi-trauma patients. Methods: Randomized, placebo-controlled study enrolling 58 multitrauma patients, requiring mechanical ventilation for >10 days. Participants were randomly assigned to receive either probiotic (n=28) or placebo (n=30) treatment. A four-probiotic formula was applied and each patient received two capsules per day from Day1 to Day15 post ICU admission. The content of one capsule was given as an aqueous suspension by nasogastric tube, while the other one was spread to the oropharynx after being mixed up with water-based lubricant. The follow-up period was 30 days, while ICU stay and mortality were also assessed. ], while no difference in 30-day mortality rate was identified between groups (10.7% probiotics vs 6.7% placebo). Conclusions: The prophylactic administration of probiotics exerted a positive effect on the incidence of VAP or other ICU-acquired infections and ICU stay in a critically-ill subpopulation being notorious for its high susceptibility to infections, namely multi-trauma patients. Use of a C-reactive protein-based protocol to guide the duration of antibiotic therapy in critically ill patients: a randomized controlled trial I Borges 1 Introduction: The rational use of antibiotics is one of the main strategies to limit the development of bacterial resistance. In this study we aimed to evaluate the effectiveness of a C reactive protein (CRP) based protocol in reducing antibiotic treatment time in critically ill patients. Methods: An open randomized clinical trial was conducted in two adult intensive care units of a university hospital in Brazil (Clini-calTrials.gov: NCT02987790). Patients were randomly allocated to: i) intervention -duration of antibiotic therapy guided by CRP levels, and ii) control -duration of therapy based on best In the intention to treat analysis, the median (Q1-Q3) duration of antibiotic therapy for the index infection episode was 7.0 (5.0-8.8) days in the CRP group and 7.0 (7.0-11.3) days in the control group (p=0.011). In the cumulative suspension curve of antibiotics, a significant difference in the exposure time between the two groups was identified, with less exposure in the CRP group (p=0.007). In the pre-specified per protocol analysis, with 59 patients allocated in each group, the median duration of antibiotics was 6.0 (5.0-8.0) days in the CRP group and 7.0 (7.0-10.0) days in the control group (p=0.011). Mortality and relapse rates were similar between groups. Conclusions: Daily levels of CRP may aid in reducing the time of antibiotic therapy in critically ill patients, even in a scenario of judicious use of these drugs. Introduction: The Macrophage Activation Syndrome (MAS) or hemophagocytic lymphohistiocytosis(HLH) is a life threatening complication characterized by pancytopenia, liver failure, coagulopathy and neurologic symptoms and is thought to be caused by the activation and uncontrolled proliferation of T lymphocytes and well differentiated macrophages, leading to widespread hemophagocytosis and cytokine overproduction [1, 2] .The etiology is unknown, but is considered to have an infectious trigger.The aim of our study is to evaluate the impact of HLH in our 9 beds infectious diseases ICU, during 83 months period (2012-2018). Methods: A retrospective study based on electronic databases, including all patients admitted in our ICU, that have matched at least 5 out of 8 criteria for HLH diagnosis (1):fever; hepatosplenomegaly; >2 cytopenia (Hb <9 g/dl, PLT</dl, PMN -N<1000/mmc); hypertriglyceridemia>265mg/dl, fibrinogen<150mg/dl; hemophagocytosis-bone marrow, spleen, and/or lymphnodes; NK activity reduced/ absent; ferritin level>500UI/L; CD 25>2400. We have evaluated the etiology established with cultures, serology, and molecular methods, treatment with corticosteroids, IV immunoglobuline, cyclosporine, etoposide and outcome (2) . Results: 2112 patients were admitted to ICU, 15 patients(0.71%) met the criteria for HLH. The average length of stay in ICU was 15 days; 6 patients died (40%) without relation with the followed treatment. Conclusions: HLH is not a rare condition in infectious diseases ICU. The etiology is more frequent established compared with literature data. Treatment (corticosteroids, immunoglobuline, cyclosporine, etoposide) is not associated with increased survival Forecasting hemorrhagic shock using patterns of physiologic response to routine pre-operative blood draws Introduction: Irreversible hemorrhagic shock (IHS), a critical condition associated with significant blood loss and poor response to fluid resuscitation, can induce multiple organ failures and rapid death [1] . Determining the patients who are likely to develop IHS in surgeries could greatly help preoperative assessment of patient outcomes and allocation of clinical resources. Methods: machine learning model of IHS is developed and validated via porcine induced bleed experiment. 36 healthy sedated Yorkshire pigs first had one 20mL rapid blood draw during a stable period, and then were bled at 20mL/min to mean arterial pressure (MAP) of 30 mmHg. 10 subjects had IHS defined as MAP<20mmHg. Arterial, central venous and airway pressures collected at 250 Hz during the blood draw [ Fig 1] were used to extract characteristic sequential patterns using Graphs of Temporal Constraints (GTC) methodology [2] , and a decision forest (DF) model was trained on these patterns to determine subjects at high risk of impending IHS. Results: In a leave-one-subject-out cross-validation, our method confidently identifies 30% (95% CI [15.6%, 44.4%]) of the subjects who are likely to experience IHS when subject to substantial bleeding, while only giving on average 1 false alarm in 10,000 such predictions. This method outperforms logistic regression and random forest models trained on statistically featurized data [Tab 1, Fig 2] . Conclusions: Our results suggest that by leveraging sequential patterns in hemodynamic waveform data observed in preoperative blood draws, it is possible to predict who are prone to develop IHS resulting from blood loss in the course of surgery. Future work includes validating the proposed method on data collected from human subjects, and developing a clinically useful screening tool with our investigations. Work partially funded by NIH GM117622. Introduction: The H 2 S and oxytocin(Oxy) systems are reported to interact with one another [1] . H 2 S plays a major role in the hypothalamic control of Oxy release during hemorrhage [2] . There is scarce information about Oxy receptor(OxyR) expression in the brain in general and what is there is ambivalent. OxyR has been immunohistochemically(IHC) detected in the human hypothalamus but not in the hippocampus, in contrast to rodents [3] , which underscores the need for additional characterization in relevant animal models. Thus the aim of this study is to map the expression of the Oxy and H 2 S systems in the porcine brain in a clinically relevant model of hemorrhagic shock (HS). Methods: Anesthesized atherosclerotic pigs (n=9) underwent 3h of HS (MAP 40+/-5mmHg) [4] , followed by 72h resuscitation. IHC detection of Oxy, OxyR, the H2S producing enzymes cystathionine-γ -lyase (CSE) and cystathionine-β -synthase(CBS) was performed on formalin fixed brain paraffin sections. Results: Oxy, OxyR, CSE and CBS were localized in the porcine brain. Proteins were differentially expressed in the hypothalamus (Fig 2) , parietal cortex and cerebellum (Fig 1) . Cell types positively identified were: magnocellular neurons of the hypothalamus, cerebellar Purkinje cells and granular neurons, and hippocampal pyramidal and granular neurons of the dentate fascia. Arteries and microvasculature were also positive for OxyR and CSE. Conclusions: Our results confirm the presence of Oxy and OxyR in the hypothalamus similarly to the human brain. Novel findings were: OxyR in the cerebellum and CSE expression in the hypothalamus and cerebellum. The coexpression of OxyR and CSE may link and help better understand neurochemical systems and physiological coping in hemorrhagic shock. Funding: CRC1149 Introduction: Septic shock is one of the main causes of intensive care unit (ICU) admission, leading to mortality up to 50% of patients. Acute kidney injury (AKI) frequently occurs and is associated to great morbidity and mortality. Hemodynamic optimization may reduce the incidence of AKI, but the use of vasopressors to increase mean arterial pressure (MAP) could have deleterious effect on renal perfusion. We aimed at investigating the effect of MAP and norepinephrine (NE) on the incidence of AKI in septic shock patients Methods: Retrospective study based on prospectively collected data on digital medical records (Digistat) at our ICU. Introduction: In patients with distributive shock, increasing mean arterial pressure (MAP) to a target of >65 mmHg can improve tissue perfusion. Patients unable to achieve the target MAP of >65 mmHg despite adequate fluid resuscitation as well as catecholamines and vasopressin standard care (SC), may benefit from the noncatecholamine vasopressor angiotensin II to increase MAP. This posthoc analysis examined whether patients from the ATHOS-3 study with a baseline (BL) MAP <65 mmHg and treated with SC plus either angiotensin II (Ang II) or placebo achieved a MAP of >65 mmHg for 3 consecutive hours, without increasing the dose of SC therapy. Methods: Patients were assigned in a 1:1 ratio to receive Ang II or placebo, plus SC. Randomization was stratified according to MAP (<65 or >65 mmHg) at screening. In patients with BL MAP <65 mmHg, we evaluated whether patients achieved a MAP of >65 mmHg for the first 3 hours after initiation (MAP measurements taken at hours 1, 2, and 3), without an increase in the dose of SC. Results: Among 321 treated patients, 102 had BL MAP <65 mmHg (Ang II, 52; placebo, 50). Median BL MAP (IQR) was 61 (57-63) and 62 (59-64) mmHg for placebo and Ang II groups, respectively. Patients with BL MAP <65 mmHg who were treated with Ang II were more likely to achieve MAP ≥65 mmHg for 3 consecutive hours after initiation without an increase in SC dose (67%, 95%CI 53-80), compared with placebo-treated patients (24%, 95%CI 13-38, OR=6.52, p<0.0001). Conclusions: In this post-hoc analysis of patients with BL MAP <65 mmHg, patients receiving Ang II plus SC were significantly more likely to achieve a MAP >65 mmHg for the first 3 consecutive hours after initiation than patients receiving SC only. This suggests that administering Ang II may help patients with catecholamine-resistant distributive shock to achieve the consensus standard target MAP. Norepinephrine synergistically increases the efficacy of volume expansion on venous return in septic shock I Adda, C Lai, JL Teboul, L Guerin, F Gavelli, C Richard, X Monnet Hôpitaux universitaires Paris-Sud, Hôpital de Bicêtre, APHP, Service de médecine intensive-réanimation, Le Kremlin-Bicêtre, France Critical Care 2019, 23(Suppl 2):P093 Introduction: Through reduction in venous capacitance, norepinephrine (NE) increases the mean systemic pressure (Psm) and increases cardiac preload. This effect may be added to the ones of fluids when both are administered in septic shock. Nevertheless, it could be imagined that NE potentiates in a synergetic way the efficacy of volume expansion on venous return by reducing venous capacitance, reducing the distribution volume of fluids and enhancing the induced increase in stressed blood volume. The purpose of this study was to test if the increase in Psm induced by a preload challenge were enhanced by NE. Methods: This prospective study had included 30 septic shock adults. To reversibly reproduce a volume expansion and preload increase at different doses of NE, we mimicked fluid infusion through a passive leg raising (PLR). In patients in which the decrease of NE was planned, we estimated Psm (using respiratory occlusions) at baseline and during a PLR test (PLR High ). The dose of NE was then decreased and Psm was estimated again before and during a second PLR (PLR Low ). . The increase in cardiac index induced by PLR Low was significantly greater than that induced by PLR High (p<0.001). Δ PsmHigh -Δ PsmLow was moderately correlated with the diastolic arterial pressure at Baseline-High (p=0.03, r=0.41) and with the NE-induced change in mean arterial pressure (p=0.004, r=0.57). Conclusions: NE enhances the increase in Psm induced by a PLR, which mimics a fluid infusion. This suggests that it may potentiate the effects of fluid in a synergetic way in septic shock patients. This may decrease the amount of administered fluids and contribute to decrease the cumulative fluid balance. Introduction: Arginine vasopressin (AVP) can be used in addition to norepinephrine (NE) for NE-resistant septic shock. However, a subgroup who will response to AVP is unknown. The purpose of this study was to determine factors which could predict the response to AVP in patients with NE-resistant hypotension. Methods: This was a single-center, retrospective analysis of patients who administered AVP for NE-resistant hypotension in our intensive care units (ICUs). Eligible patients were adult patients who administered AVP in addition to NE due to hypotension (mean arterial pressure (MAP) < 65) in our ICUs between August 2014 and December 2017. We divided all patients into two groups by response to AVP; responders and non-responders. The responders were defined as an increase of MAP ≥ 10 mmHg at 1h after AVP initiation. We conducted univariate and multivariate logistic regression analysis to evaluate the effect of variables on AVP response. Results: A total of 163 patients were included; 107 responders (66%), 56 non-responders (34%). There was no significant difference for MAP at the time of AVP initiation (51 vs 52 mmHg; p = 0.40), initiation dose of AVP (0.028 vs 0.031 U/min; p = 0.67), and dose of NE at the time of AVP initiation (0.18 vs 0.18 μ g/kg/min; p = 0.95). MAP at 1h after AVP initiation was significantly higher in responders than non-responders (74 vs 55 mmHg; p < 0.01). Responders were older (69 vs 66; p = 0.02) and had lower heart rate (HR) (99 vs. 108; p = 0.01) and lactate (3.2 vs. 4.8 mmol/L; p = 0.02) at the time of AVP initiation. The multivariate logistic analysis revealed that HR ≤ 103 (OR 2.56, 95% CI 1.29-5.10, p < 0.01), lactate ≤ 3 (OR 3.11, 95% CI 1.56-6.19, p < 0.01) and age ≥ 63 (OR 2.16, 95% CI 1.05-4.45, p = 0.04) were significantly associated with the response to AVP. Conclusions: HR, lactate levels and age before AVP initiation can predict the response to AVP in ICU patients with NE-resistant hypotension. The maximum norepinephrine dosage of initial 24 hours predicts early death in septic shock D Kasugai 1 , A Hirakawa 2 , N Jinguji 3 , K Uenishi 3 1 Nagoya university Gtaduate School of Medicine, Department of Emergency and Critical Care, Nagoya, Aichi, Japan; 2 Fujita Health University, Department of Disaster and Traumatology, Fujita Health University, Toyoake, Japan; 3 Fujita Health University Hospital, Department of Emergency and General Internal Medicine, Fujita Health University Hospital, Toyoake, Japan Critical Care 2019, 23(Suppl 2):P095 Introduction: The mortality of septic shock refractory to norepinephrine remains high. To improve the management of this subgroup, the knowledge of early indicator is needed. We hypothesize that maximum norepinephrine dosage on the initial day of treatment is useful to predict early death in septic shock. Methods: In this retrospective single-center observational study, septic shock patients admitted to the emergency intensive care unit (ICU) of an academic medical center between April 2011 and March 2017 were included. Cardiac arrest before ICU admission and those with do-not-resuscitate orders before admission were excluded. The maximum dosage of norepinephrine initial 24 hours of ICU admission (MD24) was used to assess 7-day mortality. Results: One-hundred-fifty-two patients were included in this study. Median SOFA score was 11 (9-13), and median MD 24 was 0.24 (0.16-0.38) mcg/kg/min. Vasopressin and steroid were administered in 58 (38 %) and 67 (44 %) cases. Nineteen patients (13 %) died within a week. Non-survivors had higher MD24, higher SOFA score, and higher rate of vasopressin use. The higher MD24 predicted 7-day mortality (area under curve 0.797, threshold 0.60 mcg/kg/min, sensitivity 50 %, specificity 91%). After adjustment of inverse probability of treatment weighing method using propensity scoring, MD24 higher than 0.6 mcg/kg/min was independently associated with 7-day mortality (OR: 9.96, 95 %CI: 2.56-38.8, p < 0.001). Conclusions: The maximum dosage of norepinephrine higher than 0.6 mcg/kg/min initial 24 hours was significantly associated with 7day mortality in septic shock, and may be useful in the selection of higher severity subgroup. The impact of norepinephrine on right ventricular function and pulmonary haemodynamics in patients with septic shock -a strain echocardiography study K Dalla Sahlgrenska University Hospital Mölndal, Göteborg, Sweden Critical Care 2019, 23(Suppl 2):P096 Introduction: Septic shock is characterized by myocardial depression and severe vasoplegia. Right ventricle performance could be impaired in sepsis. The effects of norepinephrine on RV performance and afterload in septic shock are not immediately evident. The aim of the present study was to investigate the effects of norepinephrine on RV systolic function, RV afterload and pulmonary haemodynamics. Methods: Eleven, volume-resuscitated and mechanically ventilated patients with norepinephrine-dependent septic shock were included. Infusion of norepinephrine was randomly and sequentially titrated to target mean arterial pressures (MAP) of 60, 75 and 90 mmHg. At each target MAP, strain-and conventional echocardiographic were performed. The pulmonary haemodynamic variables were measured by using a pulmonary artery thermodilution catheter. The RV afterload was assessed by calculating the effective pulmonary arterial elastance (Epa) and pulmonary vascular resistance index (PVRI). Results: The norepinephrine-induced elevation of MAP increased central venous pressure (38%, p<0.001), stroke volume index (7%, p<001), mean pulmonary artery pressure (19%, p<0.001) and RV stroke work (20%, p=0.045), while neither pulmonary vascular resistance index nor Epa was affected. Increasing doses of norepinephrine improved RV free wall strain from -19% to -25% (32%, p=0.003), tricuspid annular plane systolic excursion (22%, p=0.010) and tricuspid annular systolic velocity (17%, p=0.029). There was a trend for an increase in cardiac index assessed by both thermodilution (p=0.079) and echocardiography (p=0.054). Conclusions: The RV function was improved by increasing doses of norepinephrine, as assessed both by strain-and conventional echocardiography. This is explained by an increase of RV preload. Pulmonary vascular resistance is not affected by increased doses of norepinephrine. Peripheral perfusion versus lactate-targeted fluid resuscitation in septic shock: the ANDROMEDA SHOCK physiology study. preliminary report G Hernandez 1 , R Castro 1 , L Alegría 1 , S Bravo 1 , D Soto 1 , E Valenzuela 1 , M Vera 1 , V Oviedo 1 , C Santis 2 , G Ferri 2 , M Cid 2 , B Astudillo 2 , P Riquelme 2 , R Pairumani 2 , G Ospina- Tascón Table 1 . Conclusions: This preliminary results suggest that using CRT as a target for FR in septic shock appears to be feasible, and not associated with impairment of tissue perfusion-related parameters as compared to lactate-targeted FR. Grant FONDECYT Chile 1170043 Introduction: Shock patients often become resistant to catecholamines which often require the addition of a non-catecholamine vasopressor. Preclinical studies suggest that in the presence of aadrenoceptor antagonism, the renin-angiotensin aldosterone system exerts the major vasopressor influence. We sought to determine the effects of AngII or Lypressin (LYP [porcine vasopressin]) on blood pressure in a norepinephrine (NE)-resistant hypotension pig model. Methods: Phentolamine (PHN), a reversible α-blocker that antagonizes the vasoconstriction by NE, was continuously infused to induce hypotension. After NE-resistant hypotension was established, LYP or AngII was then co-infused with PHN. Mean arterial pressure (MAP) and heart rate were continuously recorded (Fig. 1) . Results: As shown in Fig. 2 Conclusions: In a background of α-adrenoceptor blockade, at clinically comparable doses, the vasopressor effect of Ang II was maintained while those of NE and LYP were attenuated. These data suggest that the blood pressure effect of vasopressin-like peptides may require a functioning α-adrenoceptor. Patients with shock who are resistant to increasing doses of catecholamines may also have vasopressin resistance potentially making angiotensin II a preferred vasopressor for these patients. Introduction: Resuscitative Endovascular Balloon Occlusion of the Aorta (REBOA) has been increasingly used for the management of both traumatic and non-traumatic hemorrhagic shock. However, there is limited evidence for its use in gastrointestinal bleeding (GIB), especially in the ICU setting. We successfully treated a patient with massive GIB using REBOA in the ICU. We will discuss the difficulty performing the procedure and its countermeasure. Methods: A case report. Results: An 83-year-old woman was transferred to our hospital with shock. Coffee grounds material was found in a nasogastric aspirate after intubation and upper gastrointestinal endoscopy identified a pulsating large duodenum ulcer without active bleeding, for which an elective procedure was planned. She was admitted to our ICU, responded to initial resuscitation, and thereafter extubated. Her systolic blood pressure (SBP) suddenly dropped to 40mmHg with massive hematochezia at that night, and did not increase despite resuscitation with blood products, crystalloid and norepinephrine. To buy time until measures for stop bleeding, we planned to place REBOA in the ICU. Following the placement of a sheath in the left femoral artery, we tried to place a 7 Fr intra-aortic balloon occlusion catheter, which unintentionally and repeatedly went into the right common iliac artery because her left femoral artery was tortuous. After compressing the right lower abdomen, we managed to introduce REBOA in Zone 1. It took approximately 60 minutes to successfully place the catheter. The patient's SBP increased immediately after the balloon inflation and bleeding was endoscopically controlled. Introduction: The natural components of the pomegranate fruit may provide additional benefits for endothelial function and microcirculation. We hypothesized that chronic supplementation with pomegranate extract might improve glycocalyx properties and microcirculation during anaerobic condition. Methods: Eighteen healthy and physically active male volunteers aged 22-28 years were recruited randomly to the pomegranate and control groups (9 in each group). The pomegranate group was supplemented with pomegranate extract for two weeks. At the beginning and end of the experiment, the participants completed a high intensity sprint interval cycling-exercise (anaerobic exercise) protocol. The systemic hemodynamics, microcirculation flow and density parameters, glycocalyx markers, and lactate and glucose levels were evaluated before and after the two exercise bouts. Results: No significant differences in the microcirculation or glycocalyx were found over the course of the study. The lactate levels were significantly higher in both groups after the first and repeated exercise bouts, and were significantly higher in the pomegranate group relative to the control group after the repeated bout: 13.2 (11.9-14.8) vs. 10.3 (9.3-12.7) mmol/L, p = 0.017. Conclusions: Chronic supplementation with pomegranate extract has no impact on changes to the microcirculation and glycocalyx during anaerobic exercise, although an unexplained increase in blood lactate concentration was observed. Introduction: Extracorporeal membrane oxygenation in adults in accompanied by high mortality. Our ability to predict who will benefit from ECMO based on currently available clinical and laboratory measures is limited. The advent of single cell sequencing approaches has created the opportunity to identify cell populations and pathophysiological pathways that are associated with mortality without bias from a priori cell type classifications. Identification of such cell populations would provide both an important prognostic markers and key insight into immune response mechanisms and therefore a possibility for advanced drug matching that may impact clinical response to ECMO in these patients. Methods: Whole genome transcriptomic profiles were generated from a total of 40,935 peripheral blood monocytes obtained from 37 patients at the time of cannulation for ECMO (Fig 1) . Differential gene expression analysis was performed with the Monocle package for the R statistical analysis framework. Time-to-event data were analyzed in a survival analysis with a log-rank test for differences. Results: Genes encoding several members of the heat shock family of proteins were up-regulated in cells from non-survivors. Notably, these genes were expressed by a small fraction of cells (2.4% on average). Nevertheless, the proportion of cells expressing these genes was a significant predictor of survival to 30 days (p = 0.02 by log rank test), with a particularly pronounced effect in the first 5 days after initiation of ECMO support (Fig 2) . Conclusions: The proportion of cells expressing genes encoding members of the heat shock proteins is predictive of survival on ECMO. Majority of pt (43%) had no known predisposing conditions, followed by immobility (21%) and cancer (15%). In ECG analysis tachycardia and V1-V3 T wave inversion were the most common findings whereas hypoxemia± hypocapnia were the most prominent features in ABG analysis. 8 pt (10%) had bleeding complications (none intracranial), 3 (3.7%) during rtPA, 5 (6.3%) in the first 36h and only 3 pt required transfusion. Mortality rate was 12%: 6% directly due to PE (all during CPR) and 6% due to late complications (newly diagnosed cancer and infections). Conclusions: In our experience, fibrinolytic therapy is safe and effective but in submassive PE should be applied after thorough assessment of risks and benefits on individual basis aiming to patient tailored precision medicine. [2] trials evaluated the role of levosimendan in preventing low cardiac output syndrome in patients undergoing cardiac surgery. The studies were similar in their design and recruited patients with preoperatively low LVEF undergoing either isolated CABG or valve surgery combined with CABG (Table 1 ). In both, a 24-hour levosimendan infusion was started at induction of anesthesia. Neither study met the primary efficacy composite enpoints, but both showed a clear tendency for better outcome in patients undergoing a CABG compared to a valve procedure. We are currently evaluating the solidity of a co-analysis based on shared end-points. We are planning a shared analysed of the data related to the CABG settings and analyze the aggregated mortality data for both studies at 1 and 3 months by Cochran-Mantel-Haenszel odds ratio. Data from individual studies would be analysed as fixed effect and Breslow-Day test was used to evaluate homogeneity of the odds ratios Results: In the placebo groups of the two studies, the mortality is similar; 7.9% (22/279) in LEVO-CTS and 7.3% (9/123) in LICORN, corroborating the working hypothesis that the two studies can be coanalysed. In a preliminary combined analysis (Fig 1) , 90-day mortality was 7.7% (31/402) in the placebo group and 2.9% (12/407) in the levosimendan group. Odds ratio was significantly in favor of levosimendan (0.36; 95% confidence interval 0.18-0.72; p=0.0026, Fig. 1 ) Conclusions: The LEVO-CTS and LICORN trials can be co-analysed in their sub-setting of patients requiring isolated CABG surgery for mortality at 1 and 3 months. A preliminary analysis on mortality reinforce the hypothesis that, in isolated CABG surgery, levosimendan lowers post-operative mortality significantly both at 1 and 3 months, when started at the induction of anesthesia Introduction: Emergency medical system (EMS) -based ST elevation myocardial infarction (STEMI) networks allows not only STEMI diagnosis in the pre-hospital phase but also reduces treatment delays; treat your fatal complications and the immediate activation of the catheterization laboratory. The aim of study was to investigate the effect of out-of-Hospital by Mobile Intensive Care (MICU) versus Hospital beginning treatment in hospitalization length and survival of patients with STEMI diagnosis Introduction: Contrast induced nephropathy (CIN) is a complex acute renal failure syndrome, which can occur after primary percutaneous coronary intervention (PCI) and is an important cause of morbidity and mortality in this subgroup of patients. The aim of our study was to establish the incidence and predictors of CIN after primary PCI. We performed a retrospective analysis of STEMI patients treated with primary PCI in the period from January until September of 2017. CIN was defined as an absolute increase in baseline serum creatinine of ≥0.5 mg/dL (44 μmol/l) or >25% relative rise within 72 hours after primary PCI. We analyzed demographic characteristics, risk factors, clinical status at hospital admission, laboratory parameters, left ventricle ejection fraction and data regarding PCI procedure. Results: The study included 729 patients, with an average age of 62.33 ± 11.78 years, 66.1% of the patients were males. An average of 175.67 ± 91.77ml of contrast medium per patient was utilized. CIN developed in 11(1.5%) patients and overall intra-hospital mortality was 8.4 %. In multivariate analysis, the independent predictors of CIN were age>75 years ( Introduction: Left main coronary artery (LMCA) disease is a disease of the main coronary branch that gives more than 80% of blood supply to the left ventricle, it carries high mortality without surgical intervention; [1] however the influence of LMCA surgery on morbidity ICU measures needs to be explored. We aim to determine whether LMCA is definitive risk factor for prolonged ICU stay as a primary outcome and whether LMCA is definitive risk factor for early morbidity Methods: Retrospective descriptive study with purposive sampling analyzing 398 patients underwent isolated coronary artery bypass surgeries (CABG). Patients were divided into 2 groups those with LMCA disease as group 1 (75 patients) and those with coronary arty disease requiring surgery but without LMCA disease as group 2 (324 patients) then we will correlate with ICU outcome parameters including ICU stay length, postoperative atrial fibrillation, acute kidney injury, re-exploration, perioperative myocardial infarction, post operative bleeding and early mortality. Results: Patients with LMS had significantly higher diabetes prevalence (43.3% vs 29%, p=0.001). However, we did not find a statistical significant difference regarding ICU stay, or other morbidity and mortality outcome measures Conclusions: Diabetes was more prevalent in patients with LMS. The latter group showed similar outcome as those without LMS in this study these findings may help in guiding decision making for future practice and stratifying the patients care. Introduction: Multimorbidity in patients admitted for acute myocardial infarction [AMI] is associated with higher risk for in-hospital mortality and adverse clinical outcomes. We investigated to what extent an increasing number of comorbidities affects the age-stratified excess risk of death and other clinical outcomes among patients with myocardial infarction. Methods: We analyzed nationwide administrative data of 174`803 admissions for an acute myocardial infarction between 2006 and 2016. We calculated multivariate regression models to study the association of four comorbidities (chronic kidney disease [CKD], diabetes mellitus, heart failure [HF], and atrial fibrillation) and excess risk of in-hospital mortality, length of hospital stay [LOS] , and 30-day readmission and stratified the analysis for different age categories. Results: The incidence of admissions for AMI increased continuously during the observed decade without an increase in in-hospital mortality, LOS, and 30-day readmission. Among admitted patients with AMI, there was a stepwise increase in risk for adverse outcomes for each comorbidity. Compared to patients with no comorbidity, patients with 4 comorbidities had 6-fold increased risk for mortality (adjusted odds ratio [OR] 6.6, 95% confidence interval [CI] 5.6 to 7.7) and a similar risk for readmission (OR 1.0, CI 0.9 to 1.2). The LOS was 5.3 days (CI 5.1 to 5.5) in patients with no comorbidity and increased by 2.5 days (CI 2.4 to 2.5) with each additional comorbidity. These associations were stronger in younger compared to older patients. CKD was the strongest predictor of in-hospital mortality and LOS, while HF was the strongest predictor of 30-day readmission. Conclusions: This study of nationwide admitted patients with AMI found a stepwise increase in the risk for adverse outcome with increasing number of comorbidities, particularly in the younger patient population. Younger, multimorbid patients may thus have the largest benefits from multidisciplinary treatments. Introduction: Certified cardiac arrest centers, sophisticated post cardiac arrest care and prehospital ECLS teams aim to increase survivor rates with a preferable neurological outcome after cardiac arrest. Centers also provide emergency ECLS and ECLS pick ups for cardiogenic shock patients before arresting. Few data answer the question of the long-term quality of life after ECLS therapy. Methods: In a retrospective single center register we included patients after emergency ECLS (eCPR and cardiogenic shock) between 10/2010 and 10/2017 discharged alive and performed a follow-up after 2 years on average at 6/2018. In our center criteria to initiate ECLS therapy in cardiogenic shock or under cardiac arrest are an observed collaps, shockable rhythm, absence of frailty and severe comorbidities. All patients were requested to take part in a telephone interview. Thus, we analyzed survival, CPC scores and SF36 scores. Results: 97 patients with hospital survival after ECLS were screened. 44% (N=43) had survived until 6/2018; 38 patients were not accessible; 16 had ceased. 33 survivors (mean±SD; min-max; 53±17; 19-78 years, 8 women) answered SF36 questionaires 29±15; 7-64 months after ECLS (45% cardiogenic shock, 55% eCPR with shockable rhythm in 89%). The participantsĆ PC scores were in median 1. The results of the SF36 were physical functioning 84±21, physical role functioning 84±28, bodily pain 92±17, general health 68±23, vitality 65±26, social role functioning 83±28, emotional role functioning 96±14 and mental health 81±22. Survivors who did not take part at the SF36 had a CPC score of in median 2 (N=10, 5 personally signed refusals, 3 language barriers, 2 vegetative states). Conclusions: After emergency ECLS therapy and hospital survival 44% of our patients survived the following 2 years up to over 5 years with a preferable neurological outcome and a general mentally and physically satisfactory quality of life. A vague outcome in 39% limits the results of our study. Introduction: Successful weaning from VA-ECMO requires the restoration of a sufficient cardiac function to ensure an adequate tissue perfusion. Skin blood flow (SBF) is among the first to deteriorate during circulatory shock and the last to be restored after resuscitation. SBF would be a good predictor of successful weaning from VA-ECMO. Methods: Patients with VA-ECMO, who required a first weaning attempt, were included. Weaning procedure (WP) was performed by a reduction of VA-ECMO blood flow to 1 L/min for 10 minutes. The weaning criterion was an aortic velocity-time integral (VTI) > 10 cm. Successful weaning from VA-ECMO was defined as hemodynamic stabilization and without the need to increase the vasopressor dose during the next 24 hours. SBF, assessed by skin laser Doppler (Peri-flux5000, Perimed, right index finger); perfusion unit: PU), together with global hemodynamic parameters were obtained before and after 3 min of weaning. Receiver operating characteristic curves (ROC) were generated to assess the ability and reliability of baseline parameters to predict a successful weaning. Results: We studied 22 WPs in patients with VA-ECMO for pulmonary embolism (n = 2), post cardiotomy (n = 3), acute coronary syndrome (n = 14), myocarditis (n = 3). These were successful (SW) in 12 and unsuccessful (NSW) in 10. At baseline, hemodynamic variables, lactate, ECMO blood flow were similar in both groups (Table1). SBF was greater in SW than NSW patients (Table1). During WP, CI rose from baseline and was similar in SW and NSW (p=0.1) ( Table 2 ). VTIs were higher in SW than NSW (13 (12-14) vs 8 (6-8), respectively, p=0.03). SBF decreased in SW and remained low in NSW (Table 2) . From the ROC curves analyses, baseline SBF had the highest area under the ROC curve with a cut off ≥ 34 PU (sensitivity 83%, specificity 92%) (Figure1). Conclusions: SBF is a good predictor of successful weaning from VA-ECMO Introduction: Postoperative cognitive dysfunction (POCD) is defined as a temporarily decline in cognition associated with surgery. Long-term POCD (3 months after surgery) occurs in 10-30% of cardiac patients and is associated with a higher morbidity and mortality. Endo-CABG is a new minimally invasive endoscopic coronary artery bypass grafting (CABG) technique that requires retrograde arterial perfusion which may be associated with a higher incidence of neurological complications. The aim of this study is to assess the incidence of POCD after endo-CABG. Methods: Sixty consecutive patients undergoing an endo-CABG were enrolled. POCD was assessed following the recommendations of the "1995 statement of consensus on assessment of neurobehavioral outcomes after cardiac surgery". A comparative group of 60 patients undergoing percutaneous coronary intervention (PCI) and a control group of 60 healthy volunteers were also enrolled. Additional tests included the digit span test and digit symbol-coding test. Patients were tested at baseline and at 3month follow-up. POCD is defined as a Reliable Change Index (RCI) ≤ -1.645 (significance level 5%), or Z-score ≤ -1.645 in at least two different tests. Results: After enrolling 60 patients in each group, respectively 46 in the Endo-CABG-group, 44 in the PCI-group and 48 healthy controls were analysed. Patients suffering from a CVA within three months after their procedure were automatically classified as having POCD (PCI: n= 1; Endo-CABG: n= 1). The total incidence of POCD was not different between groups (PCI: n= 7; Endo-CABG: n= 6, p=0.732). Conclusions: Our results suggest that the risk of POCD after Endo-CABG is low and comparable with the risk of POCD after PCI. Introduction: Rhabdomyolysis ( RML) post aortic surgery probably affects the renal outcome adversely [1, 2] . There is no robust data regarding the same in literature. Methods: Retrospective single center data review; prior approval from Institutional review board. Patients were divided to two groups Group 1-with RML ( CK above cut off levels 1050 U/Litre) and Group 2 without RML. The determinants of RML and the impact of the same on outcome; predominantly renal function was evaluated. Chi-square tests are performed for categorical variables whereas, student t tests (un-paired ) are performed with continuous variables. Correlation is performed between Creatine kinase and creatinine rise. P value 0.05 (two tailed) is considered for statistical significant level. Results: Out of 33 patients, 21 patients (63.64%) developed Rhabdomyolysis ( GROUP RML) and 12 did not( GROUP NON RML). Demographic and intraoperative factors had no significant impact on the incidence of RML. There was a significantly higher incidence of renal complications including new postoperative dialysis in the RML group. Other morbidity parameters were also higher in the RML group. Conclusions: There is high prevalence of RML after aortic dissection surgery -Identification of risk factor and early intervention might help to mitigate the severity of renal failure Introduction: We investigate whether central venous pressure (CVP) pressure waveform signal can be informative in detection of slow bleeding in post-surgical patients. We apply a novel machine learning method to analyze CVP datasets to characterize bleeding in a porcine model of fixed rate blood loss. Methods: Thirty-eight pigs were anesthetized, instrumented with catheters, kept stable for 30 minutes, and bled at a constant rate of 20ml/min to mean arterial pressure of 30 mmHg. CVP waveforms were extracted from inspiration and expiration phases of respiration and statistically featurized. The proposed machine learning method, Canonical Least Squares (CLS) clustering, identifies correlation structures that differ between subsets of observations. We extend it to supervised classification. Both clustering and classification methods yield human-interpretable models that reflect distinctive patterns of correlations within CVP waveforms. Results: We conducted three experiments to discover structure in the physiological response to bleeding. First, we clustered respiration cycles with full knowledge of blood loss. The color-coded cluster assignments are shown in the Figure 1 . They are consistent with escalation of bleeding. Second, we deployed clustering on only CVP features without blood loss. Temporal structure was complemented with some subject-specific clusters (Fig 2) . Third, we ran CLS classification to decide whether an observation came from before or after the onset of bleeding (performance shown in the Results: Over the last decade, the number of patients with HLHS who underwent Norwood 1 has increased. Interstage mortality has decreased, and is currently 30-40%. Significant morbidity was not seen at a rate higher than in the international literature. Discharge planning, and community access to allied health professional services remained a concern. Conclusions: The paediatric congenital cardiac surgical service in the United Arab Emirates is relatively new (compared to some services around the world). Interstage mortality in HLHS is improving as a result of programme development, surgical progress and postoperative care. In the interstage period, there is currently no home monitoring programme in place. Some patients were found to have had very extended hospital admissions. Improved community support may reduce interstage mortality further, as well as improve the social situation of many of these patients. Postoperative complications were observed in 112(10.9%) patients. We lined out the prevalence of cardiac complications, such as heart failure and rhythm disturbances, observed in 87 (8.5%) and 53 (5.1%) patients respectively. Hospital mortality rate was 2.8% (29/1020). The cause of mortality in all cases was acute heart failure, due to the initial severity of the disease, and in 11(1.07%) cases an acute myocardial infarction was diagnosed. Duration of postoperative period was 7.8 ± 0.9 days. Conclusions: Off-pump Coronary artery bypass grafting can be safely performed with relatively low incidence of mortality and postoperative morbidity. Prognostic value of mid-regional pro-adrenomedullin and midregional pro-atrial natriuretic peptide as predictors of multiple organ dysfunction development and ICU length of stay after cardiac surgery with cardiopulmonary bypass in adults Introduction: One of the most harmful complications after cardiac surgery with cardiopulmonary bypass is a syndrome of multiple organ dysfunction (MODS). We consider that mid-regional proadrenomedullin (MR-proADM) and mid-regional pro-atrial natriuretic peptide (MR-proANP) plasma concentrations can be used as predictors of MODS development and LOS in ICU. Methods: Thirty six adult patients (mean age 60 years, 27 male) with cardiovascular diseases undervent cardiac surgery with cardiopulmonary bypass (heart valve(s) replacement -20 (55.6%) patients, aorta and it`s branch surgery -14 (38.9%) patients, valvular surgery and coronary artery grafting -2 (5.5%) patients). NYHA heart failure class II was in 3 (8.3%) patients, IIIin 27 (75%) patients, IVin 6 (16.7%) patients. In the dynamics levels of MR-proADM and MR-proANP were measured in the venous blood with the Kryptor compact plus analyzer (Thermo Fisher Scientific, Germany) before 1 day and on the 1st and 6th days after surgery. All patients were divided into subgroups according to the lengths of stay in the ICU and the development of MOD in the postoperative period. The data are shown as median and 25th and 75th percentiles. The data were compared by Mann-Whitney U-test, pvalue of <0.05 was considered statistically significant. Results: Levels of MR-proANP did not significantly change at the study stages and did not have a significant difference between subgroups. The levels of MR-proADM increased in the first postoperative day and remained elevated for 6 days. This increase was significantly higher in subgroups of increased LOS in ICU and with MODS. The data are shown in the Table 1 . Conclusions: MR-proADM can be used as predictor of MODS and LOS in the ICU for adult patients underwent cardiac surgery with cardiopulmonary bypass. Introduction: Prolonged intensive care unit (ICU) stay after cardiac surgery is associated with increased mortality and cost .The aim of this study was to investigate factors influencing prolonged ICU stay. Methods: Consecutive patients who underwent cardiac surgery from June 2012 to October 2018 in our Cardiothoracic department, were retrospectively investigated. Group A consisted of pts with prolonged stay defined as more than 3 days and group B the rest of the cohort. The following characteristics and perioperative factors were compared between the groups: smoking, diabetes, COPD, REDO(re-operation), ejection fraction (EF)<50%, emergent procedure, cardiopulmonary bypass time (CPB)>120 min, low cardiac output syndrome (LCOS), acute kidney injury(KDIGO) and mortalityChi square test was used for the statistical analysis. Introduction: Hemorrhagic complications of extracorporeal membrane oxygenation (ECMO) pose a major morbidity and mortality. Optimal anticoagulation strategies balancing risks of bleeding and thrombosis in children are poorly understood. We aimed to identify factors associated with non-surgical bleeding in the first 12 ECMO hours. Methods: We evaluated all pediatric (<18 yrs) post-cardiotomy patients requiring ECMO between Dec 2002-July 2017 stratifying them by presence/absence of surgical bleeding. Non-surgical bleeding was defined as chest tube output >3cc/kg/hr during the first 12-hours not requiring reoperation. Patient characteristics and coagulation parameters at various time points after ECMO initiation were compared between groups, and receiver operator characteristic (ROC) curves were constructed to identify models and thresholds with optimal predictive performance. Figure 1 . Conclusions: Deranged coagulation parameters, particularly kaolin Rtime may predict non-operative bleeding in pediatric ECMO patients. These findings may guide therapeutic anticoagulation while avoiding hemorrhagic sequelae in at risk patients. Introduction: Elevated cardiac troponin (cTn) level in patients (pts) admitted in the intensive care unit (ICU) is multifactorial and has been associated with a worse prognosis. The aim of the study was to review the frequency and the main cause of cTn elevation and to calculate a discriminating index. Methods: We retrospectively assessed all pts admitted in our eightbed general ICU during a 6-month period with at least one measurement of cTn during their ICU stay. We recorded clinical characteristics, the level of cTn on admission, the maximum cTn during ICU stay and the possible causes of elevation. Variables are expressed as mean ± SD or as median and Interquartile Ratio (IR), according to the normality of their distribution. Student´s Ô test or the Mann Whitney U tests were used to compare the group of elevated cTn with the group of normal cTn. The prognostic performance of elevated cTn was evaluated by the Receiver Operating Characteristics (ROC) curve. Statistical analysis was performed using SPSS version 21.0 (SPSS, Inc., Chicago, Illinois). Results: In 84 out of 92 pts that cTn was measured at least once, abnormal levels (>15.6 pg/ml) were found in 58 (69%) of them, and the maximum cTn value was 633 (1718.25) pg/ml. The clinical characteristics of the pts are depicted in Table 1 . Sepsis was the main cause of troponin elevation, which complicated by Acute Kidney Injury (AKI) in 20 pts (34%). Maximum cTn, AKI and the difference of maximum -admission cTn (ÄcTn) differed significantly between pts who survived and pts who died (p=0.029 and 0.001, respectively). The Area Under the Curve (AUC) was 0.753 and the optimal prognostic cut-off value of ÄcTn was 57 pg/ml with a sensitivity of 0.656 and a specificity of 0.833 Conclusions: Raised cardiac troponin values is a frequent finding in ICU pts and sepsis is the driving cause. AKI and the difference between maximum and admission cTn measurements differ significantly between pts who survive and pts who die. An elevation of cTn during ICU hospitalization >57 pg/ml seems to be a threshold indicating poor prognosis regarding both mortality and AKI. The prognostic role of NT-pro-BNP in septic patients with elevated troponin T level Introduction: Sepsis is frequently accompanied with release of cardiac troponin T (TnT) and NT-pro-BNP, but the clinical significance of this myocardial injury and cardiac dysfunction remains unclear [1] . TnT is known to be an independent predictor of mortality, whereas the prognostic role of NT-pro-BNP is uncertain. Methods: Here, we report data of VA-ECMO-patients, treated with dobutamine, levosimendan, suprarenin or no inotropic agens, in respect of 30-day survival. All data were collected retrospectively (10/ 2010 to 10/2018) at a single center, all patients with a survival below 24 hours were excluded. While treatment of VA-ECMO patients is strongly guided by standard operation procedures at our institution, no recommendation on positive inotropic therapy could be made. Results: A total of 232 VA-ECMO patients were evaluated, of which 47 patients were treated with levosimendan within 48 hours after cannulation. 30day survival in the whole cohort was 41.9%. A total of 99 patients did not receive any positive inotropic therapy at 24 hours after implantation (survival 48.0%). Survival was best in the levosimendan plus dobutamine group 50%, followed by dobutamine mono-therapy 46.2% and levosimendan mono 41.2%. Survival with suprarenin mono was 33.3%, suprarenin plus levosimendan 25.0% and suprarenin plus dobutamine 18,8%. Pooling data, we found no evidence that levosimendan and/or dobutamine (survival 45.8%, n=83, p=0.882) improves survival over no inotropic therapy (Fig 1) . Therapy with any combination including suprarenin however resulted in poor survival (27.7%, n=65, p=0.009). Adjustment for lactate levels or eCPR did not change the results. Conclusions: This retrospective analysis of 232 VA-ECMO patients shows no evidence that early inotropic therapy improves outcomes in VA-ECMO patients. This conclusion is obviously biased by retrospective design. Until randomized data are available, suprarenin however should be avoided. Survey of non-resuscitation fluids in septic shock A Linden-sonderso 1 Introduction: Positive fluid balance is associated with poor outcome in septic shock. The objective of the present study was to characterize non-resuscitation fluids in early septic shock. Methods: Consecutive patients >18 years of age were screened for inclusion criteria during a 4-month period in 8 ICUs in Sweden and in Canada. Inclusion criteria were septic shock per SEPSIS-3 definition within 24 hrs of ICU admission. A maximum of 30 patients per center were included. Type, indication and volume of non-resuscitation fluids were recorded during the first 5 days of admission. Fluids other than colloids, blood products and crystalloids given at rate > 5ml/kg/h were considered to be non-resuscitation fluids. The study was registered on Clini-calTrials.gov (NCT03438097). Data are presented as median (interquartile range). Results: A total of 201 patients were included between March 1st and June 30th 2018 (see Table 1 for demographics). Patients received 7886 (4051-12670) milliliters (ml) of non-resuscitation fluids Introduction: We aimed to ascertain the extent and make-up of fluid overload in critically ill patients and to identify whether delivery of more concentrated medications could reduce this. Positive fluid balance is associated with increased mortality [1] . A recent study has shown that the predominant component of fluid overload was from IV medications and maintenance fluid [2] . Methods: We reviewed 20 sequential patients admitted to our ICU with an APACHE II score of greater than 15 and a length of stay (LOS) greater than 72 hours. The patients' electronic admission summary was interrogated to establish: length of stay (LOS) fluid balance at 72 hours, total volume administered as IV medications, total volume administered as maintenance fluid and total fluid administered Introduction: In children less than 30 kilograms, maintenance fluids are routinely added to the resuscitation requirements calculated using Parkland's or other formulae. The contribution of this component for fluid resuscitation in children can add a significant quantity to total estimated fluid requirements. For example, in a child who is 10 kilograms with a 25% burn, the maintenance fluid requirement is 1000 mls per 24 hours and the resuscitation component per Parkland's will be 4 x 10 x 25%=1000 mls. Hence, the maintenance requirement can exceed the resuscitation requirement in this child if the burn surface area is less than a 25 % burn. The contribution of maintenance fluids to the total fluid requirements in small children with thermal injuries is under-recognised and not frequently studied. Methods: To understand the contribution of maintenance fluids to the total fluid requirements in children less than 30 kilograms who need resuscitation for thermal injuries of different sizes, we numerically simulated 1. children who had similar weights but different burn sizes and 2. Children with similar burn size but different weights. The results are as shown in Fig Introduction: Accurate quantification of fluid in resuscitation of thermal injuries is important for benchmarking, comparing and improving outcomes. In adults, it is usually expressed as mls/kg/%TBSA. In children, maintenance fluids are added to the resuscitation requirements. This is kept constant and the resuscitation component is titrated to meet pre-defined end points-usually urine output. Maintenance fluids are not uniformly stratified across the weight ranges. We propose that quantification of fluids in mls/ kg/%TBSA in children does not accurately capture fluid needs for resuscitation due to the maintenance component of the fluid requirement. Methods: We conducted this retrospective study in children admitted to a single-center Burns Intensive Care Unit (BICU) between January 2010 and December 2014. Children ≤30 kilograms with TBSA ≥15% admitted within 8 hours of their injury were included. OE (Observed to expected ratio) and fluid in mls/kg/% TBSA were calculated as shown in Figure 1 . Results: There were 33 children in the cohort with half requiring invasive mechanical ventilation in the BITU and nearly a quarter requiring inotropic support. The demographic details are as shown in Table 1 . The OE ratio at the end of 24 hours in the cohort was 1.01 (0.68-1.36). The total fluid given was 6.9 (6.1, 8) mls/kg/ % TBSA. The Titrated resuscitation component was 4.2 (3.7, 5.6) mls/kg/TBSA. Total fluid (which included the maintenance fluid) had a poor correlation with OE ratio R2=0.34 (Fig 2) . Exclusion of the maintenance fluid had a better correlation with the OE ratio R2=0.75 Conclusions: To capture differences in the titratable resuscitation component rather than differences in the maintenance requirements, fluid should be quantified in children by excluding the maintenance component when expressed as mls/kg/%TBSA. Dynamic arterial elastance for predicting mean arterial pressure responsiveness after fluid challenges in acute respiratory distress syndrome patients P Luetrakool 1 , S Morakul 1 , V Tangsujaritvijit Introduction: Dynamic arterial elastance (Eadyn; pulse pressure variation/stroke volume variation; PPV/SVV) is a dynamic parameter of arterial load that can be continuously monitored. Previous study proposed that Eadyn was able to predict mean arterial pressure (MAP) responsiveness after fluid challenge [1] [2] [3] [4] [5] . The objective of this study was to assess whether the Eadyn was able to predict MAP responsiveness in acute respiratory distress syndrome (ARDS) patients ventilated with low tidal volume. Methods: We performed a prospective study of diagnostic test accuracy in adult ARDS patients with acute circulatory failure and fluid responsiveness. All patients are continuously monitored blood pressure via arterial line connected with Flotrac® transducer and Vigileo® monitor. Once the attending physicians decided to load intravenous fluid, we recorded PPV/SVV and also other hemodynamic parameters before and after fluid bolus. MAP responsiveness was defined as an increase in MAP ≥ 10% from baseline after fluid challenge. Results: Twenty-three events were included. Nine events (39.13%) were MAP-responsive. Cardiac output, heart rate and stroke volume were similar in both MAP-responder and MAP-nonresponder group. Baseline MAP, diastolic blood pressure (DBP) and pulse pressure (PP) were significantly different after fluid challenge in MAP-responder group. Eadyn of preinfusion phase was failed to predict MAP Conclusions: One of the arterial load parameters such as Eadyn derived from non-calibrated pulse contour analysis method was unable to predict MAP responsiveness in ARDS patients with low tidal volume ventilation. The Our aim is to test the hypothesis that in FR septic shock patients, fluid load will determine a significant increase in Pmsf but not in CVP. We prospectively included all mechanically ventilated patients with diagnosis of septic shock with invasive hemodynamic monitoring (transpulmonary thermodilution VolumeView-EV1000 Ed-wards©). We collected hemodynamic and metabolic data and Pmsf with the inspiratory holds technique, before and after a fluid challenge (FC) of 500 ml of ringer lactate in 10 minutes). FR was defined as an increase in cardiac output (CO)>15%. Results: 13 measures were obtained in 11 patients. In 8 case we observed FR. We found a significant increase in Pmsf after a FC (mean difference(md) 16.9±21.5 mmHg, p=.015). CVP increased significantly (md 2.5±2.4 mmHg, p=.002). Pmsf increased significantly in non-FR (md 27±10mmHg, p=.013) but not in FR while CVP was higher after FC only in FR (md 2.4±2.1mmHg, p=.008). Venous return gradient (Pmsf-CVP) globally increased after FC (md 14±22 mmHg, p=.03), but only in non-FR such increase was significant (md 24±12 mmHg, p=.03). No correlation was found between the variation CO and venous return gradient. We did not find any improvement in metabolic parameters after the fluid challenge. Conclusions: Pmsf and combined CVP variations do not correlate with FR in our cohort of septic shock patients. Inspiratory holds may not be adequate to infer Pmsf in such context. Further studies are warranted to investigate the effect of FC on Pmsf in this field. Evaluation of pre-load dependence over time in patients with septic shock I Douglas 1 , P Alapat 2 , K Corl 3 , M Exline 4 , L Forni 5 , A Holder 6 , D Kaufman 7 , A Khan 8 , M Levy 3 , G Martin 9 , J Sahatjian 10 , W Self 9 , E Seeley 9 , J Weingarten 9 , M Williams 9 , C Winterbottom 11 , D Hansell 12 is an effective method to predict fluid responsiveness (FR) or cardiac response to preload expansion. We have previously shown that fluid responsiveness is a dynamic state, changing frequently over a 72 hour monitoring period. Methods: FRESH is a currently enrolling prospective randomized controlled study, evaluating the incidence of FR and patient centered outcomes in critically ill patients with sepsis or septic shock (NCT02837731). Patients randomized to PLR guided resuscitation were evaluated every 6-12 hours over the first 72 hours of care and classified as FR if the SV increased > 10% when measured with non-invasive bioreactance (Starling SV, Cheetah Medical). The time of first FR was noted. Results: A total of 608 PLR assessments were performed in 86 patients over a 72 hour monitoring period. 56 % were female, and the average age was 61 years. PLRs were evaluated over time, with time 0 representing initial fluid resuscitation ( Figure 1 ). When individual subjects were evaluated over time, 100% of subjects who became FR only after 24 hours showed evidence of LV/RV dysfunction ( Figure 2 ). Conclusions: Fluid responsiveness or preload dependence frequently changes for septic shock patients over the first 72 hours of care. Evidence suggests it is beneficial to periodically perform an assessment of preload responsiveness to guide fluid administration, as preload dependence is a dynamic and changing state. Preload dependence provides additional information beyond fluid responsiveness. Those patients who remain primarily fluid non-responsive (preload independent) are more likely to demonstrate ECHO confirmed LV/RV dysfunction, as the delay in return to cardiac function may be related to underlying cardiac deficits. Further evaluation may be indicated in preload independent patients. Introduction: Hydroxyethyl Starch (HES), a synthetic colloid, has been used as a volume expander, and is associated with renal impairment in patients with sepsis. However, a small dose of HES (6%, 130/0.4) has sometimes been used in acute ischemic stroke. Therefore, we investigated whether a small dose of HES was linked with renal deterioration in patients with acute ischemic stroke. Methods: A consecutive 524 patients with acute ischemic stroke within 7 days from onset were included between January 2012 and May 2016 (Fig 1) . We collected admission serum creatinine (SCr), estimated glomerular filtration rate (eGFR), and renal function was assessed using KDIGO definition of acute kidney injury on hospital days 5 to 9 as to patient's hospitalization period. is crucial for venous return and volaemic status, and as such it is a useful parameter in physiology and clinical settings alike. We tested whether: Near Infra-Red Spectroscopy (NIRS) could be effective at measuring MSFP both in healthy individuals and in conditions with a rise in interstitial pressures; after an occlusion pressure is relieved, the decrease in venular blood volume could allow calculation of τ (time constant) and thus Venous Resistances (Rv). In order to verify these hypotheses we used a forearm NIRS probe on healthy individuals at rest and during different degrees of Maximal Voluntary Contraction (MVC). Methods: 10 healthy subjects volunteered in the study that took place at Sant'Andrea Hospital in Rome (Italy). All subjects had venular pressures and volumes assessed via a NIRS probe positioned on the forearm using a pressure-cuff in steps of 5 mmHg from 0 to 50 mmHg, at rest and at 10% and 20% MVC. For each patient MSFP, unstressed Volume (Vu) and stressed Volume (Vs) were measured. A temporary 30 mmHg occlusion was obtained and volume time course was calculated upon release, to derive τ . Results: P-V relationship was found to have a 3-slopes shape reflecting venular network changes. We measured Vu, Vs, and obtained MSFP values of 6.74 ± 1.83 mmHg, p<0.001; during exercise no changes in Vu and Vs were noted but MSFP values rose; value was found to be 2.9 ± 0.1 sec at rest and 0.9 ± 0.03 sec after exercise, reflecting a reduction in Rv. Conclusions: NIRS measurements on healthy subject may have implications in the clinical assessment of critical care patients where changes in interstitial pressure are possible. Introduction: In the pathogenesis of multiple organ dysfunction syndrome (MODS) important role plays the development of hepatic dysfunction. A known method for assessing hepatic blood flow is reohepatography (RHG). However, it requires the analysis of a large number of parameters of the rheogram curve. The aim of this study was to develop a method for assessing arterial hepatic blood flow based on the RHG in patients with MODS after abdominal surgery. Methods: 55 patients in the department of anesthesiology and intensive care unit were included in a prospective study (36 men and 19 women, age 60.1 ± 16.1 years, weight 78.5 ± 13.9 kg.). All patients were divided into two groups: group 1 -patients after orthopedic and trauma surgery (n = 28), group 2 -patients after abdominal surgery with MODS (n = 27). Patients in the groups did not have statistical differences by sex, age, body weight, height. RHG was carried out using the "Reo-Spectr" (Russian Federation). We have compared the RHG indicators between the groups ( Table 1) . We have developed a method for assessing hepatic arterial blood flow, which consists in determining the area under the arterial part of RHG curve using the Simpson's rule. Its normal values range from 43.2 mΩ *s to 49.0 mΩ *s. The method is non-invasive, can be applied at the patient´s bed. Its advantage is simplicity, it can be used for rapid diagnosis and monitoring the effectiveness of treatment. Area under the RHG curve in the group 1 were 46.1 ± 8.1 mΩ *s and 20.0 ± 16.8 mΩ *s in the group 2 (p <0.05). Conclusions: Patients after abdominal surgery with MODS have impaired hepatic blood flow, which may be associated with liver pathology caused by main surgical disease (obstructive jaundice) and hemodynamic disorders caused by acute cardiovascular failure. The method we developed allows us to determine disorders of hepatic arterial blood flow in the early stages before signs of liver dysfunction appear. Comparison of pulse oximetry hemoglobin with laboratory measurement of arterial and central- Results: 80 patients: 54% male, median 68years (59-73); P:F ratio 174 (132-208); PEEP 10 (7-12); APACHE III 80.5 (26); median ventilation time 6 days (3-9). Fair agreement was seen in subjective assessment vs objective measures with binary assessment of RV size and function. Ordinal data analysis showed poor agreement with RVfwS ( Figure 1 ) and RV dimensions. If onestep disagreement was allowed the agreement was good ( Table 1 , 2). Significant overestimation of severity of abnormalities was seen comparing subjective assessment with RV EDA and TAPSE, s' and FAC. There was no difference in agreement values when accounting for clinician echo experience, perceived expertise (at level of cardiologist) or type of qualifications. Conclusions: Relatively low levels of agreement were seen with subjective assessment vs objective measures of RV size and function assessed by echo. It seems prudent to avoid subjective RV assessment in isolation and a combination of objective and subjective measures should be used. Introduction: Even short periods of hypotension are associated with increased morbidity and mortality. Using high-density numerical physiologic data, we developed a machine learning (ML) model to predict hypotension episodes, and further characterized risk trajectories leading to hypotension. Methods: A subset of subjects with 1/60Hz physiological data was extracted from MIMIC2, a richly annotated multigranular database. Hypotension was defined as >5 measurements of systolic blood pressure ≤ 90 mmHg and mean arterial pressure ≤ 60 mmHg, within a 10-minute window. Derived features using raw measurements of heart rate, respiratory rate, oxygen saturation, and blood pressure were computed. Random Forest (RF), K-Nearest Neighbors (KNN), and Logistic Regression models were trained with 10-fold cross validation to predict instantaneous risk of hypotension using features extracted from the data leading to the first episode of hypotension (cases) or ICU discharge in subjects never experiencing hypotension (controls). For a given subject, risk trajectory was computed from the collation of instantaneous risks. Results: From a source population of 2808 subjects, 442 subjects met our definition of hypotension, and 724 subjects without hypotension comprised the control group. 204 features were generated from the four vital signs. The area under the curve (AUC) for Random Forest classifier was 0.829, out-performing Logistic Regression (AUC 0.826) or K-Nearest Neighbors (AUC 0.783) (Fig 1) . Risk trajectories analysis showed average controls risk scores <0.3 (<30% risk of future hypotension), while the hypotension group had a rising risk score (0.45 to 0.7) in the 8 hours leading to the first hypotension episode, and significantly higher scores leading into subsequent episodes (Fig 2) . Conclusions: Hypotension episodes can be predicted from vital sign time series using supervised ML. Subjects developed hypotension have an increased risk compared to controls at least 8 hours prior to the episode. Introduction: In critically ill patients or in patients undergoing major surgery, monitoring of CO is recommended [1] [2] [3] . Less-invasive advanced hemodynamic monitoring with PWA is increasingly used in perioperative and critical care medicine. In this study, we evaluate the measurement performance of an uncalibrated pulse wave analysis (PWA) device (MostcareUp, Vygon, Ecouen, France) compared with cardiac output (CO) assessment by pulmonary artery thermodilution (PATD) in patients after cardiac surgery. Methods: In patients after cardiac surgery, we performed seven sets of PATD measurements to assess PATD-CO. Simultaneously, we recorded the PWA-CO and compared it to the corresponding PATD-CO. To describe the agreement between PWA-CO and PATD-CO we used Bland-Altman analysis showing the mean of the differences and 95%-limits of agreement and calculated the percentage error. Results: We included 17 patients in the analysis. The bias between PWA-CO and PATD-CO was 0.01 L*min-1. Upper and lower 95% limits of agreement were +1.40 L*min-1 and -1.38 L*min-1. The percentage error was 28.1%. Conclusions: PWA-CO estimated with using the MostcareUp device shows good agreement with pulmonary artery thermodilutionderived CO in patients after cardiac surgery. Introduction: Non-invasive continuous blood pressure monitoring devices have been investigated, however, these devices did not have sufficient accuracy and precision. We developed a continuous monitor using the photoplethysmographic technique and tested the accuracy and precision of this system to ensure it was comparable to conventional continuous monitoring methods used for critically ill patients. Methods: The study device was developed to measure blood pressure, pulse rate, respiratory rate, and oxygen saturation, continuously with a single sensor using the photoplethysmographic technique. Patients who were monitored with arterial pressure lines in the ICU were enrolled. The physiological parameters were measured continuously for 30 minutes at 5-minute intervals using the study device and the conventional methods. The primary outcome variable was blood pressure. Results: Pearson fs correlation coefficient between the conventional method and photoplethysmography device were 0.993 for systolic blood pressure, 0.985 for diastolic blood pressure, 0.998 for mean blood pressure, 0.996 for pulse rate, 0.995 for respiratory rate, and 0.963 for oxygen saturation. Percent errors for systolic, diastolic and mean blood pressures were 2.4% and 6.7% and 6.5%, respectively. Percent errors for pulse rate, respiratory rate and oxygen saturation were 3.4%, 5.6% and 1.4%, respectively. Conclusions: The non-invasive, continuous, multi-parameter monitoring device presented high level of agreement with the invasive arterial blood pressure monitoring, along with sufficient accuracy and precision in the measurements of pulse rate, respiratory rate, and oxygen saturation. Conclusions: Stroke volume measurement using bioreactance technique had strong correlation with ODM while PWTT had moderate correlation. Both devices had small bias with wide limits of agreement and percentage error compared with ODM. Therefore, these devices are not interchangeable with ODM. However, using trends in stroke volume to guide treatment might still be acceptable. Introduction: Hemorrhage is the most common cause of trauma deaths and the most frequent complication of major surgery. It is difficult to identify until profound blood loss has already occurred. We aim at detecting hemorrhage early and reliably using waveform vital sign data routinely collected before, during, and after surgery. Methods: We use waveform vital sign data collected at 250 Hz during a controlled transition from a stable (non-bleeding) to a fixed bleeding state of 93 pigs. These vital signs include airway, arterial, central venous and pulmonary arterial pressures, venous oxygen saturation (SvO2), pulse oximetry pleth and ECG heartrate, continuous CO, and stroke volume variation (LiDCO). We used Gated Recurrent Units (GRU), Long Short-Term Memory (LSTM) and dilated, causal, one-dimensional convolutional neural (Table 1) . However, outside of the very low FPR range (cf. ROCs in Fig. 1 and 2), our models appear inferior to a referenced Random Forest (RF) classifier. Conclusions: Our work demonstrates the applicability of deep learning models to diagnose hemorrhage based on raw, waveform vital signs. Future work will address why the RF classifier can address the greater homogeneity of subjects when they bleed compared to an apparently wide dispersion of their statuses when being stable. This work is partially supported by NIH GM117622. Can myocardial perfusion imaging with echo contrast help recognise type 1 acute myocardial infarction in the critically ill? Introduction: Many instances of significant bleeding may not occur in highly monitored environment, contribution in the delay in recognition and intervention. We therefore proposed a noninvasive monitoring for early bleeding detection using photoplethysmography (PPG). Methods: Fifty-two Yorkshire pigs were anesthetized, stabilized and bled to hemorrhagic shock, and their invasive arterial blood pressure (ABP), and PPG data were collected [1] . Time series of vital signs were divided into data frames of 1 minute updated every 30 seconds and beat to beat features were computed. The final feature matrix contained 18 ABP features and 85 PPG features. A supervised machine-learning framework using Least Absolute Shrinkage and Selection Operator regularized logistic regression model was constructed to score the probabilities for hemorrhage of each data frame. Data in stabilization was set as negative and data in bleeding was set as positive. Model performance was evaluated by receiver operating characteristic (ROC) area under the curve (AUC) with leave-one-out cross validation, and its precision was assessed with activity monitoring operative characteristic (AMOC). Results: Two different models were proposed using ABP and PPG features separately. Figure 1 showed the PPG model could classify the hemorrhage with AUC = 0.89, where the AUC of ABP model was 0.91. Figure 2 showed the PPG model could detect the hemorrhage on average 15.5 minutes (equals to 320 ml blood loss) if the false alarm rate of 1/100 was tolerated, whereas the average detection time of ABP model were 12.5 minutes at same threshold of false alarm rate. Conclusions: We proposed a novel non-invasive bleeding detection approach using PPG signals only. This method potentially can improve the identification of hemorrhage with in patients and environments where invasive monitoring is unavailable. Table 1 , catheter and procedure characteristics are shown in Table 2 . The median angle of bed position was 26°. No patients were positioned in neutral or TP. All procedures were successful with a mean of 1.3 punctures per patient, and a maximum of 3. The median procedure time was 12.5 minutes. No major complications occurred in any of our patients. Conclusions: Central venous catheterisation in moderate upright position is feasible and can be done safely when using realtime ultrasound by well-trained physicians. We recommend performing clinical assessment and pre-procedural ultrasound to choose the optimal puncture site and position in order to attain an optimal ultrasound visualisation of the vessel and patient comfort. Methods: A retrospective analysis of 10 patients presenting to tertiary-care emergency department who required CVC for vasopressor administration was carried out. All central venous cannulation into the right brachiocephalic vein was performed with ultrasound guidance using the high frequency linear probe. Right brachiocephalic vein was visualised in its long axis. The needle was positioned just beside the centre of ultrasound probe 15 degrees below the coronal plane and 10 degrees angle to the ultrasound probe and advanced just behind the clavicle. Results: The mean puncture time taken to perform this procedure, calculated from the needle piercing the skin until to the aspiration of blood from the brachiocephalic vein through the needle, was 23 ± 6.8s. No procedure-related complications were detected. Conclusions: The oblique needle trajectory of right brachiocephalic vein CVC in adult is feasible and able to visualised well the anatomical structure, hence avoid complications. Introduction: Central venous cannulation, a routine procedure on intensive care units, is associated with a low complication rate. As a consequence, the routine use of chest X-ray (CXR) or ultrasound (US) to assess these complications is under discussion. Our aim was to identify risk factors for central venous catheter (CVC) placement associated complications that can help decide whether or not follow-up using CXR and/or US is indicated. Methods: Multicenter prospective, observational study. Consecutive critically ill adult patients who underwent CVC placement. Either the internal jugular vein or subclavian vein was cannulated. Complication rates were determined. Predicting factors were obtained through a questionnaire filled in by physicians after placing a CVC. If the questionnaire was incomplete or data was missing, analyses were performed using the available data. Patient characteristics were duplicated if a patient recieved more than one CVC. Outcomes were iatrogenic pneumothorax and malposition. Pneumothorax was detected using US, whereas CXR was used to determine CVC malposition. Table 1 . USguidance, insertion site, and setting were predictive for complications. The overall CVC placement associated complication rate is low and multiple risk factors associated with the occurrence complications were identified. A complication rate this low, strongly suggests that routine post-procedural diagnostics is superfluous. Therefore, we suggest, provided that uneventful execution of the procedure is assured, post-procedural diagnostics are only necessary in selected cases with (multiple) risk factors. Introduction: The use of ultrasound for subclavian vein cannulation (SCV) has developed poorly due to the difficulty of visualizing this vein via the classical infraclavicular approach. We explored the feasibility of ultrasound-guided subclavian vein catheterization via a supraclavicular approach Methods: Prospective study conducted over six-month period in intensive care unit. After approval of the ethics committee, we included patients over 18 years of age and requiring central venous access. Exclusion criteria were: hemostasis disorders, puncture area infections and cervico-thoracic vascular malformations The procedure consisted of catheterization of the VSC with a supraclavicular approach under ultrasound guidance using an ultrasound in plane approach (Fig 1 and  2 ). Data collection included clinical and ultrasound data: SCV depth, diameter and length, catheterization time, number of needle redirection, cannulation success and complications. Results: Thirty four patients were included. age: 57 ± 15 (Mean ± SD), 60% of whom were male. The success rate of SCV catheterization was 97% (one failure). The depth of the SCV was 11 ± 4.5 mm and its diameter was 11 ± 3.5 mm. The puncturable length of the SCV was 33 ± 7mm and the puncture angle was 36 ± 15°. The time required to obtain an adequate ultrasound image was 25 ± 9 seconds. The interval between the beginning of the puncture and the insertion of the guidewire into the vein was 42 ± 29 sec. The total catheterization time was 69 ± 33 seconds. The number of needle redirection 0.8 +/-1.2 redirects. The quality of the ultrasound image was excellent or good in 87.5% of cases. An arterial puncture was observed in two patients Conclusions: This preliminary study demonstrated the feasibility of the subclavian vein cannulation via the supraclavicular approach. More study are required to confirm its safety and to compare this approach to the infraclavicular acces using ultrasound. Introduction: Lung ultrasound B-lines, a comet-like reverberation artefacts arising from water-thickened interlobular septa, indicate extravascular lung water which is a key variable in heart failure management and prognosis. Aim of this study is to measure the correlation between lung ultrasound B-lines and NYHA functional classification. Methods: This is a 6 months prospective study on congestive heart failure patients conducted in 2 urban emergency departments in Malaysia. Following enrolment, patients had their functional capacity categorised based on NYHA classification, followed by Point of Care Ultrasound (POCUS) lung scan using a 12MHz linear probe. The scanning was performed by trained emergency physicians. The longitudinal scan done at the recommended 6 zones of both left and right lungs and the total number of B-lines identified were summed up as the comet score. Comet Score of 0, 1, 2 and 3 were categorised based on amount of Blines of less than 5, 5-15, 15-30 and more than 30 B-lines respectively. Results: Hundred and twenty-two patients were analysed (69 males(56.6%) and 53 females(43.4%)) ranging from 24 to 88 years old. Comet Score of 1,2 and 3 were found to be statistically significant with presence of paroxysmal nocturnal dyspnoea, elevated jugular venous pressure, lung crackles, bilateral pitting oedema and chest radiographic findings. A moderate correlation between NYHA classes with Comet Score 1,2 and 3 (rs= 0.77 (P<0.01)) was documented. Conclusions: Our study demonstrated a moderate correlation between NYHA classes and lung ultrasound B-lines. Lung ultrasound may be a potential tool to objectively determine the functional capacity in patients with congestive heart failure and monitor its changes in response to treatment and disease progression. The Introduction: Point of Care Ultrasound (POCUS) is a tool of increasing utility in the management of the critically ill patient. Guidelines exist for training and accreditation in POCUS [1, 2] however the widespread use of POCUS has been hampered by a lack of mentors. Online communication with end-to-end security, such as WhatsApp ™ are increasingly used in medicine as a communication aid [3] . Some individuals are using such communications to share POCUS images for review-the overall sentiment around these tools is unknown. Methods: An online survey of POCUS users was conducted via Twitter ™. The question was "in situations where an expert opinion on an ultrasound is not immediately available, is it acceptable to get an expert review via an online medium such as WhatsApp, and would you be happy to be that expert?" Results: 304 votes were received. Voters were a mix of POCUS users from the USA, Europe, and Australia. 58% said the medium was acceptable, and that they would be happy to provide expertise. 34% voted "no", with 8% voting "other" (Fig 1) . Conclusions: In this international survey of POCUS users, 58% were happy to provide and receive mentorship using remote software such as WhatsApp. Distance mentorship for POCUS training should be explored. [2] . A description of the development and refinement of INSIGHT -a feasibility and clinical effectiveness randomized controlled trial. Methods: A modified Delphi exercise was used to select the most beneficial ultrasound windows and imaging questions to ask for each window in scheduled inter-professional ultrasound. 260 nurses, 46 doctors and 6 physiotherapists from critical care were given the same information regarding potential utility of each window. The windows and associated questions were individually ranked; each window and question tested against three further criteria; and filtered by ease of training to level 1 standard; clinical usefulness; time of practical delivery and applicability across an inter-professional group. Results: The Modified Delphi exercises and prioritization exercise ranked ease of adoption by training; feasibility within the time frame and clinical usefulness to develop a core INSIGHT scan of 5 domains, each with set binary questions (Tables 1 and 2 ) Conclusions: We have developed a research intervention that will allow us to test the effectiveness of inter-professional scheduled whole body assessment of critically ill patients by ultrasound. We now plan to conduct a clinical effectiveness trial with an internal pilot to confirm feasibility. To search for optimal pressing time, the plots from the color sensor during nail bed compression were analyzed. We found two phases in the color sensor plots. In the initial part of compression, the plots changes rapidly (rapid phase) and then the slope of plots reduces (slow phase). The pressure release during the rapid phase could destabilize the measurement. The longest period of the rapid phase was 1.9999 s among all the study subjects. Thus, a pressing time of 2 s seems to be needed to obtain stable CRT measurements. Conclusions: On our study for the investigation of standard pressing time and strength for CRT measurements, pressing the nail bed with 3-7 N and 2 s appears to be optimal. Detection of pancreas ischemia with microdialysis and CO2sensors in a porcine model Introduction: Pancreas transplantation is associated with a high rate of early graft thrombosis. Current postoperative monitoring lack tools for early detection of ischemia, which could precipitate a graft-saving intervention. We are currently exploring the possibility of ischemia detection with microdialysis and CO 2 -sensors in the organ tissue or on the surface in a porcine model. Methods: In anesthetized pigs, CO 2 -sensors and microdialysis catheters are inserted into the parenchyma or attached to the surface of the pancreas. PCO 2 is measured continuously and lactate is sampled with microdialysis every 15 min. Ischemia is induced by sequential arterial and venous occlusions for 45 minutes, with 120 minutes of reperfusion in between. Results: PCO 2 increased and decreased in response to ischemia and reperfusion within minutes. Lactate increased and decreased with the same pattern, but with a considerable delay as compared to PCO 2 . An example is depicted in Figure 1 . The values are presented in Introduction: Reliable automated handheld vital microscopy (HVM) image sequence analysis is a prerequisite for use of sublingual microcirculation measurements at the point-of care according to the current consensus statement. We aim to validate a recently developed advanced computer vision algorithm [1] versus manual analysis in a wide spectrum of populations and contexts. Methods: Our collaborators were invited to contribute raw data of published or ongoing institutional review board approved work. Inclusion criteria were use of the Cytocam HVM device, manual analysis with the AVA software, and image quality as independently assessed by Massey score of <10 in >50% of recordings in a random subset of each study. 233 subjects from 11 studies were included, covering clinical and experimental populations, major shock forms and interventions to recruit the microcirculation (Table 1) . Results: 2,599,710 red blood cells were tracked by the algorithm across 150,163 frames in 1462 measurements in real time. A good to excellent correlation was found between algorithm-determined and manual capillary density (p<0.0001, r 0.6-0.9, Figure 1 ). Capillary perfusion was classified using space-time diagram derived red blood cell velocity (RBCv), yielding good correlation with manual analysis for functional capillary density und proportion of perfused vessels. Microcirculatory alterations during disease and interventions were equally detected by the algorithm and manual analysis. Change in flow short of severe abnormality was reflected in absolute RBCv but not microcirculatory flow index. Conclusions: We demonstrate the validity of automated software for HVM image sequence analysis across broad populations, disease conditions and interventions. Thus, microcirculatory assessment at the bedside may finally complement point-of-care evaluation of disease severity and treatment response in critically ill patients and during surgery. Introduction: In 2016, Naumann et al introduced the POEM score as a real-time, point-of-care score to assess sublingual microcirculation [1] . Our study aimed to determine the reproducibility of the POEM score. Methods: Two expert operators used a sidestream darkfield (SDF) videomicroscope (Cytocam, Braedius, Netherlands) to separately acquire four high-quality video clips and assign a POEM score to each image in 20 adult mechanically ventilated patients. Each operator was blinded to the other's images and analysis. Video clip scores and acquisition times were recorded. Results: Of the 20 patients enrolled in this study, 45% (n=9) required vasopressors. We categorized POEM scores 4-5 as "normal" and POEM scores 1-3 as "impaired." (Fig 1) . With only one instance of interrater disagreement (i.e., a single image scored as 3 versus 4), Cohen's kappa (0.86) confirmed a strong correlation between interpreters. The mean time to complete a study session was 9 minutes. Conclusions: The present inability to quickly characterize the quality of sublingual microcirculation as either normal or impaired at the point of care limits real-world clinical application of this resuscitative endpoint. The rapidly obtained POEM score appears to be reproducible between bedside interpreters. Future studies should assess the effect of POEM score-guided resuscitation. . Sublingual microcirculatory images were obtained using a Cytocam-IDF device (Braedius Medical, Huizen, The Netherlands) and analyzed using standardized published recommendations. Results: The median age of participants was 32 years. We found no significant difference in proportions of hemodynamic responders before and after marathon (64% vs 55%, p=0.819). Also we did not find differences between PLR induced changes of total vessel density (TVD) and proportion of perfused vessels (PPV) of small vessels before and after marathon. Correlations between changes of sroke volume and changes of TVD or PPV of small vessels during PLR were not significant. Conclusions: Marathon running did not change microcirculatory responsiveness. Introduction: Clinical measurement of mitochondrial oxygen tension (mitoPO 2 ) has become available with the COMET system [1] . A question with any novel technique is whether it is feasible to use in clinical practice and provides additional information. In elective cardiac surgery patients we measured cutaneous mitoPO 2 and tissue oxygenation (StO 2 ). Methods: Institutional Research Board approved observational study in patients undergoing cardiopulmonary bypass (CPB). mitoPO 2 measurements were performed on the left upper arm (COMET, Photonics Healthcare B.V.) by oxygen-dependent delayed fluorescence of 5aminolevulinic acid (ALA)-induced protoporphyrin IX [2] . Priming of the skin was done with ALA (Alacare, Photonamic GmbH) applied the evening before surgery. StO 2 measurements (INVOS, Medtronic) were done in close proximity to the COMET sensor. Results: At the time of writing 28 of 40 patients were enrolled and mitoPO 2 measurements were feasible in this clinical setting. mitoPO 2 appeared sensitive with a high dynamic range. For example, highdose vasopressor therapy decreased mitoPO 2 and blood transfusion increased a low mitoPO 2 but not a high mitoPO 2 . In the example in Figure 1 , mitoPO 2 is clearly dependent on CPB flow and the restored cardiac circulation is able to maintain good cutaneous oxygenation after CPB even before returning of cellsaver blood. StO 2 had the tendency to provide relatively stable values within a small bandwidth and little response to even major hemodynamic changes. Conclusions: mitoPO 2 shows the effect of interventions on mitochondrial oxygenation and provides additional information compared to standard monitoring and StO 2 . Introduction: Traumatic asphyxia is a rare condition in which breathing and venous return is impaired due to a strong compression to the upper abdomen or chest region, and induces swelling, purplish red appearance, and petechiae around the face and neck. To our knowledge, there are no reports describing details of traumatic asphyxia including the clinical course and the therapeutic reactivity from cardiac arrest. We focused on cardiac arrest among all traumatic asphyxia patients treated at our hospital, and investigated their clinical features and therapeutic reactivity. Methods: Sixteen cases of traumatic asphyxia involved with our hospital between April 2007 and March 2018 were reviewed by using the pre-hospital activity record, medical record, and Hyogo prefectural inspection record. These patients were divided into three groups. The first group had already cardiac arrest at the time of rescue from the trapped place (Group A; 6 cases). The second group became cardiac arrest after the rescue (Group B; 5 cases). The third group did not experience cardiac arrest (Group C; 5 cases). Results: All cases had abnormal findings in skin or conjunctiva (Table  1) . Total mortality rate reached 56%, but among 11 cases of Group A and B who resulted in cardiac arrest, there were 10 cases with Injury Severity Score 16 or more and Abbreviated Injury Scale in the chest 3 or more. They had pneumothorax, flail chest, pericardial hematoma. Seven of them restored spontaneous circulation, and two cases achieved neurologically full recovery. Conclusions: There are some cases of traumatic asphyxia whose therapeutic reactivity is very good even after cardiac arrest, so it is important not to spare efforts for life support in such cases. rhythm and 75% witnessed arrest, five hundred ten (25%) patients had a good functional outcome at 6-months. Physiological derangements were each negatively associated with outcome in bivariate analysis at the p <0.001 level. A summary score of physiological derangements was included with potential confounders in the final regression model, and was independently associated with outcome with the chance of a good outcome decreasing by 46% for each increase of one physiologic derangement (95% CI 0.46-0.63). Conclusions: Uncorrected physiological derangements are independently and cumulatively associated with worse outcome after cardiac arrest. Although causality cannot be established, it is reasonable to consider that the correction of physiological parameters may be an important step in the chain of survival after resuscitation. Characteristics Introduction: Glan Clwyd Hospital (GCH) was recently designated one of three Cardiac Arrest Centres for Wales. It has offered a 24/7 Percutaneous coronary angiography (PCI) service to a geographically dispersed North Wales population of approximately 690,000 since June 2017. Prior to this, urgent coronary angiography was available on a more limited basis to patients requiring PCI. The aim of this study was to investigate factors associated with hospital mortality after critical care admission following cardiac arrest. Methods: Retrospective review of the Ward Watcher critical care database at GCH to identify patients who had undergone CPR in the 24 hours prior to critical care admission in 2013-18. Patients likely to have sustained OOHA of cardiac aetiology (OOHA-C) were identified from primary and secondary diagnoses and free text entry. Data were subsequently analysed using Excel and SPSS. The project was registered as a service evaluation with GCH Audit Department. Results: There were 190 cardiac arrest admissions over this period, increasing from 25 in 2013-14 to 69 in 2017-18. Of these 122 were OOHA, of which 103 were considered OOHA-C. Although OOHA-C hospital mortality appeared to decrease over the time period (89%% to 56%), this was not statistically significant (p=0.149). Factors associated with survival to hospital discharge are presented in the Tables below. On logistic regression, only PCI and low pH within the first 24 hours of critical care remained statistically significant (p=0.027 and p<0.001 respectively). Conclusions: Although we have been unable to make a distinction between patients presenting following STEMI and NSTEMI, and appreciating a potential influence of selection bias, the significant association between PCI and survival to hospital discharge supports the introduction of clinical pathways enabling PCI access following OOHA-C [1] . chest radiography. [1] Here, we aimed to derive and validate rules to estimate P_max.LV using anteroposterior chest radiography (ches-t_AP), which is performed for critically-ill patients urgently needing determination of personalised P_max.LV. Methods: A retrospective, cross-sectional study was performed with non-cardiac arrest adults who underwent chest_AP and computed tomography (CT) within 1 h (derivation:validation=3:2). On chest_AP, we defined CD (cardiac diameter), RB (distance from right cardiac border to midline) and CH (cardiac height, from carina to uppermost point of left hemi-diaphragm) (Fig 1, 2) . [1] Setting P_zero (0, 0) at the midpoint of xiphisternal joint and designating leftward and upward directions as positive on x and y axes, we located P_max.LV (x_max.LV, y_max.LV). The coefficients of the following mathematically-inferred rules were sought: x_max.LV=a0*CD-RB; y_max.LV=ß0*CH+γ . (a0: mean of (x_max.LV+RB)/CD; ß0, γ : representative coefficient and constant of linear regression model, respectively 1) . Conclusions: Evaluable echocardiographic records were reached in most of the patients. EtCO2 positively correlated with all parameters under consideration, while the strongest correlation was found between CImax and EtCO2. Therefore, CImax is a candidate parameter for real-time monitoring of haemodynamic efficacy of chest compressions during CPR. Introduction: The UK Resuscitation Council has set out guidelines for management of patients post cardiac arrest [1] . This is in line with European Resuscitation Council guideline. We set out to find if we are following the guideline. Methods: We did a retrospective audit over the course of 2 years looking at the data of 24 patients who had in hospital and/or out of hospital cardiac arrest and after the return of spontaneous circulation were admitted to the Intensive Care Unit (ICU). We focused on whether the care they received was as per the standards set by the UK Resuscitation Council. Results: We had 11 in the hospital and 13 out of hospital cardiac arrests; 11 patients had less than 10 minutes of CPR, 9 had more than 10 minutes CPR and 4 patients the data was not recorded; 16 patients needed more than 61 minutes to reach from the site of arrest to the ICU. The partial pressure of carbon dioxide was >5.5 Kpa in 11 patients at two or more occasions. Target MAP was not documented in 9 patients; blood sugar target was not documented in 15 patients and was not maintained within limits in 5 patients. Target temperature was not documented in 10 patients. The withdrawal of treatment was not delayed for 72 hours in 1 patient out of 24. In 4 patients neurological tests were not documented. Multimodal assessment tools were not used in 1 patient. Electroencephalography and serum Neuron Specific Enolase were not used to diagnose brain deaths as they were not available at our trust. 11 patients were discharged, 12 died in the ICU and 1 died in hospital after discharge from ICU. Conclusions: The audit reflected our local practice and showed that our mortality was in line with the acceptable limits; poor documentation of plan of care which posed problems in analyzing the care that these patients received; some of the parameters were not being maintained as set by UK Resuscitation guideline. Introduction: High-quality chest compressions (CC) with minimized interruptions are one of the most essential prerequisites for an optimal outcome of resuscitation. Therapy of reversible causes of cardiac arrest often requires intra-hospital transportation (IHT) during ongoing CPR. The present study investigated CC quality during transportation depending on the position of the provider. Methods: 20 paramedics were enrolled into a manikin study with four groups: a reference group with the provider kneeling beside manikin on the floor (group 1), and 3 groups performing CC during a simulated IHT of 100 meters: walking next to the bed (group 2), kneeling beside the patient in bed (group 3, Fig. 1 ) or squatting above the patient in bed (group 4, Figure 2 ). Indicators of CC quality were measured as defined in the ERC Guidelines 2015 (pressure point and depth, compression frequency, complete relief, sufficient pressure depth) [1] . All 20 paramedics performed CC during each scenario (group 1-4). Results: There were no statistical differences in quality of CC between groups 1, 3 and 4. Notably, group 2 performed significantly worse in respect to the proportion of CC with correct pressure point (p = 0.044 vs group 1), correct CC depth (p=0.004 vs. group 1, p=0.035 vs. group 3, p=0.006 vs. group 4). The results are shown in Table 1 . Conclusions: Carrying out guideline-compliant CC [1] during IHT is feasible with multiple provider positions. Based on the present results, kneeling or squatting position next to the patient ( Figure  1 and 2) is recommended, whereas "walking next to the bed" while performing CC should be avoided. Methods: A retrospective review of clinical notes was undertaken for patients admitted to ICU following return of spontaneous circulation but whom remained comatose. This audit encompassed three-month periods before and after introduction of the care bundle in October 2017. Audit standards were assigned from target parameters documented in the bundle and reflected guidance from the Cheshire and Merseyside Critical Care Network. Results: 39 patients were included in our audit; 15 admitted prior to and 24 admitted following implementation of the care bundle. In patients whom targeted temperature management was indicated, improved adherence to thermoregulation between 34-36°C was observed (50 vs 71%). Significant improvements were since in the observance to target values for oxygen saturation (0 vs 70.8%, p=0.0023) and mean arterial pressure (20 vs 95.8%, p<0.0001) following the introduction of the care bundle. Improved observance of ventilation targets was also seen; maintenance of P a CO 2 >4.5 kPa (40 vs 75%, p=0.0441) and tidal volumes <8 ml/kg ideal body weight (0 to 37.5%, p=0.0069). Conclusions: The introduction of a post-cardiac arrest care bundle in our ICU has improved care by providing discrete physiological targets to guide nursing staff and standardising management between clinicians. Variations in care are associated with poorer patient outcomes [2] and introduction of this bundle has reduced disparities in practice. array of cardiac diseases and reported survival rate is low in spite of advances in resuscitation and EMS services. Methods: Single-centre retrospective study analyzed outcomes of 42 OHCA patients admitted to cardiac ICU between 2010.-2015. We studied demographic data, initial rhythm, type of CPR, comorbidities and various post admission diagnostic findings in order to identify their impact on survival. Results: OHCA comprised 0,5% of all admissions. Mean LOS was 8.24 days (0-64). Mean age was 65,4y (29-90), M: F ratio 29:13 and bystander CPR was performed in only 19% OHCA patients. The most common initial rhythm was VF (45.2%), followed by VT (16.6%), PEA was found in 2,9% and asystole in 1.9% of pt More than half of pt received adrenalin (55%) and defibrillation (57%) and only 17% required a temporary pacemaker. 38% of pt had an ECG consistent with MI after ROSC, 19% underwent coronary angiography resulting in PCI in 75% of cases. In 5 pt (12%) therapeutic hypothermia protocol was performed. Most OHCA pt had hypertension (60%) and hyperlipidaemia (69%) as the most common risk factors followed by cardiomyopathy (33%), diabetes (29%) and CAD (21%). Only 9% had a preexisting significant valvular disease and the rest were extracardial comorbidities: chronic renal disease (17%), COPD (9%) and cerebrovascular disease (9%). 14 patients survived (33%) and GCS on admission was the only significant impact factor on survival along with comorbidities (mean GSC was 6 in survivors vs. 3 in deceased). Interestingly, age, initial rhythm, troponin I level, pH and therapeutic hypothermia had no impact on survival. Conclusions: Our data demonstrate the importance of early onsite resuscitation as the most important factor of neuroprotection and outcome and puts an emphasis on the importance of CPR education for layman population. Prediction of acute coronary ischaemia and angiographic findings in patients with out-of-hospital cardiac arrest J Higny 1 , A Guédès 2 , C Hanet 2 , V Dangoisse 2 , L Gabriel 2 , J Jamart 3 Introduction: Coronary artery disease (CAD) is the leading cause of out-of-hospital cardiac arrest (OHCA). However, diagnosis of acute coronary ischaemia (ACI) remains challenging, particularly in patients without ST-segment elevation on the post-resuscitation ECG. In this regard, a consensus statement recommends the implementation of a work-up strategy in the emergency room (ER) to exclude noncoronary causes of collapse within 2 hours. Methods: Retrospective single-centre study performed on 64 consecutive patients with resuscitated OHCA who underwent a diagnostic coronary angiography (CA). We present data on coronary angiograms for patients who underwent cardiac catheterization after resuscitation. Afterwards, we sought to identify parameters associated with ACI. Results: ST-segment elevation was noted in 29 patients (45%). STsegment depression or T-wave abnormalities were noted in 35 patients (55%). Invasive coronary strategy allowed to identify an acute culprit lesion in 46 cases (72%). 29 patients with ST-segment elevation underwent an immediate angioplasty for an acute coronary occlusion. 17 patients without ST-segment elevation underwent an ad hoc percutaneous coronary intervention for a critical lesion. Stable CAD was found in 9 cases (14%) and a normal angiogram was found in only 9 cases (14%) (Figure 1 ). Conclusions: ACI was the leading precipitant of collapse. STsegment elevation was highly predictive of coronary occlusion. In addition, a culprit coronary lesion was identified in nearly 50% of patients undergoing CA despite the lack of STsegment elevation. Finally, our findings suggest that the identification of risk criteria may help to improve the recognition of ACI after OHCA. The prediction of outcome for in-hospital cardiac arrest (PIHCA) score E Piscator 1 , K Göransson 1 , S Forsberg 1 , M Bottai 2 , M Ebell 3 , J Herlitz 4 , T Djärv 1 Figure. Predictive value for classification into <3% likelihood of favorable neurologic survival was 97.4%. False classification into < 3% likelihood of favorable neurologic survival was 0.57%. The PHICA score has potential to be used as an aid for objective prearrest assessment of the chance of favorable neurologic survival after IHCA, as part of decision making for a DNAR order. Introduction: Prognosis of survival in patients with cardiac arrest remains poor. During and after cardiopulmonary resuscitation, pathophysiological disturbances in relation with a cytokine storm, are described as "post-resuscitation" disease like a combination of cardiogenic and vasodilatory shocks. Veno-arterial extracorporeal membrane oxygenation (VA ECMO) allows to restore adequate perfusion but little is known about its effect on left ventricular (LV) function and about the role of cytokines. Methods: This study was performed in an experimental model of cardiac arrest performed in 3 groups of 3 anesthetized and mechanically ventilated pigs. Cardiac arrest was obtained by application of electrical current to epicardium inducing ventricular fibrillation. After a no-flow period of 5 minutes, medical resuscitation with catecholamines and vasopressors was performed in "CONTROL" group while VA ECMO was started in "ECMO" group and VA ECMO in combination with CYTOSORB (extracorporeal blood purification therapy designed to reduce excessive levels of inflammatory mediators such as cytokines) was started in "ECMO-CYTO" group. LV function was assessed with transthoracic echocardiography and arterial pressure with aortic pressure catheter. Results: Hemodynamic stability was obtained after 22 ± 6 and 24 ± 7 minutes in ECMO and ECMO-CYTO groups, respectively. No return of spontaneous circulation was observed in CONTROL group. At 15 minutes following cardiac arrest, LV area fractional change on short axis was normalized in ECMO and ECMO-CYTO groups (31± 3 and 34 ± 4 %, respectively). Vasopressor requirements were significantly lower in ECMO-CYTO group than in ECMO group. Conclusions: After cardiac arrest (no-flow) of 5 minutes duration, VA ECMO allowed complete LV recovery and hemodynamic stability within 30 minutes of "post-resuscitation" disease. CYTOSORB added to VA ECMO could contribute to reduce post-resuscitation vasodilatation. Impact of rapid response car system on ECMO in out-of-hospital cardiac arrest: a retrospective cohort study M Nasu 1 , R Sato 2 , K Takahashi Introduction: Extracorporeal life support (ECLS) has been reported to be more effective than conventional cardio-pulmonary resuscitation (CPR). In ECLS, a shorter time from arrival to implantation of extracorporeal membrane oxygenation (ECMO; door-to-ECMO) time has been reported to be associated with better survival rates. This study aimed to examine the impact of the physician-based emergency medical services (P-EMS) using a rapid response car (RRC) on door-to-ECMO time in patients with out-of-hospital cardiac arrest (OHCA To study the interest and the educational contribution in the short and medium term of medical simulation compared to a classical training. Methods: Cohort, prospective, observational, single-center, randomized study with control group including 30 residents (20 in anesthesia resuscitation and 10 in emergency medicine). All benefited from a theoretical training with a reminder of the latest recommendations on the management of cardiac arrest and anaphylactic shock. They were randomized into 2 groups and received practical training on a high-fidelity simulator for the management of either cardiac arrest (ACC group) or anaphylactic shock (CA group). Each group was evaluated at 6 weeks (T0) and at 6 months on two scenarios: refractory ventricular fibrillation (FV) scored on 20 points and grade 3 anaphylactic reaction (RA3) scored on 30 points. Each group served as the control group for the pathology in which they did not receive specific simulator training. The results are expressed on average with their standard deviations with "p" <0.05. Introduction: Simulation is a tool for improving the quality and safety of care, and its recognized as an essential method of evidence-based education. Emergency Medicine is a discipline in which there is a constant concern for the safety of patients. The emergency physician is often called upon to take charge of critical situations that use knowledge, know-how and knowledge as skills that must be mastered and whose theoretical learning alone is insufficient. Methods: It´s a prospective study including residents in emergency medicine performing their specialty courses in emergency services and emergency medical assistance in the region of Sousse from January to June 2018. They were randomized into two groups: the one benefiting from a traditional education and the other from an education based on simulation sessions. The chosen scenario was the management of a cardiac arrest. A pre-test and a post-test were performed in both groups. Results: We included 30 emergency residents who did not receive specialized training in the management of cardiac arrest, there was a female predominance with an average age of 27, there was no significant difference regarding the pretest between the two groups with 10.08 There was no significant difference with respect to the pre-test score between the two groups 10.08 ± 2.7 / 20 for the control group versus 10.34 ± 3.3 / 20 for the simulation group. There was a significant progression after the course with an average posttest score of 13.87 ± 1.8 in the simulation group while this score was 11.94 ± 2.3 in the control group with a statistically significant difference (p <0.001). Conclusions: Simulation learning has led to a better acquisition of cognitive knowledge by learners. The simulation is not intended to replace bed-based teaching, nor theoretical or faculty teaching, but it is an essential complement . In Tunisia, the simulation must continue its current integration in the initial and continuous training of doctors. Introduction: Recent studies have shown that obesity and its related metabolic dysfunction exacerbates outcomes of ischemic brain injuries in some brain areas, such as the hippocampus and cerebral cortex when subjected to transient global cerebral ischemia (tGCI). However, the impact of obesity in the striatum after tGCI has not yet been addressed. The objective of this study was to investigate the effects of obesity on tGCI-induced neuronal damage and inflammation in the striatum and to examine the role of mTOR which is involved in the pathogenesis of metabolic and neurological diseases. Methods: Gerbils were fed with a normal diet (ND) or high-fat diet (HFD) for 12 weeks and then subjected to 5 min of tGCI. HFD-fed gerbils showed the significant increase in body weight, blood glucose level, serum triglycerides, total cholesterol, and low-density lipoprotein cholesterol without affecting food intake. Results: In HFD-fed gerbils, neuronal loss occurred in the dorsolateral striatum 2 days after tGCI and increased neuronal loss were observed cholesterol days after tGCI; however, no neuronal loss was the in NDfed gerbils after tGCI, as assessed by neuronal nuclear antigen immunohistochemistry and Fluoro-Jade B histofluorescence staining. The HFD-fed gerbils also showed severe activated microglia and further increased immunoreactivities and protein levels of tumor necrosis factor-alpha, interukin-1beta, mammalian target of rapamycin (mTOR) and phosphorylated-mTOR in the striatum during pre-and postischemic conditions compared with the ND-fed gerbils. In addition, we found that treatment with rapamycin, a mTOR inhibitor, in the HFD-fed gerbils significantly attenuated HFD-induced striatal neuronal death without changing physiological parameters. Conclusions: These findings reveal that chronic HFD-induced obesity results in severe neuroinflammation and significant increase of mTOR activation, which could contribute to neuronal death in the stratum following tGCI. Abnormal mTOR activation might play a key role. Associations between partial pressure of oxygen and neurological outcome in out-of-hospital cardiac arrest patients Introduction: Exposure to hyperoxemia and hypoxemia is common in out-of-hospital cardiac arrest (OHCA) patients following return of spontaneous circulation (ROSC) but its effects on neurological outcome are uncertain and study results are inconsistent. Methods: Exploratory post-hoc substudy of the Target Temperature Management (TTM) trial [1] , including 939 patients after OHCA with ROSC. The association between serial arterial partial pressures of oxygen (PaO 2 ) during 37 hours following ROSC and neurological outcome at 6 months, evaluated by Cerebral Performance Category (CPC), dichotomized to good (CPC 1-2) and poor (CPC 3-5), was investigated. In our analyses, we tested the association of hyperoxemia PaO 2 > 40 kPa and hypoxemia PaO 2 < 8 kPa, time weighted mean PaO 2 , (TWM-PaO 2 ) (Fig 1) , maximum PaO 2 difference (Δ PaO 2 ) and gradually increasing PaO 2 levels (13.3 -53.3 kPa) with poor neurological outcome. A subsequent analysis investigated the association between PaO 2 and a biomarker of brain injury, peak serum Tau levels. Results: 869 patients were eligible for analysis. 300 patients (35%) were exposed to hyperoxemia or hypoxemia after ROSC (Table 1) . Our analyses did not reveal a significant association between hyperoxemia, hypoxemia, TWM-PaO 2 exposure or Δ PaO 2 and poor neurological outcome at 6-month follow-up after correction for co-variates (all analyses p= 0.146 -0.847) (Fig 2) . We were not able to define a PaO 2 level associated with the onset of poor neurological outcome. Peak serum Tau levels at either 48 or 72 hours after ROSC were not associated with PaO 2 . Conclusions: Hyperoxemia or hypoxemia exposure occurred in one third of the patients during the first 37 hours of hospitalization and was not significantly associated with poor neurological outcome after 6 months or with the peak s-Tau levels at either 48 or 72 hours after ROSC. Introduction: Cerebral hypoperfusion may aggravate the developing neurological damage after cardiac arrest. Near-infrared spectroscopy (NIRS) provides information on cerebral oxygenation but its clinical relevance during post-resuscitation care is undefined. We wanted to assess the possible association between cerebral oxygenation and clinical outcome after out-of-hospital cardiac arrest (OHCA). Methods: We performed a post hoc analysis of a randomised clinical trial (COMACARE) where both moderate hyperoxia and high-normal arterial carbon dioxide tension (PaCO 2 ) increased regional cerebral oxygen saturation (rSO 2 ) as compared with normoxia and low-normal PaCO 2 , respectively. RSO 2 was measured from 118 OHCA patients with NIRS during the first 36 h of intensive care and neurological outcome was assessed using the Cerebral Performance Category (CPC) scale at 6 months after cardiac arrest. We calculated the median rSO 2 for patients with good (CPC 1-2) and poor (CPC 3-5) outcome and compared the results using the Mann-Whitney U test. We compared the rSO 2 over time with outcome using a generalised mixed model. Finally, we added median rSO 2 to a binary logistic regression model to control for the effects of possible confounding factors. Results: The median (interquartile range [IQR]) rSO 2 during the first 36 h of intensive care was 70.0% (63.5-77.0%) in patients with good outcome compared to 71.8% (63.3-74.0%) in patients with poor outcome, p = 0.943. We did not find significant association between rSO 2 over time and neurological outcome ( Figure  1 ). In the binary logistic regression model rSO 2 was not a statistically significant predictor of good outcome (OR 0.99, 95% CI 0.94-1.04, p = 0.635). Conclusions: We did not find any association between cerebral oxygenation during the first 36 h of post-resuscitation intensive care and neurological outcome at 6 months after cardiac arrest. Fig. 2 Introduction: Near-infrared spectroscopy (NIRS) provides a noninvasive means to assess cerebral oxygenation during postresuscitation care but its clinical value is unclear. We determined the possible association between cerebral oxygenation and the magnitude of brain injury assessed with neuron-specific enolase (NSE) serum concentration at 48 h after out-of-hospital cardiac arrest (OHCA). Methods: We performed a post hoc analysis of a randomised clinical trial (COMACARE) comparing two different levels of carbon dioxide, oxygen and arterial pressure after OHCA and successful resuscitation. We measured rSO 2 continuously with NIRS from 118 patients during the first 36 h of intensive care. We determined the NSE concentrations at 48 h after cardiac arrest from serum samples using an electrochemiluminescent immunoassay kit. The samples were tested for haemolysis and all samples with a haemolysis index > 500 mg of free haemoglobin per litre (n = 2) were excluded from the analyses. We calculated the median rSO 2 for all patients and used a scatterplot and Spearman's rank-order correlation to assess the possible relationship between median rSO 2 and NSE at 48 h. In addition, we compared the NSE concentrations at 48 h after cardiac arrest in patients with good (Cerebral Performance Category Scale [CPC] 1-2) and poor (CPC 3-5) neurological outcome at 6 months using the Mann-Whitney U test. Results: We did not find significant correlation between median rSO 2 and serum NSE concentration at 48 h after cardiac arrest, rs = -0.08, p = 0.392 (Figure 1 ). The median (IQR) NSE concentration at 48 h was 17.5 (13.4-25.0) μg/l and 35.2 (22.6-95.8) μg/l in patients with good and poor outcome, respectively, p < 0.001. Conclusions: We did not find any association between cerebral oxygenation during the first 36 h of post-resuscitation intensive care and NSE serum concentrations at 48 h after cardiac arrest. The association between lactate, cerebral oxygenation and brain damage in post-cardiac arrest patients Introduction: patients admitted to the intensive care unit (ICU) after being successfully resuscitated from a cardiac arrest (CA) have a large cerebral penumbra at risk for secondary ischemic damage in case of suboptimal brain oxygenation. Therefore, resuscitation during ICU stay should be guided by parameters that adequately predict cerebral hypoxia. The value of lactate as resuscitation parameter may be questioned in post-CA patients since the brain critically depends on aerobic metabolism. We aimed to investigate the relationship between arterial lactate, cerebral cortex tissue oxygenation (SctO2) by near infrared spectroscopy (Foresight) and unfavorable neurological outcome at 180days (CPC score 3-5) Methods: Subanalysis from the Neuroprotect post-CA trial. Lactate values and SctO2 were recorded hourly in 102 post-CA patients during 24hours TTM33 and subsequent rewarming. Results: In total 3290 paired lactate/ SctO2 measurements were analysed. We found no correlation between paired lactate and SctO² (Fig.1) . Moreover, temporary trends in lactate did not correlate with corresponding trends in SctO2 during the same one-hour time interval (R²=0.003) (Fig 2) . If lactate values above 2.0 mmol/l are considered to be abnormal, lactate could not adequately detect clinical important brain ischemia (SctO2 < 60%): sensitivity 62% and specificity 53% (Table 1 , 2). Nevertheless, time weighted lactate at 6h (OR 1.38; p 0.01), 12h (OR 1.38, p 0.01), 24h (OR 1.53; p 0.003) and 36h (OR 1.60; p 0.005) were inversely correlated with unfavorable neurological outcome at 180days (Fig 1, 2) . Conclusions: Although lactate was a marker of prognosis in post-CA patients, it should not be used to guide resuscitation since lactate values were not correlated with SctO2 and changes in lactate do not correspond with changes in SctO2 during the same time interval. Simplified Introduction: The aim of the study was to investigate whether simplified continuous EEG monitoring (cEEG) [1] post-cardiac arrest can be reliably interpreted by ICU physicians after a short structured training, and whether acceptable interrater agreement compared to an EEG-expert can be achieved. Methods: Five ICU physicians received training in interpretation of simplified cEEG (Fig 1) consisting of lectures, hands-on cEEGinterpretation, and a video tutorial -total training duration 1 day. The ICU physicians then interpreted 71 simplified cEEG recordings. Basic EEG background patterns and presence of epileptiform discharges or seizure activity were assessed on 5-grade rank-ordered scales based on a standardized EEG terminology [2] . An experienced EEG-expert was used as reference. Results: There was substantial agreement (κ 0.69) for EEG background patterns and moderate agreement (κ 0.43) for epileptiform discharges between ICU physicians and the EEG-expert. Sensitivity for detecting seizure activity by the ICU physicians was limited (50%), but with high specificity (87%). Among ICU physicians interrater agreement was substantial (κ 0.63) for EEG background pattern and moderate (κ 0.54) for epileptiform discharges. Conclusions: After a one-day educational effort clinically relevant agreement was achieved for basic EEG background patterns after cardiac arrest. Assessment of epileptiform patterns was less reliable, but bedside screening by the ICU physician may still be clinically useful for early detection of seizures. Interpretation of simplified cEEG requires awareness of its limitations and support from an EEG-expert when clinically indicated. Introduction: Hypoxic-ischemic injury on head computed tomography (CT), which manifests with varying degrees of cerebral edema and loss of gray-white matter differentiation, is a poor prognostic sign after resuscitated out-of-hospital cardiac arrest that may influence early clinical decision-making. Agreement among physicians on the presence of hypoxic-ischemic injury on early head CT is unknown. Methods: We recruited 10 faculty physician participants (2 emergency medicine, 3 critical care, 3 neurocritical care, and 2 general radiology; average 8.2 years of practice) across 3 academic medical centers each with >100 admissions for resuscitated out-of-hospital cardiac arrest each year. Participants, blinded to clinical context, reviewed 20 unique head CTs obtained within 2 hours of cardiac arrest that were randomly selected from a local registry. A blinded neuroradiologist also reviewed all scans (gold standard). Participants determined if hypoxic-ischemic injury was present on each CT, and agreement was determined using multi-and dual-rater kappa statistics with 95% confidence intervals. Results: Overall agreement among physicians regarding the presence of hypoxic-ischemic injury on head CT was fair (kappa 0.34; 95% CI, 0.19-0.49) with agreement consistent across most specialties (Table 1) . When compared to the neuroradiologist, individual physician agreement ranged widely, from poor (kappa 0.12) to substantial (kappa 0.68), with 6 of 10 physicians having fair or worse agreement compared to the gold standard interpretation. Conclusions: The finding of hypoxic-ischemic injury on early head CT after cardiac arrest had high interobserver variability as interpreted by acute care physicians and general radiologists. Pending the development of objective diagnostic criteria, clinicians should bear in mind the subjectivity and subtlety of cerebral edema or loss of graywhite matter differentiation soon after return of spontaneous circulation in these patients. Figure 1 ). Baseline characteristics and differences between the WLST and no-WLST groups are shown in Table 1 . Utilization of neuro-prognostication tests is shown in Table 2 . While CT and EEG were commonly employed, SSEP and MRI were used less frequently. Basic multimodal neuroprognostication (arbitrarily defined as at least one CT or MRI, plus EEG, plus SSEP) was performed only in 34.1% of all patients undergoing WLST but the rate increased significantly over six years (p<0.001) and was higher in the time period after 2015, compared to the one prior to 2015 ( Figure 2 ). This association remained significant after adjustment for confounders such as age, arrest rhythm, downtime, targeted temperature management, APACHE II score and organ failure in a logistic regression model (p=0.004). In an institution with access to a wide range of imaging and neurophysiology tests, MRI and SSEP remained underutilized but the rate of basic multimodal neuro-prognostication increased significantly over the study period, especially in the period after 2015. Introduction: Although multiple reports using animal models have confirmed that melatonin appears to promote neuroprotective effects following ischemia/reperfusion-induced brain injury, the relationship between its protective effects and the activation of autophagy in cerebellar Purkinje cells following the asphyxial cardiac arrest and cardiopulmonary resuscitation (CA/CPR) remains unclear. Methods: Rats used in this study were randomly assigned to 6 groups as follows; vehicle-treated sham-operated group, vehicletreated asphyxial CA/CPR-operated group, melatonin-treated shamoperated group, melatonin-treated asphyxial CA/CPR-operated group, melatonin plus (+) 4P-PDOT (the MT2 melatonin receptor antagonist)-treated sham-operated group and melatonin+4P-PDOT-treated asphyxial CA/CPR-operated group. Results: Our results demonstrate that melatonin (20 mg/kg, IP, 1 time before CA and 4 times after CA) significantly improved the survival rates and neurological deficits compared with the vehicle-treated asphyxial CA/CPR rats (survival rates ≥ 40% vs 10%). We also demonstrate that melatonin exhibited the protective effect against asphyxial CA/CPR-induced Purkinje cell death. The protective effect of melatonin in the Purkinje cell death following asphyxial CA/CPR paralleled a dramatic reduction in superoxide anion radical (O2·-), intense enhancements of CuZn superoxide dismutase (SOD1) and MnSOD (SOD2) expressions, as well as a remarkable attenuation of autophagic activation (LC3 and Beclin-1), which is MT2 melatonin receptor-associated. Furthermore, the protective effect of melatonin was notably reversed by treatment with 4P-PDOT. Conclusions: This study shows that melatonin conferred neuroprotection against asphyxial CA/CPR-induced cerebellar Purkinje cell death by inhibiting autophagic activation by reducing expressions of ROS, while increasing of antioxidative enzymes, and suggests that MT2 is involved in the neuroprotective effect of melatonin in cerebellar Purkinje cell death induced by asphyxial CA/CPR. Introduction: Fucoidan is a sulfated polysaccharide derived from brown algae and possesses various beneficial activities, such as antiinflammatory and antioxidant properties. Previous studies have shown that fucoidan displays protective effect against ischemiareperfusion injury in some organs. However, few studies have been reported regarding the protective effect of fucoidan against cerebral ischemic injury and its related mechanisms. Methods: Therefore, in this study, we examined the neuroprotective effect of fucoidan against cerebral ischemic injury, as well as underlying mechanisms using a gerbil model of transient global cerebral ischemia (tGCI) which shows loss of pyramidal neurons in the hippocampal cornu ammonis 1 (CA1) area. Fucoidan (25 and 50 mg/kg) was intraperitoneally administered once daily for 3 days before tGCI. Results: Pretreatment with 50 mg/kg of fucoidan, not 25 mg/kg fucoidan, attenuated tGCI-induced hyperactivity and protected CA1 pyramidal neurons from ischemic injury following tGCI. In addition, pretreatment with 50 mg/kg of fucoidan inhibited activations of resident astrocytes and microglia in the ischemic CA1 area. Furthermore, pretreatment with 50 mg/kg of fucoidan significantly reduced the increased 4-hydroxy-2-noneal and superoxide anion radical production in the ischemic CA1 area after tGCI and significantly increased expressions of superoxide dismutase 1 (SOD1) and SOD2 in the CA1 pyramidal neurons compared with the vehicle-treated-group. We found that treatment with diethyldithiocarbamate (an inhibitor of SODs) to the fucoidan-treated-group notably abolished the fucoidanmediated neuroprotection in the ischemic CA1 area following tGCI. Conclusions: These results indicate that fucoidan can effectively protect neurons from tGCI-induced ischemic injury through attenuation of activated resident glial cells and reduction of oxidative stress following increasing SODs. Thus, we strongly suggest that fucoidan can be used as a useful preventive agent in cerebral ischemia. The effects of cold fluids for induction of therapeutic hypothermia on reaching target temperature and complications-a sub-study of the TTH48 study A Holm 1 , M Skrifvars 2 , FS Taccone ). There was no difference in early bleeding incidences (Fig 1) . During late observation, TTM patients had fewer minor bleeding (55.2% vs. 100%) and more intracranial bleeding (23.1% vs. 0%; Fig 2) . Adjusted calculated risk ratio for major bleeding (including intracranial) for TTM was 0.24 (95%CI 0.10-0.54) at baseline and 1.45 (95%CI 1.18-1.77) over time. Conclusions: Bleeding complications were common. Although the risk ratio for major bleeding increased over time in TTM patients, residual and unmeasured confounding in addition to selection and detection bias may limit the clinical relevance of this finding. Methods: Patients with neurological deficit > 10 by NHISS were included. The t°of the brain was recorded non-invasively using radiothermometer RTM-01-RES (Russia). We measured t°in 18 symmetric regions of left & right hemispheres, calculated the average t°of brain, Fig. 1 (abstract P213) . Temperature of patients given and not given pre-ICU fluids (Table 1) . Conclusions: Observed moderate brain t°heterogenecity in HP, marked increase brain t°heterogenecity in IS & sharp decline of t°h eterogenecity in CCI. Supposedly, correcting the impairment of cerebral TB (increase or decrease t°) through physical (selective cerebral hypothermia, magnetic stimulation etc.) or pharmacological (sedation) can contribute to positive therapeutic results in IS & CCI. Nonivasive radiothermometry of the brain can be an objective method of patients' condition evaluation & their rehabilitation potential. Introduction: Basilar artery stroke has a multitude of different presentations and may not be captured on plain Computed Tomography (CT). It can progress to severe disability, locked in syndrome and death [1] . With the advent of thrombolytic and endovascular therapies, prompt diagnosis can change the outcome. We present a case of basilar artery stroke, which was heralded by tongue spasticity and dysarthria, indicative of pseudobulbar palsy. Methods: Case reviewed with consent. A literature search was conducted using PubMed and Medline. Results: A 53-year-old presented with pulmonary oedema and hypertension. He was transferred to our intensive care unit for treatment of a suspected anaphylaxis. His marked lingual swelling was associated with dysarthria. Glyceryl-trinitrate and labetalol infusions were started for hypertension. He developed left sided weakness and deteriorated over several days to the point that he could only move his right foot (Table 1) . Magnetic Resonance Imaging (MRI) showed midbrain ischaemia and angiogram showed no flow in the basilar artery (Fig 1,2) . Conclusions: Common presenting features of basilar artery occlusion include dysarthria, vertigo, vomiting, headache and motor defects; these may evolve gradually or be intermittent [1, 2] . Presentation with pseudobulbar palsy is described in early literature [2] . Delayed recognition of the stroke led to aggressive treatment of hypertension, potentially compromising perfusion to the penumbral area [2, 3] . This case highlights the need for a wide index of suspicion with posterior strokes. Consent: Informed consent to publish has been obtained from the patient prognosis is related to GCS < or = 8 on admission (P = 0.003) and to malignant cerebral edema (P = 0.004). Conclusions: Our study has shown some predictive factors closely related to mortality and morbidity in patients with acute ischemic stroke. GCS at admittance < or = 8 and onset of malignant cerebral edema lead to a worst prognosis at discharge from NICU. Coherence analysis of cerebral oxygenation using multichannel functional near-infrared spectroscopy evaluates cerebral perfusion in hemodynamic stroke TJ Kim 1 Table 1 ). In addition, severe stroke patients were more likely to have higher phase coherence in interval III (P =0.078). Conclusions: Our results demonstrated that the higher phase coherence of OxyHb in myogenic signal, which was originated locally from smooth muscle cells in brain was related to impaired cerebral perfusion. This suggests that monitoring cerebral oxygenation using fNIRS could be a useful noninvasive measuring tool for evaluating impaired cerebral autoregulation in stroke patients. Is esmolol associated with worse outcome at the acute phase of ischemic stroke that receives thrombolysis? Introduction: Ischemic stroke patients experienced frequent early neurological deterioration (END) events. Since ischemic stroke has also been shown as inflammatory disease, the neutrophil-tolymphocyte ratio (NLR) may associated with END events. However, the direct study regarding this association has not been addressed. Poor grade SAH, use of vasopressors, mechanical ventilation, intracranial pressure monitoring, external ventricular drainage, blood transfusions and renal replacement therapy were all more frequent among nonsurvivors (all P<0.001). Mortality was also higher with initial lactate above 2 mmol/L, in those admitted to public hospitals and when admission to ICU was delayed more than 24 hours after ictus. After adjusting for common predictors (age, gender and WFNS) SAPS 3 non-Neuro, SOFA non-Neuro, early vasopressor use and admission to a public hospital were independently associated with hospital mortality. Moreover, the area under the curve for prediction of mortality with SAPS3, SOFA and WFNS was 0.86 ( Figure 1 ). Hospital, Austria. The association of intensity and duration of intracranial hypertension episodes with 12-month Glasgow Outcome Score (GOS) was visualized using the methodology introduced by Güiza et al. [1] . Results: In both cohorts, it could be demonstrated that the combination of duration and intensity defined the tolerance to intracranial hypertension, and that a semi-exponential curve separated episodes associated with better outcomes from those associated with worse outcomes. The association with worse outcomes occurred at a lower pressure-time burden than what has been previously observed in patients with TBI. Nevertheless, the percentage of monitoring time spent by every patient in the zone associated with poor GOS was independently associated with worse 12-month neurological outcome, even after correcting for age and Fisher score ( Introduction: Apnea test is an essential component in the clinical determination of brain death, but it may incur a significant risk of complications such as hypotension, hypoxia and even cardiac arrest [1] . We analyzed the risk factors associated with failed apnea test during brain death assessment in order to predict and avoid these adverse events. Methods: Medical records of apnea tests performed for brain-dead donor between January 2009 and January 2016 in our institution, were reviewed retrospectively. Age, gender, etiology of brain death, use of catecholamine and results of arterial bleed gas analysis (ABGA), systolic/diastolic blood pressure (SBP/DBP), mean arterial pressure (MAP) and central venous pressure (CVP) prior to apnea test initiation were collected as variables. A-a gradient and PaO2/FiO2 were calculated for more precise assessment of the respiratory system. In total, 267 cases were divided into a group which was completed apnea test and the other which was failed the test. Introduction: Tunisia has already suffered recurrent outbreaks since 1997. 2018 outbreak started relatively earlier this year. We were interpellated by the frequency of neuroinvasive presentation of the disease. Methods: We report a case series of 11 patients presented to ICU with NIWND. Results: We report 11 cases of NIWND with different severe presentations overlapping neurological manifestation including encephalitis (n=8/11), meningitis (n=10/11) and flaccid paralysis (n=8/11). Almost all patients live in the locality of Sousse. Six patients presented a long course of isolated fever before developing neurological signs. Cerebrospinal fluid was consistent with encephalitis within the 11 patients. Cerebromedullar MRI identified brain lesions (n=8/10), myelitis (n=1/10) and polyradiculoneuritis (n=1/10).Three patients had electromyography for flaccid paralysis showed diffuse axonal polyneuropathy with motoneuron involvement. Ten cases had a positive WNV IgM antibody and nine had a positive WNV IgG antibody in serum. Urine polymerase chain reaction was positive for WNV in 8/10 patients. Ten patients were mechanically ventilated. All patients were managed symptomatically. Two received high doses of methylprednisolone for 3 days, one patient received polyclonal immunoglobulin intravenous and one patient had plasmapheresis. Two patients died consecutive to brainstem lesions. Two patients recovered significantly and discharged with no complications. Five other patients evolved to persistent flaccid paralysis with a minimal consciousness state and weaning difficulties requiring tracheostomy. The last remaining patient is still evolving. Conclusions: Modification of the regional climatic conditions accounted probably for the early 2018 outbreak of NIWND. This initial case series displays the severity and the poor outcomes of NIWND with higher incidence compared to past epidemics. Noninvasive estimation of intracranial pressure with transcranial Doppler: a prospective multicenter validation study C Robba 1 , C Fig. 1 ], mean bias was -3.24 mmHg (limits of agreement are ± 2 SD 24.6 mmHg). 7.5% measures were outside the limit of agreement in the overall population. However, when ICP was high, 43% of measures were out of the limit of agreement. The AUC [ Fig. 2 Introduction: Surgical treatment of aortic aneurysm needs extracorporeal circulation (ECC), aorta clamp and hypothermia, and it is often related to poor systemic perfusion and blood flow velocity. One of the main concerns of intensive care team is to prevent secondary neurological injury after long time without blood flow pulsatility, such as brain edema and seizure. The most common parameters for neuromonitoring would be intracranial pressure and EEG, however, for non-neurological patients this information is unusual and prevents optimal management. Methods: We aimed to assess brain compliance and neurological condition of ICU patients on immediate post-operative recovery of Bentall-De Bono procedure and/or other aortic aneurysm surgical treatment using a novel non-invasive intracranial pressure (ICP) device. This device uses mechanical displacement sensor capturing extracranial continuous volumetric variation of the skull and this information proportionally reflects intracranial dynamic [1] . Results: Twenty patients were included in this study. ECC mean time was 111 minutes for 19 patients and only one did not need it. Eleven presented altered ICP curves with poor brain compliance (P2/P1 ratio > 1.0) assessed by ICP curve morphology analysis. Volemic optimization and neuroprotective measures were taken based on this ICP information for acute case management. Among these patients with altered ICP curves, eight were discharged from ICU with good clinical condition and Glasgow Coma Scale of 15. Overall mortality rate was six out of twenty (30%) and three of these had altered ICP curves. Conclusions: Brain monitoring of cardiovascular post-operative patients is important to prevent secondary neurological complications and can be a helpful tool for neuroprotective acute management on ICU. The technique supplies electrical current to muscle, combined with passive cycling. Prior to a clinical trial, we first investigated the effects of one session of FES in healthy volunteers. Methods: Healthy male volunteers (n=15) were recruited. The participants had their postural sway assessed on a pressure sensitive board, and measurement of Maximal Inspiratory Pressure (MIP). Ultrasounds were taken assessing thickness of the quadriceps and rectus abdominis. They performed 20 minutes of supine passive cycling, with FES supplying the lower limbs and abdomen. After a 1 minute rest, the tests were repeated. A further 14 participants performed just the initial baseline tests, to help assess muscular factors affecting balance and sway. Results: The current needed for palpable contraction was significantly correlated to weight in the abdomen (r=0.79, p<0.001) and quadriceps (r=0.82, p<0.001). Current required to stimulate the abdominal muscles was also correlated to depth of the subcutaneous fat layer (r=0.85, p<0.001) and echogenicity of the muscle (r=0.65, p=0.012). Pre-cycling, left and right vastus lateralis thickness inversely correlated to postural sway in the antero-posterior (r=-0.638, p<0.001) plane. Compared to pre-cycling, postural sway in the antero-posterior and lateral planes increased significantly after cycling. There was a significant decrease in MIP after cycling and greater reductions in MIP were found in participants who had thinner rectus abdomni. Conclusions: Sway at baseline is related to quadriceps thickness, which atrophies during critical illness, and could worsen balance. MIP is reduced during FES and the severity of reduction is related to the thickness of the abdominal wall muscles at baseline, suggesting that FES can fatigue the diaphragm and abdominal muscles. In awake healthy volunteers, FES is a safe, comfortable technique. Introduction: In most cases postoperative cognitive dysfunction (POCD) is transient, but still some patients suffer from persistent cognitive impairment which is associated with increased length of hospital stay, early withdrawal from labor market and higher mortality. Available data on the prevalence of POCD after cardiac surgery is very diverse from 20% to 90% upon discharge and up 20% 3 months after surgery. We aimed to investigate the prevalence of short-term and long-term POCD after off-pump coronary artery bypass grafting (CABG) surgery. Methods: Psychometric testing was performed in 230 (mean age 63.5±8.2) patients before, 10 days and 6 months after the surgery. We used following tests to assess cognitive capacity: auditory verbal learning test (AVLT), digit span test (DST), digit-letter substitution test (DLST), Stroop's test and trail making test (TMT). A decline in comparison to preoperative test results for 20% or more in two or more tests was declared as POCD. Results: The prevalence of POCD after 10 days was 31.7% (73 patients) and 9.1% (21 patients) after 6 months. When comparing patients who developed POCD with those who did not we found the former were older (69.2±8.7 vs 61.1±10.3 years; p<0.001), had lower education level (13.7±2.1 vs 10.6± 2.4 years; p<0.05) and had longer surgery duration (250.4±12.2 vs 232.1±15.9 minutes; p<0.05). The most affected cognitive domains were long term memory (AVLT) and executive function (TMT) and least affectedworking memory (DST) and selective attention (Stroop's test). Conclusions: In our prospective study the prevalence of long-term POCD after cardiac surgery was slightly less (9.1%) in comparison to available data (from 9% to 20%). It might be due differences in psychometric testing and interpretation of its results among authors. Advanced age, low cognitive reserve and long duration surgeries are linked with higher incidences of POCD. Introduction: Postoperative cognitive dysfunction (POCD) is a common and widely described phenomenon in surgical patients. Advanced age, major surgery, certain general anesthetics, genetic factors, sleep deprivation and other factors were described as contributing factors to POCD. The hospital stay itself is a major 'social' trauma for patients; social isolation, sleep deprivation and changes in daily regimen may effect neurocognitive behavior of patients. In this trial we tried to assess the link between POCD and the length of hospital stay in cardiac surgery patients. Methods: 94 patients who underwent 'off-pump' coronary artery bypass grafting (CABG) surgery selected for this trial. Neuropsychological testing was performed prior to the operation and upon discharge. We used auditory verbal learning test (AVLT), digit span test (DST), digit-letter substitution test (DLST), Stroop test and trail making test (TMT). A 20% or more decline in two or more tests in comparison to preoperative test results was declared as POCD. Patients were allocated into two groups according to the length of hospital stay: the SHORT-STAY group (group 1) included patients (n=36) who were discharged on the 8th day after surgery or earlier and the LONG-STAY (group 2) group consisted of patients (n=58) who were discharged on the 9th day after surgery or later. Patients received similar anesthesia, postoperative care and were operated by the same surgical team. Reasons for prolonged duration of hospital stay were mainly surgical. Results: 11 patients (30.6%) in group 1 and 21 patients (36.2%) in group 2 had POCD upon discharge (p<0.05). Mean length of hospital stay were 7±1.2 and 10±1.4 days in group 1 and group 2 patients respectively (p<0.05). Conclusions: Prolonged length of hospital stay increased the prevalence of POCD in our trial. Studies with various types of surgical procedures and larger patient populations needed to further understand the effect of length of hospital stay to POCD. The influence of multiple trauma with head trauma on posttraumatic meningitis: a nation-wide study with hospital-based trauma registry in Japan Introduction: Posttraumatic meningitis is one of severe complications and results in increased mortality and longer hospital stay among head trauma patients. However, it remains unclear whether there is a difference in the incidence of post-traumatic meningitis due to single traumatic brain injury (TBI) and multiple trauma including head injury. Methods: This study was a retrospective observational study during 12 years We included trauma patients registered in Japanese Trauma Data Bank whose head AIS score was >3 in this study. Multivariable logistic regression analysis was used to assess potential factors associated with posttraumatic meningitis such as CSF fistula, skull base fracture, type of injury that divided into single TBI and multiple trauma. Introduction: The aim of this study was to determine if regional cerebral oxygenation (rScO2) can be used as an indicator of tissue perfusion in ICU patients with TBI [1, 2] , and to determine the prognostic value of cerebral oxygenation rScO2 in survival prediction. Methods: Patients were enrolled retrospectively from January 2012 through July 2018 in the ICU of Derince Kocaeli Training Hospital. 250 patients with trauma patients and traumatic braine injury patients who were admitted to the ICU from the emergency room were included in the study. The sedation levels of the patients were followed up with BIS. The rScO2, BIS was taken as well as blood lactate level, mean arterial blood pressure and cardiac output at baseline time, 6, 12, 24, 48 and 72 hours. Results: No significant difference was also detected between the value of rScO2 in all patients . It was Average ScO2 (right) 60.54 ±4.8 and Average rScO2 (left) 55.63± 5.4. Conclusions: Cerebral regional oxygen saturation might be helpful as one of the perfusion parameters in patients with TBI but it could have no prognostic value in mortality prediction. However, further studies with larger sample size are still needed to validate these results. Introduction: TBI in elderly is an increasingly cause of admission in ICU. Data regarding management and prognosis of these patients are lacking. Validated prognostic models refer to younger patients and do not adequately consider the influence of pre-injury functional status, which often compromises with aging. Frailty has been defined as a state age-related of increased vulnerability and decline in autonomy of daily life activity. Aim of the study is to evaluate the impact of frailty on outcome in TBI elderly patients. Methods: moderate and severe TBI patients >65years, admitted in NeuroICU from January 2017 to May 2018, were prospectively enrolled. Data of age, comorbidity, Glasgow Coma Scale (GCS), pupils' reactivity, CT scan characteristics, neurosurgical intervention and GOSE (Extended Glasgow Outcome Scale) at 6-months were collected. Frailty status was measured by Clinical Frailty Scale (CFS) [1] and patients were divided as frail (CFS>4) and not frail (CFS<4). Bad outcome was defined as GOSE<4. Results: 22(37%) of the 60 studied patients were frail. Frailty was not related to age. Frail patients had more comorbidities and worse pupils' reactivity at admission (Table 1) . Other variables did not differ between groups. In univariate analysis neurological diseases, GCS, tSAH (traumatic subarachnoid haemorrhage), compressed/absent basal cisterns, non-reactive pupils and CFS were significantly associated to bad outcome. In multivariate analysis only GCS and CFS remained associated to bad outcome ( Table 2) . Conclusions: pre-injury frailty is strongly associated to outcome in TBI elderly patients. The age of the patients was 48.4±15.6 years. Patients were operated on for intracranial traumatic (39 cases) and non-traumatic hematomas (5), brain tumors (4) and the need for plastic of postoperative skull defects (6). General endotracheal total intravenous anesthesia with fentanyl, propofol, rocuronium, or tracrium was used. After tracheal intubation, 4-12 nerves were blocked (e.g., supraorbital, supratrochlear, zygomaticotemporal, auriculotemporal, great auricular, greater and lesser occipital nerves), depending on the surgical site. 0.75-1.0% ropivacaine was used. For blockade of one nerve used 0.5-2.0 ml of local anesthetic. Fentanyl was applied on section of a periosteum, dura matter and at inefficiency of blockade of nerves. Anesthesiology monitoring included HR, ECG, SpO2, NIB, respiratory parameters, EEG (CSI), body temperature, blood glucose and lactate levels. In 2 and 10-12 hours post-surgery, the intensity of pain was ranked by alert patients using VAS. Results: the volume of local anesthetic for blockade in one patient was 7.7± 1.7 ml. In 5 (9.3%) from 54 patients, an additional fentanyl injection was required to skin incision due to an increase in blood pressure and heart rate by 25% of the baseline values, and an increase in CSI until 80 un. Patients available to productive contact in 2 hours post-surgery ranked the pain by VAS at 1 (0;2) point, and in 10-12 hours post-surgery ranked it at 2 (1;3) p. Conclusions: At patients with craniotomies scalpe-block with lowvolumes of a ropivacaine showed high efficiency (90.7%). were transferred to hospital ward or 53 (13.1%) to the center of intensive nursing care; 2 (0.5%) went to the surgical recovery room. Acute renal failure, hypernatremia and hyperphosphatemia were independent predictors of mortality as described in Table 2 . Conclusions: Hypernatremia and hyperphosphatemia were independent predictors of mortality in critically ill patients. Introduction: The Strong Ion Difference (SID) is essential for the assessment of acid-base equilibrium, thus requiring an accurate measurement of plasma electrolytes. Currently there is no gold standard for electrolyte measurements and SID computation. Differences in electrolyte values obtained with point-of-care (PoC) and central laboratory (Lab) analyzers have been reported [1, 2] . In previous studies [3, 4] we have shown that changes in PCO2 induce electrolyte shifts from red blood cells to plasma (and vice versa), yielding variations in SID. Aim of the present in-vitro study was to induce SID changes through acute changes in PCO2 and compare values of electrolytes and SID obtained with PoC and Lab techniques. Methods: Blood samples from 10 healthy volunteers were tonometered (Equilibrator, RNA Medical) with 3 gas mixtures at fractions of CO2 (FCO2) of 2, 12, and 20%. Electrolytes were measured quasisimultaneously with a PoC analyzer (ABL800 FLEX, Radiometer) and a routine Lab method (COBAS 8000 ISE, Roche). For both techniques a simplified SID was computed as sodium + potassiumchloride. Results: Bland-Altman analysis of SID calculated with PoC and Lab showed a proportional bias (slope = 0.64, r2 = 0.55, p <0.0001), indicating a variable agreement between methods according to the average SID value (Fig.1) . SID values measured with PoC and Lab at different FCO 2 differed significantly (p<0.001, Fig.2) . A similar discrepancy was observed for chloride (p <0.001, Fig.2 ), while sodium (p=0.439) and potassium (p=0.086) were similar. Conclusions: SID measured with PoC and Lab differed significantly, mainly due to a variable discrepancy in chloride. Our findings suggest that our POC analyzer is superior to the Lab in measuring electrolytes and thus compute SID. Introduction: This study evaluated the safety of half dose insulin (HDI) versus standard dose insulin (SDI) for the treatment of hyperkalemia in a medical intensive care unit (MICU) population with renal insufficiency. Recent emergency medicine data demonstrated a lower incidence of hypoglycemia in patients with renal insufficiency when HDI was used for the treatment of hyperkalemia [1] . There is limited data describing the safety of HDI in a MICU population with renal insufficiency. Methods: This was a retrospective, chart review of patients admitted to the MICU with a diagnosis of AKI and/or CKD stage 3-5 with a serum potassium ≥ 5.8 mEq/L from January 2013 to September 2018. SDI is defined as 10 units of regular IV insulin and HDI as 5 units. The primary outcome was the incidence of hypoglycemia within 12 hours of insulin administration. Secondary outcomes included severe hypoglycemia and change of serum potassium after insulin administration. Results: A total of 265 patients were screened and 98 were included for analysis. The incidence of hypoglycemia occurred in 6/ 45 patients (13.3%) and 6/53 patients (11.3%) who received SDI and HDI, respectively. One patient in the SDI group and two patients in the HDI group developed severe hypoglycemia. The mean decrease in serum potassium after insulin administration was 1.0 mEq/L in both groups. Patients in the HDI group who were re-dosed with 5 units of regular insulin did not have any hypoglycemic events. Conclusions: In a MICU population with renal insufficiency, SDI and HDI regimens appear safe and effective for the treatment of hyperkalemia. Introduction: Sepsis and septic shock are common causes of admission in the Intensive Care Unit with a high mortality rate [1, 2] . Hence, electrolyte disturbances are common in this group of patients. Acute hypernatremia is one of the multiple features of homeostasis disturbances and available data in the literature suggest that its incidence can reach 47% [3, 4] . (Fig 1, 2) . The main source of sepsis was pneumonia with 37 affected patients (41.57%). Conclusions: Hypernatremia is significantly associated with higher mortality in septic patients. 2 (abstract P246) . The outcome versus the sodium levels higher in the group 2 -80% vs 39.1% (p=0.031). There were no significant differences between the groups in length of stay in the ICU. In group 1, there was an increase of serum phosphorus level and in the group 2the tendency to decrease. However, statistically significant differences were obtained only on the 2nd day after surgery 1.74±0.84 mmol/l (group 1) vs 0.88±0.26 mmol/l (group 2) (p=0.01). The ROC curve was constructed to assess the predictive significance of serum phosphorus levels (Fig. 1) . AUC was 0.687; 95% CI 0.574-0.799; p=0.002; sensitivity 55.2%, specificity 83.9%. The Kaplan-Meier survival analysis (Fig. 2) Introduction: The rate of extubation failure might be higher in obese patients than in non-obese patients. Effect of obesity on mortality is controversial [1, 2] (obesity paradox). Several pathophysiological changes contribute to an increase of respiratory complications [1] . We sought to identify incidence of extubation failure in obese and non-obese patients. Methods: The primary endpoint of this post-hoc analysis of a prospective, observational, multicenter study [3] performed in 26 intensive care units was extubation failure, defined as the need for reintubation within 48 hours following extubation. Only patients with body mass index (BMI) recorded were included. Results: Between December 1, 2013 and May 1, 2015, among the 1427 patients with BMI available undergoing extubation, 301 obese patients (21%) and 1126 non-obese patients (79%) were enrolled. Extubation-failure rate was 7.6% (23/301) in obese patients, and 11.0% (124/1126) in non-obese patients (p=0.09). Delay of reintubation did not differ between obese and nonobese patients (Figure 1 ). Length of intubation > 8 days was significantly more frequent in obese patients (84/301, 29%) than in non-obese patients (195/1126, 18%, p<0.0001). Precautions to anticipate extubation failure were more often taken in obese patients (153/301, 51%) than in non-obese patients (426/700, 38%, p<0.0001). Spontaneous Breathing Trial (SBT) characteristics differed between obese and non-obese patients (Table 1) . Physiotherapy was more often used in obese patients (173/301, 57%) than in non-obese patients (556/1126, 49%, p=0.01). Conclusions: Incidence of extubation failure did not differ between obese and non-obese patients. In obese patients, clinicians anticipate more a possible extubation failure, delaying the moment of extubation, performing more physiotherapy and providing an optimal SBT. Introduction: In the acute phase of critical illness, growth hormone (GH) resistance develops, reflected by increased GH and decreased insulin-like growth factor-I (IGF-I), mimicking fasting in health. The EPaNIC RCT observed fewer complications such as muscle weakness and faster recovery with accepting a macronutrient deficit in the first ICU week, as compared with early full feeding [1, 2] . We characterized its impact on the GH axis in relation to the risk of acquiring muscle weakness. Methods: In this EPaNIC RCT sub-analysis, for 564 matched patients per group, and all patients assessed for muscle weakness (n=600), serum GH, IGF-I, IGF binding protein 3 (IGFBP3) and IGFBP1 were measured upon ICU admission and at day 4 or the last ICU day for patients with shorter ICU stay (d4/LD). For 10 matched patients per group, GH was quantified every 10 min between 9 PM and 6 AM, and deconvolved to estimate GH secretion. Groups were compared with Wilcoxon test or repeated-measures ANOVA. Associations between changes from baseline to d4/LD and muscle weakness were assessed with logistic regression analysis, adjusted for baseline risk factors, baseline hormone concentrations and randomization. Results: In the fully fed group GH, IGF-I and IGFBP3 increased, whereas IGFBP1 decreased from admission to d4/LD (all p<0.001). Accepting an early macronutrient deficit prevented the rise in GH and IGF-I and the decrease in IGFBP1 (all p<0.005) but did not affect IGFBP3, whereas basal, but not pulsatile, GH secretion was lowered (p=0.005). A stronger rise in GH and IGF-I was independently associated with a lower risk of acquiring muscle weakness (OR (95%CI) per ng/ml change 0.88 (0.81-0.96) for GH; 0.98 (0.97-0.99) for IGF-I). Conclusions: Accepting an early macronutrient deficit suppressed basal GH secretion and reduced IGF-I bioavailability during critical illness, which may counteract its protection against muscle weakness. Introduction: Aim of the study was to relate hypokalemia (hypoK) and hypoglycemia as diabetic ketoacidosis (DKA) treatment complications and precocious insulin interruption also use of sodium bicarbonate with length of stay (LOS) in intensive care unit (ICU). Methods: Analysis of retrospective cohort study data of 100 patient (pt) treated for DKA at ICU of Hospital Kaunas Clinics of Lithuanian University of Health Sciences during 2012-2018 has been carried out. Serum kalemia, glycaemia; rate of episodes of hypoK, hypoglycaemia and precocious insulin interruption; use of sodium bicarbonate, in relation with LOS in ICU were analysed. SPSS 23.0 was used for statistic calculations. Traits evaluated as significant at p<0.05. Results: At the beginning of DKA treatment hypoK (3.1±0.3 mmol/l) was recorded in 53/100 (53%) pt. Due to disregarding of blood pH (6.8 -7.3 (7.1 ± 0.1) kalemia was falsely misinterpreted as "normo-" or "hyperkalemia" 3.5 -6.1 (4.8 ± 0.8mmol/l) in 49 of 63 (78%) pt, as normo-and hyperkalemia thus not treated and complicated by hypoK additionally in 20/49 (41%) pt. In hypoK LOS in ICU was 52.2 ± 31.3 vs 31.7 ± 18.2 h, p<0.05. Insulin use has caused hypoglycaemia (1.2 -3.3 (2.5 ± 0.7 mmol/l)) in 20/100 (20%) pt, LOS in ICU 62.1 ± 40.3 vs 37.7 ± 21.4 h, p<0.05. Insulin use was interrupted in case of normo -and hypoglycaemia with still persisting ketoacidosis in 34/ 100 (34%) pt, LOS in ICU was found to be 56.2 ± 32.1 vs 35.5 ± 22.5 h, p<0.05. Sodium bicarbonate was given for symptomatic treatment of acidosis during the first 10 h of DKA in 21/100 (21%) pt with stable hemodynamic: HCO3buffer has increased (4.2 ± 3.1-7.5 ± 3.0 mmol/l), p<0.05, but ketoacidosis has still persisted, LOS in ICU was 54.8 ± 30.0 vs 39.3 ± 26.5 h, p<0.05. Conclusions: HypoK (53%), hypoglycemia (21%), precocious interruption of insulin use (34%) have prolonged LOS in ICU almost twice. Symptomatic treatment of ketoacidosis with sodium bicarbonate (1/5 pt) didn't control it and has prolonged LOS in ICU. Introduction: Cystathionine-γ -lyase (CSE), a regulator of glucocorticoid (GC)-induced gluconeogenesis [1] , correlates with endogenous glucose production in septic shock [2] . The hyperglycemic stress response to noradrenaline (NoA) is mediated by the kidney [3] and less pronounced with low CSE [4] . GC receptor (GR)-mediated gene expression is differentially regulated: the GR monomer is considered to repress inflammation, and GC side effects are attributed to the GR dimer; recent reports challenge this view [5] . GC-induced gluconeogenic gene expression is reduced in GR dimerization deficient (GRdim) mice [6] . The aim of this study is to investigate renal CSE expression and systemic metabolism in GRdim and GRwt mice in a resuscitated model of LPS-induced endotoxic shock. Methods: Anesthetized GRdim (n=10) and GRwt (n=9) mice were surgically instrumented, monitored, resuscitated and challenged with LPS. NoA was administered to maintain MAP and 13 C 6 glucose was continuously infused. 6h after LPS, CSE expression was determined via immunohistochemistry of formalin-fixed paraffin sections (n=8 p.gr.). Results: GRdim required 2.5-fold more NoA than GRwt and had 1.5fold higher glucose and 1.9-fold higher lactate 6h after LPS. This was concomitant with elevated endogenous glucose production (2-fold), 10% lower glucose oxidation and 1.8-fold higher renal CSE expression in GRdim. Conclusions: Increased CSE expression together with higher glucose production (confirming [2, 4] ) and glucose levels in GRdim mice suggest an association that may link CSE to GC signaling. The higher NoA administration in GRdim mice could contribute to these effects. Introduction: To achieve safe glycemic control in critically ill patients frequent blood glucose (BG) measurements and according titration of insulin infusion rates are required. Automated systems can help to reduce increased workload associated with diabetes management. This bi-centric pilot study combined for the first time an intraarterial glucose sensor with a decision support system for insulin dosing (SGCplus system) in critically ill patients with hyperglycemia. Methods: Twenty-two patients (3 females, 19 males, 12 with preexisting diabetes mellitus, age 65.7 ± 12.7 years, BMI 27.3 ± 3.5 kg/ m2, creatinine level 1.4 ± 1.2 mg/dl, SAPS (Simplified Acute Physiology Score) 64.6 ± 18.3, TISS-28 (Therapeutic Intervention Scoring System) 39.2 ± 5.5 who were equipped with an arterial line and required iv insulin therapy were managed by the SGCplus system during their medical treatment at the intensive care unit. Results: 1845 SGCplus-based BG determinations were performed and 0.8 ± 0.8 sensor calibrations per day were required. Sensor glucose readings correlated well with reference BG (Figure 1 ). Mean treatment duration was 6.0 ± 3.8 days. Time to target was 100 ± 178 min (80-150 mg/dl) and 135 ± 267 min (100-160 mg/dl). Mean blood glucose was 142 ± 32 mg/dl with seven blood glucose values <70mg/dl. Mean daily insulin dose was 62 ± 38 U and mean daily carbohydrate intake 148 ± 50 g /day (enteral nutrition) and 114 ± 50 g/day (parenteral nutrition). Acceptance of SGCplus suggestions was high (>90%). The novel intraarterial glucose sensor demonstrated to be highly accurate. The SGCplus system can be safely applied in critically ill patients with hyperglycemia and enables good glycemic control. Introduction: We aimed to assess the effect of frailty as assessed by Clinical Frailty Scale (CFS) and Karnofsky performance score (KPS) on critical care (CC) and hospital mortality in this group at a nonspecialist tertiary critical care unit. Methods: Patients admitted to critical care were identified from our electronic database by screening for liver disease or cirrhosis in the admission diagnoses. Those with an aetiology of liver disease other than alcoholic liver disease (ALD) were excluded. Data was collected on patient demographics, length of stay, status at discharge from critical care and hospital and CFS. KPS was also calculated where sufficient in-formation was available in the medical record. Data was analysed using logistic regression multivariate analysis with Stata 14 software. [1] . Results: TG13 diagnosis criteria and severity grading criteria for acute cholangitis and acute cholecystitis were judged from numerous validation studies as useful indicators in clinical practice and adopted as TG18 diagnostic criteria and severity grading without any modification. Provide initial treatment, such as sufficient fluid replacement, electrolyte compensation, and intravenous administration of analgesics and full-dose antimicrobial agents, as soon as a diagnosis has been made. In new flowchart for the treatment of acute cholecystitis (AC) in the TG18, Grade III AC was indicated for gallbladder drainage, but some Grade III AC can be treated by laparoscopic cholecystectomy (Lap-C) at advanced centers with specialized surgeons experienced in this procedure and for patients that satisfy certain strict criteria. We also redefine the management bundles for acute cholangitis and cholecystitis. Introduction: 13 C-acetate breath tests provide a non-invasive assessment of gastric emptying [1] and could, hence, be used to judge tolerance to enteral nutrition. Result values like t50 (time for 50% absorption) correlate with scintigraphic measurements. The data evaluation is based on model equations like the β -exponential function (BEX) [1] . It considers a mono-phasic breath gas response. This may not be the case during critical illness, which could reduce precision too low for a reliable personalized assessment [2] . Methods: We recently developed an evaluation of irregular gastric emptying patterns, which separates absorption from post-absorptive distribution and retention of tracer and from the terminal respiratory release of the oxidized tracer [3] . Using breath test data of 31 ICU patients (mean SAPS2 36 +/-14) the precision of this approach was compared with a BEX analysis to explore how often an extended analysis is warranted and whether it improves the reliability of estimates. Results: 15 patients had a release profile consisting of series of peaks with a periodicity of 60-90 min. A first dominant peak carries about 95% of the released moiety, as reported [4] for controls. For these patients the precision in t50 for the BEX approach was 70 +/-30% of that observed for the new approach. For the other patients, the secondary peaks had a similar periodicity but were more pronounced, indicating persisting peristaltis, which has been linked to tolerance to enteral nutrition [4] . The BEX approach achieved a precision of 40 +/-27 % relative to the new one, challenging its applicability for these patients. Introduction: Clinical scoring systems used to prognosticate the severity of acute pancreatitis (AP), such as APACHE II, are cumbersome and usually require 24 hours or more after presentation to become accurate, at which time the window for early therapeutic intervention has likely passed. SIRS at presentation is sensitive but poorly specific for severe AP. We postulated that SIRS and accompanying hypoxemia would specify at presentation patients with AP who have severe inflammation and are at risk for clinically severe disease. Methods: Patients with AP who had SIRS and hypoxemia at presentation were enrolled in an open-label study evaluating the safety and efficacy of CM4620-IE, a Calcium Release-Activated Calcium (CRAC) channel inhibitor (NCT03401190). Hypoxemia was defined as an estimated PaO2 <75 mm Hg calculated using a log-linear equation and the SpO2 on room air at the time of presentation. A contrastenhanced computed tomography (CECT) was performed at presentation and a CBC with differential, D-dimer and CRP were analyzed daily. The CECT was read by a blinded central reader who assessed the degree of inflammation using the Balthazar scoring system (Table  1) . Results: 13 patients, seven men and six women, have been randomized in the study. The mean estimated PaO2 at presentation was 74 mm Hg. 10 patients had 2 SIRS criteria present and the other 3 patients had 3 SIRS criteria present. The median value for age was 53.5 (IQR 48-56), initial neutrophil-lymphocyte ratio (NLR) 10.6 (6. 3 Introduction: To investigate whether circulating immune profiles were able to serve as early biomarkers in predicting persistent organ failure (POF Methods: Thirty-nine patients with predicted severe acute pancreatitis (pSAP) and 9 healthy control subjects were prospectively enrolled in our study. We measured the expression of monocytic human leukocyte antigen-DR (mHLA-DR), the proportions of dendritic cells (DC) and its subtypes (including myeloid dendritic cell (mDC) and plasmacytoid dendritic cell (pDC)), the different cytokineproducing CD4+ T helper (Th) cells and regular T (Treg) cells. Plasma CRP and several inflammatory mediators levels were measured by ELISA. Results: Compared with healthy controls, there is a significant decrease in the expression of mHLA-DR, the frequencies of total circulating DCs and its subsets, and percentage of Th1 cells in patients with pSAP. However, we found significantly higher frequencies of Th17 cells, higher proportion of Treg cells than healthy subjects. Of interest, we observed that there was a significant decrease in the positive percentage and mean fluorescence intensity (MFI) of mHLA-DR, the proportions of total DCs and pDC, and Th1 cells in patients with POF compared with transient organ failure (TOF). Besides, there is a significantly higher frequency of Th17 cells in POF than those in TOF. Area under the receiver-operating characteristic curve analysis showed that disease severity scores had a moderate discriminative power for predicting POF in patients with pSAP. More importantly, the expression of mHLA-DR and the percentage of DCs and pDC had a significantly higher AUROC and thus, better predictive ability than disease severity in patients with pSAP. Conclusions: Circulating immune profile show multiple aberrations in patients with pSAP who have developed POF. Both the expression of mHLA-DR and the percentage of total DC and pDC may be early good biomarkers for predicting risk of POF in patients with pSAP. Introduction: Pancreatic fistula (POPF) due to anastomosis insufficiency is a common (12-30%) complication after pancreaticoduodenectomy and often discovered with delay, causing severe morbidity, ICU stay and deaths. Microdialysis (MD) catheters have been shown to detect inflammation and ischemia in several postoperative conditions and organs. The aim was to investigate if MD catheter monitoring could facilitate earlier detection of POPF than current standard of care. Methods: In a prospective, observational study 35 patients (44 to 88 years) were investigated. A MD catheter was fixed to the pancreaticojejunal anastomosis. Samples for analysis of glucose, lactate, pyruvate and glycerol were acquired hourly during the first 24 hours, then every 2-4 hours to discharge. POPF was defined according to the International Study Group of Pancreatic Fistula 2016 update definition. Results: Patients who developed POPF (N=7) had significantly higher glycerol levels (P<0.01) in microdialysate than did patients without POPF (N=28) during the first 24 h. Thereafter, the difference diminished. A glycerol concentration >400 μmol/L during the first 12 h detected patients who later developed POPF with a sensitivity of 100 % and a specificity of 92%. Lactate and lactate to pyruvate ratio were significantly higher (P<0.05) and glucose was significantly lower (P<0.05) in patients with POPF from about 24 h. Fig. 1 shows microdialysis measurements in patients with (red lines) and without (blue lines) POPF. Conclusions: A high level of glycerol in microdialysate is an early (first 12 hours) indicator of POPF. Glucose, lactate and lactate to pyruvate ratio are indicators of peritonitis caused by the leakage. Thus, MD monitoring detects POPF several days earlier than current methods and may play an important clinical tool in the future. We are currently conducting a RCT to explore if MD monitoring will improve prognosis in these patients The phenomenon of total impaired of metabolic activity of gut microbiota in critically ill septic patients Introduction: During a critical condition, dramatic disturbances occur not only in the change of species diversity, but in gut microbiota metabolism as well, that might lead to nonreversible breakdowns of host homeostasis and death [1] . Metabolic activity of microbes can be assessed by the measurement of the levels of aromatic microbial metabolite (AMM) in blood serum, which are associated with the severity and mortality of ICU patients. Critically ill patients are characterized by the totally different SFS profile than in healthy people, particularly by the absence of PhPA; but dominated by p-HPhAA and p-HPhLA [2] . The purpose of our study is to assess the gut metabolic activity via AMM in sepsis. Methods: In this study 40 simultaneously serum and fecal samples (SFS) were taken from ICU patients: 14 -with sepsis, 21-chronic critical ill (CCI) patients and control -5 SFS from healthy people. After liquid-liquid extraction from serum and fecal samples, 9 phenylcarboxylic acids (AMM) were measured using GC/MS (Thermo Scientific). Results: The sum of the level of 9 most relevant AMM in serum samples were higher in patients with sepsis (median -4.7 μM) than in CCI patients (1.1 μM) and healthy people (1.3 μM). At the same time the opposite pattern was observed in the fecal samples -2.4, 31.9 and 68.7 μM, respectively. The ratios of sums AMM gut/serum were higher in healthy people than ICU patients (Fig. 1) Introduction: The aim of this study is to describe the characteristic of Bioelectric Impedance Vector Analysis (BIVA) and muscular ultrasound during the first week after admission in the ICU, and their correlation with indices of metabolic support. BIVA is a commonly used approach for body composition measurements [1] . Muscular ultrasound represents a valid tool to provide qualitative and quantitative details about muscle disease [2] . Methods: Consecutive patients admitted to ICU and expected to require mechanical ventilation for at least 72 hours were enrolled in the study. Within the first 24 hours of ICU admission (T1), patients were evaluated with muscular ultrasonography comprehensive of diaphragm thickness (DTH) and rectus femoris cross-sectional area (CSA). At the same time, BIVA and biochemical analysis. All the same measures were repeated at day 3 (T3) and 7 (T7) (Figure 1 (Table 1) . Dividing the patients in two groups based on prealbumine changes (T3 vs T1: increase, anabolic vs decrease, catabolic), those in which prealbumine increased had a higher reduction in muscle mass ( Figure 2 ). Conclusions: This study showed how the PA tends to be reduced in the first week of ICU stay. It is correlated with a concomitant Introduction: The modified Nutrition Risk in Critically ill (mNUTRIC) has been developed in order to identify critically ill patients who may receive benefit from nutrition support [1] . Several evidences showed the association between the mNUTRIC score and clinical outcomes [2, 3] , however there are no data in Thai critically ill patients. The purpose of this study was to find the association between mNU-TRIC score and 28-day mortality in medical intensive care unit (ICU) patients, Ramathibodi hospital. Methods: We retrospectively reviewed the medical patient records from June 2016 to January 2017. A mNUTRIC score of each patient was calculated to evaluate the risk of malnutrition. Statistical analysis of the association between mNUTRIC score and 28-day mortality, length of stay in ICU and hospital were performed. Results: A total of 78 critically ill patients were included in the study. The 28-day mortality was 53.06% in patients with high mNUTRIC score (5-9) and 13.79% in patients with low mNUTRIC score (0-4). Modified NUTRIC score was significantly correlated with 28 day mortality (r = 0.390, p<0.001), length of stay in ICU (r = 0.394, p<0.001) and length of stay in hospital(r = 0.414, p<0.001). In the receiver operating characteristic (ROC) curve analysis, the AUC of mNUTRIC score and 28-day mortality was 0.788 (95% confidence interval (CI), 0.686-0.889) (Fig 1) . Optimal cut-off value of 6 showed sensitivity of 66.7% and specificity of 77.1% in mortality prediction (Youden's index, 0.438). Additionally, patients who received adequate nutrition supplement within 7 days was 87.20% for calorie and 64.10% for protein. There was no association between nutrition support and 28-day mortality. Conclusions: In Thai medical intensive care population, the mNUTRIC score was associated with 28-day mortality in critically ill patients. Fig. 1 (abstract P264) . Within the first 24 hours of ICU admission (T1), patients will be evaluated with muscular ultrasonography comprehensive of diaphragm thickness and rectus femoris (medial vastus) cross-sectional area. At the same time, anthropometric measure will be collected (such as body height, ideal body weight, real body weight declared, right arm circumference) as well as BIVA measure (Xc, R, PA, lean body weight and % of extracellular body weight) and biochemical analysis (inclusive albumin, pre-albumin, blood count, lymphocyte count, magnesium, phosphorus, reticulocytes, renal and hepatic function test). The day after, the fluid balance will be calculated as well as the nitrogen balance. All the same measures will be repeated at day 3 (T3) and 7 days (T7) Introduction: Ultrasonography is an essential imaging modality in critical care to diagnose and guide for therapeutic management of shock, multiple organ failure, etc. Enteral tube feed intolerance occurs frequently in hospitalized patients and more so in critically ill patients. In present study, we consider that nursing staff may be able to use bedside ultrasound as an alternative to standard aspiration protocol or radiographic studies to assess gastric volume and nasogastric (NG) tube in patients with enteral feed intolerance. Methods: In present prospective, single-center study, we performed ultrasound residual stomach volume and NG tube placement assessments of adult critically ill patients (Figure 1 ) compared to standard protocol of stomach volume assessment (routine daily shift 60-ml syringe aspirations) and NG (nasogastric) tube placement verified by abdominal X ray. We used an abdominal (Linear Ultrasound Transducer) probe (4-10 MHz). The residual volume was calculated according to formula: GV (ml) = 27 +14.6 x right-lateral CSA-1.28 x age). Results: Hundred simultaneous double (ten critically ill patients) ultrasound measurements sessions were performed by nursing staff of our intensive care (ICU) (Fig 1) . Double simultaneous measurements of the ultrasound assessments were compared to standard nurse ICU protocol for assessment of residual volume of stomach. The new ultrasound assessment method demonstrated excellent intra-class reliability (ICC-0.96 (0.95-0.98, p<0.001) and strong correlation with standard residual volume assessment method (ICC-0.82 (0.7-0.89, p<0.001). NG tube placement was successfully verified by ultrasound measurements in all ten critically ill patients and, thereafter, confirmed by abdominal X-rays. Conclusions: Preliminary results of our study demonstrated good correlations between both methods of NG tube placement and residual stomach volume: standard ICU nurse protocol and ultrasound assessment. Evaluating the documentation of nasogastric tube insertion and adherence to safety checking L Roberts 1 Introduction: Enteral feeding into a misplaced nasogastric (NG) tube is recognised by the National Patient Safety Agency as a Never Event. NG tubes are commonly indicated in level 2/3 patients, thus we set out to evaluate current practice in critical care. The aim was to evaluate: documentation of insertion, adherence to safety guidance pertaining to checking safe use, chest x-ray interpretation. Methods: This prospective cohort study was based on inpatients in critical care who had insertion of NG tubes over four weeks; there were 57 insertions. Data was analysed from patients' medical notes and the hospital's imaging system. Results: 65% of insertions were documented using proformas. 97.1% of proforma documentations included 4 or more details: type of tube, tube length at the nostril, NEX measurement, aspirate adequacy, chest x-ray adequacy, whether it was safe to feed. Only 13.6% of hand-written documentations included 4 or more details. 51% of initial aspirates were obtained on insertion, of these, 57% had an appropriate pH between 1 and 5.5. This led to 74% of patients having chest x-rays to confirm initial placement of the NG tube. Only 54% of chest x-rays adequately satisfied the four criteria. Written documentation in medical notes stating if it was safe to feed was completed in 67% of cases. Conclusions: We found that proformas ensure a higher level of detail and uniformity in the documentation of NG tube insertions. There was a high incidence of chest x-rays performed to confirm correct placement of tubes due to difficulties in obtaining aspirates and failure to follow guidelines. A need for a uniform, ward-specific proforma on NG tube insertion has been identified, as well as a teaching session on chest x-ray interpretation and on techniques to aid obtaining aspirates. We have established critical care's shortcomings in NG tube insertion documentation and tube safety checking. Introduction: Pressure ulcers(PU) are considered as important types of public health problems, due to high mortality and cost. We aimed to investigate the efficiency of curcumin and fish oil on prevention and treatment of PU using a feasible mice model. Methods: 28 mice were randomly divided into Control(Group 1), Curcumin(Group 2), Fish oil(Group 3), Curcumin and Fish oil(Group 4) groups. 5mm skin bridge between two 1000 gauss magnets was formed on the back of mice, followed by 3 ischemia reperfusion cycles as 12 hours of rest after 12 hours of magnet placement [1] . A single dose of curcumin and fish oil was injected intraperitoneally. Tissue samples had taken 10th day of first compression, rates of PU, inflammation, reepithelisation, neovascularisation and granulation were examined histopathologically. The data analyzed by Pearson chi-square test. Results: Third degree PU were observed in all groups.There was no significant difference between groups in terms of inflammation.The formation of reepithelisation showed a significant difference between groups.Partial reepithelisation ratios in group 3 and group 4 was elevated.There was significant difference between groups in terms of neovascularisation, the highest rate as 75% was observed in group 4.Formation of granulation was observed at maximum rate as 46.2% at group 3. Conclusions: Depending on positive results of curcumin, fish oil, cur-cumin+fish oil on wound healing it may be advised to use them in treatment of acute PU.After similar rate of PU with control group we consider that it should be beneficial to evaluate the effect of these therapies with more studies by changing the mode of administration, time of initiation and duration of therapy. Introduction: Inflammation is a key driver of malnutrition during acute illness and has different metabolic effects including insulin resistance and reduction of appetite. Whether inflammation influences the response to nutritional therapy in patients with disease-related malnutrition remains undefined. We examined whether the effect of nutritional support on the risk of mortality differs based on the inflammatory status of patients. Methods: This is a secondary analysis of a multicentre trial in eight Swiss hospitals, where patients with a nutritional risk score (NRS) of ≥3 upon hospital admission were randomly assigned to receive protocol-guided individualized nutritional support according to nutrition guidelines (intervention group) or a control group. The inflammatory status was defined based on admission CRP levels as low inflammation (CPR <10 mg/dl), moderate inflammation (CRP 10-100 mg/dl) and high inflammation (CRP >100 mg/dl). Results: We included a total of 1,950 patients of which 27.33%, 45.85% and 26.82% had low, moderate and high inflammation levels on admission. While overall there was a significant reduction in 30day mortality associated with nutritional support (adjusted OR in the overall cohort 0.65, 95%CI 0.46 -0.91), the subgroup of patients with high inflammation did not show reduced mortality (adjusted OR 1.36, 95%CI 0.76 -2.41, p for interaction = 0.005). There was no difference in other secondary endpoints when stratified based on inflammation. Nutritional support did not affect CRP levels over time (kinetics). Conclusions: This secondary analysis of a multicentre randomized trial provides evidence, that the inflammatory status of patients influences their response to nutritional support. These findings may help to better individualize nutritional therapy based on patients initial presentation. Introduction: Low plasma glutamine levels have been associated with unfavourable outcomes in critically ill patients. This study aimed to measure plasma glutamine levels in critically ill patients and to correlate glutamine levels with biomarkers and severity of illness. Methods: We enrolled critically ill patients admitted to three ICUs in South Africa, excluding those receiving glutamine supplementation prior to admission. We collected clinical, biochemical and dietary data. Plasma glutamine levels were determined within 24 hours of admission, using Liquid Chromatography Mass Spectrometry and categorized as low (<420 μmol/L), normal (420-700 μmol/L) and high (>700 μmol/L). Results: Of the 330 patients (average age 47.42±16.56 years, 56% male), 68% were mechanically ventilated, with a mean APACHE II score of 18.57±8.55 and a mean SOFA score of 7.05±3.78. Plasma glutamine levels were low in 58.5% (median plasma glutamine of 382.15 μmol/l). Baseline plasma glutamine correlated inversely with CRP (r=-0.287, p<0.001) and serum urea (r=-0.124, p<0.026), and positively with serum bilirubin (r=0.262, p<0.001) and serum ALT (r=0.119, p=0.032). Significantly more patients with low admission glutamine levels required mechanical ventilation (Chi2=12.65, p<0.001) and had higher APACHE scores (p=0.003), higher SOFA scores (p=0.003), higher CRP values (p<0.001), higher serum urea (p=0.008), higher serum creatinine (p=0.023), lower serum albumin (p<0.001) and lower bilirubin levels (p=0.054). Using multiple logistic regression analysis, APACHE score (odds ratio, [OR] 1.032, p=0.018), SOFA score (OR 1.077, p=0.016) and CRP (OR 1.006, p<0.001) were significant predictors of low plasma glutamine levels. ROC curve analysis revealed a CRP threshold value of 87.95mg/L to be indicative of low plasma glutamine levels (AUC 0.7, p<0.001). Conclusions: 58.5% of critically ill patients had low plasma glutamine levels on admission to ICU. This was associated with increased disease severity and higher CRP. Introduction: The East of England deanery Operational Delivery Network in the United Kingdom came together as a group of Intensive Care Units to comply an evidence-based care bundle. One of the branches of this care bundle is on parenteral nutrition and states: 'Parenteral nutrition should not be given to adequately nourished, critically ill patients in the first seven days of an ICU stay.' This is based on evidence [1] [2] [3] that showed that 'In patients who are adequately nourished prior to ICU admission, parental nutrition initiated within the first seven days has been associated with harm, or at best no benefit, in terms of survival and length of stay in ICU.´The objective of this second cycle was to assess whether or not we are adhering to the guidelines, last year we were failing to hit targets and after some action I reassessed how we performed in the year 2017 compared to 2016. Methods: A retrospective audit of the whole year of 2017 for all patients admitted to ICU who had parenteral nutrition started at any point during their stay. Results: There is a significant improvement in the percentage of patients who are being started incorrectly on TPN before 7 days (36% compared to 69%) (Fig 1, 2) . I also found a total reduction in the number of patients prescribed TPN, a reduction in the number of bags being used and a reduction in length of hospital stays. Conclusions: As we have recently switched over to an electronic ICU programme for all documentation and prescriptions, as part of our plan and act in the PDSA cycle we are organising for several things to be put in place on the new system on prescription: pharmacy authorisation, links to guidelines and alert/justification boxes. I will do a further cycle in another year. JG and MPC contributed equally. Introduction: Recent RCTs revealed clinical benefit of early macronutrient restriction in critical illness, which may be explained by enhanced autophagy, an evolutionary conserved process for intracellular damage elimination [1] . However, in the absence of specific and safe autophagy-activating drugs, enhancing autophagy through prolonged starvation may produce harmful side effects. A fasting-mimicking diet (FMD) may activate autophagy while avoiding harm of prolonged starvation, which also improved biomarkers of age-related diseases in an experimental study [2] . We evaluated if short-term interruption of continuous feeding can induce a metabolic fasting response in prolonged critically ill patients. Methods: In a randomized cross-over design, 70 prolonged critically ill patients receiving artificial feeding were randomized to be fasted for 12 hours, followed by 12 hours full enteral and/or parenteral feeding, or vice versa. Patients were included at day 8 in ICU and blood glucose was maintained in the normal range. At the start and after 12 and 24 hours, we quantified total bilirubin, urea, insulin-like growth factor-I (IGF-I) and beta-hydroxybutyrate (BOH) in arterial blood. Insulin requirements were extracted from patient files. Changes over time were analyzed by repeated-measures ANOVA after square root transformation. Results: As compared to 12 hours of full feeding, 12 hours of fasting decreased bilirubin (-0.23±0.06mg/dl; p=0.001) and IGF-I (-13.94 ±1.62ng/ml; p<0.0001), and increased BOH (+0.47±0.07mmol/l; p<0.0001), without affecting urea concentrations (Fig 1) . Fasting reduced insulin requirements (-1.16±0.14IU/hour; p<0.0001). Conclusions: Short-term fasting induces a metabolic fasting response in prolonged critically ill patients, which provides perspectives for the design of a FMD, aimed at activating autophagy and ultimately at improving outcome of critically ill patients. Introduction: Recent evidence has led to changed feeding guidelines for critically ill patients, with a shift towards lower feeding targets during the acute phase [1] . When micronutrients are not provided separately, prolonged hypocaloric feeding could induce micronutrient deficiencies and increase risk of refeeding syndrome once full feeding is restarted, which are both potentially lethal complications [2] . Since there is limited evidence how to optimize micronutrient provision in order to avoid deficiencies, we hypothesized that there is a great variation in current practice. Methods: Within the MEN section of the European Society of Intensive Care Medicine (ESICM), we designed a questionnaire to gain insight in the current practice of micronutrient administration. In 3 email blasts, invitations were sent to all ESICM members, with currently more than 300 respondents. The survey will be closed at December 10, 2018. Results: First, we will describe demographic characteristics of the respondents, including geographical location, ICU and hospital type, and function. Second, we will describe some aspects of the current practice of micronutrient administration. We will identify the proportion of respondents having a protocol, on which evidence such protocol is based and whether it takes into account the stability and daylight sensitivity of micronutrients. Next, bearing refeeding syndrome in mind, we will identify whether there are respondents who never measure and/or separately administer micronutrients and phosphate. Finally, we will make a top 5 of the most measured and most supplemented micronutrients. Conclusions: This survey will deliver more insight in the current practice of micronutrient provision across different types of ICUs and may identify areas for future research. Furthermore, we will evaluate whether there is need to increase awareness for refeeding syndrome. Introduction: Large gastric residual volumes (GRVs) have been used as surrogate markers of delayed gastric motility to define enteral feeding intolerance (EFI). Recent studies have challenged the definition of EFI. Study objectives: 1) investigate the potential relationship between GRVs and clinically outcomes, 2) develop an algorithm for early identification of patients at increased risk of mortality due to EFI. Methods: A retrospective study of inpatient encounters from electronic health record charts within the Dascena Clinical Database. 4,018 patients were included in the study; 295 patients had EFI. Eight vital signs (diastolic/systolic BP, heart rate, temperature, respiratory rate, GRV, Glasgow Coma Scale, and feeding rate) and their trends were input to the classifier. Machine learning classifiers were created using the XGBoost gradient boosted tree method with 3-fold cross validation. Results: Rate of change in GRV (Δ GRV) was measured over a 5-day period, beginning at the time of EFI onset (Figure 1a) . Figure 1b shows a high likelihood of mortality for patients with none or modest GRV reduction. Patients with an increase in GRV over the five-day period after EFI onset had the highest mortality likelihood. A stratification algorithm was developed to identify EFI patients who died inhospital despite GRV reduction at 1, 12, and 24 hours in advance of EFI onset. Area under the receiver operating characteristic (AUROC) curves demonstrated high sensitivity and specificity of algorithm predictions of in-hospital death up to 24 hours in advance of EFI onset (Table 1) . Conclusions: The analysis suggests an association between GRV and mortality, especially in patients with persistent GRV increase over the 5-day period after EFI onset and the potential of algorithmic models to predict EFI development. Prospective validation of these Fig. 1 (abstract P274) . Changes in metabolic markers of fasting over time for both randomization groups algorithms may assist in clinical trial design to develop treatments for patients at highest risk of experiencing serious outcomes due to EFI. A quality improvement project to improve the daily calorific target delivery via the enteral route in critically ill patients in a mixed surgical and medical intensive care unit (ICU) B Johnston, D Long, R Wenstone Royal Liverpool and Broadgreen University Hospital Trust, Critical Care, Liverpool, United Kingdom Critical Care 2019, 23(Suppl 2):P277 Introduction: 'Iatrogenic underfeeding' is widespread with the CALO-RIES study reporting only 10%-30% of prescribed daily kCal was actually delivered to patients [1] . In the present project, quality improvement methodology was utilised with the aim of delivering greater calories by implementing 24-hour volume-based feeding and allowing increased feeding rates for, 'catch up' of missed daily feed volume. Methods: Baseline data assessing the percentage of daily kCal delivered to ventilated patients was collected in September 2017. Data was presented and new intervention guidelines agreed based upon the PEPuP protocol [2] . Nurse champions were identified and were responsible for cascade training of the PEPuP protocol. Educational tools to help determine daily calorific requirement and volume of feed required were provided. Repeat data was collected at 6 months (cycle 2) after PEPuP implementation. Results: Ten patients were included in cycle 1. During cycle 1 the percentage of kCal achieved via enteral feeding was 45%. Following intervention this increased to 82% (p<0.001) during cycle 2. This increased further to 95.3% of daily kCal when calories obtained from Propofol were included. Conclusions: A 24-hour volume-based feeding regimen is a simple and cost-effective method of improving enteral feeding targets. Through the use of quality improvement methodology, we demonstrated that this approach is achievable. The success of this project has led to the adoption of the protocol in other ICU units in a regional critical care network. Effect of non-nutritional calories on the calory/protein ratio in ICU patients S Jakob, J Takala University Hospital Bern, Dept of Intensive Care Medicine, Bern, Switzerland Critical Care 2019, 23(Suppl 2):P278 Introduction: Nutritional diets are composed to match the needs of critically ill patients. While effective calory needs can be measured or calculated, the needs of proteins are more controversial. We aimed to calculate non-nutritional calories and assess how they influence the ratio of calories to protein delivered to the patients. Methods: In this retrospective analysis, nutritional and nonnutritional calories and protein delivery were calculated in 220 consecutive ICU patients receiving enteral nutrition in 2016. Introduction: Marked protein catabolism is common in neurocritical patients. Optimal nutritional monitoring and protein nutritional adequacy could be associated with outcome in neurointensive care unit (NCU) patients. We aimed to evaluate the impact of monitoring and optimal support of protein using nitrogen balance on outcome in neurocritical patients. Methods: A consecutive 69 patients who were admitted to NCU were included between July 2017 and February 2018. Nitrogen balance was calculated using excreted urine urea nitrogen during ICU admission. Follow-up nitrogen balance monitoring was performed in 33 patients. We divided patients into two groups based on the results of nitrogen balance (positive balance and negative balance). Moreover, we evaluated improvement of nitrogen balance in 33 patients. We assessed the outcome as length of stay in hospital, length of stay in NCU, and in-hospital mortality. We compared the clinical characteristics and outcome according to nitrogen balance. Results: Among the included patients (age, 58.1; and male. 59.4%), 53 (76.8%) patients had negative nitrogen balance. The negative balance group was more likely to have lower Glasgow Coma Scale (GCS), longer length of stay in hospital, and longer length of stay in NCU. In 33 patients with follow-up nitrogen balance monitoring, improvement of nitrogen balance group had lower in-hospital mortality (10.5% vs. 50.0%, P = 0.019), and received adequate protein intake (1.8 g/Kg/day vs. 1.1 g/Kg/day, P = 0.001) compared to no change group (Table 1) . There was no significant difference in baseline nitrogen balance, baseline body mass index, and GCS between two groups. Conclusions: This study demonstrated that critical illness patients in NCU are underfeeding using nitrogen balance, however, adequate provision of protein was associated improvement of nitrogen balance and outcome. This suggests that adequate nutrition monitoring and support could be an important factor for prognosis in neurocritical patients. Increased protein delivery within a hypocaloric protocol may be associated with lower 30-day mortality in critically ill patients Introduction: To test the hypothesis, using real world evidence that increasing protein delivery and decreasing carbohydrates (CHO) may improve clinical outcomes. Methods: Retrospective analysis of existing electronic medical records (EMR) of patients admitted to the intensive care units (ICU) at the Geisinger Health System. Logistic regression analysis was used to determine correlation between protein delivered (which was proportional to the concentration of protein in the formula utilized) and clinical outcomes. Results: 2000 medical encounters for a total number of 12,321 ICU days were collected and analyzed. Average age was 62.2 years (55.2% male) and 68.1% were obese and overweight. Primary diagnoses included sepsis or septic shock, acute and/or chronic respiratory failure (or illness), cardiovascular diseases, stroke and cerebrovascular diseases among others. Median hospital LOS was 13.6 days, 6.9 days in the ICU, median days of invasive mechanical ventilation of 4. 30-day readmission rate among patients discharged alive was 19.3%. Patients in the High protein group received lower amounts of CHOs (data not shown). Unadjusted 30-day post-discharge mortality was inversely proportional to the amount of protein delivered (Table 1) . Conclusions: A significant improvement in mortality is observed with increased protein delivery while decreasing carbohydrate loads. Prospective randomized trials are warranted to establish causality. Introduction: Acute kidney injury (AKI) is associated with high mortality. The risk increases with severity of AKI. Our aim was to identify risk factors for development and subsequent progression of AKI in critically ill patients. Methods: We analysed 2525 patients without end-stage renal disease who were admitted to the ICU in a tertiary care centre between January 2014 to December 2016 and did not have AKI on admission. We identified risk factors for development and non-recovery of AKI as defined by the KDIGO criteria. Results: The incidence of new AKI in 7 days was 33% (AKI I 42%, AKI II 35%, AKI III 23%). Multivariate analysis revealed BMI, SOFA score, chronic kidney disease (CKD) and cumulative fluid balance as independent risk factors for development of AKI. Among patients who developed AKI in ICU, 69% had full renal recovery, 8% partial recovery and 23% had no recovery of renal function by day 7. AKI patients without renal recovery in 7 days had significantly higher hospital mortality (60%) compared to the other groups. Independent risk factors for non-recovery of renal function were CKD, mechanical ventilation, diuretic use and extreme fluid balance before and after first day of AKI. (Table 1) The association between cumulative fluid balance before AKI and 48 hours after AKI with risk of AKI non-recovery are shown in Figure 1 and 2. Conclusions: AKI is common and mortality is highest in those who do not recover renal function. Cumulative fluid accumulation impacts chances of AKI development and progression. (Table 2 ). All were in R2. 5/8 (63%) of those with an admission CK>10000 had AKI 2 or 3. All 12 (15%) patients who required CRRT for AKI associated with RM were at risk for AKI regardless of initial CK: vascular surgery (4/12), multi-organ dysfunction (7/12), and/or pre-existing renal disease (3/12). Conclusions: Raised CK is common in ICU but its cause is multi factorial thus an isolated measure >5000 does not require immediate high output treatment for RM AKI. AKI is more common in patients who have more than 1 CK>5000 on 2 sequential days or those whose first CK was >10000 as RM may be contributing. A single CK>5000 in patients with a clear reason to develop RM should also start treatment. Surgical outcomes of end-stage kidney disease patients who underwent major surgery P Petchmak 1 , Y Wongmahisorn 1 , K Trongtrakul 2 Introduction: Acute kidney injury (AKI) occurs in more than 40% of successfully resuscitated out-of-hospital cardiac arrest patients treated with targeted temperature management (TTM) [1] . The effect of the duration of cooling on AKI has not been well studied. In this post-hoc analysis of the TTH48 randomized controlled trial that compared 24 vs 48-hours of TTM (33°C) after cardiac arrest [2] , we studied the impact of TTM length on the development of AKI. Fig. 1 . Duration of TTM had a significant impact on the development of creatinine values during the first 7 days in the ICU, p<0.05. This was primarily driven by an increase in creatinine during rewarming on day 2 for the 24hour and day 3 for the 48-hour group (Fig 2) . Conclusions: In a trial of 24 vs 48 hours of TTM after out-of-hospital cardiac arrest, the length of TTM did not affect the incidence of AKI. Fig. 2 (abstract P287) . Creatinine over time patients [1] , but there are no published data on longer-term renal outcomes in adult patients. The purpose of this study was to assess longer-term trends in serum creatinine in this cohort. Methods: A retrospective study was conducted of all patients admitted to an adult regional referral centre for ECMO at a UK university hospital between 2010 and 2016. Those who survived for >12 months were included. Demographics, baseline serum creatinine, presence of AKI during ICU admission, and serum creatinine at hospital discharge were determined. Serum creatinine and dependence on renal replacement therapy (RRT) were assessed at 6 and 12 months post ECMO. Results: 13 patients had a complete (or near-complete) data-set available. The mean age was 45.5 years, 38% of whom were male. 8/13 had AKI during their critical care admission. None were dependent on RRT at 6 or 12 months post ECMO. Most patients had lower serum creatinine results at hospital discharge compared to their pre-hospitalisation baseline, but creatinine concentrations at 6 and 12 months post ECMO tended to be higher than at hospital discharge ( Figure 1) . Conclusions: In this cohort of ECMO patients who were discharged from hospital alive, serum creatinine tended to be lower at hospital discharge compared to baseline and rose again in the following 12 months. Decreased creatinine production due to deconditioning and muscle wasting may offer a biological rationale for the lower creatinine results at hospital discharge [2] . Therefore, caution should be exercised in the use of serum creatinine at hospital discharge to assess renal dysfunction -further research is warranted. Introduction: AKI complicates more than half of ICU admissions [1, 2] and is associated with development of chronic kidney disease (CKD), need for renal replacement therapy (RRT) and increased mortality [3] . We prospectively evaluated all ICU admissions during a one-year period in order to determine incidence, etiology and timing of AKI as well relevant clinical outcomes. Methods: Prospective observational study of all patients admitted from Jan to Dec 2017 to a multidisciplinary ICU in Greece. Patients with end-stage renal disease and anticipated ICU stay less than 24 hrs were excluded. AKI diagnosis and classification was based on KDIGO criteria [4] . Lowest creatinine level within 3 months before admission or first creatinine after ICU admission served as reference. (Fig 1) . Conclusions: Although AKI alert does not include urine output criterion or AKI risk factors, it remains a helpful tool to point out patients with AKI. Education and diagnostic algorithms are still needed to early diagnose and treat AKI patients. Influence of severity of illness on urinary neutrophil gelatinaseassociated lipocalin in critically ill patients: a prospective observational study C Mitaka, C Ishibashi, I Kawagoe, D Satoh, E Inada Untendo University, Anesthesiology and Pain Medicine, Tokyo, Japan Critical Care 2019, 23(Suppl 2):P291 Introduction: Neutrophil gelatinase-associated lipocalin (NGAL) is a diagnostic marker for acute kidney injury (AKI). NGAL expression is highly induced not only in kidney injury, but also in epithelial inflammation of intestine, bacterial infection, and cancer. However, the relationship between uNGAL and severity of critically ill patients has not been well understood. The purpose of this study was to elucidate whether uNGAL is associated with severity of illness and organ failure in critically ill patients. Methods: We prospectively enrolled patients with sepsis (n=34) and patients who underwent esophagectomy with gastric reconstruction for esophageal cancer (n=39). Sepsis was defined according to Sepsis-3. uNGAL levels were measured on ICU day 1, 2, 3, 4 and 7. uNGAL levels and AKI rate in patients with sepsis were compared with those in patients who underwent esophagectomy. AKI was defined according to KDIGO. Acute physiology and chronic health evaluation (APACHE) II score and sequential organ failure assessment (SOFA) score were calculated. Results: Median uNGAL level (413 ng/mg creatinine) was significantly higher in patients with sepsis than that (26 ng/mg creatinine) in patients who underwent esophagectomy on day 1. Median APACHE II score and median SOFA score in patients with sepsis were significantly higher than those in patients who underwent esophagectomy. Four patients with sepsis developed AKI, and 3 out of them underwent continuous renal replacement therapy, whereas no patients who underwent esophagectomy developed AKI. uNGAL levels were positively correlated with APACHE II score and SOFA score in patients with sepsis. uNGAL levels were remarkably elevated (> 4000 ng/mg creatinine) in urinary tract infection (n=3), loops enteritis (n=1), and obstructive jaundice due to cholangiocarcinoma (n=1). Conclusions: These findings suggest that uNGAL level is associated with severity of illness and organ failure in patients with sepsis. uNGAL levels might be influenced by severity of illness and inflammation. To assess the quality of the course 15 US renal images had to be evaluated in "post-renal obstruction" (p-ro) or "no p-ro". The rate of correctness (RoC Farius ) was determined. In 2018 we, once again, contacted the students to attend a web-based online "follow-up". This online survey was created with "Google Formular". 15 new and unknown US images were presented and rated in "p-ro" or "no p-ro" (RoC FUP Introduction: Septic-induced kidney injury worsen the patient's prognosis [1] . Renal resistance index (RRI) is correlated with an increased mortality in septic patients [2] . The aim of this study was to describe the evolution of RRI in a rat sepsis model. Methods: The local ethics committee approved the study (APA-FIS#9385-2016113016181432). Sepsis was induced in 3-month-old male rats by caecal ligation and puncture (CLP) [3] . The RRI was assessed before and 24h after CLP by pulse Doppler on the left renal artery (RRI=(peak systolic velocityend diastolic peak)/ peak systolic Values expressed as % per column. Abbreviations in alphabetical order: AKI acute kidney injury; AKIN acute kidney injury network definition; CKD chronic kidney disease. There were statistical differences between subgroups with and without AKI for the subgroups of patients with previous CKD (p = 0.010*), sepsis at admission (p = 0.031**), hypotension (p= 0.003***) Fig. 1 (abstract P290B) . Target comparing accuracy and precision of AKI alert and actual AKI diagnoses velocity) (Fig 1) . RRI were compared by a paired Wilcoxon test (R software v.3.3.4). A p value < 0.05 was considered significant. Results: 14 rats were included. 24 hours after sepsis induction, all rats were in septic shock with cardiac dysfunction. The RRI increased after sepsis induction compared to baseline (0.61 ± 0.03 vs 0.66 ± 0.02, p<0.05) and mean renal artery velocity decreased (11.8 ± 1.41 vs 7.7 ± 0.94, p<0.05) (Fig 2) . Systolic and diastolic peaks velocity of the renal artery were unchanged. Conclusions: Sepsis induced changes in RRI and mean velocity on the left renal artery whereas no changes in systolic or diastolic velocities were seen. These results are consistent with available clinical datas. The RRI could be an additional tool to assess renal failure in septic rats. Further studies are needed to confirm the validity of this marker during sepsis. Kidney failure is one of the most common organ dysfunction during sepsis. The RRI could be an additional tool in small animals to assess the effects of potential therapeutic targets on renal function induced by sepsis. (Fig 2) . The eGFR improved more with the heparin group (53% vs 48%; p=0.21) (Fig 1) . Interruptions of the filter circuit were as expected less with the citrate group (144mins vs 45mins; p=0.17). Finally, inotropic requirements increased following therapy interruptions, more so with patients receiving citrate (7.2% vs 14.5%; p=0.4). Conclusions: Our analysis suggests that using citrate anticoagulation for RRT results in a monitoring cost saving of approximately £9 per 24 hours, alongside the other conferred savings previously reported. Furthermore, results demonstrate the efficacies of both systems are similar in the initial 24 hours, although there is a suggestion that heparin systems improves renal parameters more quickly. Finally, interruptions and 'filter downtime' caused an increase in the patient's inotropic requirements, however results suggestive that this is greater in the citrate group. Mmol/L respectively. Demographic characteristics of the study group and the main parameters of the procedure were presented in Fig 1. Conclusions: Regional citrate is a safe and effective anticoagulation method for CRRT in children, when it is applied following a protocol. It significantly prolongs circuit survival time and thereby should increase CRRT efficiency. We did not find any serious adverse effects of regional citrate anticoagulation. -5) , deceased at 1 year n=67 (14%). The MDRD trend is more indicative than creatinine of decline of renal function in the post operative period (Fig 1) . CRRT was used in 14.4% (69 pts) and was associated to a greater LOS and mortality (Fig 2) . Preoperative bilirubin, BUN and creatinine are among the greatest risk factors for its use ( Table  2 . At 1 year follow up n=6 pts (1.5%) were on hemodialysis. Conclusions: AKI requiring CRRT in after LT is associated with higher mortality and LOS. Identify patients at risk and adopt preventive strategies in the perioperative period is mandatory. Introduction: We developed a new CO2 removal system, which has a high efficiency of CO2 removal at a low blood flow. To evaluate this system, we conducted in vivo studies using experimental swine model. Methods: Six anesthetized and mechanically ventilated healthy swine were connected to the new system which is comprised of acid infusion, membrane lung, continuous hemodiafiltration and alkaline infusion. In vivo experiments consist of four protocols of one hour; Baseline= hemodiafiltration only (no O2 gas flow of membrane lung); Membrane lung = "Baseline" plus O2 gas flow of membrane lung; "Acid infusion" = "Membrane lung" plus continuous acid infusion; "Final protocol" = "Acid infusion" plus continuous alkaline infusion. We provided an interval period of one hour between each protocol. We changed the respiratory rate of the mechanical ventilation to maintain PCO2 at 50-55 mmHg during the experiment. Results: The amount of CO2 eliminated by the membrane lung (VCO2ML) significantly increased by 1.6 times in the acid infusion protocol and our final protocol compared to the conventional membrane lung protocol, while there was statistically no significant difference observed in the levels of pH, HCO3-, and base excess between each study protocol. Minute ventilation in the "Final protocol" significantly decreased by 0.5 times compared with the hemodiafiltration only protocol (P <0.0001), the membrane lung (P=0.0006) and acid infusion protocol (P=0.0017). We developed a novel ECCO2R system which efficiently removed CO2 and is easy-to-setup to permit clinical application. This new system significantly reduced minute ventilation, while maintaining acid-base balance within the normal range. Further studies are needed for the clinical application of this easy setup system comprising of the materials typically used in a clinical setting. , and psychomotor agitation (10%) while the most common symptoms of hypertensive emergency were chest pain (30.4%), dyspnea (28.6%) and neurological deficit (29%). Clinical manifestations of hypertensive emergency were cerebral infarction (26.4%), acute pulmonary edema (24.8%), hypertensive encephalopathy (20.6%), acute coronary syndromes (20.4%), cerebral hemorrhage (4,.5%), congestive heart failure (12%), aortic dissection (0.8%), preeclampsia and eclampsia (2.6%). Conclusions: Hypertensive urgencies were significantly more common than emergencies (78.1% vs. 21.9%, p<0.0001). There was no statistically significant difference in the number of patients with hypertensive urgency and emergency in relation to age, gender, duration of hypertension, except for the 60-69 age group, where urgency was statistically significantly higher (p=0.0204). Introduction: Emergency Department (ED) crowding is a major public health concern. It delays treatment and possible ICU admission, which can negatively affect patient outcomes. The aim of this study was to investigate whether ED to ICU time (ED-ICU time) is associated with ICU and hospital mortality. Methods: We conducted an observational cohort study using data from the Dutch NICE registry. Adult patients admitted to the ICU directly from the ED in 6 academic centers, between 2009 and 2016, were eligible for inclusion. For these patients NICE data were retrospectively extended with ED admission date and time. ED-ICU time was divided in quintiles. The data were analyzed using a logistic regression model. We estimated crude and adjusted (for disease severity; Apache IV probability) odds ratios of mortality for ED-ICU time. In addition, we assessed whether the Apache IV probability (divided into quartiles) modified the effect of ED-ICU time on mortality. Results: A total of 14,787 patients were included. Baseline characteristics are shown in Table 1 . The median ED-ICU time was 2.0 [IQR 1.3-3.3] hours. ICU and hospital mortality were 18.1 and 22.2%, respectively. The crude data showed that an increased ED-ICU time was associated with a decreased ICU and hospital mortality (both p<0.001, Figure 1A ). However, after adjustment for disease severity, an increased ED-ICU time was independently associated with increased hospital mortality (p<0.002, Figure 1B ). Figure 2 shows that only in the sickest patients (Apache IV probability >60.9%), the association between increased ED-ICU time and hospital mortality was significant (p=0.019, Figure 2D ). We found similar results with respect to ICU mortality. Conclusions: This study shows that a prolonged ED-ICU time is associated with increased ICU and hospital mortality in patients with higher Apache IV probabilities. Strategies aiming at rapid identification and transfer of the sickest patients to the ICU might reduce inhospital mortality. Reliability and validity of the SALOMON algorithm: 5-year experience of nurse telephone triage for out-of-hours primary care calls E Brasseur, A Gilbert, A Ghuysen, V D´Orio CHU Liege, Emergency Departement, Liège, Belgium Critical Care 2019, 23(Suppl 2):P308 Introduction: Due to the persistent primary care physicians (PCP) shortage and their substantial increased workload, the organization of PCP calls during out-of-hours periods has been under debate. The SALOMON (Système Algorithmique Liégeois d'Orientation pour la Médecine Omnipraticienne Nocturne) algorithm is an original nursing telephone triage tool allowing to dispatch patients to the best level of care according to their conditions [1] . We aimed to test its reliability and validity under real life conditions. Methods: This was a 5-year retrospective study. Out-of-hours PC calls were triaged into 4 categories according to the level of care needed: Emergency Medical Services (AMU), Emergency Department visit (MAPH), Urgent PCP visit (UPCP), Delayed PCP visit (DPCP). Data recorded included patients' triage category, resources and potential redirections. More precisely, patients included into the UPCP + DPCP cohort were classified under-triaged if they had to be redirected to an Emergency Department. Patients from the AMU+MAPH cohort were considered over-triaged if they did not spend at least 3 resources, 1 emergency specific treatment or any hospitalization. Results: 10207 calls were actually triaged using the SALOMON tool, of which 19.1% were classified as AMU, 15.7% as MAPH, 62.8% as UPCP and 2.1% as DPCP (Fig 1) . As concerns the AMU+MAPH cohort, the triage was appropriate in 85.3% of the calls, with an over-triage rate of 14.7%. As concerns the UPCP + DPCP cohort, 97.1% of the calls were accurately triaged and only 2.9% were under-triaged. SAL-OMON sensitivity reached 93.9% and its specificity 92.5%. These results indicate that SALOMON algorithm is a reliable and valid nurse telephone triage tool that has the potential to improve the organization of PCP out-of-hours work. Introduction: Inappropriate visits to the Emergency Department (ED), such as patients manageable by a primary care physician (PCP), have been reported to play some role in the ED crowding [1] . Indeed, non-urgent patients directly managed by PCPs could reduce ED workload [2] . Triage and diversion to alternative care facilities, eventually co-located within the ED, could offer a solution [3] provided Fig. 1 (abstract P308) . Distribution of different calls, their triage using the SALOMON algorithm and the inappropriate triages (over and undertriages) based on the preselected criteria the availability of a reliable triage tool for their early identification. We created a new triage algorithm, PERSEE (Protocoles d'Evaluation pour la Réorientation vers un Service Efficient Extrahospitalier) and tested its feasibility, performance and safety. Methods: After initial evaluation with a 5-level ED triage scale [4] , ambulatory self-referred patients classified as level 3 or below benefited from a simulated triage with PERSEE identifying 2 categories of patients: ED Ambulatory patients and primary care (PC) treatable patients. We collected patients data and resources. Patients requiring less than 3 resources, no specific emergency treatment and no hospitalization were considered as manageable in a PC facility. Results: 1999 patients were included in the study of whom 66.9% were self-referred (Fig 1) . Among those self-referrals, 58.6% were triaged as level 3 or below. 38.9% patients were triaged as Ambulatory patients of whom 10% were as PC treatable. We noted a redirection rate of 10% of the global visits or 15% of the self-referrals, an error rate of 7%, a sensitivity of 24.6% and specificity of 97.6%. Conclusions: Using advanced ED triage algorithm in addition to classical ED triage might offer interesting perspectives to safely divert self-referrals to PC facilities and, potentially, reduce ED workload. Introduction: Generally, prehospital medical provider should minimize staying prehospital scene to reach the patient to definitive care as soon as possible in prehospital medical activity. In addition, Some textbook and report saids that medical provider minimize the number of procedure or limit minimum requirement procedure because unnecessary procedure may extend the staying time in prehospital scene. However, there are few studies evaluating this hypothesis and that this "extension is significant or not. Therefore, we perform this study. Methods: We evaluated the operated air ambulance(Doctor-Heli) case from 1st April 2015 to 31st March 2018, in Gifu University Hospital using our mission record. We evaluated about time from landing to ready for taking off(activity time), operation doctor, mission category (i.e. trauma), number of procedure in the each activity and work load. We only focused on prehospital care and exclude transportation from hospital to hospital . In addition, we exclude the case which are not suitable for analysis. Results: 1299 cases were operated in these period. 511 cases were suitable for analysis. Average activity time in prehospital scene was14.32±6.09. There was weak correlation between the number of procedure and activity time. (r=0.3452) The length of the activity time did not depend on mission category. If the doctor perform 6 and over procedures, staying time was 7minutes longer, this was significantly longer than that of under 5 and under procedures. Conclusions: We confirmed that we have to minimize the number of procedure or limit minimum requirement procedure in prehospital scene. And our result suggest we may have to limit appropriate number of procedures. Introduction: Organ failure is a critical condition, but the prevalence is largely unknown among unselected emergency department (ED) patients. Knowledge of demographics and risk factors could improve identification, quality of treatment, and thereby improve the prognosis. The aim was to describe prevalence and all-cause mortality of organ failure upon arrival to the ED. Methods: This was a cohort-study at the ED at Odense University Hospital, Denmark, from April 1, 2012 to March 31, 2015. We included all adult patients, except minor trauma. Organ failure was defined as a modified SOFA-score > 2 within six possible organ systems: Cerebral, Circulatory, Renal, Respiratory, Hepatic, and Coagulation. The first recorded vital, and laboratory values were extracted from the electronic patient files. Primary outcome was prevalence of organ failure; secondary outcomes were 0-7-day and 8-365-day mortality. Results: Of 70,399 contacts 52.1% were female and median age 62 (IQR 42-77) years. The prevalence of new organ failure was 11.8%, individual organ failures; respiratory 4.9%, circulatory 3.0%, cerebral 2.3%, renal 1.7%, hepatic 1.4%, and coagulation 0.6%. The 0-7-day and 8-365-day all-cause mortality was 11.7% (95% CI: 10.9-12.6) and 19.3% (95% CI: 18.2-20.4), respectively, if the patient had new organ failures at first contact in the observation period, compared to 1.4% (95% CI: 1.3-1.5) and 6.6% (95% CI: 6.4-6.9) for patients without. Seven-day mortality ranged from hepatic failure, 5.6% (95% CI: 4.0-7.5) to cerebral failure, 33.3% (95% CI: 30.4-36.2), and the 8-365-day mortality from cerebral failure, 13.2% (95% CI: 11.2-15.4 to renal failure, 26.7% (95% CI: 23.6-29.9). Conclusions: New organ failure is frequent and serious, with a prevalence of 11.8% and a one-year mortality of 31% with wide variation according to type of organ failure. Results: We proceeded to a descriptive study that showed that 39% of patients were male and 62% of them were female with a sex ratio of 0.62.The average age of patients was 34 years old and ranged between 15 and 90 years old.We found that 59 patients of our population had medical background, dominated by diabetes in 12 cases, high blood pressure in 8 cases and asthma in 6 cases.The results also showed that 29.5% of patients had a history of abdominal surgery while 13% of them had history of other types of surgery.The patients were oriented according to their severity level as following: 21% care unit of emergency department, 1.5% close monitoring room .The VASPI score was ranged between 1 and 10 with an average of 4±2. It was higher than 5 in 32.5% of cases.The results of physical examination found an isolated pain in 67,5% of cases, a reactionnal pain syndrom in15% of cases, a peritoneal syndrome in 12% of cases and an occlusive syndrome in 7% of cases.The final diagnosis was mostly represented by the following causes: 45.5% of gastroenteritis 11.5% of constipation and 9% of ulcer disease.The final orientation of patients according to the diagnosis led to hospitalization in 21% of cases and to outpatient clinic in13% of cases while 66% of them did not need any more care. Conclusions: Appropriate diagnostic evaluation and decision for or against hospitalization is a challenge in the patient who comes to the emergency department with acute abdominal pain it need an adequate evaluation and management. Introduction: We assessed patients' impressions of a selfadministrated automated history-taking device (tablet) to gather information concerning emergency department (ED) patients prior to physicians' contact. The quality of communication was compared with the traditional history-taking. Methods: The algorithm content was developed by two emergency physicians and two emergency nurses through an iterative process. Item-Content Validity Index (I-CVI) was measured by five experts rating the relevance of each item (from 1: not relevant to 4: highly relevant) [1] . Next, quality control was realized by research team. To assess the feasibility, we used a computerized randomization. Low acuity, ambulatory adult patients presenting to the ED were assigned either to a control group (CG, n=65) beneficiating form a traditional history-taking process or to the experimental group (EG, n=66) assigned to use the tablet with further history-taking by the ED physician. Communication was analyzed by the Health Communication Assessment Tool [2] and satisfaction assessed by questionnaires. Results: After two rounds, validity was excellent for each item (I-CVI > 0.8). The Universal agreement method was of 0.9. Refusals (n=11) to participate were analyzed: they fear using an electronic device or the experimentation. Content satisfaction revealed that 93% of patients understood the questions. 94% of patients indicated that the device was easy to hold and use. Medical communication was not affected by the device (p=0.15). We noticed that, among the subsections, physicians significantly introduced themselves better in the EG (p=0.04). Conclusions: In this feasibility study, patients were highly satisfied. The use of a self-administrated automated history-taking device does not generate miscommunications and allow physicians better introduce themselves. . A positive point we have established is the possibility for the detorsion of a twisted retention ovarian cyst after its transvaginal aspiration. We used this method only in cases when the onset of torsion did not exceed 4 hours. 28.5% of all emergency conditions associated with retention cysts were recurred by conservative therapy, and 50.4% of patients with the retention cysts rupture were successfully treated in this way. Conservative management is possible in the case of a small loss of blood (up to 150.0-200.0 ml), hemodynamic stability and the absence of signs of continuing bleeding. The detorsion and resection of the cyst when torsion is not more than 180°and even longer than 4 hours, in most cases did not reveal necrosis in the appendages. Conclusions: Improvement of organs of preservation and reproduction in women. Criteria for admission to an intensive care unit of a tertiary hospital: analysis of the decisions of the outreach intensivist and 28 day in-hospital mortality Introduction: The aim of this study was the analysis of ICU admission criteria and evaluation of in-hospital mortality of patients assessed by our Critical care outreach team. Criteria for admission to the ICU should be defined to identify the patients most likely to benefit from ICU admission. This triage process is complex, associated with several factors, including clinical characteristics of the patients, but also subjective factors because it depends on the judgment of the intensivist who decides whether to admit or not the patient and is obviously conditioned to the structure and size of the ICU. Methods: The outreach intensivist records the patient observation in a form with 5 questions (reversibility of acute illness, objective of admission in ICU, comorbidities, functional reserve and intuitive prognosis of the doctor). Analysis of 6 months (January 1 through June 30, 2018) of admission decisions in ICU, mean delay, ICU mortality, and 28 day in-hospital mortality (28HM). Results: The intervention of the intensivist in "outreach" was requested on 855 occasions. The main places of observation were the emergency room (40.7%) and the wards (40.4%). The 28HM increased with the degree of comorbidity decompensation. Functional reserve also influenced 28HM, reaching 42.2% in partially dependent patients and 63.4% in totally dependent patients. There was agreement between the mortality and the physician´s intuitive prognosis in 71% of the cases. Conclusions: A larger sample is needed to draw sustainable conclusions, however, the evaluation algorithm correlated well with hospital mortality. Decompensated comorbidities and low functional reserve have a negative impact on prognosis, regardless of acute disease. There was agreement between mortality and the physician´s intuitive prognosis. Electrochemical methods for diagnosing the severity of patients with multiple trauma Introduction: Multiple trauma is one of the leading causes of death worldwide [1] . Timely diagnosis and treatment is crucial in this state. One of the promising areas is the use of new electrochemical methods they are simple, flexible, efficient and of low cost. Among these methods, attention is paid to the measurement of open circuit potential (OCP) of the platinum electrode and cyclic voltammetry (CVA). The OCP is a reflection of the balance of pro-and antioxidants in the body, and the amount of electricity (Q) determined by CVA is proportional to the antioxidant activity of the biological environment. Methods: A total of 51 patients with severe multiple trauma (38.7 ±13.5 y.o., 33 men and 18 women) were enrolled; APACHEII 17.4±7.2; ISS 40.9±8.3; blood loss 2356±997 ml. Blood plasma was collected from patients. Measurement of the OCP was carried out according to [2] , CVA analysis -according to the original method on a platinum working electrode. Results: A shift in the OCP towards more positive potential values (Fig. 1) , while the antioxidant activity of blood plasma decreased (Fig.  2) . A more significant change of OCP, as compared to the Q values, may indicate not only a deficiency in the components of the antioxidant defense system of the body, but also an increase in the concentration of prooxidants (e.g., reactive oxygen species), which are involved in oxidative stress. who underwent surgical fixation). Information was collected from TARN, ICNARC and surgical team databases. Our primary outcome was ITU resource utilisation (ITU LOS and mechanical ventilation days). Our secondary outcomes were morbidity and mortality (Hospital LOS, infection burden, inotrope use and death before discharge). Data was collected and analysed in Microsoft Excel and R. Results: 691 patients were included (Group 1 = 246, Group 2 = 414, Group 3 = 31). Mortality was significantly higher when comparing the post 2014 groups undergoing conservative (12%, 51/414) vs. surgical fixation (0%, 0/31), p-value = 0.026. Regarding potential temporal changes, there was no significant difference in mortality between the non surgical groups; pre-2014 (Group 1: 28/218) and post 2014 (Group 2), p-value 0.683. Group 3 patients did spend more time mechanically ventilated (p-value 0.019) and used more antimicrobials (p-value 0.032) ( Table 1) . Conclusions: Patients undergoing surgical rib fixation at the RLH had significantly improved mortality with more days spent mechanically ventilated. Pilot study on ultrasound evaluation of epiglottis thickness in normal adult A Osman 1 Introduction: As the prevalence of epiglottitis is decreasing due to immunization, the difficulty in early detection remained. The aim of this study is to determine the thickness of epiglottis in normal adult with the utilization of bedside ultrasound. Methods: This was a prospective observational study of convenience selection among healthy staff in Emergency Department, University Malaya Medical Centre. The identification and measurement of epiglottis were performed using a 10MHz linear transducer by trained emergency physicians and registrars in EM. Subjects were scanned in either standing or upright seated position with the neck neutral or mildly extended. The epiglottis, thyroid cartilage and vocal cord were visualized and the epiglottis anteroposterior(AP) diameter was measured. Difference in categorical parameters were analyzed by independent-sample t-test. The relationship between height, weight and epiglottic size was analyzed using Pearson's correlation. Results: Fifty-six subjects were analyzed with 25 males and 31 females age ranging from 18 to 50 years old. The epiglottis AP diameter ranged from 0.15cm to 0.23cm, with average of 0.18cm. There was significant difference in epiglottic AP diameter between male (M=0.19cm, SD=0.19) and female (M=0.17cm, SD=0.12; t(54)=5.27, p=<0.001, twotailed). Moderate positive correlation between height and epiglottic AP diameter (R=0.49) and weight (R=0.33) was documented. Conclusions: Our study demonstrated the identification and visualization of epiglottis was feasible and easy with the use of bedside upper airway ultrasonography. There was a little variation in the AP diameter of epiglottis in adults. Indoor vs. outdoor occurrence in mortality of accidental hypothermia in Japan Y Fujimoto 1 , T Matsuyama 2 , K Takashina Introduction: The impact of location of accidental hypothermia (AH) occurrence has not been sufficiently investigated so far. Thus we aimed to evaluate the differences between indoor and outdoor occurrence about baselines, occurrence place, mortality, and length of ICU stay and hospital stay. Methods: This was a multicenter retrospective study of patients with a body temperature ≤35°C taken to the emergency department of 12 hospitals in Japan between April 2011 and March 2016. We divided the included patients into the following two group according to the location of occurrence of AH (Indoor versus Outdoor). The primary outcome of this study was in-hospital death. Secondary outcomes were the length of ICU stay, and hospital stay. Results: A total of 537 patients were enrolled in our hypothermia database. There were 119 and 418 patients with the outdoor and indoor occurrence. The indoor group was older (71 versus 81.5 years-old, p<0.001) and worse in ADL than the outdoor group. The proportion of in-hospital death was higher in the indoor group than the outdoor group (28.2% [118/418] versus 10.9% [13/119], p<0.001). The multivariable logistic regression analysis demonstrated that Adjusted Odds Ratio of the indoor group over the outdoor group was 2.48 (95%CI; 1.18 to 5.17) ( Table 1) . As for secondary outcomes, both of the length of ICU stay and hospital stay in survivors were longer in the indoor group than the outdoor group. Conclusions: Our multicenter study indicated that Indoor occurrence hypothermia accounts for about 78% of the total in this study, and the proportion of in-hospital death was higher in the indoor group. We have to raise an alert over the indoor onset accidental hypothermia and need to take countermeasures for prevention and early recognition of AH in indoor location. Conclusions: During acute asthmatic attack, arterial hyperlactatemia is frequently present at ED arrival. Nevertheless, the plasma lactate level was no significant difference between ED admission and 2 hr after treatment. The Introduction: This is a case series of traumatic aortic injury (TAI) which was diagnosed by transesophageal echocardiography (TEE) in the emergency department. The number of patients with blunt thoracic aorta injury arriving at emergency department is on the rise and survival rate is time-dependent on early diagnosis. TEE offers several advantages over transthorasic echocardiography (TTE) including reliability, continuous image acquisition and superior image quality. Methods: All trauma patients who presented to emergency department from 1st January 2017 until 30th November 2018 at Hospital Raja Permaisuri Bainun, Perak, Malaysia with suspected TAI were evaluated with transesophageal echocardiography. Over the 2 years period, TEE was performed in 30 patients. 6 patients had positive findings suggestive of TAI. Results: The first case was an old lady who presented after a deceleration injury in a car accident. TEE was performed due to hemodynamic instability and found an intimal flap along the ascending aorta. The second case, a Stanford Type A (Figure 1) , was complicated with pericardial tamponade. The intimal flap was visualised from the aortic arch extending to the descending aorta by TEE. The third case was a case of intramural haematoma involving distal aortic arch extending to the descending aorta which survived until corrective surgery. In the fourth case, TEE revealed a motion artefact which mimicked an intimal flap in the ascending aorta. In the fifth case, TEE showed intimal flap at aortic isthmus which was not detected by TTE. In the last case, a traumatic aortic dissection was complicated by aortic regurgitation (Figure 2) . Conclusions: TEE can be a useful point of care tool use by emergency and critical care physicians for early diagnosis of blunt traumatic aorta injury. Introduction: REBOA is an endovascular intervention intended to preserve central perfusion in the context of shock due to noncompressible torso haemorrhage. More so, it is less invasive than the traditional approach of resuscitative thoracotomy (RT) and aortic crossclamping. Though its use dates back to the Korean War, it has not been widely adopted in trauma management, as evidence demonstrating clear benefit compared with conventional RT is lacking [1] . We aimed to evaluate feasibility, outcomes and complications after REBOA for haemorrhagic shock and traumatic cardiac arrest. Methods: We performed a systematic literature review, searching SCOPUS and PUBMED databases using relevant terms (July 2018). We included studies enrolling patients with haemorrhagic shock or cardiac arrest after civilian trauma who had undergone REBOA and reported hospital mortality (our primary outcome). Abstract-only studies and single-patient case reports were excluded. We collated and analysed data using Review Manager v5.3. The Newcastle-Ottawa scale was used to assess risk of bias. Results: Sixteen in-hospital studies met inclusion criteria (n=2034). Ten were case series and six were cohort studies comparing REBOA outcomes with those of RT. There were wide differences between studies' inclusion criteria, case-mix (including cardiac arrest), injury severity, insertion details, and reported outcomes. Overall hospital mortality post-REBOA was 61.4%. Meta-analysis of cohort studies indicated notably lower mortality in patients undergoing REBOA (OR 0.41, 0.32-0.54) than RT with low statistical heterogeneity between studies (I 2 = 0%), shown in Fig 1. Conclusions: Whilst our findings are limited by methodological differences and biases in the included studies, almost 40% of patients undergoing REBOA for haemorrhagic shock and/or cardiac arrest survived to discharge. Furthermore, REBOA appeared to offer a consistent mortality benefit compared with RT. Introduction: Trauma related coagulopathy remains a primary contributor to mortality on battlefields and in civilian trauma centres. Fibrinogen is considered to be the first to drop below critical level and correspondingly compromised coagulation process. However, it is unclear if fibrinogen concentrate at a very early stage is feasible and effective to prevent from coagulopathy. Methods: A total of 66 acutely injured patients in Austria, Germany and Czech Republic were screened and enrolled in this controlled, prospective randomized placebo controlled double blinded multicentre and multinational trial. Upon the completion of randomization, fibrinogen concentrate (50 mg/kg, FGTW©, LFB France) or placebo was reconstituted and given to the patients at the scene or during helicopter transportation from the scene to nearby hospitals. Blood samples were taken at baseline (scene of accident before study drug administration), at the emergency room, three hours, nine hours and twentyfour hours after admission to the hospital as well as after three and seven days after admission, for measurements of blood gases and coagulation, together with clinical data and outcome records. Results: The demographic and injury characteristics and the estimated blood loss, ISS, and GCS at the scene were similar in both groups. In the placebo group, fibrinogen concentration dropped from 200 mg/dL at injury site to 160 mg/dL () at ER admission and clot stability reduced from 13.5 mm (4, 21 mm) to 10 mm (p=0.013) (Fig 1) . Fibrinogen concentrate administration prevented the drop of fibrinogen level (baseline of 170 mg/dL to 210 mg/dL and improved clot stability from 13 mm at baseline to 16 mm at ER. Conclusions: Pre-hospital administration of fibrinogen concentrate in traumatic bleeding patients is feasible and effective in preventing the development of coagulopathy. Data from this study support the use of fibrinogen to prevent trauma related coagulopathy. Fibrinogen concentrate vs cryoprecipitate in pseudomyxoma peritonei surgery: results from a prospective, randomised, controlled phase 2 study Results: The per-protocol set included 43 pts (HFC, n=21; cryo, n=22). The mean total intraoperative dose of HFC was 6.5 g vs 4.1 pools of cryo (containing approx 8.8 g of fibrinogen). Median duration of surgery was 7.7 h. Overall haemostatic efficacy of HFC was non-inferior to cryo and was rated excellent or good for 100% of pts receiving HFC and cryo, with similar blood loss. Intraoperatively, only red blood cells were transfused (median: 1 unit). Intraoperative efficacy is shown in Table 1 . Infusions were initiated 0.4 h earlier with HFC than cryo due to faster product availability. Preemptive HFC led to a greater mean increase vs cryo in FIBTEM A20 ( Figure 1 ) and plasma fibrinogen (Figure 2 ). There were 6 serious adverse events (SAEs) in the HFC group and 17 in the cryo group, including 7 thromboembolic events (TEEs; 2 deep vein thromboses, 5 pulmonary embolisms). No AEs or SAEs were deemed related to the study drug. Conclusions: HFC was efficacious for treatment of bleeding in pts undergoing surgery for PMP. No related AEs and no TEEs occurred in pts treated with HFC. Fig. 1 (abstract P326) . Fib MCF T1 to T7 with 95% CI Fig. 1 (abstract P327) . FIBTEM A20 prior to and following the preemptive dose of HFC/cryoprecipitate Introduction: Patients in the intensive care unit often suffer from thrombocytopenia. In dealing with this problem, we need to figure out not only the cause of thrombocytopenia but also the risk of bleeding. However, there is no reliable method for evaluating bleeding risk. Methods: In this preliminary study, four thrombocytopenic patients who required platelet transfusion before undergoing invasive procedure were enrolled. Written informed consent was obtained from all patients for participation in the study. Bleeding was graded using the WHO bleeding scale. Thrombogenic activity was evaluated using total thrombus-formation analysis system (T-TAS), rotational thromboelastometry (ROTEM), and multiplate impedance aggregometry. For T-TAS analysis, we prepared a novel microchip, named HD chip, which is suited for analyzing low platelet samples rather than those with normal platelet counts. , key patient groups in which it was wasted and the use of standard laboratory tests (SLTs) to guide its use. The purpose was to assess the potential benefit a point of care viscoelastic haemostatic assay (VHA) could have on FFP transfusion and waste. The National Blood Transfusion Committee and NHS Blood and Transplant committee have published data showing that up to 25% of FFP is transfused inappropriately [1] . Methods: Blood bank data was obtained evaluating haemorrhaging patients in whom FFP was requested across a nine-month period in 2018. Patient bleeds were categorised by speciality. The mean time FFP dispensed and wasted was recorded, as were timings of SLT requests. Where available, the INR result was recorded. Results: 66 patients were identified. 74 transfusions were requested. Table 1 shows that the highest transfusion requirements are for acute medical emergencies and major trauma. 70% of transfusion were surgical specialities, it would be expected that these patients would have anaesthetic or critical care input. 242 units were wasted. Acute medical emergencies wasted the highest amount of FFP (82 units). Table 2 demonstrates that 29.7% of transfusions had an INR available one hour prior to FFP being dispensed. Conclusions: We conclude that use of SLTs to guide FFP transfusion is low. This suggests transfusion decisions are being made clinically. A point of care VHA could give treating physicians better access to timely haemostatic data. Introduction: We developed the process for the out-of-hospital packed red blood cells (pRBC) transfusion in the HEMS of Castilla-La Mancha CLM according to criteria of medical indications, security, monitoring and tracking. Haemorrhage is a preventable cause of death among population suffering accidents or bleeding injuries in regions with low population density where health services should reach people in remote areas. HEMS of CLM is the first out-ofhospital Emergency Service in Spain that provides pRBC transfusion there where the accident takes place. This program has been developed jointly between hematologists of the Center for Transfusions CT and the HEMS team. Methods: Observational retrospective study with data collected from June 2014 to August 2018. The medical helicopter was provided with two pRBC O Rh(D) negative (Fig 1) . Shock Index was selected as indication for transfusion. To achieve feasibility and preservation of the pRBC it was established a prospective monitoring and microbiological culture for both groups: case group for the pRBC kept in the HEMS and control group in the Hospital (Fig 2) . Controls and comparison of hematologic analysis were performed immediately and 35 days after collection. Statistics used SPSS 23.0 (signification p<0.05). Results: 138 pRBC were evaluated, 67 case -71 control. Analyses were tested days 1 and 35 after collection. Hemolysis was not observed. All cultures were negative. Results obtained of the pRBC after 35 days transported in the HEMS related to monitoring parameters were not different than those observed on pRBC conserved in the CT. 35 pRBC were transfused to 28 patients in out-of-hospital assistance. Neither post-transfusional reactions or undesirable events have been registered. pRBC units are changed every 30 days. Conclusions: The process designed (collection, conservation, tracking and tests) to make pRBC available in the medical helicopter has demonstrated to keep the standard conditions and properties to be transfused in critically ill patients out-of-hospital. Outcomes in patients with a haematological malignancy admitted to a general intensive care unit A Corner East Sussex Healthcare NHS Trust, Intensive Care, Eastbourne, United Kingdom Critical Care 2019, 23(Suppl 2):P331 Introduction: Recent published data have challenged the view that critically ill patients with a haematological malignancy have a poor prognosis [1] . Reports have largely originated from tertiary centres. The aim of this audit was to evaluate the intensive care unit (ICU), in hospital and one year mortality for a cohort of patients admitted to a mixed medical and surgical ICU in a district general hospital. Methods: Details were obtained for all patients with a haematological malignancy admitted to Eastbourne and Hastings ICU between March 2012 and August 2017. Patient characteristics, type of malignancy, reason for admission, degree of organ support and survival rates at ICU discharge, hospital discharge and 1 year postadmission were collected. Results: 60 patients, 65% male, were identified. Median (interquartile range, IQR) age was 68 (60-73) years. 55% had neutropenia. The commonest malignancies were acute leukaemia 43%, lymphoma 27% and myeloma 17%. Reasons for admission were respiratory 48%, cardiac 18% and renal 10%. Organ supports used were noradrenaline 68%, intubation and mechanical ventilation 42%, renal replacement therapy (RRT) 32% and dobutamine 10%. Overall survival rates are shown in Figure 1 . 7 patients were discharged from hospital following a period of mechanical ventilation. For these patients, median (range) age was 64 (26-72) years. All were male. Median (IQR) time in hospital prior to admission was 0 (0-2) days, 7/7 patients required vasoactive support, 3/7 required RRT, median ICU length of stay was 12 (3-25) days. 3/7 were admitted following surgery for an unrelated condition. To date, only 1/7 patient has survived 5 years post ICU admission. Conclusions: Although survival rates were disappointing, particularly in those patients requiring mechanical ventilation, selected patients have the potential for a good outcome. These results outcomes have been presented to our Haematology Department to aid patient counselling. analyses. Cox regression was used for the survival analysis. Organ failure was defined as the occurrence of renal failure based on Acute Kidney Injury Network (AKIN)-creatinine or need for; vasopressors, invasive ventilation or continuous renal replacement therapy (CRRT) the first 28 days after admission. Length of stay was only analysed in survivors. Results: The study included 3585 unique patients. Prolonged APTT was associated with mortality with a 95% confidence interval (CI) of hazard ratio 1.001-1.010. Prolonged APTT correlated also with the occurrence of renal failure and the need for vasopressor and CRRT with 95% CI of odds ratio (OR) 1.009-1.028, 1.009-1.033 and 1.009-1.029 (Fig 2) . Increased PT-INR was associated with the need for vasopressors and invasive ventilation with 95% CI of OR 1.138-2.056 and 1.151-1.871. Both APTT and PT-INR correlated with length of stay with 95% CI of OR 1.007-1.020 and 1.034-1.438. Conclusions: Activated partial thromboplastin time on admission to the ICU is independently associated with mortality. Both APTT and PT-INR are independently associated with length of stay and the need of organ support. All regression models were adjusted for SAPS 3 score which means that APTT prolongation and PT-INR increase on admission represent morbidity that is not accounted for in SAPS 3. Introduction: The goal was to assess if daily venous thromboembolism (VTE) assessment was being done in our critical care (CC) unit, and if not, what changes could be made. A mortality review showed the need for a dynamic VTE assessment in CC patients, who are subject to daily changes influencing VTE risk. A daily risk assessment was introduced, and a 'tab' on our clinical information system, Metavision(r)(MV) was created. Recently published National Institute for Health and Care Excellence guidelines on VTE risk assessment in CC provided us cause to assess our compliance [1] . Methods: Data was collected from MV. Review of daily VTE assessment was made and a percentage completion of daily VTEassessments was calculated per patient.Interventions were done using standard improvement methods through PDSA cycles. Results: Baseline data, of 53 patients, was collected in July,2018.Compliance with daily VTE assessment was 51%. The results were presented at the clinical governance forum(CGF), and posters were displayed in CC. The second cycle, of 32 patients, was collected in October. Compliance had increased to 68%.Following discussion from presenting results at the CGF, the VTE tool was appropriately modified.The responsibility of VTE assessment was also shifted to becoming more shared, including all clinical staff, rather than mainly consultants. The third cycle, of 30 patients, was collected in November. Compliance had increased to 80%.Introducing a nursing care bundle with VTE is in progress. Conclusions: Despite the identification of a risk in our clinical practice and the development of an appropriate IT tool to facilitate improved practice, the advent of new national guidance revealed poor compliance with agreed standards. This shows the difficulties with achieving practice change in complex multiprofessional clinical environments. A sustained effort is required focusing on dissemination and engagement across the whole team. Introduction: We describe the changes in anti factor Xa (aFXa) activity, thrombin generation and thromboelastography (TEG) in critically ill patients with and without acute kidney injury (AKI) following routine administration of Tinzaparin as part of venous thromboembolism (VTE) prophylaxis. Methods: Pilot prospective observational study. 18 patients divided into those with and without AKI were administered Tinzaparin by subcutaneous injection as per established local guidelines. 6 patients who did not receive Tinzaparin were recruited as a 'control'. Plasma aFXa activity and thrombin generation were measured at intervals over a 24 hour period. TEG parameters were collected at t0 and t24. Results: aFXa activity: Results are shown in Figure 1 . 3/18 patients failed to achieve a prophylactic aFXa level of >0.2 at any point. 14/18 patients achieved a level of >0.2 however in all cases this was at the lower end of the prophylactic range and was achieved for only a short time (median 1.5 hours). 1/18 achieved a level of >0.2 for the whole 24h period. There was no difference between the AKI and no AKI groups. Endogenous thrombin generation: there is no significant difference in thrombin generation between the AKI and no AKI groups. There is a significant decrease in thrombin generation between 0h and 5h (p<0.01) and a significant increase between 5h and 24h (p<0.01) (Figure 2 ). There is no significant difference between 0h and 24h (p=0.90). TEG: all TEG parameters for all patients were within normal range Conclusions: Standard VTE prophylactic dose Tinzaparin rarely achieves an aFXa range that has been suggested for VTE prophylaxis. However, as assessed by thrombin generation, a hypo-coagulable state is generated in response to LMWH. There is no difference between critically ill patients with or without AKI that would suggest the need for dose reduction in this context. 2 (abstract P334) . Thrombin generation at 0h, 5h and 24h. t0 = time of Tinzaparin administration, with the sample taken just prior to administration. Patients from AKI group shown with dotted line and from no AKI shown with solid line 12% which takes the third place between CPB-associated complications . Current data demonstrates the importance of researching of changes in haemostatic system in paediatric patiens after CPB. Provided below data is an intermediate result of our research. Methods: 39 patients in age up to 11 mohth 29 days (median age -5,5 months, youngest age -2 days after birth, oldest -11 months 29 days), who underwent cardiac surgery with CPB to treat congenital heart diseases, were enrolled in this study. All patients were divided into two groups: 1stwithout TC, 2ndwith TC. Protein C (PC) and fibrin-monomer (FM) plasma levels were assessed in there points: before surgery, 24-hours and 72 hours after surgery. Thrombotic cases were provided by Doppler ultrasound or MRI. Results: Thrombotic complications were diagnosed in 7 chidren (18%). Between all TC ischemic strokes were diagnosed in 57% (4 cases), arterial thrombosis in 29% (2 cases), intracardiac thrombus in 14 % (1 cases). In group with TC FM-mean values in points 1,2 and 3 respectively were 9.62; 37 and 108 mcg/ml, meamwhile in group without thrombosis -7.5; 36.04 and 9.25 mcg/ml .PC-mean value in 1st groupwere 50; 56 and 76%, in the 2nd group -49; 52 and 39% respectively in the points 1, 2 and 3. Statistically significant differences between groups in 3rd point (p<0.05) and correlation between PC and FM (r=-0.93; p<0.05) were detected. Conclusions: CPB causes hypercoagulation with increasing of PC consumtion and FM level. Moreover, CP associated with a high risk of TC on the 3rd day after cardiac surgery. Further studies to investigate prognostic values of FM and PC in thrombosis are required. These studies would help to asses FM and PC as markers of TC and possibility of PC-prescribing for prevention and treatment of these complications. Introduction: Thrombocytopenia is a common condition in critically ill patients and an independent predictor of mortality. The relevance of a supranormal platelet count remains unclear. Septic patients with Disseminated Intravascular Coagulation (DIC) are also known to have a high mortality, but the influence of sepsis on mortality rates in coagulopathic patients is less well characterised. Our objectives were to: 1) Evaluate mortality amongst patients with sepsis and nonsepsis associated DIC. 2) Assess incidence of DIC during the first 7 days of admission. 3) Assess the relationship between platelet count and mortality. Methods: Records of 935 adult critical care patients admitted to the Royal Liverpool University Hospital between 2008-2014 were retrospectively reviewed. The presence of sepsis (using the definition of SIRS with infection), coagulopathy, degree of thrombocytopenia and 28 day mortality were noted. Modified ISTH DIC score was used to define DIC. Results: The overall mortality rate was 19%. 522 patients were identified as having sepsis (56%) and 413 non septic patients (44%). Mortality rates of patients with sepsis were significantly higher than without sepsis (71% vs 29% respectively, p<0.0001). In patients with DIC, their DIC scores tended to be 'positive' for the first 4 days of admission. Fibrin-related markers were often not available for DIC scoring. Mortality rates amongst patients with sepsis-associated DIC were greater than patients with non-sepsis related DIC. Thrombocytopenia severity was associated with mortality, and patients with platelets above the upper limit of normal had lower mortality rates (11% when platelets > 400x10^9/L, 30% when platelets <50x10^9/L). Conclusions: Sepsis-associated coagulopathy is associated with a higher mortality rate than non-sepsis associated coagulopathy. Supranormal platelet counts may be associated with a mortality benefit. Introduction: Deep vein thrombosis (DVT) is a major problem in ICU and affects overall lethality. DVT is widespread complication in ICU, especially in elderly patients, when early activisation may not be achieved. Aim of this study is comparison of haemostatic potential and analgesia methods of elderly patients who underwent major urological surgery during their stay in ICU. Methods: A cross-sectional study was employed. Participants were ≥70 y.o., underwent major urological surgery, have had normal initial hemocoagulation data (thromboelastography was performed to all of them), had received analgesia with epidural catheter or IV by opioids use and were treated in ICU >3 days due to non-coagulopathy states, were included. Data were collected from October 2017 till October 2018. The patients were examined with thromboelastograph "Mednord" for thromboelastogramm (TEG) and with eSaote USG for thrombi occurrence in lower limb deep veins. The anticoagulants were prescribed under the ESA guidelines 2017. Results: Participants (n=30) were divided in two groups -non-opioid analgesia with epidural catheter (n=12) and opioid analgesia (n=18). We received moderate decrease in anticoagulants dosage to the patients with epidural analgesia with the same TEG goals compared to the patients with opioid analgesia. Other factors as comorbidities may provoke DVT events, but was not evaluated in this study. The DVT events were monitored by expert with the use of USG to locate thrombi in the vein. Conclusions: Use of epidural catheter analgesia provides moderate decrease of anticoagulants dosage compared to opioid analgesia patients; however strict control of TEG data must be presented. Comorbidity need to be monitored for early detection and prevention of DVT events. Introduction: Patients with morbid obesity (MO) have a high risk of thromboembolic events. In patients with a BMI >35, the hypercoagulable state is due to impairment of all parts of the blood coagulation as well as anticoagulation mechanisms by obesity. Methods: The hemostasis system was studied in 100 patients with a BMI> 35 kg/m2 with various pathologies that were admitted to ICU. All patients were divided into 2 groups depending on the type of therapy: 1 group (n=50) received monotherapy with Enoxaparin sodium 0.1% 0.2 ml SC 2 times a day every 12 h; group 2 (n=50) received combination therapy with Enoxaparin sodium 0.1% 0.2 ml SC 2 times a day every 12 h and Pentoxifylline 100 mg 2 times a day every 12 h. To study the hemostasis system, we used LPTEG immediately after hospitalization, on 1, 3, 5 days. Results: In both groups, prior to treatment: Contact Coagulation Intensity (ICC) was increased by 23.57%, Intensity of coagulation drive (ICD) -by more than 32.68%, clot maximum density (MA) -by 74.52%, index of retraction and clot lysis (IRCL) -91.18% above normal. Patients of the 1st group: ICC increased by 12.62%, ICD was close to normal values, MA increased by 18.63%, IRCL was increased by 31.17%. Patients of the 2nd group on the 5th day: ICC decreased by 15.22% compared with the norm; the coagulation and fibrinolysis parameters were close to normal values and the decrease in fibrinolysis activity reaches to normal. Conclusions: Combined therapy of thromboembolic complications in patients with obesity Sodium Enoxaparin sodium and Pentoxifylline is more effective than Enoxaparin sodium monotherapy because it affects all parts of the hemostatic system. Introduction: A laryngeal injury secondary to blunt neck trauma can lead to life-threatening upper airway obstruction [1, 2] . Ultrasound enables us to identify important sonoanatomy of the upper airway [3] . The purpose of this report is to discuss role of POCUS airway in blunt neck trauma and to determine airway management based on standard Schaefer 5 subgroups classification. Methods: Three cases of blunt neck trauma presented to our centre with either subtle or significant clinical signs and symptoms. Standard airway management was performed prior to POCUS airway using 15MHz linear transducer and it findings were later compared to flexible fibreoptic laryngoscopy and Computed Tomography (CT). Results: POCUS airway had identified one out of 3 cases to have Schaefer 2 and the remaining as Schaefer 3. All POCUS airway findings were confirmed with flexible fibreoptic laryngoscopy and CT scan (Figs 1, 2) . Based on Schaefer, supportive care and early steroid administration are advisable for group 1 and 2. For groups 3 to 5, immediate open surgical repair is deemed necessary due to extension of injuries.All cases were intubated using Glidescope.All including those presented with Schaefer 3 were managed conservatively and discharge well with proper follow-up. Conclusions: Upper airway ultrasound is a valuable, non-invasive and portable for evaluation of airway management even in anatomy distorted by pathology or trauma. An organised approach using POCUS airway as an adjunct can expedite care and prevent early and long term complications in facilities without flexible laryngoscope and CT. Introduction: High-flow nasal oxygen (HFNO) and helmet noninvasive ventilation (hNIV) are increasingly used for the early management of acute hypoxemic respiratory failure (AHRF). We compared the physiological effects of HFNO and hNIV during AHRF. Methods: In this randomized cross-over study, we enrolled patients with acute-onset (<7 days), non-cardiogenic respiratory distress (respiratory rate>25/min), pulmonary infiltrates at the chest-x-ray and hypoxemia (SpO2<90% while breathing on room air). All patients received hNIV (PEEP 10 cmH2O, pressure support adjusted to achieve a peak inspiratory flow of 100 L/min) and HFNO (flow 50 L/min) for one hour each, in a randomized cross-over manner. At the end of each period, arterial blood gases, inspiratory effort (esophageal pressure) and respiratory rate were recorded. Self-assessment of dyspnea and device-related discomfort ( [3] [4] [5] [6] [7] ). Conclusions: As compared to HFNO among critically ill patients with AHRF, hNIV ameliorates oxygenation, limits inspiratory effort and relieves dyspnea, without affecting PaCO2, respiratory rate and comfort. Introduction: Pre-intubation hypoxemia is a predictor of negative patient outcomes including in-hospital mortality. While successful first intubation attempt is also an important factor of patient outcomes, little is known about whether physicians achieve successful first intubation attempt for the hypoxemic patients in the emergency department (ED). The aim of this study is to investigate the first-pass success for patients with pre-intubation hypoxemia in the ED. Methods: This is an analysis of the data from the second Japanese Emergency Airway Network study (JEAN-2 study)a multicenter, prospective, observational study of 15 EDs in Japan. We included all patients who underwent intubation in the ED from 2012 through 2018. We excluded patients 1) aged <18 years and 2) patients who underwent intubation for cardiac arrest. We grouped pre-intubation hypoxemia as follows: non-hypoxemia (oxygen saturation [SpO2], ≥97%), moderate-hypoxemia (SpO2, 96%-90%), and severehypoxemia (SpO2, <90%). Primary outcome was the first-pass success rate. To demonstrate the association between pre-intubation hypoxemia and the first-pass success in the real-world setting, we fit two unadjusted logistic regression models 1) using grouped preintubation hypoxemia as a categorical variable and 2) using the preintubation SpO2 as a continuous variable. Results: Among 10,144 patients who underwent intubation in the ED (capture rate, 97%), 5,463 patients were eligible for the analysis. Compared to the non-hypoxemia, the first-pass success rate was low in moderate-hypoxemia (72% vs 67%; OR=0.77 [95%CI, 0.67-0.89]) and severe-hypoxemia (72% vs 69%, OR=0.85 [95%CI, 0.72-0.99]). Additionally, there was a linear association between pre-SpO2 and lower first-pass success rate (OR for the success, per one pre-SpO2 decrease, 0.99 [95%CI, 0.98-0.99]). Conclusions: Based on the large, multicenter data, the first-pass success rate was low in hypoxemic patients compared to nonhypoxemic patients in the ED. Introduction of rapid-sequence induction guideline to reduce drug-associated hypotension in critically unwell patients Introduction: The aim of this project was to assess whether the introduction of a Rapid Sequence Induction (RSI) agent guideline changed drug choice and the incidence of peri-intubation vasopressor use at St John's Hospital, Livingston. It is well documented that emergency airway management in the critically ill can be a source of significant morbidity and mortality [1, 2] and the choice of induction agent matters [3] . Methods: An RSI agent guideline was instituted for all critically ill patients being intubated in ICU and the ED [ Figure 1 ]. Following this, we set up an intubation registry to collect data from all intubation events. This data was then compared to a previous audit of intubations completed in 2012. Results: The choice of agent used pre-and post-intervention are summarized in Figure 2 . Forty-five intubation events were included in the initial audit in 2012, of which, 7 (16%) required vasopressor support immediately following intubation. Of the 41 intubation events following the guideline's introduction, 2 (5%) required vasopressors. Ketamine use changed from 0% to 51%, Propofol use from 43% to 29% and Midazolam from 14% to 0%. Thirty-eight of these intubation events (93%) were compliant with the guideline. Conclusions: The introduction of the RSI guideline dramatically affected the choice of induction agent and reduced the incidence of significant hypotension requiring vasopressors (5% versus 16%). Overall compliance with the guideline was excellent (93%). Introduction: The purpose is to test the feasibility of using the I-Gel® device for airway maintenance during bronchoscopic-guided percutaneous dilatational tracheostomy (PDT). Usually PDT is accomplished via the tracheal tube. Failure to position the endotracheal tube correctly can result in further complications during the procedure. The alternative implies extubation and reinsertion of an I-Gel® airway device. Methods: The PDT was performed using the Blue Dolphin method in 5 patients in intensive care unit. Before undertaking bronchoscopicguided percutaneous dilatational tracheostomy (PDT), the patient's tracheal tube (ET) was exchanged for I-Gel®, as a ventilatory device for airway maintenance. The insertion of the I-Gel®, the quality of ventilation, the blood gas values, the view of the tracheal puncture site, and the view of the balloon dilatation were rated as follows: very good (1), good (2), barely acceptable (3), poor (4), and very poor (5) [1] . Results: The I-Gel® successfully maintained the airway and allowed adequate ventilation during percutaneous tracheostomy in all patients. The ratings were 1 or 2 in 100% of cases with regards to ventilation and to blood gas analysis, for identification of relevant structures and tracheal puncture site, and for the view inside the trachea during PDT. Conclusions: The I-Gel® successfully maintained the airway and allowed adequate ventilation during percutaneous tracheostomy in all patients. The ratings were 1 or 2 in 100% of cases with regards to ventilation and to blood gas analysis, for identification of relevant structures and tracheal puncture site, and for the view inside the trachea during PDT. No damages to the bronchoscope, reports of gastric aspiration or technical problems were detected. The bronchoscopic view obtained via an I-Gel® seems to be better than that obtained through an endotracheal tube (ET) or through traditional laryngeal mask [1] . Introduction: The purpose of this study was to investigate the efficiency of nasal airway inserted in the oral airway (ON airway) in securing the airway patency during mask ventilation [1] (Fig 1) . Methods: Fifty eight patients undergoing general anesthesia were randomly assigned to either oral airway group (group O) or ON airway group (group N). In both group, 2 mg/kg of propofol was infused intravenously and mask ventilation was performed in the sniffing position without head extension or jaw thrust. The patients were ventilated with a volume-controlled ventilator with O2 flow of 10 l/min, tidal volume of 10 ml/kg (IBW), and respiratory rate of 10 /min. Before the start of mask ventilation, airway was placed in the oral cavity. Oral airway was used in group O and ON airway was used in group N. Peak inspiratory pressure (PIP), tidal volume and EtCO2 were compared between the two groups. The location of airway tip was graded by fiberoptic bronchoscope as; 0: airway obstructed by tongue, 1: epiglottis visible, 2: airway touches epiglottis tip, 3: airway passes beyond epiglottis tip [2] . Methods: A prospective uncontrolled observational study in 2017-18 in 4 Ukrainian hospitals. 10 SMA-pts from 6-18 mo were involved. All pts. ready for extubation: afebrile, no infiltrations on chest x-ray, normal WBC. However, each SMA-pts. failed SBT (T-tube or PSV). We evaluated: extubation success (no reintubation in 48 hours), ICU LOS, one year survival. Three pts. were excluded: two pts. by staff decision, 1 family have choosen tracheostomy. 7 SMA-pts. included. A cuff leakage test performed -with a negative, dexamethazone 1mg IV was administered. After extubation NIV was started by Ventilogik LS in ST mode via nasal mask Giraffe. The EPAP and IPAP settings were titrated to reach the chest excursion and target levels of SpO2 (92-96%) and EtCO2 (40-45 mmHg). A sputum was draining by mechanical insufflation-excuflation (MIE) and aspirator Results: All pts, were extubated successful. The mean ICU LOS was 8.5 days (7-10 days), one year survival rate was 100%, respiratory failure fully compensated by NIV, there was no ICU admission. Every SMA-pts. are in good condition, gaining weight Introduction: Aerosol delivery has previously been assessed during simulated adult HFNT, delivered by various stand-alone humidification systems [1] . The objective of this study was to evaluate aerosol delivery during simulated HFNT delivered by a mechanical ventilator, across three clinically relevant gas flow rates. Methods: 2ml of 2 mg/ml salbutamol was nebulised using an Aerogen Solo nebuliser (Aerogen, Ireland). An adult head model was connected to a breathing simulator (ASL5000, Ingmar, US), Vt 500 ml, BPM 15 and I: E, 1:1 (Fig 1) . HFNT was supplied via the Servo-U ventilator (Maquet, Getinge, Sweden), using the integrated nebulisation option. Tracheal dose was recorded at two nebuliser positions; A (after the humidification chamber) or B (before of the cannula), at three gas flow rates (10LPM, 30LPM and 60LPM) (n=3). The mass of drug captured on a filter placed distal to the trachea (tracheal dose) was quantified using UV spectroscopy at 276 nm. Results: Presented in Table 1 . Conclusions: To our knowledge, this is the first study to successfully demonstrate aerosol delivery during simulated HFNT, delivered by a mechanical ventilator. Increasing gas flow rate was associated with a reduced tracheal dose (p= <0.0001). At 10LPM, a significantly greater tracheal dose was observed when the nebuliser was positioned before the nasal cannula (p= <0.0001). At 60LPM, a greater tracheal dose was yielded when the nebuliser was positioned after the humidifier (p= <0.0001). Introduction: Tracheotomies are often performed in critically ill patients who are in need of prolonged mechanical ventilation and respiratory care. Our aim was to evaluate the possible effect of percutaneous and surgical tracheotomies on thyroid hormone levels. Methods: Eighty seven adult patients were included in our study from January 2017 to September 2018. Patients were in need of prolonged mechanical ventilation and tracheotomies were performed after consent was taken. We have excluded patients with preexisting thyroid diseases. Forty five patients were undergone percutaneous tracheotomies and forty two patients were undergone for surgical. Thirty eight female patients and forty nine male, age range 18-85. We studied TSH, T3 and FT4 serum levels using Chemiluminescence Immunoassay method before either procedure and 2 hours post each procedure.: Statistical analysis was performed using SPSS 15. Significance was estimated at the level of p< 0.005 Results: TSH levels were increased in surgical group compared to percutaneous group at 2 hours post procedure but the difference was not found statistically significant (p> 0.005). The rise in post operative levels of T3 compared to preoperative was found statistically significant for surgical tracheotomy group (p< 0.005).Elevated FT4 levels for both groups have shown statistically significant difference between preoperative and postoperative period for the surgical tracheotomy group (p< 0.005) Conclusions: We analyzed the effect of surgical versus percutaneous tracheotomy on thyroid hormones and it was found that both Introduction: Insertion of a tracheostomy for weaning purposes is associated with prolonged critical length of stay (LOS) and several adverse patient outcomes [1] . Previous work has suggested that protocolised weaning may reduce weaning times [2] . We aimed to assess the impact of protocolised weaning on LOS following introduction of a standardised weaning protocol in 2017. Conclusions: Introduction of a standardised weaning protocol for patients with a tracheostomy in our unit has had a beneficial effect on several patient outcomes, notably duration of weaning and length of critical care admission. Introduction: Delirium is a relatively frequent neurologic complication in liver transplantation (LT) recipients, which is an important cause of increased morbidity, mortality, extended ICU stay, and increased cost of medical care. Extubation of the endotracheal tube at an appropriate timing is an essential part of intensive care after LT, suggested to improve graft perfusion and systemic oxygenation, and thus decrease intensive care unit (ICU) stay and positively affect prognosis. The aim of this study was to compare the incidence of delirium between early and late extubation groups after LT. Methods: Medical records from 247 patients who received LT from January 2010 to July 2017 in a single university hospital were retrospectively reviewed. Patients were divided into 2 groups: Those who underwent early extubation after LT (group E, n = 52) and those who underwent extubation within few hours of ICU admission after surgery (group C, n = 195). The data of patients´demographics, perioperative management, and postoperative complications were collected. Early extubation was defined as performing extubation in the operating room after LT. A propensity score matching analysis was performed to minimize the effects of selection bias. Results: Postoperative delirium occurred in 4/52 (7.69%) in group E and 30/195 (15.38%) in group C, respectively (P = 0.15). After propensity score matching, there was no difference in ICU stay (P = 0.96), time to discharge after surgery (P = 0.12), and incidence of delirium between groups (P = 1.00). Conclusions: Although this study is retrospective in nature, limited by small sample size, early extubation did not affect the incidence of delirium after LT. Further prospective studies on this area are required. Weight estimation and its impact on mechanical ventilation settings in Queen Elizabeth hospital intensive care unit A Nasr, A Iasniuk, A Roshdy Queen Elizabeth Hospital, ICU, London, United Kingdom Critical Care 2019, 23(Suppl 2):P353 Introduction: Documented weight in the intensive care unit (ICU) can be the total, ideal, adjusted or predicted body weight (PBW). Lung protective ventilation depends on tidal volume (VT) delivery which is based on accurate calculation of patients´weight [1] . The weight is most probably documented on admission to the ICU using estimation or one of many available equations. The aim of this study is to assess the documented versus the PBW and its impact on tidal volume delivery for mechanically ventilated patients in Queen Elizabeth Hospital ICU. Methods: Data was collected prospectively from all ventilated patients over a period of 2 weeks in June 2018. VT delivered in the first hour was calculated for each patient. Documented body weight and height of each patient was obtained from the nursing chart. PBW was calculated and compared with the documented weight. The difference in VT attributable to the difference in weight has been subsequently calculated. Results: 29 ventilated patients were included (17 males). The mean tidal volume delivered according to the documented body weight was 7.16 ml/kg versus 8.28 ml/kg based on PBW. VT more than 8 ml/ kg was delivered in 31% of patients based on documented weight versus 48% when correcting the weight according to the PBW equation. Conclusions: Inaccuracy in documenting weight on patients´admission to the ICU is a potential cause of delivering unsafe tidal volume [2] . The harm can extend to drug dosage, nutrition provision and renal replacement therapy. Introduction: Ventilator-associated pneumonia (VAP) is the leading cause of death among mechanically ventilated critically ill patients [1] . Chest radiography (CXR) is essential in the diagnosis of VAP. In the past decade lung ultrasonography has proven to be a valuable tool in the diagnosis and monitoring of lung diseases. The aim of the study is to assess sensitivity and correlation between CXR, lung ultrasound and Clinical Pulmonary Infection Score (CPIS). Methods: In this retrospective, non-randomized study seven patients with proved VAP were enrolled. In all patients CPIS and Lung Ultrasound Score (LUS) [2] were assessed. Comparison of patients that had LUS≥18 and CPIS≥6 points was performed. The correlation between LUS and CXR was done using the Pearson model. Results: We found significant difference between positive CXR patients with LUS≥18 and CPIS≥6 (100% vs 57%, p<0.05). There is a very high correlation between CXR and LUS. These results render lung ultrasound as a highly sensitive tool in the diagnosis of VAP. Conclusions: Our study shows that lung ultrasonography could be used as a reliable supplementary method in the diagnosis of VAP. The benefits of lung ultrasound include the ability to perform it at the patient´s bed without need for transportation, no radiation exposure and repeatability. The high correlation between CXR and lung ultrasound makes echography a valuable adjunct in the diagnosis of VAP. Color Introduction: It is difficult to differentiate between pneumonia and atelectasis as cause of lung consolidation in Intensive Care Unit patients. Tools like the Clinical Pulmonary Infection score are of little help (Sensitivity 60% and Specificity 59% for detecting pneumonia) [1] . The objective of this study was to determine the accuracy of ultrasound assessed vascular flow within the consolidation to distinguish these causes. Methods: Adult patients with pulmonary symptoms and lung consolidation on lung ultrasound that were scheduled for Chest-CT were included. Vascular flow was analyzed with color Doppler imaging (flow velocity scale was chosen at 0.25m/sec.). The final diagnosis made by the treating physician was regarded as the gold standard. Results: 20 patients were included of which nine (45%) were diagnosed with pneumonia. Vascular flow in the consolidation was present in seven (78%) out of nine patients with pneumonia, compared to three out of 11 (27%) patients with atelectasis (p = 0.07). The diagnostic accuracy in differentiating between pneumonia and atelectasis was 75%. The sensitivity and specificity were 78% and 73% respectively. The positive predictive value was 70% while the negative predictive value was 80%. Conclusions: Vascular flow in lung consolidations assessed by lung ultrasound in ICU patients aids in differentiating between pneumonia and atelectasis. It outperforms the frequently used Clinical Pulmonary Infection Score. Methods: Three intubated patients for various causes of respiratory distress undergoing mechanical ventilation were subjected to TEE. At the level of mid-esophagus, the descending aorta short-axis view (0°) the imaging plane is directed through the transverse axis of the descending aorta. Sector depth was increased to image the left pleural space beneath the aorta. For the right lung, the TEE is rotated to the right at the level of atria until lung is seen or until the image of the liver is seen and the probe was withdrawn until the right lung is seen. Recruitment manoeuvres were performed after identifying PBL atelectasis. Atelectatic lungs were visually observed to open up during and after the recruitment manoeuvres. Results: The time to acquire the image of PBL atelectasis from the time of insertion by TEE is short. The images of posterior lung and the effect of lung recruitments is successfully viewed (Fig 1) . No immediate complication seen. Conclusions: TEE provides an excellent view of PBL atelectasis and able to directly monitor the success and failures of recruitment manoeuvres. Introduction: High respiratory driving pressure (Δ PRS) is strongly associated with increased risk of lung injury and increased mortality during mechanical ventilation. Δ PRS consists of the pressure required to distend the lung the transpulmonary driving pressure (Δ PL) and the pressure required to distend the chest wall. Δ PL is the pressure that increases the risk of lung injury. Data on Δ PL is limited because its measurement requires an esophageal catheter. We aimed to assess changes in Δ PRS and Δ PL during proportional assist ventilation (PAV+) at different experimental conditions. Methods: We retrospectively analyzed patients ventilated with PAV+ who had esophageal pressure measurements before and after dead space or chest load addition. We calculated end-inspiratory plateau pressure (Pplateau), Δ PRS, respiratory system compliance (CRS) and Δ PL during occluded breaths in PAV+ (Figure 1 ). Data were compared with Wilcoxon signed rank test and p value<0.05 was considered significant. Results: 16 patients were analyzed. Dead space increase (9 patients) did not affect the studied parameters. Chest load (7 patients) significantly increased Pplateau (p=0.03) and Δ PRS (p=0.02) and decreased CRS (p=0.02) but Δ PL remained the same (p=0.4). Median (IQR) changes were 9.5 ml/cmH2O (7.7-13.2) for CRS, 2.9 cmH2O (2.4-5.1) Introduction: Particle flow in exhaled air from mechanically ventilated patient's mirrors the opening and closing of small airways and can be detect by optical particle counter [1] . We hypothesized that this particle flow is affected by cardiac function. Methods: Exhaled air from 20 mechanically ventilated patients was analyzed using a customized optical particle counter PExA, Figure 1 . Introduction: We assessed the diagnostic accuracy of Mechanical Power (MP) and Driving Pressure (DP) alone and combined with Stress Index (SI) to identify ventilator settings likely to produce ventilator induced lung injury caused by tidal hyperinflation [1] [2] [3] . Methods: Secondary analysis of a previous database of ARDS patients [1] . Computerized tomography markers of tidal hyperinflation (were used as a "reference standard". Analysis of the area under the receiver-operating characteristics curve (AUC) was used using a two-fold cross-validation. Results: In a cluster of 44 patients, a "training set" of 28 not hyperinflated patients was compared with a "validation set" of 14 hyperinflated patients. (Figure 1-2) . Conclusions: SI seems to be more accurate than MP and DP in identifying tidal hyperinflation in patients with ARDS. Specificity and sensibility were not improved combining SI with MP or DP. The Introduction: The PaO 2 /FiO 2 (P/F) ratio is widely used to assess the severity of lung injury. Conceptually, the P/F ratio should be independent of the FiO 2 and solely depend on the pulmonary condition. However, effect of FiO 2 modulation on the P/F ratio has not been well characterized in ventilated intensive care (ICU) patients. The purpose of the present study was to investigate the relationship between FiO 2 and the P/F ratio in ICU patients on mechanical ventilation. Methods: In a prospective, interventional study 10 patients with a Swan Ganz catheter in situ were included. The P/F ratio was calculated at FiO 2 levels ranging from 0.21 to 1.0 with 10 minute intervals. During the study other ventilator settings were not modulated. To understand the physiological effects of FiO 2 modulation on gas exchange and hemodynamics, mixed venous oxygen saturation and cardiac output were assessed. Shunt fraction was calculated as described by West [1] . Results: Patient characteristics and ventilator settings are reported in Table 1 . All patients were admitted to the ICU after elective cardiac surgery. Modulation of FiO 2 did have a significant effect on the P/F ratio, following a U-shaped pattern (P < 0.05) (Figure 1 ). The shunt fraction varied with altering FiO 2 levels, also exhibiting a U-shaped pattern (P < 0.05) (Figure 2 ). Cardiac output was not affected by FiO 2 . Conclusions: In contrast to current thinking, the P/F ratio varied substantially with altering FiO 2 levels in mechanically ventilated ICU patients. This is an important novel physiological observation. In addition, it demonstrates that the assessment of the severity of respiratory failure by using the P/F ratio should be standardized to a fixed FiO 2 level. Conclusions: In patients undergoing prolonged mechanical ventilation, we must take into account all the factors that may affect our patients. The assessment of diaphragmatic dysfunction is key to preventing weaning failure. An optimal level of consciousness as well as a good management of secretions are key to a successful weaning. Prognostic value of the minute ventilation to CO 2 production ratio as a marker of ventilatory inefficiency in the ICU R Lopez 1 , R Pérez 1 , Á Salazar 1 , I Caviedes 2 , J Graf 1 Introduction: Ventilatory inefficiency for CO2 clearance may provide better severity stratification in acute respiratory failure than oxygenation [1] . Ventilatory inefficiency (VI) is best assessed by the Bohr-Enghoff physiological dead space [2] . We recently reported that the minute ventilation to CO2 production ratio (VE/VCO2), a simplified VI index from exercise testing that obviates the PaCO2 measurement, correlates better than other VI indices to physiological dead space in mechanically ventilated patients [3] . Here we report the prognostic performance of this index using a survival analysis. Mean±SEM VE/VCO2 was higher in patients who died than those who survived (58±8 vs 43±1, p<0.001, Figure 1 ). We found a VE/ VCO2 cutoff value of 45. Mortality was higher in patients with High-VE/VCO2 (≥45) as compared to those with Low-VE/VCO2 (21% vs 4%, p=0.021) with an Odds ratio of 6.7 [95%-CI 1.2-35.8]. Cumulative mortality was higher in the High-VE/VCO2 than in the Low-VE/VCO2 group (Log-rank p=0.015, Figure 2 ). Conclusions: In this unselected cohort of mechanically ventilated patients an early high VE/VCO2 ratio was associated to 28-days mortality. The VE/VCO2 ratio may be a simple and non-invasive VI index with prognostic value in this population. Introduction: Sodium thiosulfate (STS) is a clinically relevant and safe hydrogen sulfide donor that improved acute lung injury (ALI) and brain ischemia/reperfusion injury in previous studies [1, 2] . Methods: In a prospective, controlled, randomized, and doubleblinded trial, twenty adult, anesthetized, mechanically ventilated and surgically instrumented swine with preexisting coronary artery disease [3] underwent 3 h of hemorrhagic shock (HS; removal of 30% of the calculated blood volume and subsequent titration of mean arterial pressure to 40mmHg). Post-shock resuscitation (72h) comprised re-transfusion of shed blood, crystalloids, and norepinephrine. Animals were randomly assigned to "placebo" or "STS" (0.1g·kg -1 ·h -1 for 24h). Before, at the end of and every 24h after shock, hemodynamics, blood gases, and lung function were recorded. Results: Survival rates did not differ between groups. STS-infusion attenuated the HS-induced impairment of lung mechanics and pulmonary gas exchange (Table 1 ,2), resulting in a significantly higher Horovitz/PEEP-ratio ( Figure 1 ). Conclusions: STS during acute resuscitation from HS may protect comorbid swine against HS-induced ALI. Introduction: Alveolar epithelial cell (AEC) death is a main mechanism of severe respiratory failure in acute respiratory distress syndrome (ARDS). Classically, cell death is classified into necrosis or apoptosis. Recent studies have reported that not only apoptosis but also certain types of necrosis are molecularly regulated and that these regulated necrosis can be therapeutic targets for various diseases. However, the relative contribution of necrosis and apoptosis to AEC death in ARDS has not been elucidated. Our study aimed to elucidate which type of cell death is dominant in AEC death and to evaluate whether the regulated necrosis is involved in LPS-induced experimental ARDS. Methods: We established ARDS model by instilling 25μ g of LPS intratracheally to mice. To estimate the relative proportion of apoptosis and necrosis in AEC death, we measured cytokeratin18 M65 level (total cell death marker) and M30 level (apoptosis maker) in bronchoalveolar lavage fluid (BALF) by ELISA, and quantified propidium iodide-positive necrotic cells and TUNEL-positive apoptotic cells in the lung sections. Moreover, we performed pathway enrichment analysis of gene expression data from PCR array to evaluate whether regulated necrosis pathway is associated with the ARDS model. Results: Both M65 and M30 levels were increased in the ARDS mice. The M30/M65 ratio (an indicator of the proportion of apoptosis to total cell death) in the ARDS mice was significantly lower than that of healthy controls. Moreover, the number of propidium iodidepositive necrotic cells was significantly higher than that of TUNELpositive apoptotic cells in ARDS mice. In the pathway enrichment analysis, the necroptosis pathway, a regulated necrosis pathway, was associated with LPS-induced experimental ARDS. Conclusions: AEC necrosis is more dominant than apoptosis in LPSinduced ARDS model. Moreover, necroptosis may contribute to ARDS pathogenesis. AEC necrosis including necroptosis is a potential therapeutic target for ARDS. Clinical ARDS diagnosis is not associated with a unique circulating neutrophil cell surface phenotype T Craven 1 , S Duncan 1 , S Johnston 2 , C Haslett 1 , K Dhaliwal 1 , T Introduction: Acute respiratory distress syndrome (ARDS) is a form of non-cardiogenic oedema due to alveolar injury secondary to an inflammatory process. The clinical diagnosis is defined by the Berlin Criteria but this may not reflect the underlying biological process. The activated neutrophil is central to the pathogenesis of ARDS, characterised by altered cell surface markers. Methods: Three cohorts of seven participants were recruited. The first cohort suffered from mild, moderate or severe ARDS as defined by the Berlin Criteria [1] . The second cohort was composed of ventilated patients on the intensive care unit with acute inflammatory lung disease (diagnosis of clinical suspicion) but did not meet the Berlin Criteria for ARDS. A third cohort was composed of age and sex matched healthy volunteers. Procurement of human tissue was approved by a regional Ethics Committee (14/SS/1074 or 08/S1103/38 or AMREC: 15-HV-013) and with the informed consent of the participant or their personal legal representative. Patients were excluded if aged under 16 or over 80 years of age, were expected to survive for less than 24 hours, if the attending physician refused, due to the absence of suitable indwelling vascular catheter, if the haemoglobin concentration was below 6.5 g/dL, or if the patient was enrolled in a trial of novel anti-inflammatory agent. Whole blood (lysed erytocytes) underwent flow cytometry to determine CD11b, 63, 66b, 88, 62L and 16. Results: A description of the enrolled cohorts can be found in Table  1 . There were no significant differences between the mechanically ventilated, critically ill cohorts for any cell surface molecule in the multiplicity adjusted p values (Fig 1) . The results support the conjecture that clinical diagnostic criteria should not be used as a surrogate to stratify patients according to biological changes, with implications for the testing of biological therapies. Introduction: Aim of the present study was to compare the global and regional diagnostic accuracy of Lung Ultrasound (LUS) compared to lung computed tomography (CT) scan in patients with the Acute Respiratory Distress Syndrome (ARDS). ARDS is characterized by a diffuse, inhomogeneous, inflammatory pulmonary edema. Lung CT scan is the reference imaging technique, but requires transportation outside the intensive care and exposes patients to X-rays. Lung ultrasound (LUS) is a promising, inexpensive, radiation-free, tool for bedside imaging. Methods: Lung CT scan and LUS were performed at PEEP 5 cmH2O. LUS was performed using a standardized assessment of 6 regions per hemithorax: superior and inferior; anterior, lateral and posterior. Each region was classified for the presence of normally aerated, alveolar-interstitial syndrome, consolidation regions and pleural effusion. Agreement between the two techniques was calculated, and diagnostic parameters were assessed for LUS using lung CT as a reference. Both a global and a regional analysis were performed. Results: Thirty-two sedated and paralyzed ARDS patients (age 65±14 years, BMI 25.9±6.5 Kg/m2 and PaO2/FiO2 139±47) were enrolled. Global agreement between LUS and CT was 0.811±0.032. The overall sensitivity and specificity of LUS are shown in Table 1 . Similar results were found with regional analysis (anterior/lateral/posterior lung regions is a common practice in our ICU. During the interruption EIT belt was positioned. When the presence of spontaneous breathing activity was evident by clinical assessment and ventilator traces analysis, NMBA were administered to reach full paralysis, in accordance with the treating physician. EIT tracing were analyzed offline and the change in EELI after NMBA bolus, as compared to before NMBA administration, was measured. Respiratory mechanics and arterial blood gas (ABG) data were collected Results: We enrolled 5 ARDS patients, undergoing controlled mechanical ventilation with muscle paralysis. Baseline respiratory mechanics and ABG data are shown in Table 1 . In 4 out of 5 patient the bolus of NMBA led to an increase of EELI. In 1 case, the NMB administration led to no changes in EELI. The mean change in EELI was 159 ±152ml Conclusions: In our small population of ARDS patients, the administration of a bolus of NMBA after the regain of spontaneous breathing activity led to an increase in EELI in 4 out of 5 patients. Further study are needed to 1) correlate this increase to global and regional respiratory system compliance and 2) correlate this increase to the time needed to wean the patient from NMBA Introduction: To analyze the use of the orthostatic board as an auxiliary device for the treatment of severe ARDS by assessing its risks and benefits. Methods: We selected 91 patients, 43 females and 48 males, hospitalized in a Neurological ICU, between June 2014 and July 2018, in a physiotherapeutic follow-up with diagnosis of severe ARDS. The patients were submitted to orthotics assisted for 40 to 60 minutes and monitored HR, PAM, FR, SatO2 at 30°and 60°of inclination and the PaO2 / FiO2 ratio after the procedure. The mean number of sessions per patient was 6.6. All patients were undergoing anticoagulation in RASS -5, in the treatment of the cause of ARDS. The mean time of mechanical ventilation was 8.5 days. Results: Among the patients selected, 36.3% presented tachycardia above 115 bpm, requiring intervention in 12.1% and interruption of the procedure in 6.6%. PAM arterial hypotension <65 mmHg was observed in 34.1%, requiring intervention (increase of vasopressor dose and / or change of plank angulation) in 22% and interruption of the procedure in 14.3%. Hypoxemia SatO2 <92% was observed in 8.8%, without interruption, but an improvement in PaO2 / FiO2 was observed in only 95.6% of the patients. Conclusions: Assisted orthostatism as an auxiliary device for the treatment of severe ARDS was shown to be an alternative, with improvement of PaO2 / FiO2 in 95.6% of the patients, safe and without significant hemodynamic repercussions that could lead to interruption of the procedure. Introduction: The EOLIA trial found that vvECMO compared to conventional mechanical ventilation (CMV) did not improve mortality in patients with severe ARDS [1] . The CMV strategy consisted of airway pressures below 30cmH 2 O. In patients with severe ARDS higher airway pressures are required to maintain lung aeration. Grasso et al. measured the transpulmonary pressure (P L ) in patients with severe ARDS and increased PEEP until P L was 25cmH 2 O, accepting airway pressures above 30cmH 2 O. Fifty percent of patients responded to an increase in airway pressure and did not require vvECMO [2] . We hypothesized that a P L guided open lung concept (OLC) improves oxygenation and prevents conversion to vvECMO in patients with severe ARDS. Methods: A retrospective study was conducted in a tertiary referral ICU. The records of patients referred to our ICU for advanced medical care were reviewed. Inclusion criteria were severe ARDS according to the Berlin definition and the EOLIA trial inclusion criteria for vvECMO. Results: Mechanical ventilation was limited to a P L of <25cmH 2 O instead of plateau pressures below 30cmH 2 O. The P L guided OLC resulted in an increase in P/F ratio and none of the patients required vvECMO. During the first 6 hours peak airway pressure was increased, but was reduced within 24 hours while PEEP was maintained ( Fig. 1 ). At 72 hours both peak airway pressures and PEEP were reduced to baseline values while P/F ratio remained stable. Only one patient (12.5%) died of disseminated invasive aspergillosis. Conclusions: The P L guided OLC improved oxygenation and none of the patients required vvECMO. These findings support a ventilation strategy guided by transpulmonary pressures instead of plateau pressures in patients with severe ARDS. Introduction: The mortality benefit conferred by early prone positioning in the treatment of acute respiratory distress syndrome (ARDS) has been well established. We also know that APRV improves oxygenation, and more recently has been shown to reduce ventilator dependent days and ICU length of stay [1, 2] . However, controlled ventilation remains the mainstay mode of ventilation used during prone position. Literature looking at combined APRV and prone positioning is scarce. We aim to explore and report our institutional experience with respect to feasibility and outcomes in combining APRV and prone positioning, and perform a literature review in this area. Methods: We undertook a single-centre retrospective cohort study within a surgical ICU of a tertiary hospital in Singapore between Jan 2013 -Oct 2017. Patients with ARDS who received combined prone positioning and APRV were reviewed retrospectively. A literature review of patients with ARDS who received combined intervention was also performed. Results: 5 adult patients aged 21-77 years old diagnosed with ARDS received a combination of APRV and prone positioning for a duration of 16-45 h ( Table 1 ). All the patients tolerated APRV with prone positioning well. Our patients saw an improvement of P:F ratio ranging from 59-225 upon completion of combination therapy. 3 out of 5 patients were extubated within 72 hours of turning supine, 1 was weaned to tracheostomy mask after 14 days and 1 died while on the ventilator. Only 1 case report and 1 randomized clinical trial were found on this topic upon literature review, which corroborated our findings. Conclusions: In our experience, APRV is a practical and feasible alternative mode of ventilation that can be employed in the prone position, yielding significant P:F ratio improvements. The synergistic effects on improving oxygenation herald potential, especially in the subset of severe ARDS patients with refractory hypoxemia, where extracorporeal membrane oxygenation is unsuitable or unavailable. Introduction: The recirculation during veno-venous extracorporeal membrane oxygenation (VV ECMO) had been a drawback, which could limit sufficient oxygenation. Purpose of this study is to compare the short-term oxygenation in acute respiratory distress syndrome (ARDS) patients under VV ECMO according to their cannula configurations, especially in the national environment of the absence of newly developed double-lumen, single cannula. Introduction: VV-ECMO is most commonly used in severe potentially reversible respiratory failure. This report looks at two patients in whom VV-ECMO was used to facilitate surgical airway stenting. Methods: Case 1-A 56-year-old with recurrent respiratory arrests, on a background of Neurofibromatosis Type 1 and kyphoscoliosis. He had complex airway pathology, including, airway neurofibromas and granulation tissue, tracheobronchomalacia, severe kyphoscoliosis and a permanent tracheostomy tube. Rigid bronchoscopy was performed and following debridement of granulation tissue, a trouser-leg stent was deployed. Case 2-A 75-year-old with progressive stridor due to recurrence of a malignant melanoma, which was causing mid-lower tracheal compression. Three tracheal stents were deployed via a rigid bronchoscope. In both cases, percutaneous bi-femoral VV-ECMO was established prior to general anaesthesia and decannulation took place the following day. Results: In these cases, VV-ECMO provided stable extracorporeal gas exchange without conventional tracheal intubation. Cardiopulmonary Bypass and Veno-Arterial ECMO have been described in patients at risk of compression of the heart and distal airway [1] . However, if the major threat is airway collapse, VV-ECMO can provide cardio-respiratory support without the problems associated with arterial cannulation and with lower anticoagulation requirements. Introduction: ECCO2R facilitates the use of low tidal volumes during protective or ultraprotective mechanical ventilation when managing patients with acute respiratory distress syndrome (ARDS); however, the rate of ECCO2R required to avoid hypercapnia remains unclear. Methods: We determined ECCO2R requirements to maintain arterial partial pressure of carbon dioxide or CO2 (PaCO2) at clinically desirable levels in ventilated ARDS patients using a six-compartment mathematical model of CO2 and oxygen (O2) biochemistry [1] and whole-body transport [2] with the addition of an ECCO2R device for extracorporeal veno-venous removal of CO2. The model assumes steady state conditions and is comprehensive from both biochemical and physiological perspectives. O2 consumption and CO2 production rates were assumed proportional to predicted body weight (PBW) and adjusted to achieve PaO2 and PaCO2 levels at a tidal volume of 7.6 mL/(kg of PBW) as reported in LUNG SAFE [3] . Clinically desirable PaCO2 levels during mechanical ventilation were targeted at 46 mm Hg for a ventilation frequency of 20.8/min as previously reported [3] . Results: Model simulated PaCO2 levels without and with an ECCO2R device at various tidal volumes are tabulated in Tables 1 and 2 , respectively. Table 1 shows a substantial increase in PaCO2 at a tidal volume of 6 mL/(kg of PBW) that is more pronounced when further reducing the tidal volume. Additional simulations showed that predicted ECCO2R rates were significantly influenced by ventilation frequency. Conclusions: The current mathematical model predicts that ECCO2R rates that achieve clinically acceptable PaCO2 levels at tidal volumes of 5-6 mL/(kg of PBW) can likely be achieved with current technologies; achieving such PaCO2 levels with ultraprotective tidal volumes of 3-4 mL/(kg of PBW) may be challenging. Figure 1A ). Pulmonary infections for each subtype of immunosuppression are shown in Figure 1B . Conclusions: ARDS VV-ECMO patients with underlying immunosuppression have higher mortality rates and higher rates of ECMO weaning failure. Immunosuppressed patients suffer from a different spectrum of pulmonary infections in comparison to not immunosuppressed patients. Introduction: Acute asthma attack in children is a life-threatening emergency that requires urgent medical intervention. In the present study, we aim to clarify the effect of non-invasive ventilation (NIV) on the heart rate (HR), respiratory rate (RR), and fraction of inspired oxygen (FiO2) in children with acute severe asthma (ASA) who failed to respond to standard medical treatment; and to evaluate the associated complications and length of stay (LOS) at the pediatric intensive care unit (PICU). Methods: This is a retrospective descriptive study of prospectively collected data. It was carried at the PICU of a tertiary university hospital, Saudi Arabia. The study included children ≤14 years old with ASA admitted to the PICU from November 2011 to November 2015 and required NIV. Outcome measures include the effect of NIV on the HR, RR, FiO2, and LOS. The study included 118 children with ASA and 52 (44%) of them required NIV. Of those 52 patients, 12 (23%) were excluded due to incomplete data, and 40 (34%) patients were included in the final analysis. They were 22 (55%) male and 18 (45%) female with a mean age of 66 months and a median Pediatric Index of Mortality 2 (PIM2) score of 6.3%. Of them, 28 (70%) had moderate asthma scores (≥5-9) and 12 (30%) had severe asthma scores (≥9). The median duration of NIV was 18 hours and the median LOS in the PICU was three days. At 6 hours, only RR showed a significant decrease compared to initiation of NIV (p-value <0.001) (Fig 1) ; while HR, RR, and FiO2 were significantly improved at 24 hours from initiation of NIV (p-value <0.001) (Fig 2) . Conclusions: Non-invasive ventilation, in association with standard medical treatment, was associated with clinical improvement in children with ASA not responding to standard medical treatment alone. NIV was not associated with significant complications or side effects. Neurally adjusted ventilatory assist (NAVA) is a partial support ventilatory mode which triggers and tailors the level of assistance delivered by the ventilator to the electrical activity of the diaphragm. The objective of this study was to compare NAVA and pressure support ventilation (PSV) in patients who were difficult to wean. Methods: A total of 99 difficult-to-wean patients who were able to sustained PSV in the critical care medicine unit (ICU) of the Zhongda Hospital, Southeast University were enrolled in the study (Fig 1) . Patients were classified according to the reason for weaning failure and were randomly assigned to receive NAVA or PSV during weaning ( Table 1 ). The primary outcome was the duration of weaning. Secondary outcomes included the proportion of successful weaning and patient-ventilator asynchrony. Results: There were 17% (8/47) and 33% (17/52) patients in the PSV and in the NAVA group never weaned from mechanical ventilation (P = 0.073). The duration of weaning was significantly shorter in the NAVA group [2.4 (1.1-5.3) days], than in that in the PSV group [4.1(1.1-7.7) days] (P = 0.041). The proportion of patients with successful weaning was 70% (n=33/47) in NAVA group which was much higher than that in PSV group (48%, n=25/52) ( Table 2) . Compared with PSV, NAVA improved the rate of successful weaning in patients with single reason (74% vs. 49%, P = 0.019) but not in patients with multiple reasons for difficult weaning (50% vs. 45%, P = 0.656). NAVA decreased ineffective efforts and improved the trigger and cycling-off delays when compared with PSV. Mortality was similar in the two groups (Fig 2) . In patients who were difficult to wean, NAVA decreased duration of weaning and increased the probability of successful weaning. NAVA which improved patient-ventilator asynchrony, is safe, feasible and effective over a prolonged period of time during weaning. Conclusions: Only MRC score is independently associated with SBT failure and difficult or prolonged weaning. HGS is also associated with these two outcomes related to MV weaning and may serve as a simple tool to identify ICUAMW. Introduction: There is evidence to support that in patients with hypoxemic respiratory failure (AHRF) under non invasive ventilation (NIV), high tidal volume (TV) and high respiratory rate (RR) are associated with NIV failure and possibly poor prognosis. We postulated that high minute ventilation (MV); or TV x RR; is associated with mortality in AHRF, when NIV is initiated. Methods: Single-center, prospective and observational study. We included consecutives AHRF adults requiring NIV. AHRF was defined as acute dyspnea with new pulmonary infiltrates on chest radiography and PaCO2 below or equal to 45mmHg. We registered demographic and clinical parameters (including RR, MV, arterial blood gases, heart rate and blood pressure) at baseline and after 6 hours of first session of NIV, APACHE II score, diagnosis, need for intubation and ICU mortality. We performed a multivariate analysis to assess independent factors associated with mortality and ROC .000) and (AUC = 0.7; P =0.019), respectively for mortality, future exacerbations and readmissions. The optimal cut-off point for the 6MWT ratio to predict mortality was 0.23 and to predict future exacerbations and readmissions was 0.4. The 6MWT ratio performed at ICU discharge reveals interesting discriminative properties to predict early mortality, future exacerbations and readmissions in AE/COPD patients. Diffuse alveolar haemorrhage in an intensive care unit -search and you will find M Matias 1 , E Ribeiro 2 , J Baptista 3 , P Martins 3 Introduction: The incidence of diaphragmatic ruptures after thoracoabdominal traumas is 0.8-5% [1] and up to 30% diaphragmatic hernias present late [2] when there is a complication. We report two cases of delayed traumatic diaphragm rupture to highlight the diagnostic difficulties. Methods: Case 1 (image 1) presented left diaphragmatic hernia containing the stomach, spleen, bowel and pancreas. The patient reported a motor vehicle accident dating 4 months. He had thoracoabdominal trauma with several broken ribs on the left side. He then reported occasional pain in his left shoulder and occasional dyspnoea. Case 2 (image 2) showed right diaphragmatic hernia containing right hemicolon, right hepatic lobe and gallbladder, he reported occasional dyspnoea and recent right chest pain. He had a 25 years car accident in which three ribs broke on the right side. Results: Almost 88% of the patients with delayed diaphragmatic rupture presented with complications between 9 and 12 months after trauma, Singh [3] reported a diaphragmatic rupture presenting 50 years after the traumatic event. The physical examination is often not helpful. Conclusions: Those cases emphasizes on the delayed presentation, patients may be asymptomatic or produce only mild, nonspecific symptoms, such as vague abdominal pain, chest pain or recurrent dyspnoea for months or years. The best tool to guide the clinician toward the appropriate diagnosis is a high index of suspicion whenever there is a history of high velocity trauma, regardless of how remote. Factors associated with asynchronies in pressure support ventilation (PSV), a bench study Introduction: Critically ill patients frequently have increased risk of ocular surface disorders (OSDs) due to poor eyelid closure and reduced tear production due to sedation during mechanical ventilation. We conducted a study to look at the incidence of OSDs in our ICU with the current eye care practices and the impact of a protocolised eye care on the incidence and outcome and to determine the correlation of risk factors with the incidence of OSDs Methods: This study was done in our mixed medical surgical ICU. It had a prospective cohort design and was done as before and after study in two phases (Phase I and Phase II). In phase I existing eye care practices were continued. In phase II protocolised eye care was implemented and incidence of OSDs was noted in both phases. Introduction: Both fentanyl and morphine are known as opioid analgesics, which blocks the brain from receiving pain signals, the route of administration and the adverse effects affect their use. We compare the efficacy of intranasal fentanyl versus intravenous morphine adults population presenting to an emergency department (ED) with acute post traumatic severe pain. Methods: We conducted a prospective, randomized, double-blind, placebo-controlled, clinical trial in a tertiary Emergency department between october 2016 and June 2017. Adults with severe post traumatic was included to receive either active intravenous morphine (3 mg immediately and then 1 mg every 3 min if persistence of severe pain maximum 10 mg) and intranasal placebo or active intranasal concentrated fentanyl (2 μ g /kg maximum 200 μ g) and intravenous placebo. Exclusion criteria: significant head injury, allergy to opiates, nasal blockage, or inability to perform pain scoring, Pain scores were rated by using a digital scale at 0, 5, 10, and 30 minutes. Routine clinical observations and adverse events were recorded. Conclusions: ISCs were related to K over-use in our BICU. Burnt patients are at risk of hepatic injury [2] , but K related hepatic injury likely occurred. Its not clearly understood mechanisms may involve a cumulative dose effect. Although involvement of concomitant medications is being investigated, K restriction policy seemed to contain hepatic disorders. Introduction: In November 2016, our institution switched from using alfentanil to fentanyl for analgesia and sedation in adult patients receiving ECMO. There is no published evidence comparing the clinical use of alfentanil vs fentanyl for sedation in ECMO patients, although some reported increased fentanyl sequestration into the circuit [1] . For these reasons, we conducted a retrospective observational study to explore whether there were any significant differences in patient outcome or adjunctive sedation before and after the switch. Methods: Outcome data and total daily doses of alfentanil or fentanyl as well as adjunctive sedation/analgesia for each patient where obtained from our clinical information system (Philips ICCA®). Data was included from ECMO patients who were sedated with alfentanil or fentanyl from 1/1/16 to 31/10/2017 until ECMO decannulation. Patients not requiring either opiate or who were switched between the two during ECMO therapy were excluded. All medicines prescribed for the management of sedation or agitation were included. For each patient an average total daily dose of each drug, was calculated. Data was analysed using STATA®. Results: Both groups were found to be statistically equivalent for mode of ECMO, age, APACHE 2 score and charlson score (p=0.202) except for BMI (p=0.007). No difference in patient outcomes were found between groups (Table 1) . Patients in the alfentanil group were found to have received significantly higher median average total daily dose of quetiapine and midazolam (Table 2) . Conclusions: No differences in patient outcomes were found between patients sedated with alfentanil compared to fentanyl. We Introduction: The European Society of Intensive Care Medicine Consensus Statement recommends that for comatose survivors of cardiac arrest 12 hours without sedation is the minimum acceptable before neurological assessment. They highlighted the need to investigate the pharmacokinetics of opioid drugs in post-cardiac arrest patients, especially those treated with controlled temperature [1] . Methods: Following approval by Research Ethics Committee, we measured the blood concentration of fentanyl in 24 post-cardiac arrest patients treated with TTM following cessation of continuous infusion. The fentanyl was discontinued when the patients were rewarmed to a temperature of 36.5 degrees Celsius and a blood sample taken 12 hours later. The blood was analysed using a commercial ELISA kit (Neogen Corporation). Using the total dose of fentanyl administered, the half-life of fentanyl was calculated for each patient. Patient physiological data, CYP3A4 and ABCB1 polymorphism and drug history were compared with half-life. Results: The median fentanyl concentration at 12 hours was 0.82 mcg/L with a very wide range (0.07-8.29 mcg/L). The results for calculated half lives are shown in Figure 1 . There was no correlation between fentanyl level and BMI, illness severity (SAPS ll), creatinine clearance, transaminase or lactate level. There was no correlation between co-administration of drugs of metabolised by the CYP3A4 and ABCB1 enzyme systems or genotype. Conclusions: There is marked variation in the concentration of fentanyl at 12 hours in patients managed with TTM following cessation of fentanyl infusion. The calculated clearance of fentanyl in some patients is greater than 48 hours and a 12 hour cut off is not safe. Introduction: Objective of this study was to compare the effects of three analgesic regimens, one opioid and two multimodal ones, on cardiovascular stability and pain intensity in patients undergoing elective surgery under general endotracheal anesthesia during the 24 h postoperative period. Methods: Sixty elderly patients, ASA II, undergoing elective knee sugary were assigned to receive 1) morphine 5 or 10 mg iv q6h, depending on body weight, and paracetamol 1 g iv q6h (MP group), or multimodal nerve block: 2) femoral nerve block, single shot (FNB group) or 3) fascia iliaca compartment nerve block single shot (FICNB group). Measurement of pain intensity was performed with numerical Introduction: Opioids are frequently used in the intensive care unit (ICU) to relieve pain and facilitate tolerance of life-support technologies. When discontinued abruptly, patients may develop a cluster of symptoms known as opioid-associated iatrogenic withdrawal syndrome (OIWS). This phenomenon is poorly described in critically ill adults although it is associated with unfavourable outcomes, such as prolonged ICU stay. The objective of this study was to describe the signs and symptoms of OIWS in adult ICU patients. Methods: A prospective observational study was conducted in two tertiary care centres in patients requiring mechanical ventilation and regular opioids for more than 72 hours. After an opioid dose reduction of at least 10%, patients were assessed daily for signs and symptoms of withdrawal using a standardized form. Concomitantly, the presence of OIWS was assessed daily by a physician using modified DSM-5 criteria. All physician evaluations were blinded and performed independently. Inter-rater reliability for DSM-5 evaluations was assessed with the kappa coefficient. Results: A total of 1128 patients were screened and twenty-nine enrolled. The majority were male (72.4%) with a median age of 65. The median APACHE II score was 27. Withdrawal occurred in 20.7% of patient within a median of three days (IQR 3 to 9 days) from opioid weaning. According to investigator assessment, restlessness, agitation, anxiety, hallucinations, insomnia/sleep disturbance, mydriasis and elevated blood pressure were more prevalent in OIWS-positive patients. DSM-5 evaluations identified dysphoric mood, muscle aches, lacrimation/rhinorrhea, pupillary dilation/piloerection/sweating, diarrhea and yawning more frequently in OIWS-positive patients. The kappa coefficient showed good agreement (0.64). Conclusions: OIWS in critically ill adults presents with a large spectrum of signs and symptoms that occur within a median of three days from onset of opioid weaning. Further studies are needed to confirm these preliminary findings. Withdrawal reactions after discontinuation or rate reduction of fentanyl infusion in ventilated critically ill adults S Taesotikul Introduction: Propofol is a well-known sedative, commonly used in Intensive Care Units (ICU s), that on rare occasions has been reported to cause green urine and has also been associated with pink or transient white urine discoloration. It can cause several adverse effects, such as low blood pressure, pain on injection, apnea, hypertriglyceridemia and when administered in high doses it may lead to the "propofol infusion syndrome". Methods: We present two examples of interesting urine discolorations observed unexpectedly in our ICU in patients under propofol sedation requiring mechanical ventilation. Results: Dark green urine discoloration as presented in Fig.1 is the result of a phenolic metabolite of propofol that is produced in the liver and is subsequently excreted in the urine, thus changing its color. It is considered a reversible phenomenon that resolves after propofol discontinuation.Respectively, pink urine discoloration as presented in Fig.2 can also be the result of propofol infusion. The increase in urine excretion of uric acid caused by propofol, in combination with a low urinary PH can lead to the formation of uric acid crystals and turn the urine pink. Discontinuation of propofol and urine alkalization can reverse the phenomenon. Conclusions: Green or pink urine discoloration due to propofol is generally a benign, reversible condition. Its presence should not compel the physician in charge to perform unnecessary testing, although other causes of discoloration should be considered. As far as green urine discoloration is concerned, other factors such as drugs, dyes, certain nutritional supplements or even a Pseudomonas urinary tract infection may be at fault. On the other hand, pink urine syndrome due to propofol infusion seems to be even rarer. Although its presentation is not alarming, it may well increase the risk of uric acid lithiasis, a fact that the physician in charge should always keep in mind. Conclusions: Hepatic changes related to propofol are frequently observed and should be systematically monitored to ensure patient safety. Fig. 1 (abstract P405) . Dark green urine discoloration Introduction: Clevidipine (CLEV) and propofol (PROP) are lipid-based medications used in the intensive care unit (ICU) for hypertension and sedation, respectively. No data exists regarding potential adverse effects of concurrent therapy with this combination. This study aims to evaluate the incidence of hypertriglyceridemia (hTG) and pancreatitis in ICU patients using concurrent CLEV and PROP. Methods: This was a single-center, retrospective chart review in patients utilizing CLEV and PROP concurrently from February 2015 to November 2018. Patients were included if they were 18 years and older, on CLEV and PROP concurrently for at least 24 hours with no more than 2 hours of interruption at a time, had at least one triglyceride (TG) level during concurrent therapy, and admitted to the medical or surgical ICU. The incidence of hTG (defined as TG equal to or greater than 500 mg/dL) and pancreatitis (provider assessment based on 2013 American College of Gastroenterology guidelines) was evaluated. Patients with and without hTG were compared to identify risk factors for the development of hTG. Results: Of 145 patients screened, 30 patients were included which comprised 36 observations. The incidence of hTG was 13.9% with no patients developing pancreatitis. Patients with hTG had a higher median age compared to without hTG (64.5 vs. 38), p=0.015. In patients with hTG the median dose of CLEV and PROP were 16 mg/h and 42.3 mcg/kg/min, respectively, which was higher but not statistically significant when compared to patients without hTG. Cumulative lipid load (g/kg/d) was non-significantly higher in patients with hTG (2.6 vs. 1.9), p=0.599. Conclusions: The incidence of hTG was comparable to what is cited in literature for PROP alone. Patients with hTG were older, had higher median CLEV and PROP doses, and a larger cumulative lipid load compared to patients without hTG. Introduction: The 2013 Society of critical care medicine guidelines for pain, agitation and delirium suggested use of nonbenzodiazepine sedatives like dexmedetomidine which is associated with a reduced duration of mechanical ventilation, shorter length of hospital stay and a lower incidence of delirium [1] . Enteral clonidine represents a potentially less costly alternative for agitated patients with prolonged dexmedetomidine infusion. Limited literature exists examining this transition for management of agitation [2] . Methods: The critical care management initiated an action plan on the transition of patients with prolonged dexmedetomidine infusion to oral clonidine. A protocol was prepared with clinical pharmacist's assistance. Risk factors were assessed and inclusion criteria were applied as per protocol. Dexmedetomidine infusion rate was reduced gradually with oral clonidine administration in selected patients. Other rescue managements were implemented as per protocol. Oral clonidine was then tapered down by reducing frequency of administration over few days. Results: Post intervention data in 2017 showed significant decrease of dispensed doses and cost of the injections compared to 2016. The annual cost saving was 24% equating to 61,249 USD (Table 1, Figure 1 ). Conclusions: Transitioning to clonidine may be safe and less costly method of managing agitated critically ill patients on prolonged dexmedetomidine infusion. More studies are needed to evaluate the efficacy and safety of this practice. Incidence of dexmedetomidine associated fever at a level 1 trauma center NA Beaupre, JT Jancik Hennepin County Medical Center, Pharmacy Department, Minneapolis, United States Critical Care 2019, 23(Suppl 2):P409 Introduction: We evaluated the incidence of dexmedetomidine associated fever (DAF) in a level 1 trauma center's medical intensive care unit (MICU). Hypotension and bradycardia are the most commonly reported adverse effects associated with dexmedetomidine (DEX) infusion. Case reports suggest DEX can cause fevers and the clinical trials that led to the approval of DEX demonstrated fever rate to be 4-5% [1] . Methods: This was a single-center, retrospective chart review of patients admitted to the MICU at Hennepin County Medical Center between March and July of 2018 that were started on a DEX infusion. Patients were included if they were 18 years and older, on a DEX infusion for at least 12 hours, and had temperature data available. Fever was defined as >38.3C and other causes of fever including infections, medications, withdrawal, recent surgery, thromboembolic disease, thyroid disorders and seizures were excluded from analysis. Results: Of the 246 patients screened, 120 were included. The mean age was 50 years and 63.3% were males. Of all the patients included, the mean change in temperature after initiation of DEX infusion was +1.1C from baseline. The mean initial dose was 0.3 mcg/kg/hr. Four of 120 patients (3.3%) had a DAF. Of those that had a DAF, the median initial dose was 0.3 mcg/kg/hr; the median time of infusion was 43.5 hours; and the median cumulative dose was 0.57 mcg/kg/hr. The median time to fever after initiation of DEX was 7 hours, with a range of 5 to 20 hours. The median time to fever cessation after discontinuation of DEX was 3 hours. Conclusions: In our population, the incidence of dexmedetomidine associated fever was relatively rare at 3.3% and similar to current literature rates. The results obtained showed a statistically significant fact that fewer points on the test, from 11 to 20 points, received older patients who underwent an urgent surgical procedure, over 60 years of age, of which 27% . Also statistically significant data were obtained that patients who used a higher amount of sedatives during emergency surgery, 47% had a worse test result than under 20 points due to increased preoperative anxiety. The older population is more susceptible to postoperative delirium, especially in emergency surgery situations, which they carry, unpreparedness for surgery, increased use of medication for Fig. 1 (abstract P411) . Flowchart of enrolled patients calm, unpredictability of the duration of surgery, and therefore anesthesia as well the use of anticholinergics, which is sometimes impossible to avoid in operative procedures such as gall bladder surgery. The results of the study suggest that in cases of emergency surgery, the use of protocols for postoperative delirium should be planned regularly to prevent or at least mitigate the clinical picture of delirium that can lead to complications postoperatively. Introduction: Delirium is a serious and often underestimated condition with implications for morbidity, mortality and healthcare costs. As it presents in a wide range of settings from admission to discharge, early prediction and risk assessment are essential. E-PRE-DELIRIC is a delirium prediction score which has been validated in ITU patients but not in other populations, and we conducted a quality improvement project using this score to assess its utility in other settings. Methods: Data was gathered from three patient categories: those undergoing elective surgery (ES), admissions to the Emergency Observation Unit (EOU) in the A&E, and patients with fractured neck of femur (NOF). Clinical notes were reviewed to collect data to calculate E-PRE-DELIRIC score at admission, along with a number of other clinical variables including incidence of delirium, and statistical analysis performed. Results: A total of 103 patients were included, with 52 in the ES group, 32 in the EOU group, and 19 in the NOF group respectively, with an overall average E-PRE-DELIRIC score of 17.5%. ES had a 5.4% average E-PRE-DELIRIC score, a mean age of 56 and no cases of delirium. The EOU group had an average age of 63, a 20.5% average E-PRE-DELIRIC score and no incidence of delirium. The NOF group had a mean age of 84 and an average E-PRE-DELIRIC score calculated on admission of 45.3%. This was the only group in which 5 patients developed delirium. A 50% cut off was demonstrated to be the most accurate to predict delirium in this population with a sensitivity of 0.80 and a specificity of 0.78. Conclusions: Despite the limitation of a small sample size, this project has shown that E-PRE-DELIRIC score could be a useful tool to predict patients at high risk of delirium in a non-ITU setting, with a 50% cut off in hip fracture patients. Further investigation should be conducted into the potential use of E-PRE-DELIRIC in non-ITU patients. Comparison of long-term mortality between patients with and without delirium during admission in medical intensive care units in a university hospital N Kongpolprom King Chulalongkorn Memorial Hospital, Pulmonary Unit, Bangkok, Thailand Critical Care 2019, 23(Suppl 2):P415 that delirium is linked with preoperatory comorbidities. The complexity of surgery has a big influence on the development of delirium, especially in the cases of aortic dissection. Delirium was associated with intraoperatory blood transfusions. Finally, our data point to a bridge between postoperatory electrolytic disturbances, as well as inflammation as factors potentially triggering delirium onset. Introduction: We did a retrospective case note study of mortality due to sepsis of our unit over three months as observational study in which we noted the causes of deaths, origin of sepsis, organism, patient characteristics and ICNARC Physiology scores and ICNARC H2015 model predicted risk of acute hospital mortality percentage. Methods: ICNARC data base was used to gather the data and coding was used to identify the patients with sepsis for three months. Patients mortality attributed to sepsis were identified from mortality list.Causes of death were noted from patients notes and death certificates.Cyber Lab was used to access the data and case note were ordered for review.Patients characteristics were noted including DNACPR orders and treatment withdrawal orders. Scores (apache scores, ICNARC Physiology scores, ICNARC H2015 predicted risk models of acute hospital mortality percentage) were noted. Results: Mortality percentage was found to be 17% as per codig which was reduced to 12% as 5% deaths were attributed to other causes. 44% patient had DNACPR in first 24hrs. Average length of stay was 5.58 days with median of 2.53 days.Median age was 66yrs in surviving age group and 78years in other. ICNARC physiology score 23 with predicted risk of 56.8%. Commonest cause was found Pneumonia 49% followed by Urine tract infection. 20% patients were with no source identification. Conclusions: Conclusion was made that we do need to improve the coding as significant percentage was mentioned as sepsis as cause of death where clinicians differed. Pneumonia was found to be the commonest killer in critical care followed by urine tract infection. It was pointed to be useful to carry out further audit targeting Pneumonia .Review of ICNARC Case Mix Program, development of ICNARC Physiology score, which provides excellent local use with downside of lacking international comparison was done also. Introduction: Hospitals vary widely in the quality of care they provide for septic patients. Since many septic patients present to their nearest hospital, local variations in care quality may lead to geographic disparities in access to optimal sepsis care. We sought to better understand geographic access to high quality sepsis care, taking advantage of publicly reported data on sepsis management and outcomes in a large US state. Methods: We performed a cross-sectional analysis of geographic access to high quality sepsis care, taking advantage of a 2013 New York state initiative that mandates public reporting of sepsis quality data to the state government. We linked these data to the locations of hospitals in New York state from the US Centers for Medicare and Medicaid Services and population data from the US Census Bureau for 2016. We defined hospital sepsis performance using self-reported risk-adjusted mortality rates (RAMR) and defined high-performing hospitals as those with a RAMR <20%, which represents the lower end of short-term mortality typically observed in sepsis. We used ArcGIS to generate drive-time estimates and assess population access to high performing acute care hospitals for sepsis care. Results: 161 hospitals publicly reported treating 47,081 cases of sepsis from a population of 19,569,040 persons. Overall access to an acute care hospital was excellent at the 45-minute drive threshold (99.3%), good at the 30-minute threshold (95.6%), and marginal at the 15-minute threshold (84.4%). We classified 46 hospitals (28.6%) as high-performing based on a RAMR <20%. High-performing hospitals reported 12,666 (26.9%) of the total sepsis cases. High-performing hospitals were geographically dispersed across the state, although population access diminished substantially with increasing drive times (92.6% at 45-minutes, 83.5% at 30-minutes, and 59.8% at 15minutes; Figure 1 ). Conclusions: One in six people do not have timely access to a high performing hospital for sepsis care using a 30-minute threshold. [1] . This poses a significant safety risk. A previous study found that the implementation of a multidisciplinary medication safety group in intensive care increased reporting of errors and near misses [2] . The purpose of our work was to set up a multidisciplinary group to provide a forum to review and improve medication safety at all stages of the process. Here we discuss some of the initiatives and outcomes implemented in the last 12 months. Methods: CCMSG was formed in 2013, under the leadership of the critical care pharmacy team, with representation from medical and nursing disciplines. The group meet fortnightly to analyse trends in medication errors, implement changes to local practice and review outcomes to improve patient safety. The cohesive, multidisciplinary nature of the group allows medication safety initiatives to be delivered in the most effective way. Results: On average, CCMSG reviewed 21 medication errors per month. The most common high risk drug classes involved are seen in Table 1 . Medication safety initiatives implemented were based on these trends and included writing guidelines and policies, bedside education, teaching and training, informatics optimisation and operational changes. Examples are seen in Table 2 . Conclusions: Initiation of a CCMSG provides a cohesive approach to facilitate the implementation of targeted safety initiatives, which are proven to reduce some of the most common medication errors in critical care. In addition, these often result in optimisation of operational and financial inefficiencies. Introduction: CIS/hospital electronic medical records downtime can cause major disruptions to workflow, patient care, key communication and information continuity [1] . Here we describe the consequences of deploying a business continuity plan (BCP) designed to support a critical care clinical informatics system (CIS) failure, during an 8-hour unplanned downtime in a large central London ICU. The institutional BCP was developed through an iterative process based on CIS provider recommendations and internal workflow knowledge. It consisted of a Web offline chart (WOC) that is accessible at every computer connected to the network (in the event of a CIS server fault), and via hard copy from designated back up computers connected to a printer (in the event of whole network loss). Operational and clinical consequences were recorded during informal and formal debrief of the informatics team. The decision making around´drop-to-paper´was reviewed. -The BCP permitted´drop-to-paper´, service continuity and controlled uptime -Patchy network loss and lack of a general institutional BCP delayed initial system failure diagnosis (network vs primary server); reduced reliability of´read-only´data and delayedd rop-to-paper-Day-to-night handover during downtime led to loss ofḿ emory´of key patient data/events, and should have accelerated decision to´drop-to-paper-Transfer of prescriptions was time consuming, distracting (occupied CIS team) and prone to error Conclusions: Previous end-to-end testing of the BCP had not identified many of the observations and recommendations that came from the analysis of an actual period of unplanned downtime. We recommend sharing of similar experiences and scheduled high-fidelity simulated downtime in other institutions to replicate real world conditions, particularly in a critical care setting. .001) were predictors of ICU transfer. We developed a simple score to predicting ICU transfer from previous variables and performed analysis of AUC of ROC, which was compared to that of APA-CHE II. The result showed the AUC of ROC of a new score was slightly higher than the APACHE II, namely 0.797 vs. 0.720 respectively. Conclusions: The immunocompromised patients take two times higher risk than the immunocompetent ones regarding ICU transfer. The other risk factors are lower GCS, lower SBP, and higher RR. A newly developed score may be a promising tool for predicting and triaging site of care in patients who require IMCU admission. Introduction: This research aims to explore the role of situation awareness in the decision-making of patient discharge from the Intensive Care Unit (ICU). The discharge of these patients is a complex and, moreover, a challenging transition of care. Readmissions are undesirable given the association with a more extended hospital stay and a possible chance of higher mortality. Little is known on how the decision-making process takes place and accordingly, the role of situation awareness of patient discharge from the ICU. In order to improve the quality of care of patient discharge from the ICU, further research is necessary. Methods: This research concerns a qualitative study in which various health care providers, working in an ICU adults of a large teaching hospital, were interviewed. Through purposive sampling, six nurses, two physician assistants, two intensivists and a physiotherapist were included. On the obtained data a thematic analysis was applied, based on the principles of the Grounded Theory. Results: The discharge decision of ICU patients seems mainly based on the team´s situation awareness, with the initiating role of the intensivist and the guiding role of the nurse. Furthermore, there is an additional role for the physician-assistant and a consultative role for physiotherapy in the process of the decisionmaking. Worries of patients and family seem not to affect the decision-making directly. In the decision-making process, the well-being of the patients and the possibility to provide the most suitable and best possible care were central. Organizational factors, such as an urgent demand for ICU beds do count but seem not to push the decision to transfer patients from the ICU to the regular hospital ward. Conclusions: The decision to dismiss ICU patients is a complex process with different disciplines and a variety of factors involved. Obtained knowledge and insights into the role of situation awareness provide starting points for improving the quality of the discharge process of ICU patients. Conclusions: Despite the fact that older people was more severe illnes, and similar frequency of respiratory failure, the use of mechanical ventilation, the use of central venous catheter and arterial catheter was less frequent. The addition of a simulation fellow within the intensive care team and introduction of in situ simulation N Bhalla, D Hepburn, G Phillips Royal Gwent Hospital, Intensive Care Unit, Newport, United Kingdom Critical Care 2019, 23(Suppl 2):P429 Introduction: Traditionally, simulation based medical education has been carried out in off site simulation centres, however, we trialled the addition of a simulation fellow, within our intensive care team, to run an in situ simulation (ISS) program on our intensive care unit over a 3 month period. Methods: Our multi-disciplinary ISS program, led by a simulation fellow, incorporated participants, observers and facilitators including doctors (junior trainees up to consultants of varying medical specialties), nursing staff, healthcare support workers, operating department practitioners, physiotherapists and medical students. We ran simulated emergency scenarios and technical skills sessions. With every scenario, we collected data on participant and observer feedback using the World Health Organisation participant feedback form and conducted a satisfaction survey at the end of our trial period. Results: Our results, highlighted in Table 1 , show participants found ISS led by a simulation fellow realistic, well structured and organised. It was useful for testing and understanding our response systems, Fig. 2 (abstract P426) . Patient journey of Group 2: those patients discharged home 2 days after step down from critical care identifying strengths and gaps and establishing individual roles/functions within emergencies; overall leaving us feeling better prepared for critical care emergencies. From our satisfaction survey, 100% of participants found the simulation fellow a useful addition to the intensive care team and expressed the need for more in situ simulation. Conclusions: The addition of a simulation fellow allowed for numerous disciplines within the critical care team to be involved in challenging emergency scenarios (Fig 1, 2) , with the additional realism of being on the intensive care unit playing the role they would in real life; as well as having opportunity for spontaneous discussion and learning. From this they reported great benefit and satisfaction. Following our initial success with this program, we plan to have a simulation fellow as an ongoing role within our critical care team. Impact of multidisciplinary team in readmission in a Brazilian cardiac intensive care unit C Bosso 1 , P Introduction: The aim of this study is to determine the importance of the multidisciplinary team at readmission rates in a cardiac intensive care unit (CICU). Methods: Retrospective study with analysis of 2945 patients in a CICU of a medium size Brazilian hospital. The years of 2012 and 2013 represent the reduced team (physician, nurse and physiotherapist) and 2015 and 2016 the complete multidisciplinary team (additional presence of phonoaudiologist, psychologist, pharmacist, dentist and nutritional professional). The risk of mortality was determined by SAPS3 score. In order to compare the teams, it was utilized Odd ratio of a logistical sample to the discrete data, and T-Student test to the continuous data. The data analysis was executed from the software RStudio (1.1.456), and the significance level adopted was 5%. Results: The number of patients was of N=2945 (1422 from the reduced team and 1523 from the multidisciplinary team). The age, sex and BMI didn`t present significant difference between groups. The average age of the sample was 68±13 years old (p=0.13). The male sex represented 58% (p=0.93), and the BMI was around 29.2±23.3 (p=0.12). The main diagnoses were similar in both groups -coronary angiography with stent (16%), unstable angina and non ST elevation myocardial infarction (9%). Table 1 shows the average, standard deviation, p-value to T-Student test to SAPS 3 score and lengh of stay (days), according to both reduced and multidisciplinary teams. Table 2 exposes the mortality rate and readmission for both teams. The figure shows the Odds ratio and its IC 95% to the comparison of the mortality, readmission, 24 hours readmission and 48 hours readmission rates between the teams. Conclusions: The multidisciplinary team performance reduced the number of hospital readmissions in 24 and 48 hours in a CICU. Methods: During the initial audit 24 hours' worth of waste from one ITU bed was manually divided into the categories above. Results: Based on these figures it was estimated that a saving of £1745 per year would be made (£87.25 per bed space) over the course of a year should domestic waste bins be placed across the 20 bed ICU/HDU. A business case was made, and every bay had a domestic waste bin installed with poster signs for explanation.The reaudit in which all domestic waste across the unit was weighed produced an even greater figure of a saving of £205 per bed space (£4100) per year. Conclusions: Introducing a domestic waste bin may save approximately £200 per year per bed. In a typical ITU such as Lewisham (10 ITU beds/10 HDU beds) that may mean a saving of £4000 per year (with 100% capacity). There are also environmental benefits, burning of plastics releases harmful dioxins. The authors wish to make intensive care units and indeed all areas of the hospital aware of the cost and environmental impact associated with disposing of waste in incorrect categories. We hope that our quality improvement project demonstrates how easily money may be saved and environmental footprint reduced. Association between resilience and level of experience in intensive care doctors in India J Gopaldas, A Siyal Manipal Hospital, Bangalore, Critical Care Medicine, Bangalore, India Critical Care 2019, 23(Suppl 2):P433 Introduction: Attrition of doctors in intensive care unit (ICU) is one of the highest amongst all medical specialities globally, and is strongly associated with stress and Burn Out Syndrome (BOS). Factors that contribute to BOS are low pre-morbid resilience and low level of ICU experience. Studies from India have shown high levels of stress in intensive care doctors (>40%), but there are no published studies measuring pre-morbid resilience and risk of burnout in relation to years of experience amongst ICU doctors. Our main aim was to measure cross sectional resilience levels in ICU doctors compared between those with less than 5 years of experience to those with 5 years or more. A secondary aim was to assess the impact of other factors that may contribute to low scores. Methods: An anonymised survey was conducted involving 125 doctors in ICUs across 6 different states in India, using the Connor-Davidson Resilience Scale (CD-RISC 25), which is validated in Indian population. Results: A statistically significant correlation was found between low levels of resilience in ICU doctors with under 5 years of experience 1.456) , and the significance level adopted was 5%. A logistic regression model was used to test the difference between the mortality and readmission rates in <90 and ≥ 90 groups, which enabled the calculation of odds ratios. Chi-square test was used to evaluate categorical variables and T-Student test to some quantitative variables. The ROC curve was constructed to verify the sensitivity of prediction of mortality through different SAPS 3 scores. Results: Among the <90 and ≥ 90 groups, respectively 59% and 37% was male (p = 0.0001). Mean weight of the> 90 years was 76 ± 16 kg and <90 years was 61 ± 11 (p <0.0001). Odds values indicated a significant difference only for the mortality rate, which was more than double among ≥ 90. Readmissions in any time, 24h and 48h as well the mortality is shown in Table 1 and odds in Figure 1 . There was a significant difference in SAPS 3 points between groups ( Table 2 ). The ≥ 90 group presented an average of 10 points higher on the severity scale when compared with those in the <90 group. There was no significant difference in lengh of stay. The highest amount provided by SAPS3 scores was 63% and a specificity of 86% for hospital mortality not group <90 years. In ≥ 90 group the highest sensitivity was 53% and the specificity was 84%. ROC curve for SAPS3 is shown in Figure 2 . Conclusions: The extremely elderly patients of a CICU is more severe, with higher mortality and have the same lengh of stay and readmission rates. Introduction: The purpose was to assess the prevalence and impact of non-urgent interruptions (NUI) within critical care (CC).A root cause analysis of a never event in our CC discussed NUI as a contributory factor, paralleled by learning from serious incidents.The negative impact of NUI is well evidenced, resulting in delayed task completion, increased stress, and affecting patient safety. Methods: Any NUI during a consultant ward round (CWR) or invasive procedure (IP), not relating directly to the current clinical episode, was included. Qualitative data was collected by a survey, assessing the CC multidisciplinary teams(MDT) perception of NUI. Results: One third of reviews during the CWR, and 40%of IPs, had a NUI. Adverse effects included prescription omissions, delayed CWR, near-miss with a CVC, and failed PICC insertion. Overall, 86% of staff considered NUI a problem; 95% had experienced NUI that led to distraction in train of thought. 45% felt that NUI had led to an error: 83% of doctors, versus 20% of nurses. 86% overall felt NUI contributed to stress at work. Reasons for interruptions included: feeling overloaded, needing to resolve concerns before forgetting/being distracted, unable to prioritise, and to shift responsibility.Lack of leadership or clinical supervision providing a point of contact for problems during shifts was mentioned as contributory. Senior staff raised that whilst attempts have been made to level hierarchy, allowing a voice for all to express concerns contributes to interruptions. Potential solutions included awareness on impact of NUI, jobs book,´sterile cockpitd uring IPs, and increased clinical supervision during shifts. Conclusions: We have demonstrated the prevalence and consequences of NUI within CC is significant.The impact on staff is significant, both for contribution to errors and also the negative impact on stress in the workplace. Identified potential solution will be implemented. The impact of an education package on the knowledge, skills and self-rated confidence of medical and nursing staff managing airway & tracheostomy/laryngectomy emergencies in critical care L O´Connor 1 , K Rimmer 2 , C Welsh Methods: The factors affecting the delivery of intensive care was elucidated by a comprehensive review of the intensive care literature. A further understanding of intensive care delivery in South Africa was obtained by "making sense of the mess" with eight workshops and 20 interviews using a systems approach. Systemic intervention served as the meta-methodology and methods and techniques from interactive planning, critical systems heuristics, soft systems methodology and the viable system model were employed. Results: Making sense of the mess emphasised the complexity of intensive care delivery, on both a situational and a cognitive level. It became clear that a single methodology would not suffice, but that a pluralist methodology was required to guide improvement in intensive care delivery. Based on this understanding, nine principles were formulated to guide the development of a framework. Systemic intervention was again used as the meta-methodology. Interactive planning was identified as the key methodology, incorporating methods and techniques used in the making sense of the mess phase to build a systemic framework for the improvement of intensive care delivery. Embedded in the proposed framework are matters relating to systemicity, complexity, flexibility, empowerment, and transformation of intensive care delivery. The proposed framework allows for multiple-perspectives, including that of marginalised stakeholders, the mitigation of multivested interests and power relationships (Fig 1) . It is both flexible and adaptable to promote learning about the complex problems of intensive care delivery and it accommodates the strengths of various relevant approaches to complex problem solving. Conclusions: The proposed framework aims to facilitate sustainable improvement of intensive care delivery and to ensure the "just-use" of resources to foster distributive justice. The perioperative management of adult renal transplantation across the United Kingdom: a survey of practice C Morkane 1 , J Fabes 2 , N Banga 3 , P Berry 4 , C Kirwan 5 Introduction: There is a limited evidence base to guide perioperative management of patients undergoing renal transplantation and no national consensus in the UK. We developed an electronic survey to provide an overview of UK-wide renal transplant perioperative practice and determine the need for future guidelines on patient management. Methods: A 29-question survey was developed to encompass the entire renal transplant perioperative pathway with input from clinicians with expertise from renal transplant surgery, anaesthesia, nephrology and intensive care. The survey was sent to lead renal anaesthetists at each of the 23 transplant centres across the UK. Results: Twenty-two centres (96%) returned complete responses. There was limited evidence of guideline-based approaches to preoperative work-up, with marked variety in modality of preoperative cardiorespiratory function testing performed. Questions regarding intraoperative fluid management (Fig 1) , blood pressure targets and vasopressor administration (Fig 2) identified a broad range of practice. Of note, the routine use of goal-directed fluid therapy based on cardiac-output estimation was reported in six (27%) centres whilst nine centres (41%) continue to target a specific central venous pressure (CVP) intra-operatively. A dedicated renal ward was the most common postoperative destination for renal transplant recipients (62% of centres), whilst a renal or transplant-specific HDU provided postoperative care in 8 (38%) centres. The need for care in an ICU setting was decided on a case-by-case basis. Conclusions: This questionnaire highlighted a high degree of heterogeneity in current UK practice as regards the perioperative management of renal transplant recipients. Development of evidence-based national consensus guidelines to standardise the perioperative care of these patients is recommended. Fig. 1 (abstract P437) . Framework for the improvement of intensive care delivery Introduction: Postoperative care of high risk patients in the ICU used to be considered the gold standard of care in terms of reducing perioperative mortality [1] . New evidence comes to question this practice [2] . The primary objective of our study was to detect any benefit of postoperative ICU care after elective surgery in terms of patient's outcome, length of hospital stay, complications and cost. Methods: A 6-month retrospective analysis of high perioperative risk patients who were about to be subjected into an elective operation were included into the study. Subsequently they were allocated into two groups. Group I patients were those admitted into the ICU for postoperative care while those admitted into the standard ward consisted Group II. Demographic data, length of hospital stay, outcome, need of mechanical ventilation, complications and total cost were recorded. Results: A total of 78 patients were recorded, 39 in each Group. There was no statistical difference regarding the demographic data between the two study groups. Seven patients died before hospital discharge (5 in Group I and 3 in Group II, p>0.005). There was no impact of ICU admission on length of hospital stay (p=0.03) which is primarily affected by the need of mechanical ventilation (p=0.04) and reoperation (p<0.05). The total cost and the postoperative cost of hospital care did not statistically differ among study groups. Conclusions: According to our study the need of postoperative care of high risk patients in the ICU is rather questionable in terms of perioperative mortality, length of hospital stay and cost of care. Introduction: TIVAP is a preferred vascular access device for patients with solid tumors and radiological-guided insertion is a standard of care. However, many hospitals have no access to interventional radiology service. Our study aimed to determine whether it is safe to place TIVAPs in ICU for immediate administration of chemotherapy. Methods: We analysed prospectively maintained database of our department and collected data for adult pts with TIVAPs implanted between 02/2008 and 10/2018. The median age was 55 (range 18-82) years, 59% were women. All procedures were performed by 3 trained physicians with experience in ultrasound (US). Puncture technique was used and tip location was controlled with electrocardiographic (ECG) and US with subsequent chest X-ray confirmation. Pts were followed up for at least 30 days after the procedure for complications, functioning of TIVAP and surgical wound healing. Results: All 277 TIVAPs were successfully implanted in 276 pts. Infraclavicular route was used in 231 cases (83.3%). Difficulties with indwelling guide wire were observed in 11 (4.76%) pts but did not precluded implantation. Placement complications included pneumothorax (n = 3), catheter malposition (n = 3) and artery bleeding (n = 1). These complications required additional therapy but were managed successfully and resolved without consequences. In the rest 46 cases internal jugular vein (JV) was used. Complications were not observed. ECG and US navigation provided optimal tip location control in these situations. Surgical wound healed after 10-14 days and chemotherapy initiation did not affect healing. All TIVAPs had adequate functioning 30 days after placement. Conclusions: It is feasible to implant TIVAPs in ICU. These devices can be used on the implantation day without jeopardizing patient safety. JV catheterization seems to be optimal approach and US navigation and ECG are sufficient methods for placement control. Introduction: There is increasing use of Clinical Information Systems to improve patient safety and quality of care in Critical Care. With all these systems, a rigorous business continuity access (BCA) plan needs to be in place so patient safety is not compromised [1] and ensure continuity of care. Here we evaluate the types of medication errors that occurred during a period of unscheduled downtime; potential contributory factors [2] and the number of errors involving critical medicines [3] were analysed. Methods: During the unscheduled downtime, all prescribing and administration of medicines were transferred to a paper based system using the patients' Web Offline Chart (WOC -Philips Healthcare). Pharmacists at the time double checked the paper charts that were transcribed, to mitigate errors but this was not consistent due to the timing of the event. We retrospectively compared the paper drug charts against the electronic prescriptions and noted all errors for 47 patients. Results: In total 26 medication errors were identified & 1 allergy omission ( Table 1) . Pharmacists double checked 68% of the paper charts. Conclusions: Our data highlights the risks associated with unscheduled electronic patient management system downtime and the heterogeneity of the types of errors & potential contributory factors. It underscores the need for robust local BCA plan implementation, critical review of the WOC document and regular staff training around potential unscheduled system downtime. Introduction: The transfer of patient care (TOC) between the intensive care unit (ICU) and hospital ward is associated with a high risk of medical errors [1] .According to UK National data between 30-70% of patients have an error or unintentional medication change made when moving between care settings [2] . Currently different prescribing systems without interoperability are used between ICU areas & ward settings in our institution, resulting in medications needing to be re-prescribed on transfer. We aimed to evaluate the time delay in medication re-prescribing, number of unintentional omissions of drug doses and reasons, as well as percentage of critical medicines [3] omitted in the first 24h following discharge. Methods: Over a 2 month period, 79 discharged patients (50% of all discharges) from two ICU units were included. The ICU discharge letter which contained the medication list on transfer was compared against the ward based electronic drug chart to identify all unintentional omitted medication doses during the first 24 hours. The starting time point was when the patient physically left ICU. Results: 13/79 (16%) of patients had their medication prescribed more than 4 hours post discharge. There were a total of 269/1,145 (23%) unintentional omitted doses (Table 1) . Of these 104/269 (39%) were considered critical medicines ( Table 2) . Conclusions: This data confirms the risk associated with TOC especially around medicines. The need of interoperable electronic prescribing systems is one solution and could improve patient safety by streamlining the process. Introduction: Staff perceptions of safety may contribute to workforce stress and be organisationally important [1] . This study explored the feasibility of capturing perceptions of safety with a bedside professional reported (BPR) shift safety score, and explored relationships between BPR and measures of staffing and workload. Methods: UK Health Research Authority approval was obtained (ID249248). Data were collected for 29 consecutive days at Imperial College Healthcare Trust (70 general critical care beds on 3 sites).The BPR asked all ICU staff to rate each shift as "safe, unsafe, or very unsafe". Responses were described and correlated with data on organisational staffing (care hours per patient day CHPPD) and nursing intensity (total number of organs in failure/ total number of nurses). Results: A total of 2836 BPR scores were recorded (response rate 57%). We noted heterogenous responses between sites and days, and within shifts, only 14 % of shifts were unanimously rated. Whilst 34% of shifts were rated by staff as "unsafe" or "very unsafe", organisational metrics recorded only 5% as 'unsafe'. We did not find a correlation between measures of staffing (CHPPD) and perceptions of safety ( Figure 1 ). Preliminary analyses suggest that staff perceptions of safety are not well correlated with nursing intensity (Figure 2 ), although these numbers commonly inform staffing metrics. Conclusions: Completing the BPR tool was feasible and acceptable to staff. Responses showed variations in perceptions of safety and a gap between organisational metrics and individual perceptions. Introduction: Delivery of intensive care (ICU) is complex because of multiple stakeholders with varied perspectives and conflicting goals that interact and are interdependent. To inform the development of a framework for the improvement of ICU delivery in South Africa, it was essential to first understand ICU delivery or "make sense of the mess". A systemic approach such as systems thinking is required to holistically explore and understand the complexity of ICU. No methodology is perfect and methodological pluralism as proposed by Systemic Intervention, a systems thinking approach, was used for a more flexible and responsive intervention. The methods used was the making sense of the mess phase of Interactive Planning, stakeholder analysis as describe by Critical Systems Heuristics, rich pictures from Soft Systems Thinking and Viable Systems Model diagnosis. Making sense of the mess was done in 2 phases: first the mess was formulated with rich pictures generated in 8 workshops and 20 interviews. The discussions of the rich pictures by the respective stakeholders were transcribed and analysed using Braun and Clark's thematic analysis. Secondly, based on the data generated from phase 1 a diagnosis of the viability of the ICU system was made. Results: The data from the 2 phases were very rich and complex and 6 themes emerged (Figure 1 ). These themes were interdependent and resulted in disorganised ICU delivery with limited opportunities for learning to improve ICU delivery with dichotomies that existed at various levels of ICU. It was a problem to present the complex data in the traditional linear manner due to the interdependence of the themes. The analysis is presented as 6 stories, a known approach in the complexity discipline, where the themes of the analyses are portrayed. The making-sense-of-the-mess phase confirmed the complexity of ICU delivery, at both a situational and a cognitive level and with this understanding a framework for the improvement of ICU delivery could be developed. Introduction: Improving prescribing practice involves changing prescriber behaviour. Education is assumed to change behaviour but other approaches may be more effective (Figure 1 ) [1] . Changes to the presentation of information and the configuration of choices have potential to rectify common prescribing errors through subtle 'nudges' [2] . The implementation of clinical information systems (CIS), including electronic prescribing, provides an opportunity to deploy strategies such as standard orders, dose limits, and product level prescribing. With an infinite number of configuration options available, clinical leaders need to know which interventions are most effective. We evaluated several of these strategies in a before and after observation study Methods: Interventions, utilising CIS nudges, were chosen to improve four areas of prescribing practice in a tertiary critical care unit using methods matched to the top 4 levels of the hierarchy. Data were collected for 2 months before and after interventions to map changes in compliance with a pre-defined standard except for the standardisation intervention where 4 months' data were collected due to low prescription numbers. No education on changes was given during the baseline data collection so any change in performance after the go-live date is entirely attributable to the intervention. Results: The change in performance for each level ranks the intervention levels in the order (highest first) forced function, automation and standardisation ( Table 1 ). The use of point of prescribing reminders was not associated with a significant difference in performance. Conclusions: The effectiveness of intervention levels seen in practice is consistent with that of the model. Further studies could be undertaken to strengthen these conclusions but in the meantime the approach to changing practice using CIS nudges should focus on standardisation or above. Introduction: Intensive care unit (ICU) sound pressure levels (SPL) are persistently above World Health Organisation recommendations for clinical areas [1] . This may impact patient recovery. Standard SPL monitoring records single values for each 24h period (LAeq24). We hypothesise this reporting rate is unsuitable for ICU. Methods: We measured SPL October 2016 -May 2018, logging frequency (Hz), SPL (dB), and loudness (perception of sound) every second [2] . The resulting dataset was of a size that conventional statistics programs would require computational resources not easily obtainable on standard university commodity hardware. We processed the full dataset without sampling by using distributed task dispatching, parallelism and scheduling of a cluster computing framework (Apache Spark). We created a system consisting of a single workstation (6 cores; 32GB RAM) running Ubuntu 18.04 LTS, Oracle Java 1.8, Apache Spark 2.3, Scala 2.11, R Core 3.5, R Studio 3.5 and sparklyr 0.8.4. We utilised the sparklyr library in R Studio to run arbitrary R code using the dplyr library. We analysed aggregate data in R Core & used ggplot (v3) to create visuals. Results: We achieved more complex analysis than standard SPL reporting with relatively modest computing resources. Specifically we identified lower SPL peaks in the early hours & loudness levels considerably higher than parallel SPL. Conclusions: Simple LAeq24 do not facilitate reflection on practice thus impetus for change is limited. Loudness data highlight the patient experience of SPL in the ICU is more intrusive than LAeq24 indicates due to high sensitivity to sounds~2-4kHz, a common frequency range for alarms. Higher fidelity increases understanding of SPL which can lead to targeted interventions to reduce patient disturbance. Introduction: Survivors of critical illness face significant long term impairments in mental and physical function. Early mobilisation (EM) in the intensive care unit has been suggested to improve functional outcomes and reduce delirium in the ICU. We hypothesized that implementing a protocol for EM in the ICU would improve mobilisation rates while remaining safe. Methods: Design: Prospective non-blinded observational cohort study, based on a Quality Improvement project. Data was collected Conclusions: Only 6 of 10 variables in Boyd criteria were significant associated with morbidity or mortality. The physiologic score and operative score were significant higher in the patient on mortality and morbidity after SICU admission. Effects of structural hospital characteristics on risk-adjusted hospital mortality in patients with severe sepsisanalysis of German national administrative data D Schwarzkopf 1 Introduction: The quick sequential organ failure assessment (qSOFA) score is a simple tool used to identify severe patients with infection. As this score is calculated from three variables that can be measured at the scene of trauma-systolic blood pressure, respiratory rate and consciousness-the prehospital qSOFA score may also be a good predictor of mortality in trauma patients. So we evaluated the discriminative ability of the prehospital qSOFA score in patients with trauma for in-hospital mortality. Methods: This is a retrospective multicenter study using the data from nationwide trauma registry in Japan. We included 42722 patients with trauma aged ≥18 years old transferred to hospitals from scene. Primary outcome is in-hospital mortality. Results: The mean age was 59.4±21.5 years old and 27069 patients (63%) were male. In-hospital mortality occurred in 2612 patients (6%). In-hospital mortality in each qSOFA score was 105/11783(0.9%), 941/17839(5%), 1280/11132(12%) and 286/1968(15%) in qSOFA score 0, 1, 2 and 3, respectively (P<0.0001 for trend). Area under receiver operating characteristics curve (AUROC) of the aqSOFA score for inhospital mortality was 0.70(95% confidence interval 0.69-0.71). If we use the cutoff ≥1, sensitivity and specificity of the qSOFA score were 0.96 and 0.29. Conclusions: In patients with trauma, the prehospital qSOFA score was strongly associated with in-hospital mortality. We can identify patients with very low risk of death by using the cutoff ≥1 of the prehospital qSOFA score. Introduction: Only one prospective study is available of the validation of the diagnostic and prognostic role of qSOFA (quick SOFA score) in the emergency department (ED). A prospective study was conducted in Greek EDs. Methods: The PROMPT study (ClinicalTrials.gov NCT03295825) run in the ED of six hospitals in Greece among patients with suspected infection and presence of at least one of fever, hypothermia, tachycardia, tachypnea and chills. Clinical data were collected and the 28-day outcome was recorded. Sepsis was defined by the Sepsis-3 criteria. Results: The sensitivity and the specificity of at least 2 signs of qSOFA for the diagnosis of sepsis was 78.4% and 96.8% respectively and for the prognosis of 28-day mortality 45.7% and 94.2% respectively. The odds ratio for 28-day mortality when qSOFA was equal to or more than 2 was 59.67 among patients with Charlson's comorbidity index (CCI) equal to or less than 2; this was 7.45 among patients with CCI more than 2 (p: 0.038 between the two ORs by the Breslow-Day's test; p: 0.040 by the Tarone's test). Conclusions: Data validated the sensitivity of qSOFA for the diagnosis of sepsis. CCI was an independent predictor of severity. qSOFA could better predict unfavorable outcome among patients with low CCI. Comparative accuracy between two sepsis severity scores in predicting hospital mortality among sepsis patients admitted to intensive care unit N Sathaporn, B Khwannimit Prince of Songkla University, Internal Medicine, Hat Yai, Thailand Critical Care 2019, 23(Suppl 2):P455 Introduction: Recently, the New York Sepsis Severity Score (NYSSS) was developed to predict hospital mortality in sepsis patients. The aim of this study was to compare the accuracy of NYSSS with the Sepsis Severity Score (SSS) and other standard severity scores for predicting hospital mortality in sepsis patients. Methods: A retrospective analysis was conducted in a medical intensive care unit of a tertiary university hospital. The performance of severity scores was evaluated by discrimination, calibration, and overall performance. The primary outcome was in-hospital mortality. Results: Overall 1,680 sepsis patients were enrolled, 895 patients (53.3%) were classified to septic shock by Sepsis-3 definition. Hospital mortality rate was 44.4%. The NYSSS predicted hospital mortality 34.6+/-21.5%, which underestimated prediction with SMR 1.28 (95%CI 1.19-1.38) . However, the SSS predicted hospital mortality 47 +/-22.2%, which slightly overestimated mortality prediction with SMR 0.94 (95%CI 0.88-1.01). The NYSSS had the moderate discrimination with an AUC of 0.772 (95% CI 0.750-0.794), in contrast to the SSS presented good discrimination with an AUC of 0.889 (95%CI 0.873-0.904). The AUC of SSS was statistically higher than that of NYSSS (p<0.0001). Nevertheless the APACHE IV and SAPS II showed the best discrimination with AUC of 0.937. The AUC of the NYSSS and SSS was significant lower than that of APACHE II, III, IV, SAPS II and SAPS 3 ( Figure 1 ). The calibration of all severity scores was poor with the Hosmer-Lemeshow goodness-of-fit H test < 0.05. The NYSSS was the lowest overall performance with Brier score 0.201. The APACHE IV present the best overall performance with Brier scores 0.107. Conclusions: The SSS indicated better discrimination and overall performance than the NYSSS. However the calibration of both sepsis severity scores and another severity score were poor. Furthermore, specific severity score for sepsis mortality prediction needs to be modified or customized to improve the performance. Introduction: Metabolic markers, especially lactate, have been shown to predict mortality in acutely unwell patients. We hypothesised that early changes in metabolic markers over time would better predict mortality and length of stay, with patients who correct their metabolic derangement having lower risk of death and reduced length of stay (LOS). Methods: Single centre, retrospective cohort study in a 31 bed ICU. We included all patients who had an arterial measurement of lactate, PaCO2, base excess (BE) and pH on admission and at 6 hours after admission to ICU between 01/01/2016 and 31/12/2017. The 'clearance' of these markers was calculated using the equation ((value at admissionvalue at 6 hours)/value at admission). Clearance calculations only included those patients with deranged results on admission (Lactate>2mmol/l, BE<-2mmol/l, pH<7.35, PaCO2>6.0kPa). ROC analysis was used to predict in-hospital mortality and length of stay, using both the initial admission values, and using the clearance value, as well as ICNARC and APACHE II scores for comparison. If a patient was admitted twice in the time period, only the first admission was included. Results: 1506 patients were included (sex ratio 1.5, mean age 61.6). Table 1 ). None of the values tested had a AUC greater than 0.6 for predicting length of stay. Conclusions: The clearances of metabolic markers over the initial 6 hours after ICU admission does not provide better prognostic information than the value at admission. Initial lactate level was the best predictor of mortality, but compared poorly to ICNARC score. Metabolic markers do not accurately predict length of stay. 8.0-22.8) vs 9.1 (IIQ 9.6-9.9), p=0.005]. The other hemogram parameters did not differ between groups (Table 1) . When adjusted for severity score, in patients submitted to emergent surgery, the MPV value was still independently associated with mortality (OR 1.146 CI 1.057-1.243, p=0.042), and its ROC curve (AUC) was 0.861 to mortality (Figure 1 ). Conclusions: MPV is a cheap and easily accessible marker which can add prognostic value in this specific population. In the future, we will validate it in a larger cohort of cancer pts admitted to intensive care. Haematological malignancy in critical care: outcomes and risk factors C Denny 1 Introduction: About 7% of patients admitted to hospital with a haematological malignancy will become critically ill [1] . Life expectancy in these patients is poor with a 6 month mortality of 60% or more in specialist units [2] . In contrast, patients without critical illness can expect a 5 year survival rate exceeding 60% for many cancers. This disparity results in differences of opinion on the best strategy for such patients among haematologists and critical care physicians. We conducted a local quality improvement project to quantify mortality and risk factors in critically ill patients with a haematological malignancy in our hospital. Methods: Patients admitted to the critical care unit of Broomfield Hospital, a district general hospital with tertiary specialist services, from January 2011 to December 2017 with haematological malignancy were included in the analysis. Patients in remission for more than 20 years and patients admitted following elective surgery were excluded from analysis. Death in critical care or in hospital after critical care discharge were the primary outcomes. Mortality was correlated with demographic data using simple statistical measures and regression analysis. Results: 93 patients were included in the analysis. Overall mortality was 45%(n=42). Survivors tended to be younger (65 vs 71 years) but had similar clinical frailty scores. Early critical care admission (within 24 hours) was associated with better survival (72.5 vs 35.7%). Nonsurvivors had a greater incidence of sepsis and respiratory failure, and required more ventilatory and vasopressor support. Mortality was higher in patients requiring more than one organ support. Conclusions: The overall mortality in our data is lesser than previously published data but supports the conclusion that mortality is determined primarily by the number of organs supported with the effects of malignancy playing a secondary role. (Figure 1 ). Increasing levels of frailty were associated with increasing risks of death at 1 year (p<0.001) (Figure 2 ). Frailty significantly increased 1-year mortality hazards in unadjusted analyses (HR 1.96; 95%CI; 1.41-2.72; p<0.001) and covariate-adjusted analyses (HR 1.41; 95%CI 1.00-1.98; p=0.0497) ( Table 2) . Conclusions: Frailty was common and associated with greater age, more severe illness and female gender. Frailty was significantly associated with heightened mortality risks in both unadjusted and covariateadjusted analyses. Frailty scoring may encapsulate variables affecting mortality which are omitted in current predictive systems, making it a promising risk stratification and decision-making tool in ICU. Fig. 1 (abstract P461) . Unadjusted survival curves stratified by frailty status. Frail patients were statistically significantly less likely to survive to 1 year plateau at day 4=18, delta Peak=2 and HPR=0.34. Were assigned respectively a point value of 1, 1, and 2 to these predictors based on their beta coefficient in the predictive model. The score yielded a ROC-AUC: (AUC=0.79; 95%CI, [0.72-0.86]; p=0.000). Using the validation data set (n=104), the score had an ROC-AUC=0.8 and similar estimated probabilities for mortality. Conclusions: The Paw-MPS seems to demonstrate interesting discriminative properties to predict mortality. What is the role of the pulmonary embolism severity index (PESI) and RV/LV ratio as clinical risk assessment tools for patients undergoing ultrasound-assisted catheter-directed thrombolysis (UACDT)? Introduction: To evaluate if the pulmonary embolism severity index (PESI) score correlates with RV/LV ratio, biomarkers of cardiac injury, fibrinogen and Length of stay(LOS). Also to evaluate the correlation between RV/LV ratio with biomarkers of cardiac injury, fibrinogen and LOS for patients who underwent UACDT. Methods: A retrospective review of patients with sub-massive pulmonary embolism (PE) who underwent ultrasound-assisted catheterdirected thrombolysis (UACDT) was performed. PESI score, RV/LV ratio, length of stay(LOS), fibrinogen levels, troponin levesl, and brain natriuretic peptide(BNP) levels, were calculated and collected prior to UACDT. Spearman's rank correlation coefficient was calculated for all non-parametric variables. Results: 31 patients, 17 males and 14 females, were included in the study. The mean (±SD) age was 54±17 years. The mean PESI score was 93±38. Mean RV/LV ratio was 1.34±0.35. A significant correlation between the RV/LV ratio and both fibrinogen and troponin level (p=0.0013, p=0.0167) was noted. No significant correlation existed between PESI score and RV/LV (p=0.85). No significant correlation existed between both RV/LV ratio and PESI score with length of stay (p=0.1649) after UACDT. There were no noted mortality or complications. Conclusions: PESI score is used as a prognostic factor for the patients with PE, however, our study shows that PESI score does not correlate with RV/LV ratio or length of stay after the UACDT. There was inverse correlation between RV/LV ratio and fibrinogen. There was also positive correlation between RV/LV ratio and troponin for patients with and without heart failure. According to our data, there may be limited use of PESI score and RV/LV ratio for risk stratification of PE patients undergoing UACDT. Introduction: Conventional scores for prediction of risk and outcome, such as SAPSII and SOFA, have not been validated for patients admitted to level II critical care units (intermediate level or ImCUs). We compared the performance of SAPSII and SOFA scores with the Intermediate Care Unit Severity Score (IMCUSS) in a general population admitted to ImCU. Methods: We conducted a prospective observational cohort study in a 31-bed level II-III ICU from a university-affiliated hospital, during a three-month period. We applied SAPSII, SOFA day one and IMCUSS to all patients admitted during that period. Primary outcome was a composite of hospital mortality and need to increase level of care. Additionally, we tested the relevance of each variable within each score to predict the outcome. Results: We included 108 patients with a mean age of 61.7±18.6 years. 63 patients were considered "step-down" (transferred from our level III beds), and the remaining originated from the emergency Conclusions: 12 months after completion, the primary care management intervention had no effect on mental health-related quality of life and physical function among survivors of sepsis. Increase in PTSD symptoms in the control group may suggest a possible protective effect of the intervention. Introduction: Critically ill patients and their families are often confronted with an overwhelming amount of clinical information shortly after hospital admission. Their reliance on Internet resources for additional information is increasing, particularly for unfamiliar medical terminology. Yet, little is known about whether these online resources meet the recommended reading level and complexity appropriate for the average reader. Methods: An online search of 40 websites containing four common critical care diagnoses in the ICU (respiratory failure, renal failure, sepsis and delirium) was performed. A total of 6 readability formulas were used. The Flesch-Kincaid grade reading level (GRL) and Flesch reading ease (FRE) were used in the final analysis. Document complexity was evaluated using the PMOSE/iKIRSCH formula. Results: Websites on respiratory failure were written at the 13th GRL with FRE of 29.6. Renal failure resources had a 9th GRL with FRE of 54.7. Sepsis websites had an 8th GRL with FRE of 48.3. Delirium websites had a 12th GRL with FRE of 31.6. When comparing website types (government, non-profit and private), ANOVA showed a difference in FRE across all 3 groups and government websites had a Conclusions: Online resources used by intensive care unit patients and families tend to be written at higher than the recommended 6th GRL, with government sites better meeting this target than nonprofit and private organizations. Online resources should be improved to lower this unfortunate barrier to patient education. Introduction: The recent enactment of the Data Protection Act 2018, the General Data Protection Regulations, and a series of data breaches in the healthcare sector, have renewed interest in how our patients' information is collected, used and shared. The complex framework of laws and regulations governing the use and disclosure of personal data may lead to professional and financial consequences if information is disclosed inappropriately. Disclosures to the police when they concern incapacitous patients are particularly challenging, as the disclosure may have no direct benefit to the patient and may cause the patient considerable harm. Methods: We have reviewed the relevant laws and regulations to identify the circumstances in which doctors must release information regarding incapacitous patients to the police. The laws and regulations are examined to identify the extent of the disclosure required, and any requirements for the disclosure to be lawful. We have also identified laws which confer a power to disclose information about incapacitous patients, and the circumstances in which these powers can be used. Results: In conjunction with a local police constabulary we have developed an information request form which makes it easier for those requesting and disclosing information to understand the legal basis of the disclosure. We have also developed guidelines to allow practitioners to understand where a disclosure is obligatory or discretionary. Conclusions: The next stage of the project is to audit disclosures of information in the intensive care unit, and identify whether information is being released lawfully and following the correct procedure. Introduction: Family members are affected both physically and psychologically when their relative is admitted to ICU. There is limited knowledge describing their experiences and structured interventions that might support them during their relative's critical illness. The aim of this review is to describe published literature on the needs and experiences of relatives of adult critically ill patients and interventions to improve family satisfaction and psychological well-being. Methods: Design: Scoping review. Standardised processes of study identification, data extraction on study design, sample size, sample characteristics and outcomes measured (Figure 1) . Results: From 469 references, 43 studies were identified for inclusion Four key themes were identified: 1) Different perspectives on meeting family needs 2) Family satisfaction with ICU care 3) Factors impacting on family health and well-being and capacity to cope 4) Psychosocial interventions Conclusions: Family members of patients in ICU experience unmet information and assurance needs which impacts on their physical and mental health. Structured written as well as oral information show some effect in improving satisfaction and reducing psychological burden. ICU's who are able to support interventions based on meeting family information needs, in addition to reducing psychological burden and increasing satisfaction will enable each family to provide more support to their relative within the ICU. Introduction: Unmet informational needs lead to dissatisfaction with care and psychological distress. Identifying interventions to help meet specific needs is a crucial and necessary step in providing family centred care in ICU. We aimed to implement and evaluate the impact of delivering a structured communication strategy on levels of anxiety, uncertainty and satisfaction with care and decision making in families of critically ill adults. Methods: A quasi experimental study with pre and post test design. A convenience sample of 52 family members were recruited from July 2016 to February 2018. The intervention group (n=26) received both oral and printed information to guide them in preparing for a structured family meeting. The control group (n=26) received usual Fig. 1 (abstract P476) . Article selection process for scoping review routine care and existing family informational support. Anxiety, uncertainty and family satisfaction were measured in the two groups on ICU admission and ICU discharge. Results: Mean anxiety, uncertainty and satisfaction with care and decision making scores pre and post intervention were compared. There were no significant differences in mean anxiety, uncertainty or satisfaction scores between the two groups before the intervention (p>0.05). Mean scores on anxiety (45.5vs 46.9), and uncertainty (72.4 vs 76.7) were lower post intervention, but not significantly so ( Figure  1&2 ). Total satisfaction, satisfaction with care and satisfaction with decision making mean scores were similar in both groups before and after the intervention (p.0.05). Conclusions: Providing relatives with a combination of targeted written and oral information delivered by nursing and medical staff reduced anxiety and uncertainty with this reduction being evident through to discharge from ICU. Although not statistically significant, there was what may be seen as a suggestion of a clinically significant drop in anxiety and uncertainty following the intervention Introduction: Clinical studies in intensive care unit (ICU) patients are warranted in order to improve healthcare. The aim of this study was to analyse barriers and challenges in the process of achieving informed consent from ICU patients. Methods: We analysed patients considered for inclusion in a prospective observational study of venous thromboembolism in the ICU, i.e. the Norwegian Intensive Care Unit Dalteparin Effect (NORIDES) study. Data were collected from the screening log, consent forms and associated research notes of the NORIDES study. Results: We observed that 58 of 279 (20%) eligible patients according to inclusion and exclusion criteria were omitted from the NOR-IDES study due to barriers and challenges in the process of receiving informed consent. 21 were categorized as psychiatric diseases consisting of known psychosis or recent suicide attempt, 19 likely or actual treatment withdrawals and 18 due to language barriers among non-Norwegians. Among the 70 patients included in the NORIDES study, 29 (41%) consents were from patients and 41 (59%) obtained from their next of kind. From the patient consents, 11 (38%) consents were oral and 18 (62%) were written. 6 patients were physically unable to sign, and 7 patients did not recognize their own signature. The study further pointed at some specific challenges in the process of consent, herein questionable competence to give consent, failure to remember being asked/included, inability to separate research from treatment etc. There were also difficulties in evaluating who was next of kin and how to reach them. Conclusions: Barriers and challenges in obtaining informed consent from ICU patients led to exclusion of one fifth of the eligible patients in our study. Informed consent directly from patients was obtained from less than half of the included patients. Obstacles in the process of achieving informed consent were practical, medical, ethical and/or legal. Determinants of end-of-life decision-making in the intensive care unit P Eiben, C Brathwaite-Shirley, S Canestrini King´s College Hospital NHS Foundation Trust, London, United Kingdom Critical Care 2019, 23(Suppl 2):P479 Introduction: Although the majority of intensive care unit (ICU) deaths follow the decision to forgo Life Sustaining Treatment (LST), variability in patterns is commonly observed [1, 2] . We reviewed End of Life (EoL) practice at our institution in order to explore: (i) patient characteristics affecting EoL decision-making, (ii) communication among surrogate decision-makers, and (iii) EoL management. Methods: We retrospectively analyzed data from consecutive patients who died in our ten-bed ICU over 16 months (study period). Patient demographics, APACHE II, functional status, diagnosis on admission, ICU length of stay (LOS) were collected; family/next-of-kin (NOK) involvement and rationale for LST limitation were recorded ( Conclusions: Our analysis shows that in our institution EoL deliberations follow a shared decision-making process. Lack of family/NOK involvement and incomplete documentation was exceptional. The significant difference in LOS between W-Group and NW-Group, in the face of similar APACHE II, warrants further investigation.  
