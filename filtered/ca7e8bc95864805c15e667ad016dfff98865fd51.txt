ca7e8bc95864805c15e667ad016dfff98865fd51
Artificial Intelligence in Drug Discovery: A Comprehensive Review of Data-driven and Machine Learning Approaches
Hyunho  Kim Eunyoung  Kim Ingoo  Lee Bongsung  Bae Minsu  Park Hojung  Nam 


As expenditure on drug development increases exponentially, the overall drug discovery process requires a sustainable revolution. Since artificial intelligence (AI) is leading the fourth industrial revolution, AI can be considered as a viable solution for unstable drug research and development. Generally, AI is applied to fields with sufficient data such as computer vision and natural language processing, but there are many efforts to revolutionize the existing drug discovery process by applying AI. This review provides a comprehensive, organized summary of the recent research trends in AI-guided drug discovery process including target identification, hit identification, ADMET prediction, lead optimization, and drug repositioning. The main data sources in each field are also summarized in this review. In addition, an in-depth analysis of the remaining challenges and limitations will be provided, and proposals for promising future directions in each of the aforementioned areas. Small molecule drug research and development (R&D) spending, in the pharmaceutical industry, has grown exponentially over the past decades, with total R&D costs per approved drug recently being about $2.6 billion [1] . Moreover, the entire process for one approved drug takes approximately 13.5 years, namely 5.5 years before clinical trials (drug discovery) and eight years for the remaining process (drug development) [2] . Therefore, reducing the overall cost and time is a major challenge in both industry and academia, whereby the modern drug R&D process may not be sustainable. The reason why the modern pharmaceutical industry spends an astronomical amount of money is the repeated attrition of drug candidates. According to recent statistics [3] , 80% of the causes for attrition were attributed to poor pharmacokinetics (39%), lack of efficacy (30%), and animal toxicity (11%) . Surprisingly, the problems mentioned above are closely related to the drug discovery process, before clinical trials, demonstrating that there is room for improvement. In general, the overall process is determined by knowledge-based decisions, which can be highly biased, as it is virtually impossible to synthesize and evaluate all the possible compounds by experiments. In this circumstance, Artificial intelligence (AI)-guided decision making is a promising breakthrough [4, 5] . Fortunately, there are many pioneer groups who have been developing fast and accurate AI-guided decision-makers for rational drug discovery by adapting or inventing novel data-driven machine learning techniques. In this review, we focused on the recent data-driven based research trends of the fields that are effectively costreducible with AI, e.g., AI-applicable fields in the drug discovery stages: i) Target identification, ii) Hit identification, iii) Lead optimization, iv) Postmarket (Fig. 1) . Taking advantage of the latest AI technologies and the potential of big data has a huge advantage in the areas mentioned drug discovery stages. First, we can explore integrated multiomics and linkage data to find data-driven patterns that are difficult for humans to extract, to identify less biased and novel drug-targets. Second, by using fast and accurate predictive models to virtually screen many compounds, we can significantly reduce the cost and time of experimental validation. Third, the novel optimized candidate structures could be generated and assessed by AI models, which in turn will lead researchers to the ideal path for further lead optimization. Lastly, AI-suggested promising off-targets of the marketed drugs will bring significant savings because the marketed drugs would have already passed rigorous tests. Therefore, here we discuss the current limitations of AI applications in each field and suggest future directions by analyzing current trends. Target-based drug discovery is a highly conventional and successful method in drug discovery. From 1999 to 2013, 70% of the Food and Drug Association (FDA) approved drugs were discovered by the target-based approach [6] . However, in clinical trials, many drug candidates have poor efficacy or increased toxicity because of the selection of targets that are poorly linked to the disease or have an unjustified hypothesis for the disease [7] . Hence, a welldefined model for the disease and biological elements association is essential in identifying adequate targets. Various types of omics data such as genomics, proteomics, and metabolomics can be used to interpret those associations. As the volume of omics data grows, computational methods are needed to analyze and to integrate the evidence of associations among the vast heterogeneous omics data. Conventional methods for computational target identification can be grouped into three categories: statistical analysis, network-based model, and machine learning. Thus, after an introduction to the computational methods for target identification, we focus on the curated omics databases supporting the target identification. For decades, statistical analyzes of omics data have been the most traditional and conventional ways for target identification. These methods are based on the Genomewide association study (GWAS) . It focuses on identifying genetic variants between healthy and disease samples. Candidate target genes are identified by association tests such as the Chi-squared test, Fisher's exact test, or t-test for the gene expression of the disease. Hsu et al. [8, 9] identified three kinases (PKC-α, CDK6, and MET) targets for triplenegative breast cancer (TNBC), by using TNBC and non-TNBC data from the Cancer Cell Line Encyclopedia (CCLE) project, Gene Expression Omnibus (GEO) breast tumor sample data, and miRNA expression data of NCI-60 cancer cell lines. They conducted a two-stage bioinformatics analysis; cell-based gene expression analysis and patient based Kaplan Meier survival test. They identified three kinases that show both high expression in TNBC and high association with patient survival. Kodama et al. [9, 10] identified CD44 as a therapeutic target of type 2 diabetes by expression-based GWAS. They ranked the genes by the Fig. 1 . The overall process of drug discovery and development. The AI techniques are applied mostly in the drug discovery stage to reduce the attrition rate in the drug development stage. The AI-applied drug discovery-related fields that are covered in this paper are shown in the corresponding process. probability of differential expression in 130 microarray experiments. For the top candidate CD44, they validated by diabetic mouse experiments. GWAS can identify the associated genetic variants for the disease. However, it is difficult to determine the effect on the gene by the selected genetic variants. To address this issue, Zhu et al. [11] proposed a method named SMR (Summary data based Mendelian Randomization) to identify genes associated with a human complex trait. They defined a pleiotropic association considering pleiotropy or causality between gene and a trait to make improved MR analysis. By using SMR, they integrated the GWAS trait summary and eQTL (expression quantitative trait locus) data and analyzed the association between a complex trait and gene expression. Network-based methods have been widely used to represent the complex connections among the various biological elements. Networks comprise nodes that represent biological elements, and edges that represent the interaction among the nodes. Furthermore, this approach can manage the multiple types of omics data by the heterogeneous network. Hence, many studies use a network-based approach for target identification. Conventionally, networks are constructed based on the similarity between the targets or disease. For gene-disease association, gene co-expression networks, represented by a gene-gene similarity matrix has been used. This network captures genes with similar biological process activity [12] and helps to find the gene sets associated with diseasepathways. Petyuk et al. [13] used network analysis to identify a late-onset Alzheimer's target. They constructed a co-expression network with peptides and transcripts data to identify the gene-protein expression relationship profiles. Moreover, they constructed causal predictive networks to give ordering or direction to the network edges. Lee et al. [14] performed network analyzes to identify targets for liver disease. They constructed gene co-expression networks for 46 human tissues to represent the functional interaction of genes. Moreover, they constructed liver regulatory networks and liver protein-protein interaction networks to investigate the physical interactions of genes. Network-based approaches for miRNA-disease associations have also increased. It is based on the theory that miRNA can regulate gene expression and has some role in some diseases [15] . Chen et al. [16] proposed a miRNAdisease association prediction model named BNPMDA based on the assumption that similar miRNA correlates to a similar disease. They integrated three disease similarity and miRNA similarity models with biased ratings based on the known miRNA-disease associations. A bipartite recommendation algorithm was used to predict the associations based on biased ratings. Ding et al. [17] proposed an algorithm for miRNA-disease and gene-disease association predictions. They built the heterogenous disease-gene-miRNA association network and predicted the disease-gene or miRNA association by their novel algorithm named DMHM. The main rationale of DMHM is to make smooth functions on data manifolds by graph-based regularization. Recently, the knowledge graph has also been used for target identification. Knowledge graphs represent entities, relations, and semantic information as a graph that can be easily interpreted for a machine. Mohamed et al. [18] proposed a knowledge graph embedding model named TriModel. They constructed a drug-target interaction knowledge graph from KEGG, DrugBank, InterPRo, and UniPRot. The entity and relationship of a knowledge graph are embedded into three embedding vectors, based on tensor factorization, and updated in iterative learning by minimizing false fact and maximizing true fact. Richardson et al. [19] suggested a potential COVID-19 target and treatment. They used BenevoletAI's knowledge graph, which is a repository of structured biomedical information from machine curated relationships between over 20 types of biomedical entities like diseases, genes, and drugs [20] . They supposed that the 2019-nCoV receptor would be ACE2 and found AAK1, which is one of the ACE2 endocytosis regulators in the knowledge graph. In the AAK1 inhibitors in the knowledge graph, baricitinib showed high-affinity and binding affinity to another regulator of endocytosis. As the mechanism of disease is complex, extracting generalized patterns of disease targets using a data-driven approach is a challenging task. Along with this difficulty, several pioneer studies have shown the potential power of using machine learning techniques in drug-target identification and can learn patterns of disease targets without prior biological dependency information. Ferrero et al. [21] constructed classifiers that predict whether the gene is a drug-target or non-target. They constructed four classifiers: Random Forest (RF), Support vector machine (SVM), Neural Net, and Gradient Boosting Machine (GBM) with gene-disease association data from the Open Targets platform. They used five data types (pathway, animal model, genetic association, RNA expression, and mutation) as input features and assessed the feature importance for target identification. They found four classifiers showed similar performances of ≈ 70% accuracy with 0.75 AUC. Mamoshina et al. [22] constructed age prediction with five regression models by using gene expression data from GEO and ArrayExpress. They performed feature importance analysis to identify the most associated genes for age prediction, with the top 20 genes included five known drug-targets. Large volumes of omics data and computational methods lead to an increase in the performance of target identification. However, problems with managing the heterogeneous omics data still exist. First, the experimental conditions for generating data and the formats or annotations for recording the data are often not identical for each omics data [23] . Second, databases or publications for human diseases are biased to specific topics [24] . To tackle these problems, many efforts have been made to provide integrated or curated databases for target identification. Table 1 describes the databases reviewed. DisGeNET collects disease-associated genes and variants from various repositories, GWAS catalogs, animal models, and publications; to overcome the heterogeneity, availability, and fragmentation of genetic information [25] . It contains 628,685 gene-disease associations (GDA) and 210,498 variant-disease associations (VDA). All data in DisGeNET are homogeneously annotated by using community-driven vocabularies and ontologies. DisGeNET uses association scores to define GDAs and VDAs according to the number of supporting data or publications for the association. In addition, DisGeNET provides Cytoscape APP and disgenet2r R packages to support the visualization or analysis of the association data. All data in DisGeNET are available as TSV, SQLite, and RDF dump files. A comparative toxicogenomics database (CTD) provides a comprehensive database for understanding environmental effects on human health [26] . It curates associations among genes, chemicals, diseases, phenotypes, and environmental exposures in 10 public resources including KEGG, GO, PubMed, etc. In the latest update, in 2019, the content volume and identifiers were updated; there are 27,054,182 curated and inferred GDAs. Approximately 40,000 GDAs are curated, and the others are inferred from the chemicalgene or chemical-disease associations in the CTD data. Inference scores in CTD are defined according to the connectivity of chemical-gene-disease association networks of CTD data. All CTD associations, interactions, and vocabularies data are available as CSV, TSV, and XML files. LinkedOmics aims to provide a comprehensive and analytical portal for a large amount of cancer molecular properties and clinical data [27] . It collects multi-omics data of 32 TCGA cancer types and clinical data of 11,158 patients in the TCGA project. Multi-omics data include genomic, epigenomic, transcriptomic data of TCGA cancer, and clinical data including information like survival time, age, and tumor status. It also collects mass-spectrometrybased proteomics data of the selected TCGA tumor samples of the Clinical Proteomic Tumor Analysis Consortium (CPTAC). LinkedOmics provides three data analysis web modules to support the analysis of the collected data. First, the LinkFinder module supports identifying the association for the query attributes to others in the database. It uses statistical tests such as Pearson's correlation test and Spearman's rank correlation to rank the associations among the attributes. Second, the LinkCompare module compares the association results from LinkFinder with different [25] A discovery platform of human-disease associated genes and variants with homogeneous annotation 628 K gene-disease associations, 17 K genes, 24 K diseases, 210 K variant-disease associations Comparative toxicogenomics database (CTD) [26] Comprehensive database for environmental effects on human health. It curates associations among chemical, gene, disease, phenotype, and exposure. LinkedOmics [27] Comprehensive database for molecular properties and clinical data of cancer. It collects multi-omics, clinical and Mass-spectrometry proteomics data of TCGA cancer. Open-Target platform [28] A comprehensive database for target-disease association. It collects genetic and chemical data to aid target identification. 6.3 M association data with 27 K targets and 13 K diseases DepMap portal [31] A web portal providing cancer analytical and visualization tools. It contains genetic information and sensitivity of cancer cell lines. Genetic characters of over 1 K cell lines HMDD [30] A database that collects miRNA-disease associations based on experimental evidence from PubMed papers 35 K miRNA-disease associations from 19 K papers STRING [38] A database of physical and functional protein-protein interactions Total 3.1 B protein interactions Therapeutic Target Database (TTD) [39] A database of known therapeutic proteins, nucleic acids and targeted disease with related drugs. queries. Finally, the LinkInterpreter module provides a biological interpretation of the association from LinkFinder. It uses pathway or network analysis to interpret the associations. All data is available as a matrix file or external link to the data source. The Open-Target platform curates 20 public databases to support target identification, validation, and prioritization [28] . The types of data sources are genetic associations, somatic mutations, drugs, pathways, RNA expression, text mining, and animal models. It has 6,336,307 data associations with 27,069 targets and 13,579 diseases. The Open-Target platform prioritizes targets by the association scores to the disease. The association scores are based on the number or strength of evidence such as p-value and sample size. Moreover, it provides a target tractability analysis to assist target prioritization. This process is based on a study for the target suitability assessment method [29] ; it provides druggability information like whether the target has ligands or binding sites for small molecules. The association and evidence data can be available as a JSON file and targetdisease lists are available as JSON and CSV files. The Human microRNA Disease Database (HMDD) curates the miRNA-disease associations based on the experimental evidence of the literatures [30] . The datasets are collected from miRNA related PubMed papers by using 'microRNA', 'miRNA', 'miR' keywords, then miRNA-disease associations are extracted from the abstract of the selected papers. There exist 35,547 miRNA-disease associations from 19,280 papers. In HMDD, miRNA-disease associations are represented as the supporting evidence of genetics, epigenetics, tissue expression, miRNA-target, and circulation assay. It also provides visualized miRNA-gene regulation networks. In the latest update HMDD v3.2, causality annotation which represents the positive/negative miRNA-disease associations were added. All data are available as text or excel files. The Dependency Map (DepMap) Portal supports researchers identifying genetic and molecular dependencies of cancers by providing datasets and tools that are used in the Cancer Dependency Map project at the Broad Institute [31]. The datasets included in the DepMap portal are divided into three parts: Genetic dependency, Cellular models, and Drug Sensitivity. First, Genetic dependency identifies genetic vulnerabilities of human cancers by project Achilles. Project Achilles analyzes the genetic dependency of cancer by the genome-wide RNAi function screens [32, 33] . Second, cellular models show the genetic and pharmacological diversity of human cancers by the CCLE project. CCLE databases contain expression, gene copy number, mutation, and RNAseq based fusion for cancer cell lines [34] [35] [36] . Finally, Drug Sensitivity contains the small molecule viability of diverse cancer cell lines by the Profiling Relative Inhibition Simultaneously in Mixtures (PRISM) approach which shows high-throughput for cancer cell line screening [37] . All data in the DepMap portal are available as a CSV file. The STRING (Search Tool for Retrieval of Interacting Genes/Proteins) database provides physical and functional interactions between proteins [38] . It contains 3,123,056,667 protein interactions of 24,528,628 proteins from 5,090 organisms. There are five categories for association evidence: genomic context prediction, experiments data, text mining, co-expression, and prior knowledge in databases. In the STRING database, 'normal' and 'transferred' scores exist. The normal score represents the evidence from the organism itself and transferred represents the evidence transferred from other homolog organisms. The score ranks 0-1, and as the score increases, confidence increases. All interaction data in the STRING database is available in a web service. The TTD (Therapeutic Target Database) provides information about known therapeutic protein and nucleic acid, disease, pathway, and the corresponding drugs of targets in literature [39] , with 3,419 targets and 37,316 drugs. The database in TTD is categorized into five groups; Advanced search, Target group, Drug group, Patient data, and Model & Study; they can be browsed in the TTD web service. In the latest update in 2020, the new information about target regulators, target interacting protein, patented agents, and targets were expanded. All data in TTD are available and can be downloaded via the web service. As mentioned in section 2.4, there are problems when using databases from different sources. These issues may be avoided by using curated databases. However, there are limitations for the curated databases. The major limitation in data curation is the absence of validation or benchmarks for the target-disease association scores. As seen above, for Open Targets and DisGeNET, the target-disease association scores are based on the number of publications or databases with supporting evidence. However, the number of supporting evidence in data sources does not directly correlate with the efficacy of target modification. Hence, the scores need to be validated by experiments, or benchmark studies. Another limitation of data curation is the lack of target druggability information. Most databases present only the supporting evidence or number of supporting evidence from the data sources. For drug discovery, besides the target's efficacy or effect on the disease, the possibility for target modification by a drug is also needed. The Open-Target platform provides target tractability, which shows whether the target has a ligand or binding site for small molecules. Pearson et al. [40] proposed a target druggability software named TractaViewer which provides molecular ligand abilities or potential risks. Finally, the utilization of curated databases needs to be increased. Most of the curated databases mentioned in section 2.4 have been activated later than the traditional databases, and reference for using these curated databases are not widely distributed. In addition, curated databases are lack of programmatic accessibility. Like the disgenet2r package of DisGeNET, a programmatic accessible package may activate the usage of curated databases. Identifying drug-target interactions is one of the crucial steps in preclinical drug discovery. Desired effects of the drugs depend on the interaction between the drug and selected target, while the possibility of side effects and drug repositioning can also come from interactions between proteins that are not targeted during drug development [41] . However, it is difficult to search the entire chemical space of compounds for druggable target proteins by experiments, as approved drugs are very sparse (Fig. 2) . Fortunately, data of compounds, drugs, proteins, and their bioactivities accumulate fast, which enables data-driven computation models to identify hits from vast chemical space. Therefore, many computational models to identify drug-target interaction and estimate binding affinities have been developed to leverage the efficiency of the early stages in drug development, which also has the advantage of delivering novel drug candidates. There are three main categories of computational methods for hit identification, as shown in Fig. 3 ; structure-based methods, ligand-based methods, and chemogenomic methods concentrating on protein structure, ligand structure, and their data, respectively. Structure-based methods take advantage of 3D structures of target proteins, which are generated from X-ray crystallography (XRC) or nuclear magnetic resonance spectroscopy of proteins (protein NMR). A molecular docking simulation is a major approach in structure-based methods and is conducted in two steps. The first step is the conformational space search of ligands, which extensively simulates possible binding pose of binding. After a conformational search, the second step, scoring function prioritizes possible ligand pose on the target protein structure and estimates binding affinity. Quality of scoring function contributes to the assessment of docking simulations. Conventionally, empirical, or knowledge-based scoring functions are devised to predict binding affinity pose. They are driven by physical theory or statistical analysis [42] [43] [44] To correct the bias of classical scoring functions, datadriven machine learning scoring functions (MLSF) are developed by using a random forest score (RF-Score) [45] and support vector machines [46] . Recently, many deep learning-based scoring functions (DLSF) were developed to estimate binding affinity. With given pose, they have applied various deep learning techniques such as a 3D convolutional neural network (3D-CNN) and graph convolutional network (GCN). 3D-Voxel based methods generate 3D-voxels from the atomic coordination of 3D complexes. Each voxel contains features (channels) describing internal properties such as hydrophobicity, aromaticity, hydrogen bond, and ionization, etc. The Convolutional neural network (CNN) is now a major method to detect patterns in deep learning. For 3Dvoxel based methods, 3D-CNN was used to detect patterns of binding pose and affinity. 3D-voxel based methods are improved by using continuous features [47] , more sophisticated CNN designs [48, 49] , and transfer learning [49] . 3D-CNN of KDEEP [48] was inspired by AlexNet, while DenseFS [49] was inspired by DenseNet, whose sublayers are densely connected, and outperform previous methods [43, 45] . Particularly, DenseFS used transfer learning for protein classes to increase performance. In addition, a recent study suggested an analysis of feature weights, helping to elaborate the design of the compound [50] . Molecular graph-based methods represent the proteinligand complex on a graph. Atoms are represented as nodes, while their interactions are represented as edges between the nodes. To virtually screen given protein-ligand complexes, the model aggregates nodes use their edges, which implies locality on the graph. DeepVS [51] learns the context of an atom from the closest atoms of its ligand and interacting protein. Later models apply GCN for model aggregation, which takes into account the locality of nodes on a graph and the adjacency matrix [52, 53] . PotentialNet used GCN for both intramolecular interaction and non-covalent interaction between molecules, with better performance than the RF-Score [45] . Lim et al. [53] later built a distanceaware graph attention model by using the adjacency matrix for non-covalent bonds. By removing the intramolecular graph attention model from the distance-aware graph attention model; this model classifies activity by learning the binding pose away from the inherent ligand structure, outperforming both docking [44] and 3D-voxel based methods [47] . Ligand-based methods are grounded on the assumption that compounds of similar structures would interact with the same target. Quantitative structure-activity relationship (QSAR) models, one of the major approaches in ligandbased methods, estimate quantitative relationships (weight) between structure and its bioactivity. There are many structural and physicochemical properties of a compound which are related to bioactivity; for example, the partition coefficient is strongly related to hydrophobic effect, yielding binding to a receptor. Likewise, from a simple count of atoms to Lipinski rule of five, many quantitative descriptions of a compound can be used for prediction. Hence, there are many programs to generate quantitative molecular descriptors of compounds. RDKit [54] , OpenBabel [55] , and chemical development kit (CDK) [56] are opensource programs for bioinformatics and cheminformatics to generate molecular descriptor. Not only is the command line supported, but the GUI, web-applications, and programming language wrapper for convenient usage of descriptor generation are also supported in PaDEL descriptor [57] , DRAGON [58] , PyDPI [59] , Rcpi [60] , and Mordred [61] . From the generated quantitative descriptors, QSAR builds a model to predict the bioactivity of molecules. Therefore, well-established QSAR models enable statistical analysis on each property descriptor, inspiring insight on the mechanism of ligands. Conventionally, many machine learning models are used for QSAR prediction [62, 63] . In 2012, Merck Molecular Activity Challenge (MMAC) published benchmark datasets (Kaggle datasets) for QSAR prediction, which comprise on-target bioactivities and ADME properties. As an advanced deep learning technique, deep learning-based QSAR predictions have outperformed previous RF QSAR predictions [64] . Further expansion of deep learning models such as multitask models, correlated assistant datasets [65] , and deep belief networks (DBN) [66] have been developed to increase performance. Still, limitations of descriptors have been reported [67] ; molecular fingerprints are sparse, and there is a possibility for collisions in hashing. To overcome these limitations, many learnable feature-based methods have been developed, where the model learns local patterns and orders in raw data itself. Lusci et al. [68] proposed the model that takes a graph structure of a compound whose nodes are compound atoms. To generate a feature of a compound, they built recurrent neural networks (RNN) on every node in a compound by building a directed acyclic graph and summed up RNN results to generate features. Neural graph fingerprints [69] mimicked Morgan algorithms [70] , bringing atom features in radius, while neural graph fingerprints yielded continuous features of hidden layers to solve sparsity of Morgan fingerprints. Mol2vec [71] generates compound features that can be used to predict bioactivities by applying the Word2vec algorithm on a compound's molecular graph. Simplified molecular-input line-entry system (SMILES) is a well-defined representation of chemical compounds, converting the molecular graph to a sequence of atoms and bonds. To deal with the sequential representation of SMILES, LSTM deals with sequential data which have ordered in a series of data points. Chakravarti et al. applied LSTM on established SMILES to predict bioactivities [72] . In contrast, Winter et al. translated the International Union of Pure and Applied Chemistry (IUPAC) representation of compounds into SMILES and InChI, which provides comprehensive latent representation [73] . This model comprises an encoder and a decoder. The encoder provides a latent representation of input characters while the decoder uses them as input to generate output SMILES or InChI. Hence, intermediate latent variables are trained to predict bioactivities. SMILES-BERT [74] applied a model and training scheme of BERT [75] on SMILES string. BERT model was pre-trained to recover masked SMILES tokens from the rest of SMILES string with the assumption that it can learn the relationships among atoms by multi-head attention. Especially, representation token is attached to original SMILES, which are trained during fine-tuning, entailing relative importance on a specific molecular property. While many QSAR studies have been developed, similaritybased ligand screening models have been improved. Oneshot learning models [76] , which are designed for overcoming a small set of data, iteratively update prediction models for test data according to similarities between the pretrained representation of the compound. Compounds are embedded using GCN while the contexts of training data are considered by bi-directional LSTM. Embeddings of samples in the test dataset are updated using attention-LSTM, in which attention is calculated by the similarity between training samples. Unfortunately, in the virtual screening benchmark [77] , RF outperforms a one-shot learning model. Besides target protein bioactivity, similarities in transcriptomic expressions can also screen for active compounds (ReSimNet) [78] . ReSimNet takes two compounds with identical extended-connectivity circular fingerprints (ECFPs) [70] as input to predict CMap score, which is a standardized measurement that indicates expression similarity between compound pairs regarding the reference gene set [79] . Each compound is fed into Multi-Layer Perceptron (MLP) and cosine similarity between their latent representation is translated to a Connectivity Map (CMap) score. Therefore, ReSimNet gives a better performance than the conventional machine learning model of ECFP and Mol2Vec [71] . Conversely, the ensemble model of the hierarchical evolutionary chemical binding similarity (ECBS) tree builds more reliable screening results [80] . While the QSAR model concentrates on compounds of a specific target, the ECBS model takes advantage of the evolutionary features of targets. It takes two compounds; a known compound of the target and a compound to predict. ECBS label compound pairs with hierarchical evolution relationship. For example, if compound C1 and C2 have targets in the same family, they have a positive relationship between family-ECBS. By integrating models of compound pairs, they provided candidate compounds for serine/arginine protein kinases 1 and 2. Chemogenomic methods use the information of both target proteins and compounds. The exponential increase of protein, compound, and drug-target interaction (DTI) data leverages the quality and diversity of chemogenomic methods [81] . Chemogenomic methods are conventionally classified in two categories, similarity methods and featurebased methods [82] . Similarity-based methods concentrate on similarities between gathered proteins and compounds to predict DTIs. Welldesigned similarity metrics between proteins and compounds can be generated by various means including topological similarity in graphs and networks, normalized Smith-Waterman scores, Tanimoto coefficient, and hamming distance between protein domains. One remarkable research using the graph-based method is the bipartite local model (BLM) [83] . BLM builds a bipartite graph between drugs and targets and predicts interactions from two sides, the target and drug sides, resulting in a final prediction by aggregating both. Similarity matrices from targets and compounds are taken as a kernel of SVM, building interaction classification hyperplanes for the interacting partners. Successful performances of similarity-based methods come from well-defined similarity metrics and well-studied kernel methods [82] . To increase the performance, many regularization techniques on the graph of BLM, such as Laplacian regularized least square (LapRLS) [84] , Gaussian interaction profiles (GIP) [85] , and Kronecker regularized least square (kronRLS) are applied [86] . Furthermore, credible negative interactions are sampled from unlabeled interactions to build clear discriminative hyperplanes in the pharmacological space (Self-BLM) [87] . Network-based methods build heterogeneous networks from proteins, drugs, diseases, side effects, and their interactions. By diversifying from the known target and compound, it prioritizes interacting partners which have an opportunity as a candidate (NRWRH) [88] . In addition, recent studies integrated diverse networks of drugs and proteins to low dimensional informative feature vectors to predict DTIs (DTINet) [89] . DTINet learns a low dimensional representation of graph topology using the DCA algorithm [90] from the heterogeneous network. DTINet trains the projection matrix from known DTIs, which translates drug representations into protein representations. Feature-based methods take feature vectors of targets and compounds, which is the fixed length of a vector that describes important physicochemical properties. They concatenate vectors of drug and target pairs and train machine learning models to classify DTIs with given feature vectors of interaction and their labels. The previous model usually took chemical fingerprints [70, 91] as compound features, and many physicochemical properties as protein features [92, 93] . On constructed features, RF and SVM are trained to classify DTI [94] . To increase prediction performance, a drug-target feature can be weighted by networks of protein-protein interaction and drug-drug interaction [95, 96] . To expand on deep learning models, many studies were suggested to apply a deep learning model on featurebased methods. A Restricted Boltzmann machine, stack of restricted Boltzmann machine, and DBN are applied for the reliable abstraction of features [97, 98] . Likewise, a sparse autoencoder was used to build deep representation [99] . Deep representation of original features builds clearer hyperplanes while they are fed into a deeper layer, outperforming previous machine learning methods [51] . However, feature-based methods have many limitations, including the loss of information during feature engineering. Fixed length of feature vectors usually describes global physicochemical properties, losing informative local information while features are aggregated [92] . One of the many advantages of deep learning is that it can deal with any data structure. For example, the CNN works well for image data with a locality, while RNN is a suitable model for sequential data. In addition, deep learning can entail connectivity in a graph, aggregating locality, which is called graph convolution. Therefore, deep learning models can extract informative local features and their dependency, outperforming previous feature-based methods. DeepDTA [100] applied CNN on protein amino acid sequences and compound SMILES, which captures local patterns in raw data. Therefore, DeepDTA performed better than the previous similarity-based [86] and feature-based methods [101] . Tsubaki et al. [102] suggested a model applying CNN on protein sequence and GCN on compounds and to aggregate compound-protein pairs, applied an attention mechanism that gives high weights on interaction sites in proteins; showing better performance than previous similarity-based [83, 85, 103] and structure-based methods [43, 44, 47] . DeepConv-DTI [104] used multiple-size kernels of CNN on the protein sequence. DeepConv-DTI demonstrated that detection of a binding region on protein by CNN could be statistically validated and detected regions featurize proteins. DeepConv-DTI performs better than previous deep learning methods [51, [98] [99] [100] . DeepAffinity [105] first represented proteins as a structural property sequence (SPS) and embedded SPS and compounds with the Seq2Seq model. They applied attention mechanisms and 1D-CNN embedded proteins and compounds to predict the affinity of binding pairs. DeepAffinity proved target selectivity of drugs for many protein classes and predicted binding sites on proteins. The most representative limitation of the structure-based approach is an insufficient number of 3D-structure datasets and the difficulty in assessing the accuracy of the 3Dstructure. Likewise, the lack of activity data relative to the complexity of the model is also a problem for the deep learning model [67] . Inevitably, overfitting in deep learning models is induced by a small data size [67] . For example, it is reported that ML-based scoring functions are not suitable for comparative assessment of scoring functions (CASF) [106] . Besides the problem of overfitting, biases in datasets are reported in the Directory of Useful Decoys-Enhanced (DUD-E) and Maximum Unbiased Validation (MUV) [107, 108] . Recently developed chemogenomic models are built and evaluated with different datasets. Consequently, extensive external validation is needed for fair comparison over bias in training. For chemogenomic models, there are many deep learning protein models, entailing inherent protein characteristics, that are not used in previous models [109, 110] . Featurization by using those models will improve performance. Currently, many compounds, target proteins, and bioactivities are deposited in a public database. The number of compounds, drugs, target proteins, their interactions, and bioactivities increases exponentially [81] . 3D structures are also quickly accumulating annually in the Protein Data Bank (PDB) [111] , enabling data-driven scoring functions. Besides an increase of screening data, the organization of data for a specific task is also more enhanced. Table 2 lists commonly used databases that can be used to build a hit identification model. One of the big challenges in drug discovery is optimizing pharmacokinetic properties such as absorption, distribution, metabolism, excretion, and toxicity (ADMET). Therefore, the early assessment of compounds' ADMET properties is needed to guide the subsequent drug discovery steps efficiently [112] [113] [114] . For decades, both pharmaceutical industries and academia have been attracted to in silico ADMET property prediction, because of the accumulation of bioactivity and property data and sophisticated machine learning methods. In this section, we focus on the recent trend of ADMET property prediction by introducing the various ADMET related properties and the characteristics of current studies. A concise summary of this section is shown in Fig. 4 . [111] Database for 3D shapes of proteins, nucleic acids, and complex assemblies Approximately 1.6 M of 3D structures PDBBind [306] A comprehensive collection of the experimentally measured binding affinity data for all types 21 K bio-molecular complexes deposited in the PDB CASF [307] Scoring benchmark, where the scoring process is decoupled from the docking process to depict the performance of the scoring function more precisely. Entries in PDBBind may be filtered by processes. 195 protein-ligand complexes DUD-E [308] Benchmark molecular docking programs by providing challenging decoy 23 K active compounds with decoys for 102 protein structures MUV [77] Collection of data sets of active compounds and corresponding decoy data sets that are unbiased with regards to both analogue bias and artificial enrichment 510 active compounds with decoys for 17 protein structures CSAR [309] Benchmark datasets of crystal structures and binding affinities for diverse protein-ligand complexes 647 active/inactive compounds for 82 protein structures of 6 targets PubChem [310] An integrated chemistry database. It contains small molecules to large molecules with structures, physical properties, bioactivities, patents, etc. ChEMBL [311] A curated database of bioactive drug-like small molecules. It mainly contains 2D structures, calculated properties, and bioactivities. Merck Molecular Activity Challenge [191] The dataset which was used for Merck sponsored Kaggle competition in 2012. It contains 15 types of molecular activities. BindingDB [312] Web-accessible database of measured binding affinities, focusing mainly on the interactions of proteins considered to be drug-targets with small, drug-like molecules  Absorption is the very first barrier to potential drugs because they must first enter the circulatory system to be active in the body. Drug absorption is complexly related to various properties [5] . Among them, representative properties that are not only actively studied but also directly related to absorption are Human Intestinal Absorption (HIA) and Membrane Permeability. HIA is the most relevant property of orally administered drug absorption [115] . Recently, some data-driven HIA predictive models have been developed. Ponzoni et al. [116] used both engineered and learned molecular descriptors to construct a robust machine learning model with a collected dataset comprising 202 molecules. Wang et al. and Yang et al. [117, 118] attempted to solve the data imbalance problem by a modified RF algorithm and various sampling methods, respectively. Both studies used molecular descriptors and conducted feature selection, but interestingly, Yang et al. [118] designed the workflow to find optimal feature sets and the optimal ensemble model set. Membrane permeability is a simple and powerful physicochemical property to predict the absorption accurately. There are two major in vitro permeability assays that can simulate and predict the absorption of potential drugs. One is the human colon carcinoma (Caco-2) cell line permeability assay, and the other is the parallel artificial membrane permeability assay (PAMPA). In recent years, there have been numerous efforts to predict the absorption potential of compounds by using machine learning and molecular descriptors [119] [120] [121] [122] [123] [124] . Fredlund et al. [119] designed an in vitro assay to measure intrinsic permeability in Caco-2 cells and built prediction models with the data measured by a designed experiment. Furthermore, Lanevskij et al. [123] proposed a nonlinear regression model fit by 1,366 collected Caco-2 cell permeability data from various literatures. Whereas, Sun et al. [121] constructed a permeability prediction model with the PAMPA dataset they had generated and used atom type-based molecular descriptors and SVM for model building. Oja et al. [124] also designed a study of pH-dependent permeability prediction because the permeability largely depends on the pH level in the gastrointestinal tract (GIT). They constructed a logistic regression model with permeability data at different pH levels, comprising around 150 compounds each. In addition, there are sophisticated studies using deep learning techniques to resolve current limitations. Shin et al. [125] collected Caco-2 cell permeability data from literatures and constructed a deep neural network (DNN) model to reduce feature selection bias. Wenzel et al. [126] proposed a multitask DNN model to relieve the data deficiency with ChEMBL dataset which contains Caco-2 cell permeability and microsomal clearances. When a drug is administered or absorbed into the bloodstream, it needs to be transported to the desired site of action to be effective. This feature of the drug is called distribution. The distribution of a drug is a complex function of diverse properties; therefore, significant efforts are made by many researchers to predict the distribution rate of potential drugs. In this section, we surveyed representative distribution-related property prediction research, plasma protein binding (PPB) rate, P-glycoprotein (P-gp) inhibition, and blood-brain barrier (BBB) permeability. PPB rate is the percentage of the molecules that bind to plasma proteins such as human serum albumin, lipoprotein, and alpha-acid glycoprotein, among others. The binding rate of drugs to the plasma protein is an important property to predict distribution because the drug molecules have no pharmacological effect when they form the protein-ligand complex, although they have reached the target tissues [127] . With the in vivo or in vitro measured data, the in silico PPB predictive models are still actively studied [128] [129] [130] [131] [132] . Wang et al. [129] collected a comprehensive PPB dataset from various literatures and the DrugBank database and constructed a prediction model with the optimized feature set and the ensemble of various machine learning models. Sun et al. [130] integrated three data sources from literature and public databases to build a robust prediction model. For modeling, engineered molecular descriptors and various machine learning models are used. Toma et al. [131] collected in vivo data for PPB prediction modeling. They calculated molecular 2D descriptors and SMILESbased features and trained the RF model. Zhuyifan et al. [132] proposed multitask DNN architecture to predict few ADME properties such as PPB rate, half-life, etc. Interestingly, the model was pretrained with the molecular property benchmark dataset from DeepChem to estimate the vast chemical space. P-gp is the membrane transporter, which is also well known as multi-drug resistance protein 1 (MDR1). The role of P-gp is to actively transport foreign substances out of cells. Thus, the inhibition of P-gp is directly related to the concentration of drug in the target tissue. Recently, several machine learning-based P-gp inhibitor prediction models were developed [133] [134] [135] , all of which constructed the prediction models with various machine learning algorithms and engineered molecular descriptors as features such as molecular fingerprints [134] , 2D or 3D descriptors [135] , and SMILES-based features [133] . Notably, Kumar et al. [135] used 3D-RISM-KH based solvation free energy descriptors to increase performance. Meanwhile, Shi et al. [136] used CNN to extract a task-specific feature of the molecule and predicted its four ADMET properties from the 2D structure image. The dataset contains CYP1A2 inhibitors, P-gp inhibitors, BBB penetrating agents, and Ames mutagens. BBB permeability is a major hurdle for developing central nervous system targeted drugs because BBB protects the brain from foreign substances in the blood. At present, many researchers are attempting to make accurate prediction models and find structural patterns. Both machine learningbased [137] [138] [139] and deep learning-based models [136, 140] for BBB prediction appear currently. While Toropov et al. [137] built a model with SMILES-based features generated by CORAL software and 291 substances, Wang et al. [138] and Yuan et al. [139] used conventional 2D molecular descriptors and fingerprints as features and fit the model with a relatively large dataset containing 2,358 and 3,538 compounds, respectively. Recently, Miao et al. [140] published an interesting study of deep learning-based BBB prediction. The author designed a unique drug-phenotype feature derived from the SIDER database and the Medical Dictionary for Regulatory Activities. The proposed feature and DNN combination outperformed other conventional machine learning-based models. Metabolism is a biotransformation process that is mediated by various metabolic enzymes. The drug can be transformed into other compounds that can be excreted or activated [127] , or can affect the metabolic process which controls the activation or excretion of other drugs. Excretion is the process of eliminating the foreign compound from the body. This is an important feature of a drug because the dose of a drug is determined by the excretion factor. In fact, drug excretion is a complex function of both chemical and physiological features and also managed by the drug metabolic process in direct or indirect ways. Therefore, there are only a few recent studies that directly predict the excretion-related properties. Thus, in this subsection, we combined two categories: metabolism and excretion, and focused on both general metabolic properties and excretionrelated properties. The most active research field of drug metabolism is Cytochrome P450 (CYP450) enzyme-related prediction. There are two major streams of CYP450 related prediction, predicting CYP450 substrates and predicting CYP450 inhibitors. Predicting the CYP450 substrate is important because it can affect drug efficacy, excretion, and toxicity. Likewise, predicting the CYP450 inhibitor is also crucial because it is directly related to drug-drug interactions and consequent toxicities. In recent years, some research groups published CYP450 substrate prediction studies [141] [142] [143] . Hunt et al. [141] proposed the multiclass prediction model for finding the CYP450 isoform that metabolizes the query molecule. With 633 compoundisoform pairs of data, they constructed the multiclass RF model with conventional molecular descriptors. Tian et al. [142] developed the prediction tool 'CypReact' with 1,632 collected compounds, including 679 CYP450 reactants. They used physicochemical descriptors and various fingerprints as a feature and employed a novel technique called Learning base model; a cost-sensitive meta-learning technique which seeks the best classifier and optimal feature sets that minimize the defined cost. Shan et al. [143] proposed a multi-label classification model with 1,299 compoundisoform pairs of data. They used a network-based label space division learning technique to make a multi-label prediction model; which enables making a multi-label model with multiple binary classifiers. Another area of in silico drug metabolism research studies is on the site of metabolism (SoM) prediction. The ability to predict SoM can guide the next stage of drug discovery, since knowing the site of metabolism is essential in the drug optimization process. Many research groups attempt to construct robust SoM prediction models. He et al. [147] proposed the prediction model of SoM by oxidoreductases, collecting the data from the BKM-react database, comprising 28,042 unique biochemical reactions and constructed the classifiers with various machine learning algorithms and chemical bond descriptors. 'FAME 2' [148] , the machine learning tool of SoM prediction by CYP450 was also published. The author collected data and trained the RF with various combinations of descriptors. The dataset contains approximately 200-600 molecules for each CYP isoform. Finkelmann et al. [149] proposed the tool called 'MetScore' which can predict the SoM by various metabolism enzymes including CYP isoforms. They collected data from the BIOVIA Metabolite Database to construct a comprehensive prediction model. Cai et al. [150] developed a prediction model of SoM for UDPglucuronosyltransferase-catalyzed reactions. This group retrieved data from the "Handbook of Metabolic Pathways of Xenobiotics" [151] and reviewed the literature for validation; they used atom environment fingerprint and decision tree-based machine learning models such as RF and Adaboost. Assessing drug's susceptibility to biotransformation is another principal issue in drug discovery. This feature of the drug, which is called metabolic stability, is explained by the pharmacokinetic properties such as intrinsic clearance and half-life. Therefore, there are attempts to predict the intrinsic clearance and half-life by in silico method to reduce the cost of experiments. Podlewska et al. [152] proposed the tool, called 'MetStabOn' stands for an online platform for metabolic stability prediction. They first collected the various datasets from ChEMBL contains approximately 60-2,500 molecules in different species like human, rat, and mouse. They built both regression and classification models with molecular 2D descriptors, the former is trained to predict the half-life and clearance values directly and the latter is trained to predict the level of the values. Esaki et al. [153] showed the positive effect of data curation by constructing an intrinsic clearance prediction model with curated and non-curated datasets. They initially collected the dataset from ChEMBL, and manually curated the dataset using several rules. They constructed conventional machine learning models such as RF, Adaboost, and SVM. Recently, deep learning-based studies [126, 132, 154] were conducted for clearance and half-life prediction. Liu et al. [154] published interesting work that used graph convolution on a molecular graph to featurize and construct a multitask model to predict human microsomal clearance, CYP450 inhibition, and other physicochemical properties. Although the excretion property studies are relatively fewer than metabolism-related studies, there are some meaningful studies that focused on predicting non-liver clearance such as renal and plasma. Zhivkova et al. [155] designed a study for predicting drug plasma clearance. They built a linear regression model with optimally selected descriptors using a genetic algorithm of relevant data comprising 659 drugs. Wakayama et al. [156] proposed a prediction model that predicts the several major clearance pathways of drugs. They used 249 drugs with nine major clearance pathways' information such as renal, OATP, and CYP450 related pathways. They constructed two-step SVM with chemical descriptors. The first step is predicting the pre-defined group of clearance pathways and the second step is predicting the exact pathway of the compound. Watanabe et al. [157] developed in silico renal excretion and clearance prediction models with manually collected 411 and 401 compounds, respectively. They constructed the models by using conventional machine learning methods with chemical 2D descriptors and fingerprints. Chen et al. [158] developed both global and local models for predicting human renal clearance of compounds with a combination of molecular descriptors and conventional machine learning methods. They collected the clearance data from various literatures and U.S. FDA Drugs Database, containing 636 compounds. Notably, the model showed less reliable performance in a global model but showed reasonable performances in local models which are constructed with specific subsets of compounds such as ionization-based and elimination route-based subsets. Undesired adverse effects, namely drug toxicity, may cause high costs if it is not investigated carefully during the drug development process. Since drug toxicity is the most crucial aspect during the drug discovery process, in silico toxicity prediction has been actively studied to reduce the late-stage failure rate. Numerous toxicity prediction studies exist, but drug-induced liver injury (DILI) and human ether-à-go-go related gene (hERG)-related cardiotoxicity prediction are studied mainly because many marketed drugs are withdrawn for these toxicities. Therefore, we categorized the numerous toxicities into DILI, hERG, and several others in this section. DILI is one of the main reasons for the withdrawal of marketed drugs. Therefore, there have been many in silico research studies to make precise predictive models and find the patterns of hepatotoxic compounds [159] [160] [161] [162] [163] [164] [165] . Kotsampasakou et al. [161] highlighted the importance of data curation by curating 1,547 compounds from various sources and testing the performance of machine learning-based prediction models. In addition, our group recently proposed a precise DILI prediction model by developing a novel Bayesian weighted fingerprint for a molecule [160] . The frequent substructures of DILI positive compounds are reflected in a molecular fingerprint to improve the performance and interpretability. The author collected data from various sources; LTKB-BD, DrugBank, and literatures to construct and validate the model. Hammann et al. [163] constructed a DILI prediction model based on DILI annotated drugs by using physicochemical descriptors and machine learning methods. Furthermore, they analyzed the interactions of hepatotoxic compounds with bioentities such as carriers, transporters, and metabolizing enzymes. They also found the relationship of defined daily doses with hepatotoxicity. Williams et al. [165] proposed an interpretable Bayesian regression model with both physico-chemical properties and related bioactivities which they measured through in vitro assay. Inhibition of the hERG channel is another major issue leading to the withdrawal of marketed drugs. The hERG channel is the voltage-gated potassium ion channel (Kv11.1) which regulates cardiac action potential to make a constant period of QT interval. Thus, when the drug inhibits the hERG channel, it causes severe cardiac arrhythmia by druginduced QT prolongation. Recently, it also has been actively studied for data-driven in silico hERG-related toxicity prediction by many researchers [166] [167] [168] [169] [170] [171] [172] [173] . Siramshetty et al. [167] collected and preprocessed 5,804 compounds from the ChEMBL database for training and validated the model with literature-derived data. Various molecular fingerprints and machine learning models were used. There is currently no standard cutoff potency of hERG blockers, therefore, the authors compared the models that were trained with multiple cutoffs. Ogura et al. [172] proposed the hERG blocker classification model constructed with the hERG integrated database from their previous work [174] . The database comprises 9,890 hERG blockers and 281,329 hERG non-blockers. They selected the optimal descriptors with a genetic algorithm and built an SVM model for classification. Cai et al. [168] and Zhang et al. [173] are the pioneers who applied deep learning in this field. Zhang et al. collected data from the literatures comprising 697 molecules. The author observed that the three-layered DNN with molecular 2D descriptors had the best performance. Cai et al. proposed a multitask DNN that learns from applied data with different half maximal inhibitory concentration (IC 50 ) cutoffs. The authors used the Mol2Vec feature and molecular descriptor for the feature, and the data was collected from ChEMBL and other literatures. Kim et al. proposed the interpretable deep learning model for hERG blocker prediction, called 'hERG-Att'. By employing a self-attention mechanism, the model learns to not only classify the hERG blockers but also capture the data-specific important substructures from molecular circular fingerprints. The authors confirmed that some of the captured substructures of predicted hERG blockers are related to known hERG-related substructures [175] . Apart from the above major toxicities, many other toxicities have been studied for developing prediction models [72, [176] [177] [178] [179] [180] [181] [182] [183] . The Lei et al. research group recently published respiratory toxicity and urinary tract toxicity prediction studies [176, 177] . Both studies used the ChemIDplus database and MOE software to collect and featurize the toxic compounds. Furthermore, they constructed both regression and classification models for generality. Liu et al. [178] proposed the 35 target organ toxicity prediction model. For the compound feature, they used the structural feature and the in vitro bioactivities of a compound. They constructed each prediction model with various machine learning algorithms, for example, SVM, RF, and k-nearest neighbors. They curated data from ToxCast and ToxRefDB databases. Zheng et al. [183] developed a hemolytic toxicity prediction model with molecular fingerprint and machine learning methods. Interestingly, they found the optimal virtual fingerprint of the toxic compound with a genetic algorithm and searched for other possible toxic compounds using similarity searching. Furthermore, few deep learningbased methods have also been developed. Xu et al. [179] proposed a novel architecture, called molecular graph encoding-convolutional neural network (MGE-CNN). It automatically extracts the task-specific features and predicts the toxicity from the raw molecular graph. Chakravarti et al. [72] proposed attention-based LSTM networks, where the raw SMILES is fed into the model directly. They benchmarked three bioactivities: Ames mutagenicity, Inhibition of Hepatitis C virus, and Inhibition of Plasmodium falciparum Dd2 which were from PubChem and other public sources. They identified the structural alerts of toxic compounds from the model by analyzing attention coefficients. Unlike other ADMET properties, toxicity data is relatively well-known to the public because of the Tox21 challenge in 2014. Therefore, methodology-based studies have recently emerged, and benchmarked against major toxicity datasets including the Tox21 dataset [76, [184] [185] [186] [187] . All of the studies proposed deep learning-based novel architectures to predict the compound toxic-related properties. Altae-Tran et al. [76] used a one-shot learning technique to relieve the data deficiency and consequently proposed iterative refinement LSTM networks combined with GCN. It showed a remarkable performance of small data with three benchmark datasets Tox21, SIDER, and MUV. Abbasi et al. [185] proposed a novel transferable deep learning architecture that used GCN and an adversarial domain adaptation network; they also deeply benchmarked their model with various physiology and biophysics datasets, such as Tox21, ToxCast, and SIDER. Here, we address the limitations of current research trends and suggest future research directions. First, the quality and quantity of the data is a huge hurdle in ADMET property prediction fields. Specifically, most predictive models comprise hundreds to thousands of small chemistry datasets that cannot cover enough chemical space [76, 118, 188] . Moreover, the data is usually dispersed to many literatures [117, 118, 122, 124, 125, [128] [129] [130] [134] [135] [136] [137] [138] [139] [140] [141] [142] [143] [144] 150, 155, [159] [160] [161] [162] [163] [164] [165] [166] [167] [168] [169] 173, 176, 179, 183] , is unbalanced, and has cutoff ambiguity challenges [118, 167] . Furthermore, the bioactivity assay data is strongly biased to its platform, and has an intrinsic experimental error which disrupts accurate prediction [189] . Acquiring more data is practically very difficult and even impossible, therefore, multitask learning and transfer learning concepts may resolve these data problems [65, 76, 126, 132, 144, 185, 187] . Besides, a comprehensive well-curated database or benchmark [190, 191] could help researchers reduce the data collecting and processing time and produce fair comparisons [65] [66] [67] 192] . Remarkably, Wu et al. [190] proposed the comprehensive benchmark called 'MoleculeNet'. They offered the data of various properties of over 700,000 compounds to compare the algorithms fairly. Furthermore, they offered large-scale data, standard metrics, basic models, and common featurizers. Second, most of the studies used engineered molecular descriptors to train the conventional machine learning models, but there are feature-intrinsic biases and a modelinherent low interpretability [72] . To address these problems, a novel data-driven feature generation [71, 73] or targetspecific feature learning from raw data offer good solutions [76, 136, 154, 179, 184, 185] . However, having interpretability in the conventional machine learning and deep learning model is still challenging. Therefore interpretable and endto-end molecular property prediction is definitely a fascinating and promising research field [72, 193, 194] . Last, a challenge in ligand-based property prediction, is the activity cliff [195] . The activity cliff is the concept of circumstance where compounds, which have similar structures, have different properties. This concept breaks down the primary assumption of QSAR, therefore, it is the most difficult problem in this field. To solve this problem, we must use the information beyond the compound structure. Remarkably, some novel strategies attempt to find the toxic compounds beyond the QSAR assumption by using machine learning techniques and using toxicogenomics data containing both compound structure and gene expression profiles [196] [197] [198] . Until now, most data-driven ADMET prediction research relies on literature-derived data. However, there have been numerous efforts that pursue constructing a freely accessible, integrated database that satisfies both quantity and quality by curating the published literatures. Here, we summarize the large-scale, integrated public databases which contain enough molecular activities or properties. The databases with simple descriptions are summarized in Table 3 . Finding a molecule that has the desired pharmacological properties or has activity against biological targets can be described by the metaphor "finding a needle in a haystack". Specifically, researchers estimate the chemical space of synthesizable compounds to comprise approximately 10 30 -10 60 possibilities, whereas the number of compounds registered in Chemical Abstracts Service has only reached about 160 million to date. Fully enumerating this vast space will require too much resource and computing power. Thus, computer-aided de novo drug design has been an active research area for the past 10 to 20 years [199] [200] [201] [202] [203] [204] . In this section, we present recent studies on de novo drug design using deep generative methods, which have gained popularity over the past 2-3 years. These approaches adopt the deep learning techniques that exhibited noticeable successes in synthetic image generation and machine translation domains. In the lead optimization domain, the common objective of the deep generative models is to learn the distribution of the chemical space, and perform targeted optimization toward desired chemical properties, as depicted in Fig. 5 . Though each method introduced here has its own distinct strengths, the deep generative methods, have several benefits over other traditional approaches, where the experts manually design with knowledge and intuition, or use exhaustive enumeration on virtual libraries [199, [205] [206] [207] . First, deep generative modeling can reduce human bias since it is completely data-driven. This is in contrast with the traditional de novo design methods where the generation of the molecules depends on expert-coded rules [208] . Second, the chemical space is directly modeled as a continuous function and learned by gradient-based optimization, which is not viable for other computational techniques such as genetic algorithms [209] . Third, it can overcome the lack of target-specific data by using transfer learning and semi-supervised learning techniques [202, 203, 209, 210] . Deep learning has enabled significant advances in machine translation and language modeling for the past decade. RNN is the core component of language modeling, and research has shown that RNN can effectively generate synthetic texts [203] . There have been several attempts to bring the deep learning techniques of language modeling into drug design, which we aim to list some of these here. Most of the studies focus on building deep generative models that learn the SMILES grammar and generate novel SMILES strings. SMILES is a type of molecular graph representation that encodes the molecular graph into a line of characters using depth-first graph traversal [203] . SMILES networks generation is usually done in a symbolby-symbol manner, where a recurrent unit of the network calculates the probability of the next SMILES symbol to appear based on the generated symbol at the previous step and the recurrent unit's state. A widely used technique for training such RNN is a teacher forcing method [211] , which Table 3 . Large-scale and integrated public databases commonly used in ADMET property prediction Database Name Description Quantity ChEMBL [311] A curated database of bioactive drug-like small molecules. It mainly contains 2D structures, calculated properties, and bioactivities. PubChem [310] An integrated chemistry database. It contains small to large molecules with structures, physical properties, bioactivities, patents, etc. admetSAR [319] Comprehensive ADMET related property data source and prediction tool 200 K activities and properties MolculeNet [190] A molecular property benchmarking dataset containing various domains of property. The dataset is embedded in the DeepChem opensource python package. Merck molecular activity challenge [191] The dataset which was used for Merck sponsored Kaggle competition in 2012. It contains 15 types of molecular activities. DrugBank [313] A free, comprehensive drugs and drug targets database. It contains various chemical and target information for each drug. 13 K compounds, 5.1 K targets SIDER [288] A database of marketed medicines and their recorded adverse drug reactions 1.4 K drugs, 5.8 K serious events BindingDB [312] Web-accessible database of measured binding affinities, focusing mainly on the interactions of protein considered to be drug-targets with small, druglike molecules 1.7 M binding data for 7 K protein targets and 796 K small molecules ChemIDplus [320] A web search portal that provides access to the chemical substances cited in the National Library of Medicine databases 112 K chemical records of various toxicities ToxCast [321] A project of the U.S. Environmental Protection Agency. They generated toxicity-related high-throughput assay data on thousands of chemicals. Dataset published for the Tox21 challenge in 2014. It contains 12 assays related to human toxicities. Information curated from over 5,000 in vivo toxicity studies; contains 10 toxicity study types allows the RNN to learn the information in the training set faster and more reliably, by enabling supervision of generative learning. Gupta et al. [212] successfully applied a generative LSTM model for the molecular generative tasks. Their LSTM model was first trained with approximately 500 K SMILES strings from ChEMBL22, and they demonstrated that the model can generate valid molecules following the distribution of the training set. The model was further trained (or fine-tuned) with some target-specific datasets of 4,367 PPARγ ligands and 1,490 trypsin inhibitors. The authors observed a more focused generation toward the specific targets, indicating the viability of the transfer learning approach for hit-to-lead optimization. Segler et al. [208] used 1.4 million molecules from ChEMBL to first train their LSTM model and produced three different finetuned models with three datasets of different targets: 5-HT2A, P. falciparum, and Staphylococcus aureus. They also made a target prediction model, predicting the IC 50 of a molecule to the target to simulate the typical cyclical drug discovery process. Many other studies followed a similar framework of generative RNN training, but have used different target-specific or property-specific small datasets, such as retinoid X receptor ligands [213] and quasibiogenic compounds [214] . Awale et al. [215] performed transfer learning to a single known drug to obtain new analogs of the drug. They prepared six different primary training-sets and performed extensive comparative analysis. Arús-Pous et al. [211] performed an extensive analysis on the capability of the generative RNN. To evaluate the performance of the RNN model, they devised an ideal model which is an abstract model that samples molecules from GDB-13 with uniform probability for every molecule of GDB-13. They concluded that the RNN suffered from the constraints of SMILES, i.e. complex graph topologies or not-chemically allowed functional groups are more difficult to learn with SMILES. Many studies, introduced in this section, adopt the canonical SMILES for molecular representation. Note that one molecule has one specific canonical SMILES format, while it can have many SMILES strings if it is not restricted to the canonicalization. Arús-Pous et al. [216] performed an extensive benchmark of SMILES-based generative models with different variants of the SMILES syntax, comparing canonical, randomized, and DeepSMILES notations. They demonstrated that randomized SMILES substantially improve the quality of the generated chemical space. Pogány et al. [217] explored the applicability of using Reduced Graph (RG) representation as input to the generative RNN, though the output is still the translated SMILES string from the RG. Although RNN is most suited to sequential data like text sentences, there was a study using RNN with non-textual representations. Li et al. [218] proposed a de novo molecular design framework based on a type of sequential graph generators that do not use symbollevel recurrent units. Their model learns the parameterized decoding policy that specifies the probability value for each graph transition, where the graph transitions include appending a new atom and connecting to atoms with a new bond. RNN language models in de novo design have produced promising results, but the majority has only shown transfer learning approaches in terms of a targeted generation with the property objective. Because of this architectural limitation, it is hard to enforce a sentence-level (compound-level) property condition. Unless introducing some additional modules for the network architecture to incorporate the chemical property information [218] . The following autoencoder-based network and reinforcement learningbased tuning can embed the condition more naturally. The goal of chemical autoencoders is to learn a mapping from raw compound data to a latent vector as a compressed representation. The training enables the autoencoder (AE) to learn the latent space where each compound is continuously located at the more chemically relevant position. The main idea in generative autoencoders is to sample a compound vector from the latent space. Autoencoders are widely used in various domains of deep learning applications for representation learning and dimensionality reduction. The autoencoders in generative models enable training with conditional properties, by simple joint learning or more sophisticated disentanglement and entanglement of dependencies [209, 219] . This subsection is dedicated to introducing studies about de novo deep generative molecular design using autoencoder-based methods, such as variational autoencoders (VAE) and adversarial autoencoders. VAEs have been widely studied in the image generation domain [203] . Its mathematical foundation is based on the theory of variational inference. VAE introduces probabilistic latent space with noise, and regularization of the latent space learning with a predefined prior, so that the generative steps are conducted by sampling from the latent prior. Gómez-Bombarelli et al. [209] pioneered the application of VAE for exploring chemical space. They used generative gated recurrent unit (GRU) for the decoder of the AE and found that CNN performed well for the encoder. They used two VAE models, trained on QM9 and ZINC, and compared these models with the genetic algorithm baseline. Then, they performed joint training for property prediction, by attaching a separate feedforward network to predict the molecule's property. Lim et al. [220] proposed a molecular design strategy based on conditional VAE. The conditional VAE has the condition property vector directly embedded into the latent space and used as an additional input for the encoder. Kang et al. [210] presented a conditional molecular design method using a semi-supervised variational autoencoder (SSVAE). The SSVAE has three separate modules: encoder, predictor, and decoder networks. The predictor network gets the SMILES input and predicts a property value. An interesting concept of the network design is to assume the Gaussian distribution on the property; this concept enabled semi-supervised learning for the VAE. Harel et al. [221] presented a deep molecular generative model called prototype-driven diversity networks, which uses VAE architecture where the encoder receives molecular prototypes as input. The studies mentioned thus far all focus on using the SMILES representation with VAE. Still, there are ongoing studies in this field venturing toward more diverse representations. Skalic et al. [222] devised a deep generative pipeline that generates new molecules from 3D volumetric representations. The pipeline comprises two modules: shape autoencoder, which is a VAE that learns the latent space of the 3D molecular representation, and shape captioning network, which is a CNN-LSTM network that translates the given 3D representation into a SMILES string. The work by Lim et al. [223] suggests the graph generative VAE model in which the generation process starts with a latent vector of an initial scaffold. In the learning phase, the encoder encodes a "whole-molecule" graph to a latent vector (scaffold), and the decoder uses the latent vector and the given desired properties to reconstruct the original whole-molecule. Instead of variational inference, an adversarial training approach can also be used for regularized learning of latent space. Introducing adversarial networks in generative modeling has been a key technical advancement in various fields adopting AI algorithms [203] . The first inception of adversarial learning was the generative adversarial network (GAN), where the generator network's output is discriminated by a separate discriminator network. There was a promising attempt to bring this adversarial training scheme into the autoencoder-based latent representation learning, called adversarial autoencoder (AAE). In AAE, which is a variant of generative autoencoder, the latent space regulari-zation is conducted with the adversarial training by using a separate discriminator network that distinguishes whether the latent vector is from the encoder network or from the prior distribution. Polykovskiy et al. [219] devised an entangled conditional AAE, which is an improvement on a simple supervised AAE architecture. In their work, they theoretically discussed the disentanglement problem in the generative autoencoder schemes. Considering the issue, they designed a novel entangled model that addresses the dependence between the latent code and the property values. Kadurin et al. [224] trained the VAE and AAE models to generate a 166-bit Molecular ACCess System (MACCS) chemical fingerprints and performed a comparison of the two models on the reconstruction quality and sampling coverage. Blaschke et al. [225] explored four different generative AE architectures on the SMILES generation task: VAEs using teacher forcing or not, and AAEs where the encoder is trained to follow Gaussian or Uniform distribution. Prykhdoko et al. [226] proposed a new molecule de novo design method, by combining a SMILES heteroencoder and a GAN. After the heteroencoder is pretrained, the output from the encoder, the latent code, is used as a true input for the discriminator of the GAN. GAN generator's output produced from random uniform noise is used as a fake input for the GAN training. For the generation phase, the generator produces a synthetic latent code, and the code is fed to the decoder to generate a SMILES string. Although the introduction of guiding conditional properties in the AE architecture seems to lead to better latent space formation, it has been observed that these approaches generate valid molecules less frequently than others [203] . This implies there is room for improvement in the quality of learned latent space. Using some manual prior design could help, but it would be challenging to devise appropriate prior to the latent representation considering the complexity of the global chemical space. The alternative approach in conditional molecule generation is using reinforcement learning (RL). The main idea behind adopting RL is to construct strategies directly or indirectly for exploring the constrained chemical space. Many researches demonstrated the applicability of RL as a fine-tuning process, where it comes after the pre-training process with a general-purpose database; though, some researchers recently pioneered the possibility of a pure-RL approach [227] . In this subsection, we introduce studies that used RL as a part of their de novo molecular design with deep generative modeling. Most works introduced here have formalized the molecular generation of targeted property as maximization of the expected return, which is the accumulated reward throughout one episode [203, 228] . In the RL formalization, the current state of the environment is the SMILES sequence which has been generated symbol-by-symbol at each step. Action to be taken at a certain time step, is adding one symbol or deleting an existing symbol. RL, can be broken into two categories, value learning and policy learning; the works listed below adopt different approaches with their own reasoning. Reward function design is the most important component of building an RL system, yet it is still a challenging opportunity for future research to discover the best rewarding scheme to effectively learn the chemical space. Olivecrona et al. [228] introduced a deep generative RNN method for SMILES generation task where, through RL, the RNN can also learn to generate structures with certain specified desirable properties. They first trained the RNN prior network on the training set of 1.5 million structures from ChEMBL. After they framed the finetuning task within an RL frame, where sampling SMILES symbols from RNN ends with an EOS token, and the return is defined by their proposed objective function, which showed better results than other traditional scorings. A similar approach was taken by Popova et al. [229] , however, they used a classic policy gradient algorithm called REINFORCE, and used Stack-RNN as a generator to address long-term dependencies effectively. Putin et al. published two studies on deep generative de novo design using the GAN concept and RL policy gradient [230, 231] . In their study on Reinforced Adversarial Neural Computer (RANC) [230] , they adopted a special generator architecture, called Differentiable Neural Computer (DNC), for tackling the problem posed by using LSTM under the adversarial training scheme. For the RL fine-tuning phase, the discriminator network was used for evaluation of the reward for the generator, which is estimated as the likelihood of fooling the discriminator. In their other work, Adversarial Threshold Neural Computer (ATNC) [231] , the overall scheme was like the RANC, but it introduces a new block called adversarial threshold (AT). AT is a copy of the discriminator, lagging behind the original discriminator for a certain number of epochs. Liu et al. [232] introduced an exploration strategy on the RL training phase. Their training process uses two networks, the exploitation network and exploration network. Each symbol generation to be measured is conducted by random selection between the two networks. Many different RL learning objectives other than policy gradients also exist. Ståhl et al. [233] presented a fragmentbased RL approach based on the actor-critic model, where the policy network (actor) and value function network (critic) are trained in parallel. They suggested an alternative way of representing molecules, where molecules are split into fragments of predefined fragment set libraries, and each fragment is represented as a binary vector encoding, that aims to make similar fragments get similar vectors. Zhou et al. [227] presented a framework called Molecule Deep Q-Network (MolDQN), which uses a value learning algorithm. Their work is notable since it was a pure-RL only approach with no pre-training process on a large dataset. Few approaches attempted to combine the variational autoencoder and RL tuning scheme. Zhavoronkov et al. [234] combined RL, VAE, and tensor-train decomposition techniques into a generative two-step machine learning algorithm. They sampled six candidate compounds targeting DDR1 kinase, and the compounds were designed, synthesized, and experimentally tested in 46 days; which is quite an impressive outcome considering the typical timeline of the drug design process. Kwon et al. [207] presented a learning method involving a graph variational autoencoder for the molecular graph generation. Their graph generation procedure does not add a node or edge one by one, but a whole graph is generated at once when the network propagation is finished. For the encoder, they used a message passing neural network, and designed an approximate graph matching method for calculating reconstruction loss of the graph autoencoder. Although the recent advancements in de novo design with deep generative methods look promising, there are still many existing theoretical and practical impediments. First, there appears to be a lack of standardized benchmarking or comparative studies tools. It is known that the evaluation of the generative models is very tricky unlike well-established supervised learning tasks, and there are still debates on which measures to use for generative models. It should be noted that studies in de novo drug design have their own objectives in focusing on a specific desired target. Still, the comparative studies between various proposed models using some standardized benchmark dataset could show the models' strengths and weaknesses, which can significantly benefit the researchers of further studies in this area. There were two prominent benchmarking platforms released recently: MOSES [235] and GuacaMol [181] . MOSES created a refined 2M molecules benchmark dataset based on ZINC Clean Leads. They suggested various performance metrics for molecular generative models, including validity, uniqueness, internal diversity, novelty, filters, Frechet ChemNet Distance (FCD), similarity to the nearest neighbor, fragment similarity, and scaffold similarity. They also implemented several deep learning models appearing in the previous literatures [185, 186, 203] and provided a comparison table between them on the suggested metrics. In contrast, GuacaMol [181] used ChEMBL 24 database for the standard dataset. They separated the generative de novo task into two different categories and proposed the two benchmark types accordingly: distribution-learning benchmarks and goal-directed benchmarks. Distribution-learning benchmarks test for the model's ability to learn the distribution of the training set. Goal-directed benchmarks test for the model to generate the molecules with high scores based on the desired scoring functions. They provided a comprehensive list of 20 different optimization tasks, including the similarity scoring to one or multiple target molecules, multi-property objectives, and scaffold hopping. GuacaMol also provided baseline model implementations, such as SMILES RNN, VAE, and genetic algorithms, and reported the models' performance on their metrics. Another space for the improvement of the generative de novo studies is that the previously mentioned studies focused mostly on models generating SMILES strings. Whether they are used as input or output of the generative model, some prospective research may benefit from using other molecular representations, such as molecular graphs defined by nodes and edges [207, 218, 223] , or with 3D geometry of molecules [222] . Although deep generative modeling has been facing a rapid surge of interest in the field of de novo design for around 2 to 3 years, it is not the only computational approach to resolve the problems. Evolutionary algorithms (EAs) are one of the popular traditional approaches that researchers have used and improved over about 10 to 20 years. EAs are optimization techniques to find the best solution set, inspired by biological evolution in nature, such as reproduction, mutation, genetic recombination, natural selection, and survival [236] . One recent development using this approach was conducted by Yoshikawa et al. [237] . They used a grammatical evolution approach, where a chromosome represents a sequence of translation rules, defined under a context-free grammar, used to produce a mapping to a SMILES. Their approach was compared with other deep learning-based methods [186] , and the result shows the computational efficiency of grammatical evolution is far better than the deep learning. Jan H. Jensen [238] presented a graph-based genetic algorithm (GB-GA), where the mutation and crossover operations are performed by a graph representation of the molecules. The result showed the GB-GA's performance is equal or better than the recent RNN and VAE methods, in terms of optimization of log P values. Many other studies also adopted EAs for their computational de novo design schemes [236, 239, 240] . EAs are robust and powerful alternatives to deep generative modeling. Deep generative modeling is not an omnipotent tool and has its own limitations; researchers and practitioners need to consider other options like EAs, depending on their domain-specific problem settings. Table 4 describes the databases used by the studies listed in this subsection. Most of the studies have used large generalpurpose databases, such as ZINC and ChEMBL, for pretraining the initial generative models. ZINC especially contains hundreds of millions of molecules, thus the studies usually randomly sample or filter with specified criteria from the whole available molecules. GDB databases are enumerated datasets of all virtually possible molecules and were used by some studies to evaluate their models' capability. The studies that aimed to find the activities against some specific targets usually extract the subsets of the ChEMBL by the known bioactivities, so they can be used for fine-tuning the models. Other studies [218, 225, 226, 228] used the ExCAPE-DB, which is a convenient tool for filtering bioactivity entries provided by both PubChem and ChEMBL. The small datasets, used by the studies that resorted to proprietary data or specific bioassays, are not listed here. Drug repositioning, also known as drug repurposing, is the process of finding new indications of drugs. This approach is based on approved drugs or tested compounds and uses information about their known pharmacology. Therefore, drug repositioning has an advantage as it significantly reduces the time and cost than traditional de novo drug discovery approaches. Previously, most of the drug repositioning has been serendipitous. For example, sildenafil, which was developed in 1989 and used to treat angina, was found to treat erectile dysfunction, and was named Viagra [241] . In the case of thalidomide, it was first developed for morning sickness but resulted in severe birth defects with malformation of the limbs [242] [243] [244] , and was withdrawn from the market. Several years later, researchers discovered the anti-angiogenesis effect of thalidomide and further used it to treat multiple myeloma and leprosy. Table 5 shows more successful drug repositioning examples, most of which were discovered using the understanding of the pharmacology of each drug. Although drug repositioning is an essential approach in drug development, the identification of drugs through experiments remains a challenge. However, several data-driven computational approaches have been developed. Here, we review various data types used for drug repositioning and recent computational approaches (Fig. 6) . Besides, we discuss the advantages and limitations of each approach, then further provide recommendations that could facilitate a more informative understanding and [311] A curated database of bioactive drug-like small molecules. It mainly contains 2D structures, calculated properties, and bioactivities. PubChem [310] An integrated chemistry database. It contains from small to large molecules with structures, physical properties, bioactivities, patents, etc. ZINC [324] Virtual molecules that are likely to be synthesizable but have not yet been made 750 M molecules DrugBank [313] A free, comprehensive drugs and drug targets database. It contains various chemical and target information for each drug. [330] 1988 Hypertension Hair loss Sildenafil [241] 1998 Angina Erectile dysfunction Thalidomide [242] [243] [244] 1998 Morning sickness Multiple myeloma Celecoxib [331] 2000 Inflammation / pain Familial adenomatous polyps accelerate the process of identifying repositioning candidates. Network-based approaches are widely used in drug repositioning since various types of data are involved along with the increase of high-throughput biological data. These approaches have advantages in integrating multiple data and dealing with heterogeneous networks. Interactions or relationships between various types of node data such as drugs, diseases, genes, and proteins, as well as each characteristic information of drug and disease, are considered in heterogeneous networks. These interactions contribute to identifying drug repositioning candidates from various perspectives [245] . Thus, most studies first constructed a heterogeneous network, and then a network-based algorithm was applied. Several studies have been proposed for decades. Among them, two types of approaches are reviewed in this paper: clustering-based and network propagation-based approaches. The clustering-based approach aims to discover groups or subnetworks within biological networks. These approaches incorporate heterogeneous network construction. Wu et al. [246] collected disease-gene and drug-target associations from public databases to generate disease-disease, drugdrug, and disease-drug pairs considering shared genes for features such as biological processes and pathways. Then, they constructed a weighted heterogeneous network and used graph clustering algorithms to identify drug repositioning candidates. They applied two graph clustering algorithms to detect modules, Louvain's modularity and ClusterONE. [247, 248] The Louvain's modularity computes the modularity of iteratively aggregated communities of nodes until the maximum value is achieved, whereas the ClusterONE calculates cohesiveness of clusters by weight of edges within and between groups. The authors found novel drugdisease associations; for example, vismodegib was predicted to treat Gorlin syndrome besides its original indication, basal cell carcinoma. Sun et al. [249] developed a data fusion model to integrate multiple data types, including drugs, genes, and diseases, through weighted n-cluster editing. The approach uses graph clustering methods performed on n-partite graphs and applied to drug repositioning, to find novel drug-disease pairs through edge-modification. Chem et al. [250] proposed a method called heterogeneous-networkbased-inference (HNBI). They collected experimentally supported drug-miRNA and miRNA-disease associations from public databases to calculate all pairwise similarities of associations among drugs, miRNAs, and diseases. Then, they constructed drug-miRNA-disease heterogeneous networks based on each similarity, and calculated strength of weight between unlinked edges to rank drug-disease associations as a missing link prediction problem. Here, information propagates from the source node to all nodes in a network. Previously, random walk algorithms were used on a heterogeneous network constructed with multiple features. Martínez et al. [251] proposed a network-based method for drug-disease and disease-drug prioritization based on ProphNet, that implemented a propagation flow algorithm [252] . They retrieved information on drugs, proteins, diseases, and associations among them, and calculated similarities of each pair. The propagation process flows from query to target networks with query nodes through a path, including propagation within and between networks. Vectors of the query and target were obtained to calculate similarity scores between them. Because of this query-target propagation step, it enables both drug-disease and disease-drug prioritization, meaning that drugs can be queried for new indications and diseases can be queried for effective medications. Luo et al. [253] presented a prediction method named MBiRW that applied a bi-random walk algorithm on the two-layer drug-disease heterogeneous network constructed by similarity measures, calculated based on each property. Each random walk process was conducted for both drug and disease networks, respectively. Outputs of both walks were averaged to represent the probability that a drug associates with a disease. They found novel disease-drug relationships such as Alzheimer's and levodopa. Luo et al. [254] constructed a heterogeneous network of six subnetworks containing information of drug, disease, target, and each relation between them. Here, they additionally used target protein information of which similarities were calculated based on amino acid sequences. The random walk process was conducted on the heterogeneous network with multiple transition matrices, considering a transition from one type of network to the other, to prioritize candidate drugs for diseases. Lastly, based on multiple similarities, Yan et al. introduced similarity selection by information entropy [255] . They analyzed correlations between drug and disease similarities after calculating similarity fusion through information entropy of each similarity measure. The final similarity matrix was calculated after adjusting similarity values, considering the range of values; they used this to construct the heterogeneous network. A bi-random walk algorithm was applied on heterogeneous networks to predict new drug-disease pairs. Machine learning (ML) techniques have been applied for drug repositioning, giving reliable performance. ML helps to discover repositioning candidates by learning patterns in drug-disease associations. There are also various ML algorithms that are evolving rapidly. Similarities between drugs and diseases are also commonly combined as features. Gottlieb et al. [256] conducted multiple drug-drug and disease-disease similarity measures to construct features that discriminate and represent drug-disease associations. They then constructed a classification model named PREDICT using a logistic regression algorithm. Napolitano et al. [257] focused on drug-centered repositioning by predicting therapeutic drug classes. They used gene expression signatures collected from CMap that show the use of transcriptomic data in drug repositioning, drug structures, and target proteins to calculate similarities of drugs. Then they combined drug similarities to train multiclass SVM model for drug-disease association prediction. There is also another SVM based model, constructed with drug structures, that targets and side effects information. [258] Kim et al. [259] constructed a drug-disease association prediction model with five ML algorithms considering both linear and nonlinear algorithms, using similarities as features representing drug-disease pairs. Besides supervised models, there are also studies using unsupervised algorithms [245] . Hameed et al. used four clustering algorithms on a drug network to predict drug ATC classes. Moreover, feature construction using heterogeneous networks has been conducted for more informative features. Zhang et al. [260] proposed the network topological similarity-based inference method to predict new drugdisease associations by using a novel representation of drugs and diseases processed from the drug-disease bipartite network. Three ML algorithms, MLP, SVM, and RF, were adopted to classify the associations. Le et al. [261] presented a semi-supervised model using an integrated drug-targetdisease network to overcome the limitation of supervised models because of uncertain negative associations. They integrated each similarity network and a bipartite network of drug-disease associations and then adopted a regularized least square algorithm. Moreover, matrix factorization methods were applied to drug-disease association matrices to find novel associations. Luo et al. [254] proposed a drug repositioning recommendation system through a heterogeneous drug-disease network, and a matrix completion algorithm was introduced to fill a drug-disease association matrix and identify potential treatments for diseases. Xuan [262] presented DisDrugPred using a method based on non-negative matrix factorization. The method integrates prior knowledge of drugs and diseases, which were represented by similarity measures with association information, then constructed a drug-disease association matrix to predict novel associations. Deep learning (DL), one of the ML approaches, has shown dramatic performance in various fields. In drug repositioning, the feature extraction process is needed because of the size of the feature dimension resulting from large datasets. However, DL does not require a feature extraction step, and has the advantage of discovering latent features in complex drug-disease networks. DL can be used to construct either features or prediction models. When constructing features, DL approaches can be used by embedding information into latent representations, having the advantage of low dimension and heterogeneity. Wei et al. [263] adopted a network embedding algorithm to learn latent representations from several biomedical resources. Previously well-studied network embedding models were applied to reduce the noise and high dimension caused by adjacency matrices. Then, the SVM model was trained with constructed features to predict drug-disease associations. In contrast, Moridi et al. [264] focused on the construction of efficient representation of drugs by using various DNN architectures for each feature type. They used drug structures, targets, related enzymes, and gene expression profiles for drug features obtained from DrugBank, PubChem, and CMap. They applied variational autoencoder [209] and stacked autoencoders [265] on drug structures, gene expression data, and ProtVec [266] on protein and enzyme sequences to embed drug-related features. Donner et al. [267] also proposed a deep embedding method using LINCS [268] gene expression data. They used standardized expression values of landmark genes as input and constructed for various prediction tasks, including drug repositioning with multiple hidden layers. This study shows the application of transcriptomic data and DL approach improved the prediction model performance. Besides feature processing, recent papers constructed DNN models for predicting drug-disease associations. You et al. [269] combined the LASSO model with the DNN model, each used for feature extraction and prediction of drug-target interactions. They applied the constructed model to predict drug repositioning for breast cancer by identifying drugs that target risk genes of breast cancer. Aliper et al. [270] constructed a DNN model on large transcriptional datasets to classify drug therapeutic classes based on transcriptional profiles. They collected gene expression data from LINCS focusing on three cell lines and processed them for pathway level analysis and landmark gene-level analysis. Compared to an SVM model, the DNN model outperformed on a multiclass classification problem. Zeng et al. [271] proposed a method named deepDR that learns high-level features from a heterogeneous network generated by integrating 10 networks via a multi-model deep autoencoder. Learned features were then decoded to predict repositioning candidates. Xuan et al. [272] presented a novel model that was based on CNN to capture local representation from feature matrices and GRU, to learn path representation from drug-disease paths. The model outperformed other previous studies. A major limitation in in silico drug repositioning is the quality and quantity of data. The known repositioned drugs and related information are limited. Most of the research relied on a relatively small dataset. Moreover, using heterogeneous data can cause small datasets because not all drugs or diseases may have information of interest. This lack of data can induce limited predictions because only drug-disease associations related to data used for training can be predicted, especially when using network-based approaches. For DL approaches, the small data size can cause an overfitting problem. Therefore, qualitatively and quantitatively improved multi-omics data are needed for broader coverage and better model applicability [255, 273] . Moreover, multiple network types of data such as signaling networks and interaction networks need to be provided [274, 275] . To overcome the limitation of available data, there has been another approach in drug repositioning using Electronic Health Records (EHR) or patient history, PubMed abstracts, and ClinicalTrials.gov by using text mining algorithms [276] [277] [278] [279] . Recent DL techniques can be applied to drug repositioning to discover underlying drug-disease associations in complex networks [280] . Despite the fast growth of DL, there are few studies applied elaborate DNN architectures to extract meaningful results. There have been many efforts in dealing with graph structures with DNN, and these may help identify novel drug-disease associations. Drug repositioning studies involve a variety of data to represent drugs and diseases, as do computational approaches used in drug discovery including genome, proteome, interactome, gene expression, chemical structures, and properties. To predict novel drug-disease associations efficiently, it is necessary to represent compounds, diseases, and their interactions. There are several databases available for drug repositioning. As drugs and diseases can be represented by various data types, we categorized databases into two groups: drug-centric and disease-centric databases. Detailed data sources are described below and in Table 6 . In computational drug discovery, the similar property principle is commonly assumed, which means that similar drugs bind similar proteins and further show similar biological activities. Under this hypothesis, structural information is the most used data to represent compounds. To represent compounds, SMILES string and various molecular fingerprints are used, indicating substructural information. Molecular fingerprints are generated by cheminformatics software such as RDKit, and are mainly fragment-based or circular-based, representing overall compound structures [281] . Moreover, 3D molecular structures can be, although not always available, informative when focused on binding [282] . Besides, compound properties are informative features to represent characteristics of drugs which can be derived from databases and software [283, 284] . Drug-related genes and target proteins are also used to represent drug properties since multiple genes and proteins are involved when drugs are taken. Therefore, drug-target interactions and further genomic network information are also commonly adopted in drug repositioning. The last data type of drug-centric database is the drug side effects and indication information. These phenotypic data are also related to diseases, containing pharmacology information. From DailyMed and DrugCentral, we can obtain overall drug information, including original drug indications [285] [286] [287] . Side effects are negative effects of drugs which imply underlying drug mechanisms and biological pathways. SIDER [288] provides adverse drug Table 6 . List of data resources for drug repositioning Database Description Quantity PROMISCOUS [332] Contains information on drugs with related targets and side effects 25 K drugs, 23 K drug-target interactions, 1.4 K side-effects DPDR-CPI [333] Contains information on drugs and 611 human protein targets Predicts off-targets and potential indications 2.5 K drugs, 611 targets repoDB [334] Contains drugs including successes, failures, and 2,051 related diseases 1.5 K drugs, 2 K diseases RepurposeDB [335] Contains repositioned drugs and diseases 256 drugs, 1.1 K indications e-Drug3D [282] Contains molecular structures of FDA drugs approved between 1939 and 2019 PubChem [310] An integrated chemistry database. It contains from small to large molecules with structures, physical properties, bioactivities, patents etc. ChemSpider [284] Chemical structure database with fast text and structure search access 81 M chemical structures DrugBank [313] A free, comprehensive drugs and drug targets database. It contains various chemical and target information for each drug. 13 K compounds, 5.1 K targets DrugCentral [285] Comprehensive drug information source for approved drugs including indications and drug mode of action 4.5 K active ingredients, 77 K FDA drug labels PharmGKB [301] Comprehensive drug information source for approved drugs including indications and drug mode of action 680 drugs, 149 pathways, 22 K variant annotations KEGG [314] Databases resource for understanding high-level functions and utilities of biological system 18 K metabolites and small molecules, 11 K drugs, 7.7 K enzymes. SIDER [288] A database of marketed medicines and their recorded adverse drug reactions (ADRs) 1.4 K drugs, 5.8 K serious events ADReCS [291] Contains 669,104 drugs and ADR pairs mined from the FDA Adverse Event Reporting System 2.5 K drugs, 10 K ADRs TWOSIDES [289] Comprehensive database for drug-drug-effect relationships 3.3 K drugs, 17 K ADR types CMap [79, 292] A library containing gene expression profiles from small molecule compounds tested in multiple cell types 1.5 M gene expression profiles, 5 K small-molecule compounds, 3 K genetic reagents Gene Expression Omnibus [293] Database of high throughput gene expression profiles 3.3 M samples ArrayExpress [295] Database of microarray gene expression profiles 2.4 M assays Genomics of Drug Sensitivity in Cancer (GDSC) [296] Contains screenings of 1,000 human cancer cell lines with over 100 compounds 809 Cell lines, 175 Compounds, 118 K IC50s CCLE [34] Contains mRNA expression and mutation data of over 1,100 cancer cell lines 1.4 K cell lines, 84 K genes, 1 M mutation data DGIdb [336] Database for drug-gene interactions and potential druggability 10 K drugs, 40 K genes DisGeNET [25] Collects disease-gene, disease-variant associations with homogeneous annotation 628 K gene-disease associations, 17 K genes, 24 K diseases, 210 K variant-disease associations The Human Phenotype Ontology (HPO) [337] The standardized vocabulary of phenotypic abnormalities in human disease 13 K terms, 156 K annotations to hereditary diseases reactions (ADRs) of marketed drugs, while OFFSIDES and TWOSIDES [289] provides information on side effects not listed on the official FDA labels and negative drug-drug interactions constructed by preprocessing the FDA Adverse Event Reporting System (FAERS) [290] . There is also another database of ADRs recently updated by integrating public medical repositories [291] . Drug-GDA are important in drug repositioning because the repositioning can be conducted by finding novel targets in terms of both drugs and diseases. Transcriptional signatures can be used to represent and link drug-disease associations. The Connectivity Map (CMap) provides gene expression profiles of over 5,000 small molecule compounds tested on multiple cell lines [79, 292] . CMap data presents drug effects on various disease conditions. Moreover, the GEO from NCBI [293, 294] and ArrayExpress from EBI [295] provide gene expression data yielded from hundreds of disease conditions in various species. These raw expression data can be processed for disease signatures. Moreover, the Cancer Cell Line Encyclopedia (CCLE) [34] and Genomics of Drug Sensitivity in Cancer (GDSC) [296] can be adopted to drug repositioning studies in cancer. To represent diseases, Unified Medical Language System (UMLS) IDs [297] , Medical Subject Headings (MeSH) terms [298] or ontologies are used, and further applied to discriminate between them. Disease-gene relation data are important to connect drugdisease associations as with drug-gene relations. DisGeNet provides information on disease-gene and disease-variant associations [23, 25, 299] . Since targeted genes or proteins interact with other genes and proteins, their interactions are important. Protein-protein interactions, gene-gene interactions, and further related pathways carry information on the biological activities between drugs and diseases [38, 300, 301] . Finally, gene mutations and variants can be enlisted, particularly when studying disease-centric drug repositioning. COSMIC provides somatic mutations in human cancer, which can describe cancer-specific disease characteristics [302] . Other resources dbGAP, dbSNP, and dbVar from NCBI contain information on human genetic variations [303] [304] [305] . AI technology can be applied to a wide range of applications. The widely used AI algorithms, particularly deep learningbased algorithms, were primarily developed in the fields of computer vision, natural language processing, and acoustic signal processing. However, because of the reasons here, applying fancy AI techniques to the drug discovery process is quite challenging. First, the drug discovery process is very complicated and it involves specialized knowledge in a variety of fields (biology, chemistry, and medicine; among others.). Second, the drug discovery process requires compelling evidence for decision making because it directly affects public health and the pharmaceutical industry's net profits. Nevertheless, many researchers proved the fact that the future of drug discovery with AI technology is obviously promising by their tremendous efforts that are covered in this review. Still, the discrepancy between the two domains is a big hurdle. Therefore, AI experts and other domain experts will need to collaborate closely to develop 'drugdiscovery-specific' AI technology for real advances in the current drug discovery. AI experts will need to understand the characteristics of drug discovery data and make an effort to develop appropriate and interpretable algorithms that can explain the modes of action, to provide evidence for further decision making. Other domain experts will need to generate biological and chemical data with minimal experimental errors and store them in unified platforms for further improvements to the AI systems. However, the most important thing for both groups is to be open to working together and actively communicating to construct a concrete framework for a new revolution in drug discovery. We hope this review provides a good starting point for closing this gap. 
