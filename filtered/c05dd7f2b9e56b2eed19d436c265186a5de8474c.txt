c05dd7f2b9e56b2eed19d436c265186a5de8474c
The Hierarchy Problem: From the Fundamentals to the Frontiers
Santa  Barbara 

University of California 
My thanks go first and foremost to Nathaniel Craig for his continual support and encouragement. From Nathaniel I learned not only an enormous amount of exciting physics, but also how to prepare engaging lessons and talks, how to be an effective mentor, how to think intuitively about the natural world and determine the particle physics to describe it, and how to be a caring, welcoming, community-focused academic. It goes without saying that Nathaniel's influence pervades every word written below and the physical understanding behind them. I would not have made it here were it not for the many senior academics who have charitably given their own time to encourage and support me. As a young undergraduate, Tom Lubensky's patient help during many office hours and his explicit encouragement for me to continue studying physics were vital. Cullen Blake took a risk on me as an undergrad with little to show but excitement and enthusiasm, taught me how to problemsolve like a physicist and a researcher, and truly made a huge difference in my life. And there were many such professors who kindly gave me their time and support-Mirjam Cvetič, Larry Gladney, Justin Khoury, H. H. "Brig" Williams, Ned Wright, and othersand if I listed all the ways they have all supererogatorily supported me and my education this section would be too long. In graduate school I have also benefited from the kindness of a cadre of senior academics. Dave Sutherland and John Mason spent hours engaging me in many elucidating conversations. Don Marolf generously included me in gravity theory activities and answered my endless questions. Nima Arkani-Hamed has graciously given me his time and made me feel welcome at every turn. And there have been many other particle theorists who have offered me their advice and support over the years-Tim Cohen, Patrick Draper, Matthew McCullough, and Flip Tanedo, among others. v Of course I have not worked alone, and little of this science would have been accomplished were it not for my many collaborators who have helped me learn and problemsolve and provided guidance and been patient when I was overwhelmed with being pulled in too many directions. Let me especially mention those undergrads I have spent significant time mentoring during my time in graduate school, namely Aidan Herderschee, Samuel Alipour-fard, and Umut Can Öktem. Indeed, they each took a chance on me as well, and working with them has taught me how to be a better teacher and physicist-not to mention all of the great science we worked out together. My physics knowledge would have also been stunted were it not for the countless hours spent discussing all manner of high energy theory with friends and peers-primarily The first four chapters of this thesis are introductory material which has not previously appeared in any public form. My intention has been to write the guide that would have been most useful for me toward the beginning of my graduate school journey as a field theorist interested in the hierarchy problem. My aim has been to make these chapters accessible to beginning graduate students in particle physics and interested parties in related fields-background at the level of a single semester of quantum field theory should be enough for them to be understandable in broad strokes. Chapter 1 introduces fundamental tools and concepts in quantum field theory which are essential for particle theory, spending especial effort on discussing renormalization from a variety of perspectives. Chapter 2 discusses the hierarchy problem and how to think about it-primarily through the pedagogical device of refuting a variety of common misconceptions and pitfalls. Chapter 3 introduces in brief a variety of classic strategies and solutions to the hierarchy problem which also constitute important frameworks in theoretical particle physics beyond the Standard Model. Chapter 4 discusses more-recent ideas about the hierarchy problem in light of the empirical pressure supplied by the lack of observed new physics at the Large Hadron Collider. Throughout I also make note of interesting research programs which, while they lie too far outside the main narrative for me to explain, are too fascinating not to be mentioned. The first half of this thesis is thus mostly an introduction to and review of material I had no hand in inventing. As always, I am 'standing on the shoulders of giants', and I have benefited enormously from the pedagogical efforts of those who came before me. When my thinking on a topic has been especially informed by a particular exposition, or when I present an example which was discussed in a particular source, I will endeavor to say so and refer to that presentation. As to the rest, it's somewhere between difficult and xii impossible to distinguish precisely how and whose ideas I have melded together in my own understanding of the topics-to say nothing of any insight I may have had myself-but I have included copious references to reading material I enjoyed as a guide. Ultimately this is a synthesis of ideas in high energy theory aimed toward the particular purpose of understanding the hierarchy problem, and I have attempted to include the most useful and pedagogical explanations of these topics I could find, if not invent. I then present some work on the subject by myself and my collaborators. Chapter 5 contains work constructing a viable cosmological history for mirror twin Higgs models, an exemplar of the modern Neutral Naturalness approach to the hierarchy problem. Chapter 6 focuses on searching for long-lived particles produced at particle colliders as a discovery channel for a broad class of such models. Chapter 7 is an initial exploration of a new approach to the hierarchy problem which follows a maximalist interpretation of the lack of new observed TeV scale physics, and so relies on questioning and modifying some core assumptions of conventional particle physics. In Chapter 8 we conclude with some brief parting thoughts. If you enjoy reading this work, or find it useful, or have questions, or comments, or recommendations for good references, please do let me know-at whatever point in the future you're reading this. As of autumn 2020, I can be reached at sethk@uchicago.edu. xiii Contents Effective field theory is simply the familiar strategy to focus on the important degrees of freedom when understanding a physical system. For a simple example from an introductory Newtonian mechanics course, consider studying the motion of balls on inclined planes in a freshman lab. It is neither necessary nor useful to model the short-distance physics of the atomic composition of the ball, nor the high-energy physics of special relativity. Inversely, it is also unnecessary to account for the long-distance physics of Hubble expansion or the low-energy physics of air currents in the lab. In quantum field theories this intuitive course of action is formalized in decoupling theorems, showing precisely the sense in which field theories are amenable to this sort of analysis: the effects of short-distance degrees of freedom may be taken into account as slight modifications to the interactions of long-distance degrees of freedom, instead of including explicitly those high-energy modes. Of course when one returns to the mechanics laboratory armed with an atomic clock and a scanning tunneling microscope, one begins to see deviations from the Newtonian predictions. Indeed, the necessary physics for describing a situation depends not only on the dynamics under consideration but also on the precision one is interested in attaining with the description. So it is crucial that one is able to correct the leading-order description by systematically adding in subdominant effects, as organized in a suitable power series in, for example, (v/c), where v is the ball's velocity and c is the speed of light. Of course when the full description of the physics is known it's in principle possible to just use the full theory to compute observables-but I'd still rather not begin with the QED Lagrangian to predict the deformation of a ball rolling down a ramp. The construction of an appropriate effective description relies on three ingredients. The first is a list of the important degrees of freedom which specify the system under consideration-in particle physics this is often some fields {φ i }. The second is the set of symmetries which these degrees of freedom enjoy. These constrain the allowed interactions between our fields and so control the dynamics of the theory. Finally we need a notion of power counting, which organizes the effects in terms of importance. This will allow us to compute quantities to the desired precision systematically. Frequently in effective field theories of use in particle physics this role is played by E/Λ, where E is an energy and Λ is a heavy mass scale or cutoff above which we expect to require a new description of the physics. We will often be interested in determining the appropriate description of a system at some scale, so it is necessary to understand which degrees of freedom and which interactions will be important as a function of energy. We can gain insight into when certain modes or couplings are important by studying the behavior of our system under scale transformations. Consider for example a theory of a real scalar field φ, with action If we wish to understand how the physics of this theory varies as a function of scale, we can perform a transformation x µ → sx µ and study the long-distance limit s 1 with x µ fixed. The measure transforms as d d x → s d d d x and the derivatives ∂ µ → s −1 ∂ µ . Then to restore canonical normalization of the kinetic term such that the one-particle states are properly normalized for the LSZ formula to work, we must perform a field redefinition φ(x) = s 2−d 2 φ (x ), and the action becomes As a reminder, in the real world (at least at distances 1 µm) we have d = 4. As you look at the theory at longer distances the mass term becomes more important, so is known as a 'relevant' operator. One says that the operator φ 2 has classical dimension ∆ φ 2 = 2. The quartic interaction is classically constant under such a transformation, so is known as 'marginal' with ∆ φ 4 = 0, and interactions with more powers of φ shrink at low energies and are termed 'irrelevant', e.g. ∆ φ 6 = −2. We have been careful to specify that these are the classical dimension of the operators, also called the 'engineering dimension' or 'canonical dimension', which has a simple relation to the mass dimension as for some operator O. If the theory is not scale-invariant then quantum corrections modify this classical scaling by an 'anomalous dimension' δ O (m 2 , λ, τ, . . . ) which is a function of the couplings of the theory, and the full behavior is known as the 'scaling dimension'. The terms 'marginally (ir)relevant' are used for operators whose classical dimension is zero but whose anomalous dimensions push them to one side. The connection to the typical EFT power counting in (E/Λ) is immediate. In an EFT with UV cutoff Λ, it's natural to normalize all of our couplings with this scale and rename e.g. τ →τ Λ 6−2d whereτ is now dimensionless. It's easy to see that the longdistance limit is equivalently a low-energy limit by considering the derivatives, which pull down a constant p µ and scale as s −1 -or by simply invoking the uncertainty principle. Operators with negative scaling dimension contribute subleading effects at low energies precisely because of these extra powers of a large inverse mass scale. Then he made the tank of cast metal, 10 cubits across from brim to brim, completely round; it was 5 cubits high, and it measured 30 cubits in circumference. God on the merits of working to finite precision 1 Kings 7:23, Nevi'im New Jewish Publication Society Translation (1985) [23] The procedure of writing down the most general Lagrangian with the given degrees of freedom and respecting the given symmetries up to some degree of power counting is termed 'bottom-up EFT' as we're constructing it entirely generally and will have to fix coefficients by making measurements. A great example is the Standard Model Effective Field Theory (SMEFT), of which the Standard Model itself is described by the SMEFT Lagrangian at zeroth order in the power counting. It is defined by being an SU (3)×SU (2)×U (1) gauge theory with three generations of the following representations of left-handed Weyl fermions: In addition the Standard Model contains one scalar, the Higgs boson, which is responsible for implementing the Anderson-Brout-Englert-Guralnik-Hagen-Higgs-Kibble-'t Hooft mechanism [24, 25, 26, 27, 28, 29] to break the electroweak symmetry SU (2) L × U (1) Y down to electromagnetism U (1) Q at low energies: The Standard Model Lagrangian contains all relevant and marginal gauge-invariant operators which can be built out of these fields, and has the following schematic form with F a gauge field strength, ψ a fermion, D the gauge covariant derivative in the kinetic term Lagrangian on the first line, and the second line containing the Higgs' Yukawa couplings and self-interactions. If a refresher on the Standard Model would be useful, the introduction to its structure toward the end of Srednicki's textbook [18] will suffice for our purposes, while further discussion from a variety of perspectives can be found in Schwartz [22] , Langacker [30] , and Burgess & Moore [31] . The SMEFT power-counting is in energies divided by an as-yet-unknown UV scale Λ, so the dimension-n SMEFT Lagrangian consists of all gauge-invariant combinations of these fields with scaling dimension n − d. At dimension five there is solely one operator, L (5) = (LH) 2 /Λ + h.c., which contains a Majorana mass for neutrinos. In even-moreschematic form, the dimension six Lagrangian contains operators with the field content where for aesthetics we have multiplied through by the scale Λ and haven't bothered writing down couplings. After understanding the structure of the independent symmetrypreserving operators (see e.g. [32, 33, 34] ), the job of the bottom-up effective field theorist is to measure or constrain the coefficients of these higher-dimensional operators [35] . Useful data comes from both the energy frontier with searches at colliders for the production or decay of high-energy particles through these higher-dimensional operators and from the precision frontier measuring fundamental processes very well to look for deviations from the Standard Model predictions (e.g. [36, 37, 38] ). For more detail, see the introduction to SMEFT by Brivio & Trott [39] . Another approach is possible when we already have a theory and just want to focus on some particular degrees of freedom. Then we may construct a 'top-down EFT' by taking our theory and 'integrating out' the degrees of freedom we don't care about-for example by starting with the Standard Model above and getting rid of the electroweak bosons to focus on processes occurring at lower energies (e.g. Fermi's model of the weak interaction [40] ). We can't necessarily just ignore those degrees of freedom though; what we need to do is modify the parameters of our EFT such that they reproduce the results of the full theory (to some finite precision) using only the low-energy degrees of freedom. Such a procedure can be illustrated formally by playing with path integrals. Consider the partition function for a theory with some light fields φ and some heavy fields Φ: This contains all of the physics in our theory, and so in principle we may use it to compute anything we wish. But if we're interested in low-energy processes involving solely the φ fields, we could split up our path integral and first do the integral over the Φ fields. The light φ fields are the only ones left, so we can then write the partition function as where this defines S eff . Thus far this still contains all the same physics, as long as we don't want to know about processes with external heavy fields 1 . But having decided that we are interested in the infrared physics of the φ fields, we can say that the effects of the heavy Φ fields will be suppressed by factors of the energies of interest divided by the mass of Φ, and we should expand the Lagrangian L eff in an appropriate series: where L 0 (φ) is the part of the full Lagrangian that had no heavy fields in it, O is a dimensionless coupling, and N ≥ d defines the precision to which one works in this effective theory. This is the procedure to find a top-down effective field theory in the abstract. 1 Since we haven't made any approximations and have the same object Z, one may be confused as to why we've lost access to the physics of the Φ fields. In fact I've been a bit sloppy. If we want to compute correlation functions of our fields φ, we must couple our fields to classical sources J φ as L ⊃ φ(x)J φ (x). Physically, those sources allow us to 'turn on' particular fields so that we can then calculate their expectation values. Mathematically, we really need the partition function as a functional of these sources Z[J φ ], and we take functional derivatives with respect to these sources as a step to calculating correlation functions or scattering amplitudes. In integrating out our heavy field Φ, we no longer have a source we can put in our Lagrangian to turn on that field, as it no longer appears in the action. A great example of a top-down EFT is in studying the Standard Model fields in the context of a Grand Unified Theory (GUT). Broadly, Grand Unification is the hope that there is some simpler, more symmetric theory behind the Standard Model which explains its structure. A GUT is a model in which the gauge groups of the SM are (partially [41] ) unified in the UV. If there is a full unification to a single gauge factor, then this requires 'gauge coupling unification' in the UV until the symmetry is broken down to the SM gauge group at a high scale [42] . While one's first exposure to this idea today may be in the context of a UV theory like string theory which roughly demands such unification, this was in fact first motivated by the observed infrared SM structure. It is frankly amazing that not only are the values of the SM gauge couplings consistent with this idea, and not only does SU (3) × SU (2) × U (1) fit nicely inside SU (5), but the SM fermion representations precisely fit into the 10 ⊕5 representations of SU (5) (see Figure 1 .2). It's hard to imagine a discovery that would have felt much more like one was obviously learning something deep and important about Nature than when Georgi realized how nicely all of this worked out. I'm reminded of Einstein's words on an analogous situation in the early history of electromagnetism-the original unified theory: The precise formulation of the time-space laws of those fields was the work of Maxwell. Imagine his feelings when the differential equations he had formulated proved to him that electromagnetic fields spread in the form of polarised waves, and with the speed of light! To few men in the world has such an experience been vouchsafed. At that thrilling moment he surely never guessed that the riddling nature of light, apparently so completely solved, would continue to baffle succeeding generations. Considerations concerning the Fundaments of Theoretical Physics, 1940 [43] And just as with Maxwell, the initial deep insight into Nature was not the end of the story. As of yet, Grand Unification remains an unproven ideal, and indeed further empirical data has brought into question the simplest such schemes. But it's hard to imagine all of this beautiful structure is simply coincidental, and I would wager that most high energy theorists still have a GUT in the back of their minds when they think about the UV structure of the universe, so this is an important story to understand. To learn generally about GUTs, I recommend the classic books by Kounnas, Masiero, Nanopoulos, & Olive [44] and Ross [45] or the recent book by Raby [46] for the more formally-minded. Shorter introductions can be found in Sher's TASI lectures [47] or in the Particle Data Group's Review of Particle Physics [48] from Hebecker & Hisano. The structure of the simplest SU (5) GUT is that the symmetry group breaks down to the SM at energies M GU T ∼ 10 16 GeV via the Higgs mechanism. 2 More generally, unifica-tion may proceed in stages as, for example, SO(10) → SU (4) c ×SU (2) L ×SU (2) R → SM , and the breaking may occur via other mechanisms, as we'll discuss further in Section 3. Back to our simple single-breaking example, as is familiar in the SM this means that the gauge bosons corresponding to broken generators get masses of order this GUT-breaking scale. As this is a far higher scale than we are currently able to directly probe, it is neither necessary nor particularly useful to keep these degrees of freedom fully in our description if we're interesting in understanding the effects of GUT-scale fields. Rather than constructing the complete top-down EFT of the SM from a GUT, let's focus on one particularly interesting effect. One of the best ways to indirectly probe GUTs is by looking for proton decay. The GUT representations unify quarks and leptons, so the extra SU (5) gauge bosons have nonzero baryon and lepton number and fall under the label of 'leptoquarks'. It's worth considering in detail why proton decay is a feature of GUTs and not of the SM, as it's a subtler story than is usually discussed. While U (1) B , the baryon number, is an accidental global symmetry of the SM 3 , it's an anomalous symmetry and so is not a symmetry of the quantum world. The 'baryon minus lepton' number, U (1) B−L , is non-anomalous, but this is a good symmetry both of the SM and of a GUT and clearly does not prevent e.g. p + → π 0 e + . What's really behind the stability of the proton is that, though U (1) B and U (1) L are not good quantum symmetries, the fact that they are good classical symmetries means their only violation is nonperturbatively by instantons. Such configurations yield SM charges affect slightly how the couplings evolve toward high energies. Indeed, adding supersymmetry makes the intersection of the three curves even more accurate than it is in the SM itself. 3 'Accidental' here means that imposing this symmetry on the SM Lagrangian does not forbid any operators which would otherwise be allowed. The SM is defined, as above, by the gauge symmetries SU (3) C × SU (2) L × U (1) Y and the field content. Writing down the most general dimension-4 Lagrangian 1.3,1.4 invariant under these symmetries gives a Lagrangian which is automatically invariant under U (1) B . This no longer holds at higher order in SMEFT, and indeed the dimension-6 Lagrangian (Equation 1.5) does contain baryon-number-violating operators. If one wants to study a baryon-numberconserving version of SMEFT, one needs to explicitly impose that symmetry on the dimension-6 Lagrangian, so U (1) B is no longer an accidental symmetry of SMEFT. But in GUTs, baryon number and lepton number are no longer accidental symmetries, so no such protection is available and the GUT gauge bosons mediate tree-level proton decay processes as in Figure 1 .3. We can find the leading effect by integrating these out- 4 Convincing yourself fully that ∆B = 3 is the smallest allowed transition is not straightforward, but let me try to make it believable for anyone with some exposure to anomalies and instantons. The existence of a mixed U (1) B SU (2) 2 L anomaly-equivalently a nonvanishing triangle diagram with two SU (2) L gauge legs and a baryon current insertion-means that the baryon current will no longer be divergenceless, ∂ µ j µ B ∝ g 2 32π 2 trW W , where W µν a is the field strength andW its Hodge dual. Instantons are field configurations interpolating between vacuum field configurations of different topology, and there are nontrivial instantons in 4d Minkowski space for SU (2) L but not for U (1) Y as a result of topological requirements on the gauge group. So while there is also a mixed anomaly with hypercharge, we can ignore this for our purposes. The SM fermions contributing to the U (1) B SU (2) 2 L anomaly are then only Q a (the left-handed quark doublet with B = 1/3, with a a color index) and similarly for the lepton anomaly only the doublet L matters. There are three generations of each, which leads simply to a factor of three in the divergence of the global currents. Thinking about it in terms of triangle diagrams, this is simply because there are thrice as many fermions running in the loop. The extent to which an instanton solution changes the topology is given by the integral of g 2 32π 2 trW W over spacetime, which as a total derivative localizes to the boundary, and furthermore turns out to be a topological invariant of the gauge field configuration known as the winding number, an integer (technically the change in winding number between the initial and final vacua). Then the anomaly, by way of the nonvanishing divergence, relates the winding number of such a configuration to the change in baryon and lepton number it induces. The factor of the number of generations means that each unit of winding number ends up producing ∆B = −∆L = n g = 3. And that is why the proton is stable. Classic, detailed references on anomalies and instantons include Coleman [49] , Bertlmann [50] , and Rajamaran [51] . in particular we'll here look just at a four-fermion baryon-number-violating operator. The tree-level amplitude is simply The job of the top-down effective field theorist is to calculate the effects of some particular UV physics on IR observables and by doing so understand how to search for their particular effects. While the effects will, by necessity, be some subset of the operators that the bottom-up effective field theorist has written down, the patterns and correlations present from a particular UV model can suggest or require particular search strategies. In the present context, a GUT may suggest the most promising final states to look for when searching for proton decay. If we wanted to calculate the lifetime and branching ratios more precisely we would have to deal with loop diagrams (among many complications), which of course is a generic feature. So we now turn our attention to the new aspects and challenges of field theory that appear once one goes beyond tree-level. Renormalization is a notoriously challenging topic for beginning quantum field theorists to grok, and explanations often get bogged down in the details of one particular perspective or scheme or purpose and 'miss the forest for the trees', so to speak. 5 We'll attempt to overcome that issue by discussing a variety of uses for and interpretations of renormalization, as well as how they relate. And, of course, by examining copious examples and pointing out a variety of conceptual pitfalls. At the outset the only fact one needs to have in mind is that renormalization is a procedure which lets quantum field theories make physical predictions given some physical measurements. Such a procedure was not necessary for a classical field theory, which is roughly equivalent to a quantum field theory at tree-level. A natural question for beginners to ask then is why we should bother with loops at all: Why don't we just start off with the physical, measured values in the classical Lagrangian and be done with it? That is, if we measure, say, the mass and self-interaction of some scalar field φ, let's just define our theory for some definitions of these physical parameters, and compute everything at tree-level. However, this does not constitute a sensible field theory, as the optical theorem tells us this is not consistent. We define S = 1 + iM as the S-matrix which encodes how states in the theory scatter, where the 1 is the 'trivial' no-interaction part. Quantum mechanics Sandwiching this operator equation between initial and final states, we find that the left hand side is the imaginary part of the amplitude M(i → f ), which is nonzero solely due to loops. This is depicted schematically in Figure 1 .4. We can see why this is by examining a scalar field propagator. Taking the imaginary part one finds (1. 13) This vanishes manifestly as → 0 except for when p 2 = −m 2 , and an integral to find the normalization yields (1.14) So internal propagators are real except for when the particle is put on-shell. In a tree-level diagram this occurs solely at some measure-zero set of exceptional external momenta, but in a loop-level diagram we integrate over all momenta in the loop, so an imaginary part is necessarily present. Now we see the necessity of loops solely from the conservation of probability and the framework of quantum mechanics 6 . The lesson to take away from this is that classical field theories produce correlation functions with some particular momentum dependence, which can be essentially read off from the Lagrangian. But a consistent theory requires momentum dependence of a sort that does not appear in such a Lagrangian, which demands that calculations must include loops. In particular it is the analyticity properties of these higher-order contributions that are required by unitarity, and there is an interesting program to understand the set of functions satisfying those properties at each loop order as a way to bootstrap the structure of multi-loop amplitudes (see e.g. [54, 55, 56, 57, 58, 59] ). So far from being 'merely' a way to deal with seemingly unphysical predictions, renormalization is very closely tied to the physics. We begin in the next section with understanding its use for removing divergences, as this is the most basic application and is often the first introduction students receive to renormalization. We will then move on to discuss other, more physical interpretations of renormalization. As a first pass, let's look again at a φ 4 theory and now treat it properly as a quantum field theory. As a simple example, let us consider 2 → 2 scattering in this theory, our discussion of which is particularly influenced by Zee [20] . At lowest-order this is extremely simple, and the tree-level amplitude is iM(φφ → φφ) = −iλ 0 . But if we're interested in a more precise answer, we go to the next order in quite rigid. After Hawking-motivated by black hole evaporation-proposed that the scattering matrix in a theory of quantum gravity should not necessarily obey unitarity [52] , the notion of modifying the S-matrix to a non-unitary '$'-matrix (pronounced 'dollar matrix') received heavy scrutiny. This was found to necessarily lead to large violations of energy conservation, among other maladies [53] . Just from power-counting, we can already see that this diagram will be divergent. In the infrared, as k → 0, the diagram is regularized by the mass of the field, but in the ultraviolet k → ∞, the integral behaves as ∼ d 4 k/k 4 ∼ dk/k which is logarithmically divergent. Though one might be tempted now to give up, we note that this divergence is appearing from an integral over very high energy modes-far larger than whatever energies we've verified our φ 4 model to, so let's try to ignore those modes and see if we can't get a sensible answer. The general term for removing these divergences is 'regularization' and we will here regularize (or 'regulate') this diagram by imposing a hard momentum cutoff Λ in Euclidean momentum space, which is the maximum energy of modes we let propagate in the loop. The loop amplitude may then be calculated with elementary methods detailed in, for example, Srednicki's textbook [18] . First we introduce Feynman parameters to combine the denominators, using (AB) −1 = where we've skipped the algebra letting us rewrite this with q = k + xP s and D = x(1 − x)P 2 s + m 2 0 . The change of variables k → q has trivial Jacobian, so the next step is to Wick rotate-Euclideanize the integral by defining q 0 = iq d , such that q 2 ≡ q µ η µν q ν = q µ δ µνq ν ≡q 2 . The measure simply picks up a factor of i, d d q = id dq , and we can then = − 1 2 (1.20) In fact it is possible to do the x integral analytically here, but we'll take Λ 2 |P 2 | m 2 to find a simple answer Now putting all that together and including all the diagrams up to one-loop, we get the form M(φφ → φφ) = −λ 0 + Cλ 2 0 log Λ 2 s + log Λ 2 t + log Λ 2 u + subleading, (1.22) where s, t, u are the Mandelstam variables and C is just a numerical coefficient. Now we see explicitly that the divergence has led to dependence of our amplitude on our regulator Λ. Of course this is problematic because we introduced Λ as a non-physical parameter, and it would not be good if our calculation of a physical low-energy observable depended sensitively on how we dealt with modes in the far UV. But let us try to connect this with an observable anyway. We note that the theory defined by the Lagrangian in Equation 1 . 15 can not yet be connected to an observable because we have not yet given a numerical value for λ 0 . So let's imagine an experimentalist friend of ours prepares some φs and measures this scattering amplitude at some particular angles and energies corresponding to values of the Mandelstam variables s 0 , t 0 , u 0 . They find some value λ phys , which is a pure number. If our theory is to describe this measurement accurately, this tells us a relation between our parameters and a physical quantity This is known as a 'renormalization condition' which tells us how to relate our quantum field theories to observations at non-trivial loop order. Since the left hand side is a physical quantity, it may worry us that the right hand side contains a non-physical parameter Λ. But we still haven't said what λ 0 is, so perhaps we'll be able to find a sensible answer if we choose λ 0 ≡ λ(Λ) in a correlated way with our regularization scheme. We call this 'promoting λ to a running coupling' by changing from the 'bare coupling' λ 0 to one which depends on the cutoff. So let's solve for λ in terms of λ phys and Λ. Rearranging we have where in the second line the replacement λ 2 0 → λ 2 phys modifies the right side only at higherorder and so that is absorbed into our O(λ 3 ) uncertainty. To see what this has done for us, let us plug this back into our one-loop amplitude Equation 1. 22 . This will impose our renormalization condition that our theory successfully reproduces our experimentalist friend's result. We find M(φφ → φφ) = −λ phys − Cλ 2 phys log where again we liberally shunt higher-order corrections into our uncertainty term. Taking advantage of the nice properties of logarithms, we rearrange to get M(φφ → φφ) = −λ phys − Cλ 2 phys log s s 0 + log t t 0 + log u u 0 + O(λ 3 phys ). (1.27) We see that our renormalization procedure of relating our theory to a physical observable has enabled us to write the full amplitude in terms of physical quantities, and remove the divergence entirely. This one physical input at some particular kinematic configuration has enabled us to fully predict any 2 → 2 scattering in this theory. We thus see how the renormalization procedure removes the divergences in a naïve formulation of a field theory and allows us to make finite predictions for physical observables. While we did need to introduce a regulator, once we make the replacement λ 0 → λ(Λ) as defined in Equation 1.25 (and similar replacements for the coefficients of the other operators), the one-loop divergences are gone. We are guaranteed that any one-loop correlation function we calculate is finite in the Λ → ∞ limit, which removes the regulator. If we wanted to increase our precision and calculate now at two loops, we would first renormalize the theory at two loops analogously to the above, and would find a more precise definition for λ(Λ) which included terms of order O(λ 3 phys ). At each loop order, replacing the bare couplings with running couplings suffices to entirely rid the theory of divergences. 20 An important question is for which quantum field theories do a finite set of physical inputs allow the theory to be fully predictive, in analogy to the example above. Such a theory is called 'renormalizable' and means that after some finite number of experimental measurements, we can predict any other physics in terms of those values. Were this not the case, and no finite number of empirical measurements would fix the theory, it would not be of much use. Within the context of perturbation theory, a theory will be renormalizable if its Lagrangian contains solely relevant and marginal operators, and indeed for our φ 4 theory three renormalization conditions are needed-one for each such operator. The simplest way to understand why we must restrict to relevant and marginal operators is that irrelevant operators inevitably lead to the generation of a tower of moreand-more irrelevant operators. To see this, imagine now including a φ 6 interaction, as depicted in Figure 1 .6a. At one loop this leads to a 4 → 4 scattering process with the same sort of divergence we saw in our previous loop diagram. So this loop is probing the UV physics, but we cannot absorb the unphysical divergence into a local interaction in our Lagrangian unless we now include a φ 8 term. But then we can draw a similar one-loop diagram with the φ 8 interaction which will require a φ 10 interaction, and so on. Note that in our φ 4 theory we also have 4 → 4 scattering at one loop, seen in Figure   1 .6b, but there it comes from a box diagram which is finite, and so there is no need to include more local operators. However, we emphasized above that the most useful description of a system depends on the precision at which one wishes to measure properties of the system. Thus in the study of effective field theories a broader definition of renormalizability should be used. For a theory with cutoff Λ, one decides to work to precision O(E/Λ) n where E is a typical energy scale of a process and n is some integer. There are then a finite number of operators which contribute to processes to that precision-only those up to scaling dimension n-and so there is a notion of 'effective renormalizability' of the theory. We still require solely a finite number of inputs to set the behavior of the theory to whatever precision we wish, but such a theory nevertheless fails the original criterion, which may be termed 'power counting renormalizability' in comparison. Above we characterized our cutoff as an unphysical parametrization of physics at high scales that we do not know and we found that its precise value dropped out of our physically observable amplitude. To some extent this is rather surprising, as it's telling us that the high energy modes in our theory have little effect on physics at long distanceswe can compensate for their effects by a small shift in a coupling. We can gain insight into the effects of these high energy modes by taking the cutoff seriously and looking at what happens when the cutoff is lowered. This brilliant approach due to Wilson [61] is aimed at providing insight as to the particular effects of these high-energy modes by integrating out 'shells' of high-energy Euclidean momenta and looking at the low-energy effects. This discussion is closely inspired by that in Peskin & Schroeder's chapter 12 [19] , as well as Srednicki's chapter 29 [18] . It is easiest to see how to implement this by considering the path integral formulation. We can equally well integrate over position space paths as over momentum modes: and here it is clear that we may integrate over particular momentum modes separately if we so choose. In the condensed matter application in which Wilson originally worked, a cutoff appears naturally due to the lattice spacing a giving an upper bound on momenta k max ∼ 1/a. In a general application we can imagine defining the theory with a fundamental cutoff Λ by including in the path integral only modes with Euclidean momentum The theory is defined by the values of the parameters in the theory with that cutoff-our familiar relevant and marginal operators m 2 (Λ), λ(Λ) and in principle all of the 'Wilson coefficients' of irrelevant operators as well, since the theory is manifestly finite. The idea is to effectively lower the cutoff by explicitly integrating out modes with bΛ ≤ k ≤ Λ for some b < 1. This will leave us with a path integral over modes with k 2 ≤ b 2 Λ 2 -which is a theory of the same fields now with a cutoff bΛ. By integrating out the high-energy modes we'll be able to track precisely their effects in this low-energy theory. Peskin & Schroeder perform this path integral explicitly by splitting the high energy modes into a different field variable and quantizing it, but since we've already introduced 7 It is necessary to define this cutoff in Euclidean momentum space for the simple fact that in Lorentzian space a mode with arbitrarily high energy k 0 may have tiny magnitude by being close to the light cone |k 0 | | k|. It is left as an exercise for the reader to determine what deep conclusion should be taken away from the fact that we generally perform all our QFT calculations in Euclidean space. the conceptual picture of integrating fields out we'll take the less-involved approach of Srednicki . To repeat what we discussed above, the diagrammatic idea of integrating out a field is to remove it from the spectrum of the theory and reproduce its effects on low-energy physics by modifying the interactions of the light fields. Performing the path integral over some fields does not change the partition function, so the physics of the other fields must stay the same. We want to do the same thing here, but integrate out solely the high energy modes of a field and reproduce the physics in terms of the light modes. We'll continue playing around with φ 4 theory and define our (finite!) theory with a cutoff of Λ, which in full generality looks like: For simplicity we will decree that at our fundamental scale Λ we have a canonically normalized field Z(Λ) = 1 and no irrelevant interactions c n (Λ) = 0, but just some particular m 2 (Λ) and λ(Λ). Let's look first at the one-loop four-point amplitude, which we must ensure is the same in both the theory with cutoff Λ and that with cutoff bΛ. In the original theory, the amplitude at zero external momentum is iV Λ 4 (0, 0, 0, 0) = −iλ(Λ) + When we evaluate this in the theory with a lowered cutoff bΛ, the modification is simply to everywhere make the replacement Λ → bΛ. In order for the physics to remain the same without the high-energy modes, the vertex function must not change. We'll take full advantage of the perturbativity of the result-that is, λ(Λ)−λ(bΛ) ∼ O(λ 2 ), m 2 (Λ)− m 2 (bΛ) ∼ O(λ 2 )-to swap out quantities evaluated at bΛ in the second-order term for those evaluated at Λ at the cost solely of higher-order terms which we ignore. The effect of high energy modes on the four-point vertex function can be simply absorbed into a shift in the coupling constant! This procedure explicitly transfers loop-level physics in the theory defined at Λ into tree-level physics at bΛ. We can repeat this for the two point function to find the behavior of Z(Λ) and m 2 (Λ). We have again liberally ignored subleading terms. We see that the wavefunction normalization Z does not run at one-loop in this theory, since the only one-loop diagram contributing to the two-point function does not have external momentum routed through the loop. This is merely 'accidental' as Z is not symmetry-protected and does run at two-loops. We also see the first hints of a somewhat worrisome situation with scalar masses. The mass m 2 (Λ) receives large one-loop corrections which tend to raise the mass up to near the cutoff, regardless of whether we originally had m 2 (Λ) Λ 2 . We will investigate this in great detail later. Now imagine we want to measure some properties of φ particles with external momenta far below our fundamental cutoff p i ∼ µ Λ. By construction, our procedure of integrating out high-energy momentum modes keeps the physics of these low-energy particles the same. But if we calculate this scattering amplitude using L(Λ), it is not easy to see from the Lagrangian how these low-energy modes will behave, since important effects are hidden in loop diagrams. If we instead first integrate out momentum shells down to some L(bΛ) with µ < bΛ Λ, then the effects of the high energy modes have been absorbed into the parameters of our Lagrangian, and we can read off much more of how φ particles will behave at low energies simply by looking at the parameters. We can see further value in this approach if we consider scattering more low-energy φ. Let's look at the 6-point vertex function at zero momentum-in the theory at Λ, we start with c 6 (Λ) = 0 and a one-loop diagram where momenta up to Λ run in the loop: (1.36) Now in the theory at bΛ, the loop only contains momenta up to bΛ, so we must account for the difference with a contact interaction c 6 (bΛ): (1.37) Again we should ensure that the physics is the same upon lowering the cutoff: So this renormalization procedure is especially useful for understanding the behavior of irrelevant interactions. In our original theory nothing about the six-particle interaction was obvious from the Lagrangian, but in our theory with cutoff bΛ we can simply read off the strength of this interaction at lowest order. Note also the inverse behavior to that of the mass corrections-for the irrelevant interaction, the most significant contributions to the infrared behavior come from the low -energy part of the loop integral, and the UV contributions are suppressed relative to this. Similarly, if we had started with a nonzero c 6 (Λ) which was small in units of the cutoff c 6 (Λ) Λ −2 (so perturbative), such a UV contribution will also be subdominant. Then fully generally here, we have as we evolve to low scales b 1. The Wilsonian approach we have discussed here gives useful intuition for how renormalization works as a coarse-graining procedure wherein one changes the fundamental 'resolution' of the theory, but in practice can make calculations cumbersome. Furthermore, the hard momentum-space cutoff we used is not gauge-invariant, which causes difficulties in realistic applications. The benefit, however, is that this is a 'physical renormalization scheme' in which the renormalization condition relates the bare parameters to physical observables. For this reason, this renormalization scheme satisfies the Appelquist-Carrazone decoupling theorem [62] , which is enormously powerful. This guarantees for us that the effects of massive fields can, at low energies, simply be absorbed into modifications of the parameters in an effective theory containing solely light fields. In the next section we'll return to a clarifying example of the meaning of the decoupling theorem, and also discuss a renormalization scheme which does not satisfy the requirements for this theorem to operate, but is far simpler to use for calculations. The winning strategy will be to input decoupling by hand, which will allow us to get sensible physical results without the computational difficulty. Before we get to that, though, we'll take a couple detours. We have seen that the need to remove divergences in our theory led to the introduction of running couplings which change as a function of scale. In our example above we see that renormalization has the operational effect of transferring loop-level physics into the treelevel parameters. This is an interesting perspective which bears further exploration-if there is hidden loop-level physics that really has the same physical effects as the treelevel bare parameters, perhaps this is a sign that there is a better way to organize our perturbation theory. Indeed, at some very general level renormalization can be thought of as a method for improving the quality of perturbation theory. For useful discussions at this level of abstraction of how renormalization operates, see [63] for its natural appearance whenever infinities are encountered in naïve perturbative calculations, and [64] for its usefulness even when infinities are not present. We'll discuss this perspective on renormalization further in the next section. However it's clear that loops also give rise to physics that is starkly different from the lowest-order result (e.g. non-trivial analytic structure), so how do we know what higher-order physics we can stuff into tree-level? In a continuum quantum field theory, a Lagrangian is a local object L(x)-that is, it contains operators like φ(x) 3 which give an interaction between three φ modes at a single spacetime point x. Such effects are known as 'contact interactions', but even at tree-level a local Lagrangian can clearly produce non-local (that is, long-range) physics effects. For example, consider the amplitude for 2 → 2 scattering in a φ 3 theory at second order in the coupling. In position space the non-locality here is obvious, as in Figure 1 .7: A simple tree-level diagram corresponds to a particle at point x and a particle at point y exchanging a φ quantum, but one may forget this important fact when working in momentum space. There the result is and indeed, Fourier transforming the cross-section for this process yields a Yukawa scattering potential for our φ particles, showing that they mediate a long-range force over distances r ∼ 1/m. We obviously cannot redefine the Lagrangian to put this effect into the lowest order of perturbation theory since this is not a local effect. But if we do have a continuum quantum field theory, then because of locality it describes fluctuations on all scales. When we go to loop-level, we must integrate over all possible internal states, which includes integrating over arbitrarily large momenta or equivalently fluctuations on arbitrarily small scales. Heuristically, when the loop integral is sensitive to the ultraviolet of the theory, it is computing effects that operate on all scales-that is, it gives a contribution to local physics. This tells us that the pieces of this higher-order contribution which we can reshuffle into our Lagrangian are connected with ultraviolet sensitivity, leading to a close connection of renormalization with divergences. A couple notes are warranted about the notion of locality we rely on here. Firstly, it's clear that this criterion of local effects appears because we began with a local Lagrangian. If we postulated that our fundamental theory contained nonlocal interactions, say L(x) ⊃ gφ(x) dyφ(y) dzφ(z) , then we could clearly absorb further nonlocalities with the same structure into this coupling as well. However, this sort of nonlocality is different from the nonlocality we saw appearing out of the local theory at tree-level. In particular it would break the standard connection between locality and the analytic structure of amplitudes-see e.g. Schwartz [22] or Weinberg [65] on 'polology' and locality. Secondly, our notion of locality should be modified in a low-energy theory with an energy-momentum cutoff Λ, as can be seen in hindsight in our Wilsonian discussion above. As Λ defines a maximum energy scale we can probe, there is equivalently a minimum time and length scale we can probe due to the uncertainty principle, heuristically ∆x µ 1/∆p µ 1/Λ. As a result, any fluctuations on shorter length scales are effectively local from the perspective of the low-energy theory. An exchange of a massive field with M > Λ or of a light field with high frequency ω > Λ appears instantaneous to lowenergy observers. This explains how it's sensible to use renormalization techniques in, for example, condensed matter applications, where systems are fundamentally discrete. We can see this concretely by imagining the light φs in the tree-level example above instead exchanged a heavier scalar Φ with mass M m. While the amplitude M = g 2 /(p 2 + M 2 ) is still nonlocal in the continuum theory, if we're only interested in physics at energies E M we may Taylor expand the result M = g 2 /M 2 + g 2 p 2 /2M 4 + . . . . We may then absorb the leading effects of this heavy scalar into an effectively local interaction g 2 φ 4 /M 2 among the light fields-as long as we work at energies below M . In the next section we'll explore concretely how these insights enable us to transfer loop-level physics to tree-level physics, and so improve our calculations. The astute reader may notice a potential issue with our one-loop results in the φ 4 theory, Equations 1.32,1.34,1.35. We've derived this behavior as the first subleading terms in a series expansion in the number of loops. In relation to the tree-level results, the oneloop contributions are suppressed by a factor ∼ λ/16π 2 log(Λ 0 /Λ), where I've switched notation to now have Λ 0 as a high scale and Λ as the lower scale we integrate down to. Higher n-loop contributions will be further suppressed by n loop factors. But what if we wanted to study physics at a scale far lower than Λ 0 ? Eventually this factor becomes large enough that we will need to compute many loops to obtain high precision, and then large enough that we have reason to question the convergence of the series 8 . Keep in mind that we are in the era of precision measurements of the Standard Model, so these one-loop expressions are very restrictive. For a concrete example, say we wanted to check the SM prediction for a measurement of a coupling λ(Λ) with λ(Λ 0 ) = 1 whose experimental uncertainty was 1%. Let's define 8 Please excuse my slang. Perturbative series in QFT are quite generally not convergent but we can trust the answers anyway to order n ∼ exp 1/expansion parameter because they are asymptotic series. So really when this parameter becomes large enough we worry that our series is not even asymptotic. Thinking deeply about this leads to many interesting topics in field theory, from accounting for nonpertubative instanton effects which are (partially) behind the lack of convergence; to the program of 'resurgence', the idea that there are secret relations between the perturbative and nonperturbative pieces. This is all far outside my purview here, but some introductions aimed at a variety of audiences can be found in [66, 67, 68, 69, 70, 71 ]. a theoretical uncertainty on a perturbative calculation to n th order as n ≡ n th order result − estimated size of (n + 1) th order result n th order result , (1.43) where our Wilsonian calculation in Section 1.2.1 gave at first order, as a reminder (and with modified notation) 44) and our heuristic estimate for the size of the second order correction is ( 3 16π 2 ) 2 λ(Λ 0 ) 3 log 2 Λ 0 Λ . When the result is simply a series, the uncertainty is very simple to calculate, as the numerator is then our estimate of the (n + 1) th order correction, which is roughly the square of the first order correction in this case for n = 1. Here we have 1 = 3 16π 2 λ(Λ 0 ) log Λ 0 Λ . Then in order for our theoretical uncertainty to be subdominant to the experimental precision, 1 < 0.01, we must go past the one-loop result if we wish to look at energies below Λ ∼ exp(−16π 2 × 0.01/3)Λ 0 ∼ Λ 0 /2, the two-loop result is only sufficient until Λ ∼ Λ 0 /400, and if we can manage to calculate the three-loop corrections that only gets us down to something like Λ ∼ Λ 0 /8 × 10 4 . If we're interested in taking the predictions of a Grand Unified Theory defined at Λ 0 ∼ 10 16 GeV and comparing them to predictions at SM energies, how in the world are we to do so? Fortunately, we can do better by applying our one-loop results more cleverly. It is clear by looking at Equations 1.32,1.34,1.35 that the results have the same form no matter the values of Λ 0 , Λ. So if we take Λ to be only slightly smaller than Λ 0 (corresponding to 1 − b 1 in our previous notation) the expansion parameter becomes very small and the one-loop result becomes very trustworthy. What we would like is some sort of iterative procedure to gradually lower the cutoff, which we could then use to find the one-loop result for energies far lower than the range of our perturbative series. This is in fact precisely the sort of problem that a differential equation solves, and we can derive such an equation by differentiating both sides by ln Λ and then taking Λ infinitesimally close to Λ 0 . That exercise yields These are known as 'renormalization group equations' and they indeed allow us to evolve the coupling down to low energies-one says we use them to 'resum the logarithm'. Then to study physics at a very low scale we can bring these couplings down to a lower scale and do our loop expansion using those couplings, which is known as 'renormalization group improved perturbation theory', and which we will discuss in more detail soon. Explicitly solving with our boundary condition at Λ 0 yields Turning back to our effective field theory language, we see that quantum corrections have generated an anomalous dimension for λ, δ φ 4 = 3λ 2 /(16π 2 ), correcting the leading order scaling behavior. Since δ φ 4 > 0, we've determined that the quartic interaction is marginally irrelevant, which we will return to later. We can now look at the theoretical uncertainty in this one-loop resummed calculation by including an estimate of the next order correction to the running of the quartic 3 and resumming that expression. This can no longer be done analytically, but numerical evaluation easily reveals that the theoretical uncertainty here stays below 1% for many, many orders of magnitude below Λ 0 . Resumming the logarithmic corrections allows us to use our loop results to far greater effect. The physical meaning and technical statement of the decoupling theorem commonly confuse even prominent practitioners of effective field theory, so it's worth going clearly through an example to refine our understanding. Indeed, one may be confused just at zeroth order about how decoupling is sensible against the background of the hierarchy problem-which is an issue of sensitivity of a scalar mass to heavy mass scales. How can we claim QFT obeys a decoupling theorem and then go on to worry at length about quantum corrections δm 2 ∝ M 2 ? The correct way to think about the decoupling theorem is not whether a top-down calculation could yield a result that depends on heavy mass scales, but whether a bottomup effective field theorist and low-energy observer could gain information about the heavy mass scales through low-energy measurements. We can clarify this important difference by looking at a one-loop mass correction to a light scalar φ of mass m from a heavy scalar Φ of mass M through the interaction λφ 2 Φ 2 . We again take a Wilsonian perspective and begin at a scale Λ 0 > M . In close analogy to what we had before, we now find  However, the effect on the light scalar is an additive shift of the mass. If we go out and measure the mass at a single scale m 2 (Λ) we can't tell empirically which 'parts' of that came from m 2 (Λ 0 ) and which came from M 2 (Λ 0 ) or whatever else is in there, so we have no idea of how this low-energy measurement depends on heavy scales. To get information about the various contributions to the light scalar mass, we can measure it at different scales and look at how it changes. Of course this information is contained in the renormalization group equation for m 2 (Λ). At O(λ), we can find this by differentiating the above, and we find Now we can see the difference. If we perform low-energy observations where we can take the cutoff below the mass of the heavy scalar Λ M , then the physics of the heavy scalar decouples from the running of the light scalar mass. It is only by studying this running at low energies that we can gain information about the ultraviolet, and we see that this information is contained solely in small corrections scaling as Λ 2 /M 2 . At low energies, to learn about short-distance physics we must make very precise measurements of the low-energy physics. This is the sense in which heavy mass scales decouple from the theory in the infrared. Now let us study another, slightly more complex theory and apply renormalization techniques to simplify our calculations. We avoid the complication of gauge symmetries and focus instead on a Yukawa theory of a Dirac fermion interacting with a parity-odd scalar. Our first improvement to perturbation theory will be to switch from 'bare' to 'renormalized perturbation theory'. Let's first recap our procedure in Section 1.2.1. We began with a Lagrangian with bare parameters m 0 , λ 0 , . . . , introduced a regulator, computed the physical parameters m phys , λ phys in terms of the bare ones, inverted those relationships, and then plugged in for the bare parameters in terms of the renormalized ones, after which we were left with an amplitude which remains finite as we remove the regulator. This procedure works to remove the divergences in any renormalizable theory, but is obviously rather cumbersome. Furthermore one may question the validity of performing a perturbative expansion in a bare parameter which we later discover is formally infinite in the continuum theory It is both conceptually and computationally easier to instead start off by performing perturbation theory in terms of the renormalized parameters which we know to be finite by definition. Fortunately we can improve our accounting simply by reshuffling the Lagrangian as follows. In terms of the bare parameters and fields, the Lagrangian reads where we've split up the free and interaction parts. Just as in our earlier example, when we compute at one-loop these parameters will get corrections such that the bare parameters are no longer the physical parameters we measure. Anticipating that fact, let us rewrite the Lagrangian to explicitly account for those corrections from the outset. Although it was not a feature of our simple example above, in general there will be 'wavefunction renormalization' which changes the normalization of our field operators, ψ ψ where ψ, φ are now renormalized fields, We do the same to define renormalized masses related to the bare masses as M 0 = Z M Z −1 ψ M, m 2 0 = Z −1 φ Z m m 2 , and for the couplings Next we use the brilliant strategy of adding zero to split these Z-factors into a piece with the same form we started with and a 'counterterm' proportional to (Z − 1). Since at tree-level there's no renormalization needed, we know Z = 1 + O(couplings). At nontrivial loop level, we must choose the Z-factors to implement our chosen renormalization scheme. The Lagrangian now takes the form where we've split off the counterterms into L ct . We can now treat the terms in L ct simply as additional lines and vertices contributing to our Feynman diagrams. We'll see how useful this is once we begin renormalizing the theory. This is done in full in Srednicki's chapters 51-52 [18] , so we will not go through every detail. We'll regulate this theory using 'dimensional regularization' (dim reg) which analytically continues the theory to general dimension d = 4 − . That this will regulate our theory is not obvious, but I recommend Georgi [17] to convince yourself of this and Collins [72] for a full construction of dim reg; we'll content ourselves with seeing it in action. Our renormalization scheme will be 'modified minimal subtraction' and denoted MS, where 'minimal subtraction' means we'll choose our counterterms solely to cancel off the divergent pieces (rather than to enforce some relation to physical observables, as we did previously) and 'modified' means that actually it's a bit nicer if we cancel off a couple annoying constants as well. Since we're using MS, the mass parameters m, M will not quite be the physical masses, which are always the locations of the poles in the propagators, and the fields will not be normalized to have unity residue on those poles. So we'll have to relate these parameters to the physical ones later. We'll briefly go through renormalizing the scalar two-point function at one loop to evince dim reg and MS. In our one-loop diagrams we use propagators given by L 0 , since we know the counterterms begin at higher order. The full details of the one-loop renormalization of this theory can be found in Srednicki's Chapter 51. At one-loop, the scalar two-point function gets corrections due to both interactions, as seen in Figure 1 .8. There is the diagram we had in the φ 4 theory above, but we must recompute this in dim reg The connection is now slightly more opaque, which is why we began by discussing a cutoff in Euclidean momentum space, but the calculations become far simpler. There's another diagram with a ψ loop with D = x(1 − x)k 2 + m 2 , whose evaluation follows similar steps but we skip for brevity. Adding these together, MS tells us the φ counterterms must take the values For the fermion, evaluating the one-loop diagrams gives us the counterterms (1.67) Since we didn't choose the counterterm to keep the location of the pole in the propagator fixed, m is no longer the physical scalar mass. But we can find the physical, 'pole' mass precisely from that condition: where we have used our favorite trick to replace m 2 phys with m 2 in the one-loop correction, since it is already higher order in couplings.  from which we'll be able to understand how the strength of the interactions changes as a function of the energy at which the theory is probed. Now the second improvement to perturbation theory is the RG-improved perturbation theory we mentioned above. This takes on an even more useful role in our continuum renormalization scheme here. In the Wilsonian picture, Λ was a high cutoff and we ensured the physics was invariant under evolution of Λ, but this scale still needed to stay far above the scales of interest in the problem Λ m, M, −k 2 , . . . . Now the scale µ is entirely unphysical and we are free to bring it all the way down to the scales of kinematic interest-in fact doing so will vastly simplify calculations. As a result we are able to make even more use of the RG-improvement than we could above. We find the running couplings by again using the fact that the bare parameters are independent of the unphysical renormalization scale µ. Having utilized a massindependent regulator, a Wilsonian interpretation of couplings running with the value of the regulator is nonsensical here and so renormalization group improvement is the way to extract predictions from this theory. We already have the relations between the bare and renormalized quantities, e.g. And since we know that the bare parameters are independent of µ by definition, we have If we expand dg d ln µ = a 1 + a 0 + . . . order by order in , then matching the O( ) terms gives a 1 = −g/2 and matching the O( 0 ) terms tells us that, in the → 0 limit, we have (1.81) This -independent piece is known as the 'beta function' for the coupling, β(g) = 5 16π 2 g 3 . Of course there are higher-order terms in dg d ln µ which are needed to match the O( −n ) terms, and which one can solve for. But these vanish in the → 0 limit, so will not contribute to the running of g(µ). We can now resum this logarithm to find the evolution of this coupling with renormalization scale where we've used the boundary condition g(μ) =ḡ. As before, the resummed version will allow us to maintain precision to far lower scale than we could with simply its leading order approximation. It's useful to keep in mind the Wilsonian picture as a clearer example because our regulator had a physical interpretation. The point is that the logarithms are really what's encoding how couplings change as a function of scale; in the Wilsonian calculation it was obvious that the logarithmic contribution log 1 b is present no matter the initial cutoff. One says that couplings which receive logarithmic quantum corrections 'get contributions from all scales'. Then it's clear why this RG-improvement is sensible-though we may start at some particular Λ or µ, a one-loop calculation offers information on the lowestorder logarithmic running over all momenta, and we may sum up those modifications to improve our perturbation theory. We've seen already the necessity of renormalization when a theory produces naïvely divergent results, and its enormous use in improving the precision of perturbative calculations in a given theory. The last facet we'll discuss is its use in connecting theories. This is necessary to use the computationally-simple scheme of dim reg with MS in theories with different mass scales, and is very closely related to the effective field theory philosophy we discussed in Section 1.1. Cohen's monograph [16] goes into far more depth than I will be able to, and is a fantastic introduction to these ideas and their application. This perspective on renormalization has also been of enormous use in condensed matter to understand behaviors that appear in many distinct systems in the long-distance limit, and has applications in formal field theory to understand better the properties of QFT itself. In the previous section we derived the beta function for Yukawa theory in the MS scheme. As promised by our terming of this as a 'mass-independent' scheme, the beta functions indeed have no reliance on the masses. But this should seem remarkably peculiar, as it suggests that there is no decoupling at all. Were that the case, by measuring the low-energy beta functions of QED we could tell how many charged particles existed up to arbitrarily large mass scales! What has gone wrong is that MS does not meet the criterion of a physical scheme which is necessary for the Appelquist-Carrazone theorem to operate. In MS the renormalization condition has nothing to do with physical values of the parameters so, while it makes calculations far simpler, MS has broken decoupling. To restore decoupling and allow us to properly use a mass-independent scheme, we must implement the mass-dependence ourselves by 'matching' the Yukawa theory at energies above the fermion mass to a theory of solely light scalars at energies below the spinor mass. To 'match', we consider some process which exists in both theories-for example, φ 4 scattering-and ensure that at the matching scale M both theories agree on the physics. In the high-energy Yukawa theory we can run the RG scale all the way down from a high scaleμ to µ = M phys , the physical, pole mass of the fermion. To get simple closedform expressions, we'll take the couplings small enough that working to lowest order gives a good approximation. We'll denote all of the UV values with bars, e.g. M (µ =μ) =M . Firstly, we use the counterterms to find the anomalous dimension of the fermion mass We then find the fermion pole mass as  Now we are ready to proceed to even lower energies. We enforce decoupling by matching to the low-energy theory of just a self-interacting scalar. We have The counterterms and beta functions in this theory can be conveniently found by truncating those found above. Of course, by construction, we find that the heavy fermion ψ no longer contributes to the running of parameters at low-energies. To make sure we're getting the physics correct, we must impose the boundary condition that the predictions match at µ = M phys , which here is quite simple-we just use the values at M phys in the UV theory as literal boundary conditions for our running in the IR theory. In the IR theory, for µ ≤ M phys , we have The benefit is now clear. While the RGEs in the UV theory were very complicated, the running of λ in the low energy theory is simple. Our mass-independent scheme allows us to explicitly factorize these and contain all the UV physics in the boundary condition, which lets us study the low-energy theory in a simple manner. The general procedure of renormalization group evolution in mass-independent schemes is called 'running and matching'. The parameters in the Lagrangian run as you evolve down in energies, but at a mass threshold M we must match the UV theory at µ = M from above to a theory without this field at µ = M from below. When we match we ensure that the physics of the low-energy fields stays constant as we cross that threshold and remove that particle from the spectrum of our theory. This becomes less trivial when we have multiple mass scales, so consider now upgrading our Yukawa theory with additional fermions. Now if we are given the theory at very high energies µ M i and we want to understand what it looks like at very low energies, there is a cascade of EFTs we evolve through. As depicted in Figure 1 .10, we run the parameters down to the largest fermion mass, match to a theory with one less fermion, run down again until the next mass scale, and so on. As an example which is closer to the real world, consider QED with our three generations of leptons (and ignoring the strong sector for simplicity). At low energies we measure the asymptotic value α(µ 0) 1/137, and in colliders we measure the value of the gauge coupling at the Z mass m Z 91 GeV. To compare these, we must run the highenergy value all the way down into the infrared. Above m τ 1.7 GeV we have a theory where all of e, µ, τ run in loops and giving an MS beta function β α = 2α 2 /π. But at m τ we should remove the τ from our theory, such that from m τ down to m µ 105 MeV we have β α = 4α 2 /3π. Below the µ we solely have the electron and recover the textbook β α = 2α 2 /3π, and finally as we cross the electron mass threshold m e 511 keV we remove the electron from the spectrum and find that the gauge coupling stops running β α ≡ 0. Physically this corresponds to the fact that pure QED is scale-invariant, meaning that the coupling will not evolve at all in a theory with no charged particles. This is the regime in which classical electrodynamics holds precisely (up to the presence of additional interactions suppressed by powers of m e , that is). A possible confusion is to conflate the mass-independence of the regularization scheme with that of the renormalization scheme, and conclude that dimensional regularization cannot be used if one wants decoupling without having to integrate out and match. So lest one confuse the roles let's quickly look at an example of using dimensional regularization with a renormalization scheme which does satisfy decoupling, known as 'off-shell momentum subtraction'. For simplicity, we'll look at the anomalous dimension of our Yukawa scalar φ, and we'll perform wavefunction renormalization by subtracting the value of the graphs at the off-shell momentum scale k 2 = µ 2 E . In symbols this amount to the prescription (1.105) Since we're still using the same regularization scheme, we have the same result for Π loop (k 2 ) as above. We can then simply calculate the anomalous scaling dimension as to check if they agree with our expectations. At energies far above the fermion mass its contribution to the scalar anomalous dimension cannot know about that scale, and at energies far below its mass we expect inverse dependence on the mass for decoupling to occur. This is precisely what we find, so the lesson is that even if we didn't want to go through the trouble of integrating out the fermion and matching, we could still make use of the magical regularization scheme that is dim reg. Our interpretation of the renormalization group thus far has been as a way of understanding what a particular theory looks like at different energies. But there is another way of looking at it that is also useful, for which we shall follow an example of Peskin & Schroeder, though I recommend Skinner's lecture notes [73] for clear explanation of these concepts which goes farther than we have time to. Let's return to the idea of the Wilsonian path integral and successively integrating out Euclidean momentum shells. In the previous section we began with a scalar field theory We then integrated over momentum shells from Λ down to bΛ with 0 < b < Λ, and found we could express our result as (schematically; see 1.32,1.34,1.35,1.41) (1.110) Above we interpreted this in terms of looking at the same theory at lower energies, having coarse-grained over the largest momentum modes, which is a useful way of comparing the two path integrals. Another useful way to compare is to get them to a form where they look similar, so let's now define a change of variables k = k/b, x = xb, in terms of which the path integral now looks like We can transform the kinetic terms back to the canonical form with the field redefinition , after which we can write the effective Lagrangian as  (1.114) Since all of our dynamical variables are integrated over in calculating the partition function, we can view this as a transition in the space of Lagrangians, L → L . So this gives us an interpretation of the renormalization group as a flow in 'theory space'. This interpretation invites us to conceptualize renormalization group flow as a path through theory space between two conformal field theories (CFTs), as depicted in Figure   1 .11. CFTs are quantum field theories with an enlarged spacetime symmetry group 9 , consisting essentially of a scaling symmetry. CFTs are fixed points of RG flows-since they possess scaling symmetry they look the same at all energy scales, so if an RG flow is to have an endpoint it clearly must be a CFT 10 . There's a terminological confusion here, which is that the 'renormalization group' isn't actually a group at all, since the operation of integrating out a momentum shell is irreversible. This came up already above when we saw that integrating out heavy fields means we can no longer compute processes which have them as external fields (see Footnote 1). Flowing to lower energies, or toward the IR, is really a coarse-graining operation which does lose information about small scales, in precise analogy to decreasing the resolution of an image. This means that RG evolution is a directed flow, so there is a difference between fixed points in the UV and in the IR. Quantum field theories can have different sorts of fixed points. As a familiar example, if the theory has a 'mass gap'-no zero-energy excitations in the infrared, which may be because one began solely with massive fields or through dynamical mechanisms like Higgsing and confinement-then one finds a 'trivial' fixed point. In the far infrared, everything has been integrated out and there is not enough energy to excite any modes. We know phenomenologically that this happens in QCD. In the other familiar case one can have a 'Gaussian' or 'free' fixed point if the theory contains massless fields which don't interact, such as in QED. At energies far below the mass of the lightest charged particle this is a theory of free electromagnetism, though one can still excite photons of arbitrarily-long wavelength. Such a Gaussian fixed point occurs in the UV for QCD-the celebrated result of 'asymptotic freedom' [77, 78] -because the strong coupling flows toward zero, giving 10 I've elided a subtlety here, which is that it is not entirely known whether scale invariance in fact implies conformality in four-dimensional QFTs, the latter of which includes also invariance roughly under inversion of spacetime through a point. No counterexamples have been found, despite much effort. Polchinski's early paper on the topic is a classic [75] , and a recent review can be found from Nakayama [76] . a free theory. This famously cannot occur for U (1) gauge theories whose couplings necessarily grow with increasing energies, leading them to herald their own breakdown with the prediction of a 'Landau pole' [79] , a finite UV energy where the perturbative theory predicts the coupling becomes infinite. From one perspective this is an inverse to the prediction of a confinement scale in QCD, where the perturbative prediction is a blowup of the coupling at low energies, as we'll discuss further in Section 1.3.3. In either case the theory cannot make predictions for energies above the Landau pole or below the confinement scale, respectively, and so can be called inconsistent. It's clear that the divergence of a coupling either in the IR or the UV is problematic for a complete, consistent interpretation of a QFT. This is precisely why the formal perspective on a well-defined QFT is that it describes an RG flow between two CFTs, such that in neither direction does a coupling grow uncontrollably. In fact this provides an incredibly important perspective on renormalizability, which we'll get to momentarily. First, let us introduce somewhat of a generalization of the 'relevant/irrelevant' terminology which we introduced in Section 1.1. We implicitly had in mind that we were studying a theory in the vicinity of the Gaussian fixed point which we perturbed with various field operators-indeed, this is precisely how we normally carry out perturbative calculations-and our terminology depended on that. Generally one wishes to imagine perturbing a CFT by a particular operator, flowing down in energy, and seeing whether the operator grows in importance-a relevant operator-or shrinks-an irrelevant operator. Perturbing a CFT by an irrelevant operator does not induce an RG flow (to a different CFT), so interesting dynamical RG flows come from CFTs perturbed by relevant operators. This clearly agrees with our power-counting notion of relevance when we're near the Gaussian fixed point, but works also if one is near a strongly-coupled, interacting fixed point where one may not know how to do such power-counting and anomalous dimensions of operators may grow to overpower their classical dimensions. The importance of this language was realized in particular by Polchinski in his pioneering article [21] . He showed that in fact the intuitive notion of 'power-counting renormalizability' that the field had been building-that for a theory near the Gaussian fixed point we could see whether it was renormalizable merely by checking whether it has any coefficients of negative mass dimensions-in fact maps on to a very general statement. This is enormously powerful, as prior arguments for renormalizability were made on a case by case basis and were complicated and messy and graph-theoretic. His derivation of this fact is brilliant but requires much work, so we'll merely try to get a sense for why it should be true by building on our intuition from our Wilsonian renormalization of φ 4 theory in Section 1. of scale Λ which reach the same λ i R will differ only by positive powers of (Λ R /Λ 0 ). For less abstract discussion, Polchinski goes through a simple example which may provide further insight, and Schwartz discusses the same example in Chapter 23 of his textbook [22] . To see why this implies renormalizability, recall that the program of Wilsonian renormalization is to define renormalized, 'running' couplings as a function of scale to keep infrared physics at Λ R fixed while 'removing the cutoff'. A bit more formally, we want a family of Lagrangians L(Λ 0 ; λ i 0 , c i 0 ) with coefficients chosen as a function of Λ 0 such that each Lagrangian yields the same low energy physics λ i R , in terms of which all the IR observables can be calculated up to subleading corrections in Λ R /Λ 0 . When the cutoff is removed by taking Λ 0 → ∞, one thus recovers precisely the correct physics, specified by those chosen values of the λ i R , which are the renormalization conditions. If we can find such a family of Lagrangians, then we say this theory is power counting renormalizable. Polchinski's argument shows that this can be done so long as one wishes solely to fix the IR values of relevant and marginal couplings. The attentive reader may at this point notice an inconsistency due to imprecise language. We've seen now that the criterion for renormalizability, which Polchinski provided a robust basis for, is power-counting of the operators near the IR fixed point. This would suggest that the λφ 4 theory we've studied by means of an example is renormalizable. However, recall the result of resumming its renormalization group equation: which has a Landau pole for Λ = Λ 0 exp( 16π 2 3λ(Λ 0 ) ), preventing us from taking the limit we required above. A mathematical physicist would say λφ 4 theory is 'trivial' or 'quantum trivial', as if we demand the existence of a continuum limit, that sets λ(Λ) = λ(Λ 0 ) = 0. The issue is that the tree-level, classical scaling dimension captures only the scaling of the operators infinitesimally close to the infrared Gaussian fixed point. If we move a finite distance upward in energy scale, we've seen above that the φ 4 operator gets an anomalous dimension δ φ 4 > 0 and so is marginally irrelevant. So it's clear that Polchinski's picture of renormalization is only getting at a perturbative sense of renormalizability, and cannot tell us whether there truly exists an RG flow from a UV fixed point down to the IR theory we want to study. So what are we to make of λφ 4 theory-or for that matter of QED, which has the same problem? Of course we know empirically that QED works fantastically well and we can absolutely make finite, accurate predictions after a finite number of inputs. To understand this, we must appeal the language of effective field theory, which we've already discussed. In fact the feature we're really relying on is effective renormalizability, which tells us we require solely a finite number of inputs to set the behavior of the theory to a given precision. It's clear that in this sense QED itself is an effective field theory whose validity breaks down somewhere below its Landau pole. Finally, let me mention another reason not to be too worried that our most beloved quantum field theories don't exist in the continuum limit: Our universe is not described by a QFT at its smallest scales! It's indeed true that only RG flows between a UV CFT and an IR CFT can hope to define fully consistent and mathematically well-defined QFTs. But the existence of gravity-and the very strong evidence that a quantum field theory of gravity is inconsistent-means that at some energy scale effects not present in quantum field theory must become relevant. And since gravity couples universally to everything [80, 81] , we have no strict empirical need for a UV complete, interacting quantum field theory that does not include gravity. It is entirely consistent, and overwhelmingly likely, that a quantum-field-theoretic description of the world works only approximately and some inherently quantum gravitational theory provides a sensible UV complete theory. Before moving on, let us reiterate what we've discussed about renormalization. As we've seen, renormalization is so important and so useful and fulfills so many purposes that an entirely general statement risks becoming vague. But if a single sentence summary is demanded: Renormalization reveals for us the scale-dependence of a quantum mechanical field theory. The effects of this seemingly innocuous statement, however, are powerful and mani-fold, including: • Correctly accounting for this scale-dependence is necessary to have well-defined quantum field theories, which otherwise appear nonpredictive. • Bringing the non-scale-invariance of the quantum mechanical theory into clear scaledependence of the couplings makes it simple to read off the qualitative behavior at different scales from the renormalized Lagrangian. • Including this scale-dependence in the couplings allows us to reorganize our perturbative series such that we can efficiently calculate the behavior of the theory over a far wider range of energies than a naïve treatment allows. • Properly accounting for the scale dependence allows us to harness the full power of effective field theory, as we can study a theory of low-energy fields which correctly accounts for corrections from the high-energy physics. • Understanding the perspective of single quantum field theories as flows through theory space as a function of the scale allowed us to develop a nonperturbative definition of a fully UV-complete quantum field theory and how it behaves. All of these various perspectives will be of use in the following chapters as we apply this technology to understanding what the hierarchy problem is and how we can solve it. Naturalness is the notion that we have the right to ask about the origins of the dimensionless numbers in our theories-past solely fitting them to the data. It was Dirac who first introduced such a notion to particle physics in 1938 [82] . In modern language, what is referred to as 'Dirac naturalness' consists of the idea that dimensionless parameters in a fundamental physical theory should take values of order 1. In the language of EFT, in a theory with a cutoff Λ and an operator O with scaling dimension ∆, we expect its coefficient to take a value c O ∼ O(1)Λ d−∆ . As stated this is essentially dimensional analysis, as Λ is the only scale we've introduced, but we will discuss below that quantum mechanics gives additional credence to this expectation-indeed we've already seen this feature in our one-loop examples above. 't Hooft pointed out a refinement of this principle [83] , which has come to be called 'technical naturalness'. If the operator O breaks a symmetry which is respected by the action in the limit c O → 0, then one says it is 'technically natural' for c O to take on a small value. The reasoning here is simple-as we saw above, in a quantum field theory defined at a high scale, one finds corrections δc O to such coupling constants as they run to low energies. If there is an enhanced symmetry of the theory in the limit that c O → 0, then such quantum mechanical corrections cannot generate that operator and break the symmetry, so we know that δc O ∝ c O . The low-energy effective field theorist says of such couplings that one can 'set it and forget it': if one has c O 1 at the cutoff Λ, that coupling will remain small as one flows to lower energies. Conversely, we can emphasize the connection to Dirac naturalness by looking at this picture in reverse. We know of mechanisms to generate small technically natural couplings at low energies from Dirac natural ones, as we will discuss in detail below. Imagine one measures a small coupling c O at long distances in the low-energy theory with cutoff Λ that does not have a Dirac natural explanation. If that parameter is technically natural, it remains small up to the cutoff, and so the next generation of physicists can explain its small size at Λ in the UV theory, as emphasized nicely by Zhou [84] . If c O is not technically natural, then its RG evolution up to the cutoff yields a value to which the low-energy physics is very sensitive, and we must explain why it has a very specific value such that the correct physics emerges at long distances. A model is fine-tuned if a plot of the allowed parameter space makes you wanna puke. David E. Kaplan (2007) It's useful to make this less abstract by looking at a simple example. Consider a d = 6 dimensional effective field theory of a real scalar field φ of mass m which is odd under a Z 2 symmetry, which we expect is a good description of our system up to a cutoff Λ with m Λ. If we add a small explicit breaking σφ 3 with σ 1 at low energies, σ is technically natural and stays small up to the cutoff, so we can easily write down a UV completion which generates this small value Dirac naturally. However, if we add another Z 2 -odd real scalar Φ and give it a large Z 2 -breaking interaction with φ, then σ is no longer technically natural. Its low-energy value becomes extremely sensitive to the values of the parameters at the cutoff. It then becomes difficult to understand an ultraviolet reason for why those values take the precise values they need to realize small σ in the far IR. Consider the bare action where we've given the two fields the same mass for simplicity. This is not stable under radiative corrections, but that's a higher-order effect which will not come into our oneloop calculation of the RG evolution of the cubic couplings. We will renormalize this theory at one loop using dim reg with MS. As discussed above, we will compute the one-loop 1PI diagrams and add counterterms to cancel solely the 1 pieces of the results. With counterterms, the action is where these parameters and fields are the renormalized parameters, and for compactness we have not written down the split of these terms as we did above in Equation 1.97. At tree level the relation to the bare quantities is trivial and so Z = 1 + . . . . To get an accurate picture of how the strength of the interactions vary as they're probed at different energy scales, we must fully renormalize the theory. Since our focus is on the interactions, we simply state the results for the quadratic part of the action, where we have Z φ = 1 − 1 6(4π) 3 σ 2 + y 2 1 + . . . (1.118) (1.120) These tell us that the physical mass of the fields and the normalization of their oneparticle states has changed. The relation to these can be found using the quantumcorrected propagator ∆(k 2 ) −1 as ∆(−m 2 phys ) −1 ≡ 0 to define the mass and to define the normalization R, but solving for these relations explicitly will not be necessary for our purposes. The one-loop three-point functions each have two diagrams, whose evaluation only differs in the coupling constants. For the correction to σ, we have triangle diagrams with either φ or Φ running in the loop. We can evaluate them in d = 6 − dimensions as The counterterm vertex contributes to this as −i(Z σ − 1), meaning that MS prescribes we set For the other vertex correction there are diagrams with either one or two of each internal line, which give leading to the counterterm This gives us the beta functions Now we are finally in a place to mathematically evince our physics point about technical naturalness. Without Φ, the coupling σ is the only one which violates the Z 2 and so the beta function is necessarily proportional to σ. 11 Let's say we recruit an experimentalist friend of ours to measure the 2 → 2 scattering cross-section of φs and we find that at µ = m, the theory fits the data for σ(m) = σ 0 with σ 0 1. Solving the beta function, we find that to lowest order So σ is indeed radiatively stable. If σ(m) = σ 0 is small, then it takes until the enormous scale µ m exp ((4π) 3 /σ 2 0 ) for σ to change by an order one fraction. So running σ(µ) up to wherever the cutoff Λ of our theory is, σ(Λ) will still be small. If by Λ we haven't discovered any explanation for the size of σ(m), we can ask the theory above Λ to produce this small value of σ(Λ) from Dirac natural parameters at yet higher energies. Perhaps the high-energy Dirac natural value of σ has been relaxed toward zero by an axion-type mechanism, or perhaps σ is the vev of another Z 2 -odd field which spontaneously broke the Z 2 via confinement. We don't need to know a particular mechanism; the fact that σ is technically natural means that if we don't find an explanation for its size there is hope yet that our academic descendants will. One says that here σ is UV-insensitive as its low-energy value does not depend strongly on the physics at high energies. On the other hand, if σ is not technically natural, we have a much more difficult issue. If we now have the theory with both φ and Φ, and our experimentalist measures σ(m) = σ 0 1 and y(m) = y 0 = O(1), then to lowest order the RG evolution of σ will be β σ −y 3 /(4π) 3 leading to In this theory σ is now a UV-sensitive parameter, whose low-energy behavior depends strongly on the high-energy physics. To say the least, it seems difficult to find a natural way to achieve the precise values needed to reproduce the observed low-energy physics in this theory. We'll return to this issue at length in Section 2.2.2. Our understanding of technical naturalness allows us to already see another warning sign of the hierarchy problem. An elementary spin-1 field comes along with a gauge symmetry In fact for discussing the technical naturalness of masses there is an even simpler argument: A massless spin-1 boson has two degrees of freedom and a massive one has three. Quantum corrections cannot generate a degree of freedom ex nihilo, so a massless gauge boson must be protected. Similarly a massless chiral fermion has two degrees of freedom, but a massive Dirac fermion has four. So for charged spinors and for vectors, it is simply the representation theory of the Lorentz group that is responsible for the stability of their masses. In either of these cases mass must arise from interactions of the field with a scalar (very broadly defined) as in the Higgs mechanism, which can pair up chiral spinors together and lend vectors another degree of freedom. But a massless scalar and a massive scalar have the same number of degrees of freedom. If we want a scalar mass to be technically natural, it must come from some symmetry past simply the Lorentz group. We'll see some examples of how to arrange this in Chapter 3. We can quickly see the utility of this by looking at a simple example of a complex scalar field φ with an interaction which explicitly breaks the U (1) global symmetry φ → φe iα . where "+ h.c." denotes the addition of the Hermitian conjugate of the non-Hermitian interaction term. A naïve effective field theorist would say that our Lagrangian has no symmetries, and so we have no control and should expect that quantum corrections give us any polynomials in φ, φ † at low energies. However, we may note that if we assign λ a charge of −3 such that λ → λe −3iα , then the theory is invariant under that U (1) global symmetry. So quantum corrections cannot violate our spurious symmetry, and as a result we know that we can only generate terms like λ 2 φ 6 , and there are no φ 4 or φ 5 interactions generated at any order in perturbation theory. We can also usefully apply this to the example of technical naturalness studied in the preceding section. If is given the spurious transformation Z 2 : → − then the φ 3 term is invariant. Then it's simple to see that no matter what other sorts of interactions we add, so long as they respect the Z 2 symmetry we must have δ ∝ . But having added yφΦ 2 we see that this term can also be made invariant with y → −y, and this allows δ ∝ y as well. An important real-world example where spurion analysis is useful is in understanding the flavor structure of the SM. With all masses turned off, the SM has a large global corresponding to arbitrary unitary reshufflings of the three generations of each fermion representation. These symmetries are explicitly broken by the Yukawa matrices which generate hierarchically different masses for the three generations. where i, j = 1, 2, 3 are generation indices, and the Yukawa couplings are matrices in this generation space. We don't understand why these hierarchies are present, but we can carry out a spurion analysis to see how worried we should be. We see that our theory will be invariant under the full flavor group if we assign the Yukawa matrices the following transformation properties under the various SU (3) symmetry groups Since these are the only flavor-violating couplings in the SM and they are all in distinct spurious flavor representations, this tells us the quantum corrections to these matrices must be proportional to the matrices themselves e.g. δY e ∝ Y e . Thus this pattern of Yukawa couplings is stable under RG evolution to higher scales, and we are justified in thinking that the generation of this pattern may take place at large, currently-inaccessible scales. This eases our minds about when we need to discover the origin of these flavor hierarchies, but this holds true only as long as these remain the only flavor-violating couplings. Fantastic work in precision flavor measurements and theory has provided lower bounds on the scale at which additional flavor violation can occur. Searches for flavor-violating processes have constrained these violations to take place at scales enormously higher than scales we are able to directly probe at colliders, which poses a puzzle. If there is new physics near the TeV scale, how is it arranged to respect the flavor structure of the SM? A phenomenological approach known as Minimal Flavor Violation [85] demands that all flavor violation is proportional to these Yukawa couplings, but no fundamental explanation for this is known. For recent introductions to flavor in the Standard Model, see e.g. [86, 87, 88 ]. Perhaps the most important example of a Dirac natural field theory generating a small number is that of 'dimensional transmutation'. In particular, in quantum chromodynamics (QCD) the theory is 'asymptotically free'-meaning that the interaction strength vanishes in the far UV-and the gauge coupling g grows as one goes to lower energies. We skip the Nobel-worthy calculation (see e.g. Srednicki's chapter 73 [18] ) and simply quote the results for the beta function of QCD (here parametrized via α s = g 2 /4π), which dictates the dependence of the gauge coupling on energy. In MS, the calculation finds where µ is the energy scale of interest and n f is the number of quarks with masses below µ, which at high energies is n f = 6. Then if we know the gauge coupling at a fundamental scale like M pl , we can follow the procedure discussed in Section 1.2.3 of running and matching to sequentially evolve the coupling down to low energies. We end up with a result like where b is somewhere between the 11 − 6 × 2/3 = 7 value it takes above the top quark mass and the 11 − 3 × 2/3 = 8 value it has below the charm quark mass. This tells us that eventually the QCD coupling blows up in the infrared, and the theory becomes strongly coupled-we expect our perturbative understanding of the theory to break down. While there is no proof of the precise effects of this, there is strong evidence that this is responsible for the observed phenomenon of 'color confinement'-at low energies colored particles form bound states which are color-neutral. The intuition being that the gluon interaction is so strong that trying to pull quarks in a color-singlet apart from each other requires so much energy that it is energetically favorable for a quark-antiquark pair to be created out of the vacuum and to end up with two color singlets. We may define a new scale Λ QCD as being the energy at which α diverges So for some reasonable fundamental coupling α(M pl ) at high energies, the theory generates a new scale which is exponentially far removed from the fundamental physics. Since the mass of the proton is mainly from QCD binding energy, m p ∼ Λ QCD this explains the huge hierarchy m p ≪ M pl . This is an extremely important mechanism and historically one of the first suggestions for how to generate the electroweak scale was by copying this strategy, as we'll discuss in Section 3.3. The Hierarchy Problem The physical question of the hierarchy problem is how to get an infrared scale v out of a microscopic theory whose degrees of freedom live at the much higher scale Λ, with Thus the fact that the Standard Model is chiral and so requires the Higgs mechanism to provide masses answers a deep and important question about our place in the universe. But it doesn't provide a full answer, as the scale at which the Standard Model sits still needs to be generated somehow. And in the absence of a mechanism to make it light, we must worry once more about losing our macroscopic existence with a mass scale which is again naturally of order the only other mass scale, the Planck scale. So, far more than the hierarchy problem being a small detail to clean up after having empirically verified the structure of the Standard Model, the question of why m H M pl has serious physical importance. 13 If we want an answer to why we live in a world with 12 We note that it's also true that the existence of a macroscopic universe relies on the smallness of the cosmological constant, which is the other pressing fine-tuning issue present in the Standard Model. This way of viewing the naturalness problems of the Standard Model has been beautifully articulated by Nima Arkani-Hamed in [89] and in many seminars. 13 It's worth noting that in the complete absence of the Higgs, electroweak symmetry is broken by the QCD chiral condensate [90, 91] . It's interesting to ponder why Nature did not choose to let QCD confinement solely fill the role, but we know empirically that there is EWSB at ∼ 100 GeV scales, so we need to understand the generation of that separate scale. macroscopic structure, we must grapple with the hierarchy problem. This section is devoted to understanding the technical statement of this physical question in the framework of effective field theory. We pursue this by introducing, discussing, and refuting some common confusions about the hierarchy problem. In this section I will introduce a few common confusions and misconceptions about the hierarchy problem. Discussion and refutation of these arguments provides a natural backdrop for introducing how the hierarchy problem should be properly understood and why it is important. A first point of confusion is that the Higgs mass is a free input parameter in the Standard Model, so a natural objection is that we should just set m H = 125 GeV and call it a day. Indeed, this hits on a basic and important point: There is no hierarchy problem in the Standard Model. The hierarchy problem exists for a more-fundamental theory which predicts the Higgs mass-that is, one in which the Higgs mass is an output parameter. We can evince this in a simple toy model of a scalar φ interacting with other general fields ψ i where a tree-level, 'bare' mass term is allowed by the symmetries. This is our toy version of the Standard Model, in which the scalar mass is likewise an input. Of course, as is familiar, m 0 itself is not measurable. When one calculates the twopoint function of φ perturbatively in couplings 14 one finds quantum corrections Γ (2) (p) = where these are generically large because the mass is not technically natural-there's no symmetry protecting it. When one measures the mass of φ with e.g. some scattering experiment, it is m 2 phys = m 2 0 + g 2 m 2 1 + . . . which one measures. And luckily so, because m 2 1 may well be formally infinite in a continuum quantum field theory, and a similarly infinite bare mass term is necessary to end up with the correct finite physical mass. Indeed, we are justified in this theory in choosing m 0 such that m phys matches the measured value. This is just the familiar procedure of renormalization, stretching back many decades and first understood in the context of quantum electrodynamics. To define QED one needs to input some definitions of the electron mass and the electromagnetic coupling based on experimental data, and these two inputs then determine all other predictions of the theory-e.g. the differential cross section of Coulomb scattering, the lifetime of positronium, and anything else you could hope to measure. However, consider now a theory which has a global SU (2) symmetry which is spontaneously broken at a high scale M . We want to understand how to get a light scalar degree of freedom out of this theory-that is, we've measured m phys M . This is a toy model of a Grand Unified Theory, where the microscopic physics exists at a high scale M GUT , and indeed it was in this guise that the hierarchy problem was first recognized. Let's say our light scalar degree of freedom φ originated from a doublet Φ a = (ϕ, φ) . Our microscopic theory now does not have a bare mass term for φ but rather solely for Φ as a whole, since it must respect the symmetry. A difference in the masses of φ and ϕ can only come from the spontaneous breaking of the symmetry-let's say when another fundamental scalar Σ gets a vev ν = M . This vev is a physical, measurable parameter related to the mass of Σ and its self-interactions. Our action is controlled by the symmetries, as ever. where M 0 is the bare mass of the Φ doublet and λ 0 is its bare interaction strength. In the absence of any other scales we generally expect M 0 ∼ M and λ ∼ O(1). In this theory there is no reason for the values of these parameters to be connected to each other in any way. When Σ gets a vev, Σ = (0, ν) , it breaks the SU (2) symmetry and gives a mass splitting between the two degrees of freedom in Φ, since Σ † Φ = νφ. We then have the In this theory our tree-level inputs are M 0 and λ 0 (and the interactions controlling the value of ν, which for simplicity we don't write down) and the scalar mass m φ is an output. In fact here the hierarchy problem occurs at tree-level, simply as a result of wishing to produce a small mass via splitting a multiplet. If we wish to have, say, M But if our theory is renormalizable, we know that quantum corrections will merely change the values of these parameters, and not the operators we have. The point is that the quantum corrections are SU (2) invariant, so the masses of both φ and ϕ will receive the same loop contributions. We will then still predict the mass of φ as m 2 φ = M 2 phys +λ phys ν 2 . Now at the level of inventing the theory we may still tune these parameters to get a small m φ , but we're tuning physical, observable parameters. In the theory described by Equation 2.1, one might have also said that we needed to fine-tune m 1 against m 0 in order to get a small m φ , especially if we calculated that the quantum correction m 1 was large. But there the fine-tuning was of a different sort, since m 0 and m 1 were only ever observable in the combination m phys . Here the fine-tuning has a much sharper meaning. This tuning translates into a physical demand on our theory that at high energies the strength of the interaction between our two scalars Φ, Σ for some reason has a value extremely close to −M 2 phys /ν 2 , despite having nothing to do 75 with either of these parameters. The tuning is now a physical feature of our theory and demands explanation. 15 So we see explicitly that the hierarchy problem is present when the light scalar mass is an output of the theory, rather than an input. If one is so inclined, one can say the words that the Higgs mass is simply an input, but this possibility spells the end of scientific reductionism. It is indeed conceivable that this is how the universe works. However, we know there is physics beyond the Standard Model at smaller length scales, and our best ideas for what those could be involve theories where the inputs are defined in the ultraviolet and the Higgs mass comes out. Whether the Higgs ultimately originates as a component of a larger multiplet, or a bound state of fermions, or an excitation of a string, we expect that the Higgs mass is a parameter that comes out in the low-energy theory. You can always use the history of physics to illustrate any polemic point you want to make. Now having exhibited that getting a light scalar truly does involve some fine-tuning of physical parameters, one may still say 'So? '. In the real world, we've observed (the analogue of) m φ , but the physics we've discussed at the heavy scale M is new physics, 15 Let me mention parenthetically a confusion one may encounter if one reads older literature on the hierarchy problem in GUTs. It was common to speak of having to 're-tune the parameters at every order in perturbation theory', as if imagining an algorithmic process where one first computed a treelevel prediction, tuned that to be correct, and then computed the one-loop corrections, re-tuned those parameters to get it right again, etc. This is framed as being 'worse' than just requiring 'one' set of tunings. This is moronic, for the simple physical reason that Nature does not compute via perturbation theory. There is a physical problem, which is how to get the electroweak scale out of other physical parameters in the theory. Whether you compute the predictions in perturbation theory, or on a lattice, or whilst standing on your head is immaterial. and we don't have experimental measurements of λ telling us the value is not that perfect value to get our light scalar. One might thus object that there's no fundamental inconsistency, and as long as our theory can fit the data anything else is just philosophy. But the criterion of it being not literally impossible for a theory to fit the data is an incredibly low bar, and scientists always use additional criteria to select theories. While there is ultimately a degree of subjectivity in any notion of 'naturalness', this is really the same subjectivity that one constantly uses in science to decide which of two explanations for some data to accept. A simple (approximately) historical example of this can be seen in epicyclic theories of the motion of the planets. The Ptolemaic, geocentric model of the universe predicted at first that the heavenly bodies orbited the Earth in circles, but eventually astronomical data was accurate enough to show that the motion of the planets and sun around the earth was not circular. In the Ptolemaic model, this was dealt with by adding an epicycle, the suggestion that the heavenly bodies moved on smaller circular orbits about their circular orbit around the Earth. As astronomical observations became more and more detailed over the ensuing centuries, multiple layers of epicyclic motion were needed to explain the data-circles on circles on circles, as depicted in Figure 2 .1. The description never stopped working though; if you'd like to, you may describe the orbit of any solar system object via r(θ) = N n=1 r n sin θ n , and for some large but finite N you'll be able to fit any orbit to within observational precision. So why did sixteenth century physicists favor Kepler's laws and heliocentrism? The discriminating factor is manifestly not which model better-fitted the data. Rather, the choice comes down to Occam's razor, to explanatory power, to simplicity and to finetuning of parameters. Physicists favor theories which do more with less-theories which explain more about the world while requiring fewer inputs. This is a subjective bias about how we think the universe should work, and it's possible that this philosophy will ultimately fail-but it's been working well thus far. 16 While keeping the above intuition firmly in the back of our minds, it can be useful to introduce a mathematical classification of this fine-tuning, with the understanding that no such measure is god-given and so what to do with such a measure is up to us. We'll discuss a couple such schemes, the first being a mathematical formalization of the dependence of an output of interest on the values of the inputs. This has the benefit that it is intuitive and simple to compute, so it is widely used in the particle physics literature. However, it lacks independence under how variables are parametrized so can lead to misleading conclusions if used without care. Furthermore, it will assign a measure of fine-tuning to individual points in the parameter space of a model, whereas we'd like to characterize the naturalness of a model as a whole-if a model only produces predictions that match the real world in a small region of its parameter space, that's another important element of fine-tuning [93] . For example, if new data removes all but a small fraction of the viable parameter space in a given model, we want to regard that model as being less natural afterward. An approach based on Bayesian statistics allows us to incorporate these issues and gives unambiguous comparisons of the relative naturalness of models upon the collection of new data [94] , but loses out on simplicity. A simple and often-used measure was introduced by Giudice and Barbieri [95] , who suggested the definition which may be called a measure of the fine-tuning of the input parameter X necessary to get out the correct output parameter m 2 . The logarithmic dependence naturally gives a measure of relative sensitivity and removes dependence on overall scale or choice of units. If ∆ X is large, this denotes a large sensitivity of m 2 to the value of X, and implies that one must choose the value of X very carefully to get out the right physics. In M 2 = 1, and we find that seesaw mechanisms are natural. So ∆ X matches our intuition here, and can be quite useful. However, for a model with free parameters a notion of fine-tuning at a single point in parameter space does not capture the full picture, and we should incorporate a notion of the volume of viable parameter space into our naturalness criterion [96] , as diagrammed in Figure 2 .2. This necessity should be intuitive in the context of constraining models of new physics. Models which achieve their aim throughout parameter space are viewed more favorably than models which only work in some small region of their parameter An approach based on Bayesian statistics can be used to better match what we want from a measure of naturalness, as is discussed well in [84] . The definition of a model in a Bayesian framework requires priors on its free parameters {θ i }, denoted p(θ i |M), and different choices of {p(θ i |M)} should be considered different models. The probabilist's notation p(A|B) may be interpreted as 'the probability of A given that B is true'. We also require a prior probability p(M) that the model M is true as a whole. After we receive data d, Bayes' theorem gives us the posterior probability for the model as where p(d|M) is known as the likelihood. Both p(M) and p(d) are explicitly subjective, but if we take the ratio of the posterior probabilities for two models M 1 and M 2 we find This expresses how the ratio of the likelihood of these two models changes after receiving new data. So while different physicists may disagree on the prior and posterior probabilities, the 'Bayes update factor' B ≡ p(d|M 1 )/p(d|M 2 ) is unambiguous and shows that the physicists agree on how the relative naturalness of the two models is affected by the new data. The likelihood in a model with free parameters {θ i } is calculated as This is an integration over parameter space of the likelihood of producing the data in this model, weighted by the prior probability distribution we've placed on our parameters. This balances the competing effects of how well parameter points fit the data with the principle of parsimony-models with large regions of parameter space which don't fit the data are penalized. The need to compare models to define naturalness in a Bayesian formalism is easily seen by the fact that the likelihood for any new physics model decreases monotonically as more data is collected and previously-viable regions of parameter space are ruled out (in the absence of a discovery, of course). An interesting playground for these ideas is the strong CP problem. In brief, this is the smallness of the so-called 'theta angle' θ in QCD, which controls the amount of CP breaking in the strong sector. While θ ∈ [0, 2π), empirical measurements now constrain Let's take p(m|weak scale SUSY) to be a logarithmic prior from m low to m high , where an upper limit m high ∼ 500 GeV − 1 TeV is justified by the requirement that the model give small Higgs mass corrections and a weak-scale dark matter candidate. If we collect collider data for a decade and find that much of the parameter space is ruled out, say with a limitm ≥ m we should update our thoughts on the naturalness of the model. This data has no effect on our non-SUSY GUT model, as it predicts no new light particles, but there has been a large effect on our SUSY model as its most favored parameter space has been ruled out. So we have a large Bayesian update factor. While these measures of naturalness are useful to help us clarify our expectations, we emphasize again that they must be used sensibly. But it's clear that some notion of naturalness appears solely from the axioms of probability, and is indeed baked-in to the practice of science inquiry. That's not to say that we should be epistemologically committed to the naturalness of the universe, but we can still see it as a useful guide toward new physics which has worked well in the past. All models are wrong, but some are useful. George E. P. Box Scientific Model Building (1979) [106] There is another obvious suggestion that is useful to discuss: perhaps the Higgs does not interact with any physics at higher mass scales, such that despite in principle worries about the hierarchy problem there is no hierarchy problem in practice. The physics which can destabilize the Higgs mass must, as seen in the above example, both be heavier than the Higgs and interact with it in order to generate a contribution. Since the Higgs mass in the SM is not technically natural (that is, it is UV sensitive), large mass corrections from such particles are generic, as we saw in Section 1.3. One can explore the idea that perhaps there are no such particles. This faces a number of challenges. The first difficulty is that we know there must be new physics. The Standard Model does not explain neutrino masses nor dark matter, though it is possible that both of these be resolved without introducing new heavy particles. But a deeper issue is that we know the SM cannot be a fundamental theory because the Landau pole in hypercharge makes it inconsistent. This demands that something must happen to rid the theory of this pole, and it will interact with the Higgs because the Higgs is hypercharged. This is one motivation for thinking that something like Grand Unification must take place, and of course its breaking introduces a heavy fundamental scale. One can try to get around this by appealing to quantum gravity coming in at scales below that of the Landau pole. But the Higgs certainly interacts gravitationally, so for this program to succeed one needs a quantum gravitational theory which does not introduce any scales. This is interesting to explore, but does not seem to be the way the universe works, though we leave detailed criticism of this idea to the gravity theorists. Furthermore, even if somehow all heavy particles are neutral under the SM gauge groups, there is no way for them to escape gravitational interactions with the Higgs. This leads to irreducible three-loop corrections to the Higgs mass [107] , as depicted in places an upper 'naturalness' limit on such fermions of ∼ 10 14 GeV. While far better than the ∼ 1 TeV limit for SM-charged particles, this is still well below the Planck scale and amounts to an enourmous constraint on UV physics. In fact the problem is a bit worse than this estimate, as we should sum over all SM loops that couple to the Higgs, but this suffices already to see the issue. So asking for the Higgs to be lonely enough to cure the hierarchy problem is a humongous requirement on the ultraviolet of the universe, and this approach faces a number of important hurdles. We mention that there is work on interesting theories which touch on some of these points, but as a whole Nature seems not to have taken this approach. One may hear the statement that the hierarchy problem disappears if you use a massindependent regularization scheme, for example dimensional regularization. Unlike the prior nonsolutions we've considered, this one is definitively incorrect. The mass scale one introduces in EFT is a stand-in for genuine physical effects of any sort which appear at shorter distance scales. So the cutoff regularization is useful for seeing an avatar of the hierarchy problem even when one does not know the ultraviolet theory. With a massindependent scheme, one must instead put in specific short-distance physics to see the problem, but we can easily see the general issue. As a simple example, take a theory with two real scalars -our light φ and a heavier ϕ. If we impose a Z 2 symmetry for simplicity, the action is Doing continuum effective field theory with dimensional regularization and the MS renormalization scheme, we must upgrade the masses and couplings to running parameters which depend on the renormalization scale µ, as usual (introduced in Section 1.2.2). Since our renormalization scheme is mass-independent, if we want to study physics at an Srednicki's textbook [18] . Where γ E is the Euler-Mascheroni constant, and we have Wick rotated k 0 → ik d , integrated in general dimension, expanded in the limit → 0 and defined µ 2 ≡ 4πμ 2 e −γ E to soak up the annoying constants. Indeed, we see that there's no quadratic divergence, which ultimately is due to the fact that scaleless integrals vanish in dimensional regularization d d k k n = 0, as must be true simply by dimensional analysis. Now we follow the MS renormalization scheme by adding a counterterm which cancels off the divergent piece and we match at µ = M ϕ to ensure that our low-energy EFT produces the same predictions as the UV theory, as discussed in Section 1.2.3. This gives We see that the high-energy degree of freedom ϕ contributes a threshold correction when we flow to lower energies and remove it from the spectrum. While there was never a quadratic divergence, we still found a large quadratic correction to the mass of the scalar φ which is proportional to the scale of new physics. This underscores the importance of not getting confused by unphysical features of renormalization. There is a physical issue, which is the sensitivity of the physical lowenergy scalar mass to the physics in the ultraviolet. Indeed if we tame the UV in different ways, we find different avatars of this sensitivity. It's true that a Wilsonian cutoff acts as a stand-in for arbitrary scaleful physics, which is why it's more direct to see the issue in that picture, but the same physical problem appears regardless of the regularization. After discussing those misconceptions, we may give a one-sentence description of the hierarchy problem: In a theory of physics beyond the Standard Model where the Higgs mass is an output, physical parameters must be finely-tuned in order to produce a mass which is far below the scale of new physics, in tension with the principle of parsimony. With that in hand, we are prepared to delve in to how the hierarchy problem may be solved in the next section. Our discussion below will not take place within a UV complete extension of the Standard Model, so one might worry that we are attacking a problem without knowing its source. While true, the point from Section 2.2.3 is that the sensitivity to UV physics is so general that we expect to need a mechanism which stabilizes the Higgs mass to whatever new heavy physics is out there. Our toy calculation of the relative naturalness of SUSY already evinces this point-SUSY tamps UV sensitivity no matter what it is, so the details of the UV completion are immaterial. As a result of this idea, we will mostly worry just about finding a way to produce a light scalar and assume that it can be embedded into whatever physics exists at high scales, rather than committing to a particular framework of grand unification or what have you. Of course it's possible that interesting mechanisms to produce an IR scale do rely on particular properties of the UV, and we'll discuss this important idea in Section 4.3. But even there, our initial goal is simply to produce a light scalar which is compatible with the Standard Model, rather than to write down a full theory of the universe on all scales. If we can first solve the problem in a toy model which shares some features of our universe, then we can hope to abstract what we learn from that to solve the real problem. Chapter 3 The Classic Strategies  In large part the story of particle physics over the past decades is the story of attempts to solve the hierarchy problem. Much theoretical effort has been put into understanding interesting symmetries and mechanisms for breaking them, and more generally ways that small numbers can pop out of physical theories; and much experimental effort has focused on locating empirical hints of these ideas. However, the past few years have seen many practitioners turn their attention toward topics like dark matter, cosmology, and astrophysics. And for good reason-on the experimental side, this is largely where the new data is and will be for the foreseeable future, and at the purely theoretical level the hierarchy problem has become a lot more challenging, as we will argue below. But this has lead to a new generation of particle theorists who are largely unfamiliar with the fantastic and brilliant ideas which drove the field in the prior couple decades. Despite the fact that we will argue below that these ideas largely appear to not be the way the world works at the weak scale, understanding this prior work can be enormously helpful for inventing novel ideas in the future. It is with this in mind that we introduce below the basics of a variety of interesting ideas and methods in particle physics against the backdrop of their relevance to the hierarchy problem. These are ideas that have not yet made their way into standard textbooks on field theory, but are nevertheless essential topics for students of particle theory to absorb. We will endeavor to explicate the core of these ideas in the simplest models possible, and will largely avoid discussing phenomenological considerations past producing a light scalar. The discussion will not be at the level of depth required for research in the field, but will hopefully be a nice overview of interesting topics for which references to serious introductions and reviews will be provided as well. So how does one solve the hierarchy problem? The classical solutions may be concep-tually divided into two steps. First one introduces some structure above the electroweak scale which protects the Higgs mass from large contributions due to UV physics. This could be something like a new symmetry which forbids a scalar mass term, or a modification to spacetime on small length scales, or the dissolution of a non-fundamental Higgs into component fields. However, the Higgs is not exactly massless, which is due to the fact that whatever structure we add is not a feature of the low-energy Standard Model. There must thus be some IR dynamics that break that UV structure at the electroweak scale to ensure that we end up with the Standard Model at low energies. Depending on the UV structure this may be something like spontaneous symmetry breaking or moduli stabilization or dimensional transmutation. There are two big categories of classical solutions. One is to find a field-theoretic mechanism which prevents contributions to the Higgs mass in the UV. Supersymmetry is the prime example here. The other is to bring the fundamental cutoff of the theory down to the infrared, such that in the UV there's no Higgs to talk about. This is exemplified by composite Higgs theories or theories where the cutoff of quantum gravity is lowered to the weak scale. To evince these strategies, we'll go through a couple examples of ways to forbid scalar masses and to break those structures. Our aim here is not to construct realistic theories of the Higgs but rather to understand these general principles, so we'll study simple toy models which allow us to appreciate the essential points. Superpartners aren't essential Supersymmetry exploits a loophole in the classic Coleman-Mandula theorem [74] by introducing fermionic symmetry generators, which in layman's terms turn bosons into fermions and vice-versa. By the Haag-Łopuszański-Sohnius theorem [110] , this is the unique extension to the Poincaré algebra. Since we know that symmetries tend to make physics easier, it is not surprising that supersymmetry is an indispensable tool in high energy theory, regardless of how or whether it is realized in the real world. Some useful general introductions to supersymmetry in d = 4 and its application to the real world are Terning's book [111] , Martin's periodically-updated lecture notes [112] , and Shih's video lectures [113] , in roughly increasing order of friendliness to neophytes. In a supersymmetric theory fields come in multiplets which include particles of different spins (so called 'supermultiplets') all having the same mass and quantum numbers. We add fermionic generators Q α and Q †α , called supercharges, with the defining (anti)commutation relations where P µ is the generator of spacetime translations and σ µ = (1, σ i ) with σ i the Pauli matrices. These may be determined simply by writing down all objects with the correct index structure. For later use, recall that spinor indices are raised/lowered with the invariant antisymmetric symbols αβ , αβ , as used for example in defining the conjugate We want to find irreducible representations of the supersymmetry algebra, called supermultiplets. Since P µ P µ = m 2 commutes with the generators Q α , Q †α , the different particles in a supermultiplet will have the same mass. As an example of how to generate supermultiplet states, consider a massive particle. We can go to its rest frame, where it has momentum P µ = (m, 0, 0, 0) with m its mass. Then the supersymmetry algebra greatly simplifies to Q α , Q †α = 2m1 αα , and we see that this is just a Clifford algebra of raising and lowering operators. We define a lowest weight state, or Clifford vacuum |Ω s such that it is annihilated by the undotted generators Now we can use the dotted generators as raising operators to generate the entire multiplet. A single fermionic supersymmetry generator must change the spin of a state by 1 2 . Starting at the top with a spin j particle gives us states of spin j − 1 2 and j + 1 2 on the middle 95 line, and another state of spin j on the bottom line. In d = 4 the supermultiplet formed from a vacuum state of spin 1 2 is called a 'vector multiplet' and contains four states with spins (0, 1 2 , 1 2 , 1). That formed from spin 0 is called a 'chiral multiplet', and has states with spins (0, 0, 1 2 ). Note that this is fewer degrees of freedom, since negative spins are not allowed. Beginning with spins higher than 1 2 leads to states with spins greater than 1, which will take us into supergravity and will not be necessary for our purposes. We could repeat this exercise for massless supermultiplets, labelling states by their energy and helicity |E, λ . We would find a Clifford algebra with only one set of raising/lowering operators, and find supermultiplets with helicities λ and λ + 1 2 for some starting λ. Then CPT invariance would force us to add states of helicity −λ and −λ − 1 2 . Merely from the definition of the symmetry group there are already a few interesting immediate results. For a start, we show that physical states have nonnegative energy in a supersymmetric theory, and the vacuum energy is an order parameter for supersymmetry breaking. First, let's give a simple expression for the Hamiltonian operator of supersymmetry. We act on our anticommutation relation with (σ ν )α α and recall various identities to note that σ µ αα (σ ν )α α = 2η µν , which gives us where we have used the fact that the zeroth component of the generator of spacetime translations is the generator of time translations, which is the Hamiltonian operator. Then we can write the energy of some state state S as Connected to that fact is that each supermultiplet contains the same number of fermionic and bosonic degrees of freedom. We can see this by defining an operator F which counts the fermion number of a state, so that bosonic states have eigenvalue 1 under (−1) F , and fermionic states have eigenvalue −1. Since the SUSY generators interchange bosonic and fermionic states, they must anticommute with (−1) F . Now, for a given supermultiplet consider the states |a with the same given fourmomentum p µ , p 0 = E = 0. Since the supercharges commute with P µ , we know that these must form a complete set of states in this subspace a |a a| = 1. Now consider the trace of the weighted energy operator ( where we have suppressed the contracted spinorial indices. This implies that the number of bosonic degrees of freedom is the same as the number of fermionic degrees of freedom in our supermultiplet, which we found to be true in the example we considered above. There is a beautiful formalism of 'superspace' which can be used to make supersymmetric theories far more transparent, but introducing this would be too large of a digression for our purposes. 17 We simply want to see the effects of supersymmetry on (in)sensitivity of low-energy physics to the ultraviolet, for which studying a simple theory of chiral superfields will do. The Wess-Zumino model is the simplest such example which is not free, consisting of a single self-interacting chiral supermultiplet, and was historically the first non-trivial four-dimensional theory proved to be supersymmetric. We may write down the Wess-Zumino Lagrangian as Let's look at the vacuum energy, which we'll calculate generally but schematically. A quantum harmonic oscillator has ground state energy ± 1 2 ω for bosonic and fermionic states respectively, with the sign being familiar from the Casimir effect. If we consider a 17 Martin's notes [112] serve as a good introduction to traditional 'off-shell' superspace for N = 1, d = 4 theories, and Thaler's TASI lecture notes [114] are also a fantastic resource. There is a related but distinct formalism of 'on-shell' superspace, which falls under the heading of the amplitudes/on-shell/Smatrix program. This was first introduced very early on by Nair [115] and was used to great effect by Arkani-Hamed, Cachazo, & Kaplan [116] much later. A pedagogical introduction to on-shell techniques including superspace can be found in the textbook by Elvang & Huang [117] . Until recently, the on-shell program was mostly restricted to massless particles. As it so happens, after Arkani-Hamed, Huang, & Huang [118] introduced a beautiful extension of the formalism to include massive particles, it was Timothy Trott, my undergrad mentee Aidan Herderschee, and myself who formulated an extension of the on-shell superspace formalism for massive particles [9] . The on-shell program is another fascinating line of work that I suggest any aspiring particle or field theorist learn about. box of side length V 1/D , the energy of the fields inside it is and as we make the box bigger V 1/D → ∞, the sum turns into an with m B a boson mass and m F a fermion mass, where the sum over species is implicit. We also recognize E 0 /V as the vacuum energy density, denoted Λ, which we can write as Now if we specialize to D = 4 and introduce a cutoff k max up to which we're confident that our description of particle physics holds, the schematic form is simply Now we see quite generally and explicitly that in a supersymmetric state the vacuum energy vanishes, since there are equal numbers of bosonic and fermionic fields with degenerate masses. Furthermore, spontaneous breaking of supersymmetry breaks the degeneracy but does not change the numbers of fields, so softly broken supersymmetry retains protection from the largest contribution. Let's look now more sharply at the one-loop contributions to the scalar mass in the Wess-Zumino model, regularized with a hard cutoff Λ. Take care that we've written the Lagrangian in terms of two-component spinors, an exhaustive guide to which can be found in [119] . The three diagrams are shown in Figure 3 .1, and their evaluation proceeds as We see that the UV sensitivity of the scalar mass in this theory has disappeared, even if the two fields have different masses. In the limit of unbroken supersymmetry, the contribution vanishes identically. physical parameters [122] . However, constraints on flavor-violating couplings and on CP violation tell us empirically that the soft terms that appear must be very non-generic. In fact, looking at the MSSM in detail it turns out there are no places for supersymmetrybreaking to enter directly, and indeed there are general arguments that such breaking must take place in another, hidden sector and be indirectly communicated to the MSSM fields (see e.g. Martin's Section 7.4 [112] ). The origins of supersymmetry-breaking being a separate sector does force us to expand our model of particle physics, but on the upshot this sequestering means we can explore interesting phenomenology in sectors which are unconstrained. One can write down models where supersymmetry breaking is mediated by supergravity effects [123, 124, 125, 126, 127, 128, 129] , communicated to the SM fields by our gauge bosons from a sector with new, massive SM-charged particles [130, 131, 132, 133, 134, 135] , or takes place at a physically separate location in an extra dimension [136, 137, 138, 139, 140, 141, 142, 143, 144, 145] , for a few examples. A full discussion of the mechanisms and strategies for models of supersymmetry-breaking is beyond our scope, but we highly recommend Intriligator & Seiberg's lecture notes [146] as a general reference along with Martin's notes [112] . Of course we would like this phase transition to originate as spontaneous symme- 18 Perhaps the more urgent reason for needing two Higgs multiplets is that the interactions in supersymmetric theories are highly constrained by 'holomorphy', a full explanation of which here would require too much machinery but which leads to the conclusion that the same multiplet cannot have Yukawa interactions with both the up-and down-type quarks. However see [121] for the interesting possibility that at high energies only the up-type Yukawa interactions exist, and the down-type and charged lepton masses are induced by supersymmetry-breaking. try breaking, rather than explicitly putting it in by hand, since we want the far UV to be supersymmetric. Such spontaneous breaking requires the generation of a scale, and so it would be great if such a scale were generated dynamically, as in the dimensional transmutation we saw in QCD in Section 1.3.3. This would then be a natural mechanism for SUSY breaking. This phenomenological prospect lead to and benefited from a fantastic body of work understanding the details of supersymmetric gauge theories e.g. [147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157] , which can be found reviewed in textbooks by Terning [111] and Shifman [158] and in TASI notes by Strassler [159] . We stated above that fields in a given supermultiplet share all the same quantum numbers, but there is in fact one exception. The supersymmetry algebra in Equation 3.1 So there is a generator of a global internal symmetry that we may add that has nontrivial commutation relations with the supercharges: This generator is known as an R-symmetry generator, and if the theory is invariant under an R-symmetry that means that each supermultiplet Φ can be assigned an R-charge r Φ and the theory is invariant under transformations of each multiplet Φ → Φe ir Φ α , schematically, where Φ is the collection of fields in that multiplet. Since R does not commute with the supercharges, the different fields in the supermultiplet have different R-charges. For example if Φ is a chiral superfield consisting of (φ, ψ) then under a global rotation by α they transform as which is simply because |ψ ∼ Q |φ . It's important to emphasize that this R-symmetry is not part of the supersymmetry algebra, so one may have supersymmetric theories which do or do not implement R-symmetry. Nelson & Seiberg showed a fascinating connection between R-symmetry and supersymmetry-breaking [160] . They show roughly that for a low-energy Wess-Zumino model (possibly after having integrated out confined strong dynamics) as long as one has a 'generic' potential (in the sense that a generic set of n equations in n unknowns has a solution), then a vacuum spontaneously breaks supersymmetry if and only if it spontaneously breaks R-symmetry. The reasoning is simply that such a symmetry imposes an additional constraint on the potential minimization equations, leading to a solution no longer being generically present. For later use we mention the possibility of 'extended supersymmetry', where additional supercharges are added where A, B = 1..N index the supercharges. The construction of supermultiplets proceeds as before, but there are now more nonvanishing combinations of supercharges to act on the Clifford vacuum, so supermultiplets are enlarged. In four dimensions, the most supersymmetry we can have without gravity is N = 4, which contains enough supercharges to relate the helicity −1 vector all the way to the helicity +1 vector; any more supercharges would necessarily yield particles of spins 3/2, 2. If we are willing to include these degrees of freedom we can only go up to N = 8 'supergravity' (SUGRA), as more charges would lead to a theory with fundamental particles of spins s > 2 which is pathological 19 . Proeyen [163] . Ultimately if supersymmetry is a field-theoretic feature of the ultraviolet of our universe we must have SUGRA as well, as it simply results from applying the supercharges to the graviton field, but we won't discuss SUGRA any further. When we enlarge our superalgebra we also enlarge the (potential) R-symmetry groupthe group of symmetries which do not commute with the supercharges-since we can now shuffle around the supercharges in addition to rephasing them. We'll revisit this in Section 3.2.3 in the context of utilizing non-trivial R-symmetry representations to break supersymmetry. I exist in the hope that these memoirs, in some manner, I know not how, may find their way to the minds of humanity in Some Dimension, and may stir up a race of rebels who shall refuse to be confined to limited Dimensionality. Flatland, 1884 [164] One of the most important ideas in theoretical physics developed in the latter part of the 20 th century is that there may be additional spatial dimensions past the familiar three of our everyday experiences. Theories in which additional spatial dimensions are present were first studied in the context of unifying gravity and electromagnetism, first by Nordström [165] (before General Relativity!) and then by Kaluza [166] and Klein [167] . These ideas saw a resurgence of interest some half-century later with the advent of string theory, and the vision of all features of the universe being fundamentally geometrized. Against that backdrop, it is clearly prudent to consider the interplay of such theories with the puzzle of the hierarchy problem. As we shall see, extra dimensional theories can produce terribly interesting physics, and the possibilities are manifold. To consider the possibility that there are additional microscopic dimensions, we develop a picture of the effects of fundamentally D-dimensional fields where D = 4 + d. On our where our fields are in irreducible representations of the D-dimensional Poincaré algebra, and the Lagrangian manifestly obeys D-dimensional Lorentz invariance. We'll use boldface for D-dimensional fields and Latin letters for D-dimensional Lorentz indices. However, since K is compact, it places constraints on the mode expansions of our fields. Then when we want to study the effective four-dimensional theory we first need to decompose our fields into irreducible representations of the 4-dimensional Poincaré algebra. The D-dimensional vectors and tensors will become multiplets of 4-dimensional fields. Then we can explicitly integrate over the compact manifold K, which will produce a tower of states. As an easy, explicit example, take the compact manifold to be the circle S 1 of length 2πR, and consider a single complex scalar. We start with From introductory quantum mechanics, we know that the boundary conditions of being single-valued around the circle constrains the mode expansion of φ. We can write where the normalization will produce canonically normalized kinetic terms in the 4dimensional action, and should generally be Vol(K) . Plugging this into the action gives where the first and second terms come from either the M 4 derivatives or the K derivatives acting on the mode expansion. The integral over the compact direction now gives, using orthogonality 2πR 0 dye i(n−m)y/R = 2πRδ nm , a 4-dimensional action We see that we now have a tower of 4-dimensional states that have arisen from the one 5-dimensional scalar, as shown schematically in Figure 3 .2. There is a 'zero-mode' which has a mass given by the 5-dimensional mass term, and then there are states with larger masses. If R is small, these will be heavy, and so we would only see them at, say, a high energy particle collider. Note that all of these higher levels are doubly-degenerate, since n ranges over all the integers. If we had included the graviton in this compactification procedure, we would see the so we see the emergence of a 4-dimensional scalar, vector, and two-index symmetric tensor. And it was in this context that compactification was originally studied as a unification of general relativity and electromagnetism. There's another very important effect of the compactification. Let's consider the Einstein Hilbert action Noting that the Ricci scalar R ∼ ∂ 2 always has mass dimension [R] = 2, we see that rewrite the action in natural units we must define a (4 + d)-dimensional Planck mass as If we take the metric to be independent of the K coordinates, then the integration over K just gives a factor of the volume V K of the compact space Putting this into the form of the four-dimensional Einstein-Hilbert action, we find  where R is the radius of the extra dimensions, we find that a parsimonious d = 3 spherical dimensions of radius R 1 nm suffice to remove the hierarchy problem and accord with This can be done by imagining we live on a (3 + 1)-dimensional topological defect which is embedded in the larger (4 + d)-dimensional space. 'Topological defect' sounds exotic, but these are just (semi-)familiar non-perturbative objects such as the branes of string theory [172, 173, 174] , or the cosmic strings or domain walls that can appear in Higgsed gauge theories and which are introduced well in Shifman's textbook [158] . The original proposal suggests a weak-scale vortex in which zero-modes of our familiar fields are trapped, which is super cool. Note that the new physics appearing at the TeV scale in this scenario is about as violent as you could imagine: quantum gravity appears at a TeV! This leads to a variety of fascinating signatures and constraints, even in the absence of concrete model of quantum gravity, though embeddings into string theory have also been found [175, 176, 177] . Very generically there are corrections to the Newtonian gravitational laws [178] , all sorts of effects on precision observables [179] , a loss of flux of high energy particles into the ambient space either astrophysically [180] or at a collider [144] , violations of the global symmetries of the SM since quantum gravity does not respect them [181, 182, 183] , and production of Kaluza-Klein gravitons [184] and black holes at TeV-scale colliders [185, 186, 187, 188] . This is a fascinating field which is well worth studying in detail, but unfortunately we do not have the space to do it justice. For more detail we refer to the reviews by Rubakov [189] and Maartens & Koyama [190] , and the more introductory notes from Csáki [191] , Kribs [192] , Pérez-Lorenzana [193] , Cheng [194] , and Csáki, Hubisz, and Meade [195] . However, shrewd readers will be eager to point out that we haven't actually solved the hierarchy problem; we've merely traded the m H M pl hierarchy for a 1/R (2+d)/2 M (4+d) pl hierarchy. And indeed, it can be difficult to stabilize the size of the extra dimensions in this scheme. But the conceptual leap of considering geometric solutions to the hierarchy problem is incredibly important and leads to many further interesting directions. The next ingredient we need is to control the existence of opposite-spin partners. Let us consider a spacetime manifold M = M 4 ×K G with M 4 four-dimensional Minkowski space and K G a d-dimensional compact 'orbifold'. An orbifold is constructed by 'modding out' a manifold K by a discrete symmetry group G. A manifold is a space that looks locally like Euclidean space; an orbifold is one which locally looks like the quotient space of Euclidean space quotiented by a finite group. In layman's terms, quotienting or modding out is just identifying points which are transformed into each other under G-our space becomes the space of equivalence classes of K under the action of elements of G. As an illustrative example, consider the line R and the action of the discrete symmetry When we form the quotient space and identify points under this Z 2 action, we find the half line R ≥0 . You'll notice that this space has a boundary at x = 0, which was a fixed point of the symmetry group. Mathematicians would say that the Z 2 acts freely except at this point. This is a generic feature, and in fact what makes orbifolds interesting for our purposes. We could have also imagined 'orbifolding' R by the translation T (2πR), Compactifying on orbifolds also gives us a way to cure our missing partner ills. Con- Modding out by the former yields an orbifold because it has a fixed point, while modding out by the latter returns a regular manifold. corresponds to folding the circle over on itself. We can produce independent fixed points by uplifting first to the real line, or equivalently by folding the circle into quarters. sider quotienting the circle by a Z 2 which folds the circle over onto itself, y 2πR − y. This produces a line segment with boundaries at both ends y = 0, πR. We can alternatively think of this as modding out the real line by both translation and mirroring, producing R/T (2πR) × Z 2 (0). There are then two different sorts of fixed points-y = 0 is fixed by Z 2 (0), and y = πR is fixed by T (2πR)Z 2 (0) ∼ Z 2 (πR). You can then envision this as the circle of length 4πR with these two discrete identifications, that is How does this affect the resulting compactification? As a first step should take our results and rewrite them in terms of eigenfunctions of our Z 2 symmetry. We write where all we have done is rearrange things by defining where the superscript denotes their eigenvalue under the Z 2 . Now the way to change our S 1 compactification to an S 1 /Z 2 compactification is to impose in our 5-dimensional action that φ transform with definite parity, which must be the case if the Z 2 is a good symmetry 20 . You'll notice our free action does not demand a particular choice of parity for φ, so we are free to choose φ either even or odd. But in either case we are forced to get rid of half of our states. If φ is even we must set φ (−) n = 0, and if φ is odd we must set φ (+) n = 0-including, importantly, getting rid of the zero mode. If we wanted to include certain interactions in the higher-dimensional theory, that could dictate the transformation of φ-for example, the interaction term φ 3 would necessitate an even φ. This is just the same as we're familiar with in four dimensions. More interestingly, let us consider the effect of the orbifold on larger Lorentz representations. Imagine a 5-dimensional gauge field with free action For another example of the usefulness of orbifolding, consider a five dimensional theory with the minimal amount of supersymmetry and we want to ensure our fourdimensional theory has only N = 1 supersymmetry. First let's recall why we cannot have more than N = 1 in four dimensions. As emphasized above, the SM is a chiral theory, wherein the different chiral components of its Dirac fields are in different gauge symmetry representations. This means that N = 2 supersymmetry in four dimensions is too much for us, since the irreducible representations of super-Poincaré in that case don't allow for chiral matter. In particular, the N = 2 vector multiplet must transform in the adjoint, and the N = 2 hypermultiplet must transform in a real representation in order for it to be CPT self-conjugate, which means the two Weyl fermions must transform in conjugate representations. Thus if we want to imagine that the world came from a higher-dimensional supersymmetric theory, orbifold compactification is necessary. The same problem appears just for five dimensional spinor fields, since there is no such thing as chirality in odd spacetime dimensions. So under dimensional reduction, one five-dimensional spinor breaks into two conjugate four-dimensional spinors, and you cannot get chiral matter. This is really the same problem as the above, since the possible supersymmety comes exactly from the possible spinor representations. Compactifying a theory on an orbifold, rather than a manifold, will allow us to solve both of these problems-obtaining chiral matter, and reducing the amount of supersymmetry we have. First note that we can build N = 2 supermultiplets out of two N = 1 supermultiplets, which is really just thinking about a particular ordering for the construction of the supermultiplet by acting with supercharges. So for each N = 1 superfield we want to have, we need to arrange for it to be even under the parity, and so have a zero-mode, and its partner superfield to be odd under the parity, and so only have n > 1 Kaluza-Klein modes. In this way we get a set of zero modes which are chiral and N = 1 supersymmetric, while the towers reflect the full N = 2 supersymmetry and are non-chiral. As an explicit example, consider an N = 2 vector multiplet, which consists of a real scalar Σ, two fermions ψ a with a = 1, 2, and a vector A M . Under the SU (2) R symmetry, the scalar and vector are singlets and the fermions form a doublet. As in the example above, the N = 2 Lagrangian for this multiplet dictates that the two N = 1 multiplets living inside it transform differently under the Z 2 . So on the four-dimensional boundary one gets a zero-mode for either (A µ , ψ 1 ) or (ψ 2 , A 4 + iΣ), corresponding to a vector multiplet or a chiral multiplet respectively. In slightly more group-theoretic language we can say that we've embedded the Z 2 in the SU (2) R , and have been forced by the physics to put the doublet in a two-dimensional representation and to pair each of these fermions with two bosonic degrees of freedom, This gives us either an N = 1 chiral multiplet or an N = 1 vector multiplet on the boundary. More discussion and details can be found in Quirós' TASI notes [196] , and in e.g. [197, 198, 199] . In fact orbifolding can do even more symmetry-breaking for us. As mentioned above, we can alternatively consider compactification on an interval as the orbifold , which then means the two boundaries are fixed points of independent Z 2 symmetries. In particular this means we can choose different embeddings of the Z 2 in the full symmetry on the two ends y = 0, πR [200, 201] . Now let's apply this technology to the N = 2 case we considered above. We saw we had two different choices for nontrivial embeddings of the N = 2 into the R-symmetry to break the supersymmetry down to N = 1 at the boundary. These correspond to choosing which half of the fields are even under the Z 2 and so get zero-modes. Now that we have independent Z 2 s on either end, we can choose these to leave different N = 1 symmetries unbroken. In a microscopic picture, what we end up with is a theory on R 3,1 × I[0, πR] where the bulk is (N = 2)-supersymmetric and the boundaries respect different N = 1 supersymmetries. When we look at the effective four-dimensional theory at distances much larger than R, we have fully broken supersymmetry with the breaking being a nonlocal effect-one must be sensitive to physics on both boundaries in order to see the full breaking. As a result of this nonlocality, supersymmetry-breaking is guaranteed to be 'soft' and the effects cannot depend on positive powers of UV scales, as we will discuss further momentarily. This leads to fantastically predictive models of BSM physics from around the turn of the millennium which really do read like they have it all figured out (see e.g. [202, 197, 203, 204, 205, 198, 206, 199] ). It's worth understanding these in some detail simply because of how beautiful they are, but they uniformly lead to lots of (thus far) unseen structure near the TeV scale as KK partners become excited. Consider a single extra circular dimension of radius R with gravity and some gauge field. Upon restriction to the four-dimensional Lorentz group, the five-dimensional graviton breaks up into a four-dimensional graviton, vector, and scalar, while the five-dimensional vector breaks up into a four-dimensional vector and scalar. Each of these must be massless by five-dimensional gauge-invariance above the scale 1/R, but below that they can pick up mass corrections up to that cutoff. Either of these possibilities then amounts to a mechanism for UV protection of a scalar mass. In implementing this in the SM, the first possibility is known as the Higgs being a radion-the scalar which controls fluctuations of the size of the fifth dimension, h−δh = h = 1/R, and the latter is denoted 'gauge-Higgs unification' for obvious reasons. A particular motivation for gauge-Higgs unification is as an extension of the strategy of grand unification. As mentioned in Section 1, the SM gauge bosons and fermions beautifully fit into representations of larger gauge groups, but in 4d GUTs the Higgs is left out in the cold as an extra puzzle piece. But 'grand gauge-Higgs unification' in higher dimensions may allow even further frugality of ingredients [207, 208, 209, 210] . A very interesting feature of this sort of construction is that the symmetry-breaking which is responsible for producing the light scalar is nonlocal -one must traverse around the fifth dimension to see the effects of the breaking. This should be intuitively clear, as at distances small compared to R the theory looks like five-dimensional Minkowski space. If it isn't obvious, I recommend musing by analogy on how we could tell whether or not the universe is a sphere with radius far, far larger than the Hubble scale R 1/H 0 . As a result of this nonlocal symmetry-breaking, the scalar mass must be finite and calculable in the low-energy theory below 1/R. There cannot be any sensitivity to ultraviolet energy scales Λ UV , as this corresponds to a 'local counterterm', but in the theory above 1/R we know that the mass vanishes identically by gauge invariance, so such a counterterm cannot occur (recall our discussion in Section 1.2.1). This is powerful because symmetry-breaking generally leaves residual logarithmic dependence on large scales even when quadratic dependence has been eliminated, as we saw in the example of supersymmetry above. Of course the theory can generate finite corrections to the mass of such a scalar at and below the scale 1/R . If we look only at the low-energy effective theory then these look divergent, but we know they must get cut off at 1/R. Since we know the high-energy theory we can ask how the scalar gets a mass at all, which seems to be impossible from gauge invariance. In fact the scalar mass comes from the Wilson loop wrapping the nontrivial cycle around the fifth dimension [211, 212, 213, 214, 215, 216, 217] . In any gauge theory there is a gauge-invariant operator called a 'Wilson loop', where g is the gauge coupling, C is some closed path through spacetime, and P denotes 'path ordering' of the operators along C in similarity to the time ordering in the definition of the Feynman propagator. In an Abelian theory, the gauge field transforms as  We note that on a circle the four-and five-dimensional gauge couplings are related as we see that this operator includes a four-dimensional mass for the zero-mode, where we note that the natural size for the Wilson coefficient σ 1/R yields a scalar mass expectation which is m 2 φ g 2 4 /R 2 , unsurprisingly as R is the only scale in the 118 problem, and the dependence on the gauge coupling reveals the scalar's five-dimensional origins. For a successful such model of gauge-Higgs unification, we need not only to get a light scalar but also to endow that scalar with dynamics pushing it to break electroweak symmetry [218, 202, 219, 220, 221, 222, 223, 224, 225, 226] . There is far too much rich physics involved here to mention, but discussions of the calculation of the one-loop effective potential in these models can be found in [227, 228, 229] . Gauge symmetry-breaking by a higher-dimensional gauge field component getting a vev is sometimes referred to as the 'Hosotani mechanism' [215, 216, 217] . As a result, the lower bound on the KK scale is now far above the Higgs mass, which makes all of these sorts of models increasingly less attractive as solutions to the hierarchy problem. We have saved for last what is, in some sense, the most obvious strategy to pursue. As discussed in Section 1.3, the Standard Model already breaks a symmetry and generates a mass scale in a natural manner with the chiral condensate of QCD. Perhaps Nature has only this one trick, and repeats the same mechanism to generate the weak scale. After all, we mentioned above that the QCD condensate does in fact break the electroweak symmetry, just not at the right scale. The idea is to introduce a new gauge sector which is asymptotically free and so becomes strong and confines at the electroweak scale. If the condensate has electroweak quantum numbers, then it breaks electroweak symmetry just as a Higgs field would, but now without any Higgs. This strategy is known as 'technicolor', and was proposed in its simplest form by Weinberg [234] and Susskind [235] . A modern pedagogical introduction can be found in TASI lectures from Chivukula [236] or Contino [237] , which has heavily influenced this discussion, and a more detailed classic review is from Hill & Simmons [238] . We introduce a technicolor sector which is an SU (N T C ) gauge group with N D technicolor fundamentals which are electroweak doublets and their singlet partners, together However, the SM Higgs not only breaks electroweak symmetry but also gives mass to the quarks, and thus far we haven't introduced any coupling between the quarks and the technicolor sector. To get the appropriate couplings we can embed both of these gauge groups in a larger 'extended technicolor' group, After this extended group undergoes spontaneous symmetry breaking at Λ ET C , the broken gauge bosons generate the appropriate four-Fermi interactions and then when the technicolor group confines at a scale Λ T C , we see the emergence of quark masses But this clearly suffices solely to generate a single quark mass scale, since the Yukawa coupling is originating from a single gauge coupling, so it was quickly realized that accounting for flavor physics required far more structure and significantly larger gauge groups [239, 240, 241, 242] . To generate the variety of quark mass scales with this mechanism would require a cascade of breakings from [243] . These efficiently parametrize deviations from the tree-level form of the vector boson propagators, and can be simply connected with experiment. Ensuing estimates for the sizes of these parameters in strongly-interacting models were very far from empirical measurements [244] . The program of technicolor lives on with 'walking technicolor', the idea that the confining dynamics may be due to a strongly-coupled gauge theory which behaves very differently from QCD [245, 246, 247, 248, 249, 250, 251] . This is too large a digression for us to introduce, but we mention that there is interesting recent work relating the existence of 'walking' dynamics to proximity of the theory to a fixed point at complex value of the coupling [252, 253, 254, 255, 256 ]. However, there is another way that compositeness can be useful for us in securing a light electroweak-symmetry-breaking scalar: It will allow us to realize the dream of a pseudo-Nambu-Goldstone Higgs. Recall that whenever a continuous global symmetry is spontaneously broken, there appear scalar Goldstone bosons π i (x) which parametrize excitations about the vacuum state in the direction of the broken generators. Since the theory had a global symmetry, the potential along these directions is flat, and thus the Goldstones are massless. More strongly, they contain a shift symmetry: where ξ is independent of x. This means they may only be 'derivatively coupled'; they may appear in the Lagrangian solely as ∂ µ π i (x). Thus, a non-zero mass for such a scalar is technically natural-if such an operator is present, π i is said to be a pseudo-Goldstone and corresponds to the breaking of an approximate symmetry. This is a familiar story in the context of the QCD condensate breaking the approximate chiral symmetry of the quarks, leading to light but not massless pions. In the case of technicolor, the phenomenon of confinement of a strongly interacting sector is directly responsible for breaking the gauged electroweak symmetry, leading to no separation between the two scales Λ T C and v. This means the technipions are immediately eaten, and there is no scale at which it looks like a scalar field is responsible for symmetrybreaking, which leads to large electroweak precision constraints. In a general composite Higgs scenario, we'll attempt to arrange for separation between the scale of confinement and the scale of electroweak symmetry breaking by having confinement break an enhanced global symmetry, leading to pseudo-Nambu Goldstone bosons whose masses are protected. We then want to radiatively generate a potential for these pNGBs, leading to them getting a vev and breaking electroweak symmetry at a lower scale. A degree of separation between confinement at the scale f and EWSB at v will reduce the difficulties with electroweak precision constraints, as this scenario returns to the SM elementary Higgs sector in the limit v/f → 0. More pressingly, now that we have gained experimental access to energies close to v it's even more clear that scale separation is needed-confining dynamics lead generically to lots of resonances at the scale f , which would be seen in all sorts of ways. The general setup is a group G of (approximate) global symmetries, of which a subgroup H 0 is gauged. We will have strong dynamics at the scale f break the global  . We diagram the general structure in Figure 3 .5a and the minimal model in 3.5b. To evince the ideas in the simplest scenario possible, we'll discuss an even simpler In (b), the same structure applied to the 'minimal' model of [257] . In (c), the composite Abelian Higgs toy model discussed in [258] , where the symmetry group after confinement coincides with the gauge group. model for a composite pNGB which then breaks a U (1) gauge symmetry. This is just a toy model to understand the features, which has already been kindly worked out in the extensive review from Panico & Wulzer [258] , and which we'll call a 'composite Abelian [259, 260] , and some modern introductions to this technology can be found in Schwartz' textbook [22] , in a pedagogical review of Little Higgs models by Schmaltz & Tucker-Smith [261] , and in exhaustive detail in the review by Scherer [262] . The big idea is one of bottom-up effective field theory: Given knowledge of the symmetry-breaking structure, we can cleverly parametrize our fields to easily see the structure of the Lagrangian which is demanded both before and after symmetry-breaking. In our case we must start with an SO(3)-invariant Lagrangian of a fundamental field Φ, which is a familiar 3d vector. a nonzero vev and the symmetry is broken down to rotations keeping Φ fixed, which is simply U (1). There are a two-sphere worth of vacua corresponding to possible angles for Φ , which parametrize the Goldstone directions. We can make the split between broken and unbroken generators explicit by parametrizing our field as where in the first equalityT i are the two broken generators, Π i (x) are the massless Goldstones corresponding to fluctuations along the vacuum manifold, and σ(x) is the massive 'radial mode' giving fluctuations about the vev. We eschew writing down the generators explicitly and assert that in this case one finds the compact latter expression, with Π = Π Π . We can find an explicit expression for the interaction of the Goldstones and the radial modes by simply plugging this parametrization into the Lagrangian above. We indeed find a mass for σ of m σ = g f , massless pions Π and a tower of all possible interactions between these fields which are consistent with the symmetries. We now have a theory of massless scalars transforming under an unbroken symmetry; our pions form a doublet of SO(2) transforming as Π → exp (iασ 2 ) Π, corresponding to rotations about the unbroken SO(3) generator. We can complexify by introducing H is an exact Goldstone boson, so cannot pick up a potential to then itself get a vev and break U (1). However, when we gauge a U (1) subgroup of our original SO(3) global symmetry we're introducing explicit breaking of the symmetry and resultingly H becomes a pNGB and can pick up a mass. The tree-level effect of this gauging is simply to upgrade derivatives to gauge covariant derivatives, Now as a result of SO(3) rotations no longer being an exact symmetry, H is free There are a variety of important aspects and interesting directions we do not have the space to discuss. Even past understanding the best sorts of group structures to which to apply this strategy, it is clearly important to understand the sorts of field theories which can confine to break G → H 1 , as well as the detailed structure of the potential radiatively-generated by SM fields. Then it's important to explore the possibility of a natural structure which dictates v/f < 1-while there has been much work on this, we mention in particular the interesting strategy of 'collective' symmetry breaking in which a symmetry is broken only by an interplay between different couplings. This class of models is known as the 'Little Higgs' [261, 264, 265, 266, 267, 268] , and can be seen as a purely four-dimensional application of the strategy of nonlocal symmetry breaking through 'nonlocality in theory space' [269, 270] , which is a fascinating topic. The TASI notes by Csaki, Lombardo, Telem [271] provide a pedagogical introduction to these topics. Finally, let me mention that composite Higgs models may be understood as being dual to a novel class of extra-dimensional models known as Randall-Sundrum models [272, 273] . Unlike in the simple cases we discussed in Section 3.2, in these models the spacetime does not have a product structure (as did M 4 × K) and the geometry is said to be 'non-factorizable'. In this scenario our four-dimensional universe is seen as a brane living on one end of a five-dimensional orbifold of anti-de Sitter space. The minuteness of the electroweak scale compared to the Planck scale is a result of a large fifth-dimensional AdS 'warp factor' between the brane we live on (the 'IR brane') and the brane on the other end of the space (the 'UV brane'). As in Section 3.2.2, this trades the electroweak hierarchy into a geometric hierarchy, but now in AdS we can find novel, natural ways of generating such a hierarchy of scales [274, 275] . Furthermore, embedding our universe into an AdS spacetime means we can take advantage of the enormously powerful program of AdS/CFT holography, which enables us to study the strong-coupling phenomena of composite Higgs models via their weakly-coupled gravitational duals. Pedagogical introductions to holography can be found in lecture notes from Sundrum [276] and Kaplan [277] , and with more background in the textbook by Ammon & Erdmenger [278] . That machinery is not all necessary to appreciate the workings of Randall-Sundrum models, though, and there are a wide variety of great lectures notes aimed at particle theorists, for example those of Sundrum [279] , Csaki & Tanedo [280] , and Gherghetta [281] . The great tragedy of science -the slaying of a beautiful hypothesis by an ugly fact. Biogenesis and Abiogenesis (1870) [282]  We've seen in Chapter 3 a cadre of theories which can produce a light scalar naturally, and there's one feature all the classic approaches have in common: they predict new states with Standard Model charges close to the mass of the Higgs. This is seemingly inevitable simply from the structure of effective field theory-whatever extra structure protects the Higgs mass at UV scales must be broken close to the electroweak scale to allow the Standard Model, which does not have that extra structure. This feature means that smashing together protons at scales much greater than the electroweak scale would surely reveal the physics of whatever mechanism solves the hierarchy problem. And so the Large Hadron Collider was eagerly awaited to tell us which of these ideas was correct. Yet even years before the LHC turned on, those who could clearly read the tea leaves were realizing that something was amiss with our naturalness expectations (see e.g. the 'LEP paradox' [283] , also [284] ), and exploring the idea that supersymmetry would not show up to solve the hierarchy problem (e.g. 'split supersymmetry' [285, 286, 287]  The first such proposal in fact appeared before the LHC had even turned on. While the space of Neutral Naturalness models has now been explored more thoroughly and we will discuss some generalities below, the mirror twin Higgs remains perhaps the most aesthetically pleasing of all these approaches and serves as a useful avatar for this general strategy. As a result, in Chapter 5 we consider cosmological signatures of the MTH specifically, so we give here a more-detailed introduction to the twin Higgs in particular. This is necessarily slightly more technical than the rest of this chapter, so the reader who is not planning on reading Chapters 5 or 6 in detail may skip ahead ∼ 3 pages to Section 4.1.2 without loss of continuity. The scalar potential in this model is best organized in terms of the accidental SU (4) symmetry involving the SU (2) Higgs doublets of the SM and twin sectors, The general tree-level twin Higgs potential is given by (see e.g. [289] ) The first term respects the accidental SU (4) global symmetry, as can be seen by writing it in terms of H = (H A , H B ) , which transforms as a complex SU (4) fundamental. The second term breaks SU (4) but preserves the Z 2 and the final term softly breaks the Z 2 . In order for the SU (4) to be a good symmetry of the potential, we require κ, σ λ. However, the gauging of an SU (2) × SU (2) subgroup constitutes explicit breaking of the SU (4), so we should worry about whether quantum corrections reintroduce large masses for the would-be Goldstones when SU (4) is broken. But writing down the oneloop corrections reveals a fortuitous accidental symmetry. The one-loop effective scalar potential gets the following leading corrections from the gauge bosons at the quadratic level: where we see explicitly that if the Z 2 symmetry is good at the level of the gauge couplings, then these largest one-loop corrections continue to respect the SU (4). It is easy to see from here by power counting that this holds for all the quadratically-divergent pieces so long as the Z 2 is a good symmetry for the interactions involved. There is radiative SU (4)-breaking at the level of the quartic, since the Z 2 symmetry no longer suffices to form the Higgses into an SU (4) This will acquire mass through the breaking of the SU (4) that is naturally smaller than the twin scale f . For future reference, it is convenient to define the real scalar degrees of freedom in the gauge basis as Indeed, at one-loop N c is really just a 'counting factor' and we are free to get those three opposite-sign contributions in a variety of ways. To some extent the space of Neutral Naturalness models is an exercise in interesting ways to find that color factor. We'll be able to see that picture more clearly in the language of orbifold projection, which will also give us good reason to expect that these models with naïvely strange symmetries in fact do have nice UV completions. In Section 3.2.3 we saw how orbifolds could be useful dynamically-that is, in affording a higher-dimensional, symmetric theory which at low energies looks like a less symmetric, four-dimensional theory due to boundary conditions imposed by the orbifold discrete symmetries. But these theories had interesting properties in their low-energy behavior below the scale of compactification; we didn't need to make reference to their origins in studying them. Now we want to understand the variety of low energy theories we can get very generally, but only at the level of the zero-mode spectrum. Rather than decomposing our fields into modes and integrating over the compact manifold and noticing that only those fields invariant under the orbifold symmetry are left with zero-modes, we're going to skip to the answer and look at the spectrum of fields left invariant under our discrete symmetry. We'll find that these theories have enhanced symmetry properties at one loop. The underlying reason lies in the 'orbifold correspondence' in large N gauge theories [306, 307, 308] . Given a 'mother' field theory, you can create a 'daughter' theory by embedding some given discrete symmetry in the symmetries of the mother theory and projecting out states which are not invariant under that discrete symmetry; we call this process 'orbifolding'. Then at leading order in large N , the correlation functions of the daughter theory match those of the mother theory. This is nothing short of amazing-a theory with no supersymmetry to speak of can nevertheless 'accidentally' exhibit supersymmetric behavior at leading order. The general case of the orbifold correspondence and how to construct Neutral Naturalness models is beautiful and I do recommend reading [306, 295, 309] , but the group theory required for a full discussion would be too large a detour from our main narrative. Fortunately we can get a good sense for what's going on by considering a few explicit examples, which will not require much mathematical machinery. Let's first consider the example of 'Folded Supersymmetry' [295] which was the first Neutral Naturalness model constructed explicitly via orbifolding. The idea is precisely to consider a supersymmetric theory and orbifold project onto a theory with no explicit supersymmetry but in which supersymmetric cancellations still occur at one loop. As a pedagogical example, consider an N = 1 supersymmetric U (2N ) C gauge theory with 2N flavors of left and right fundamental chiral superfields Q = (q, q) which enjoy a U (2N ) F,L × U (2N ) F,R global flavor symmetry. Let's decree also that the theory respects R-symmetry. We will orbifold by the discrete group Z 2 as before, but we must choose how to embed the Z 2 in each of these symmetry groups. That is, our original 'mother' theory which contains many Z 2 subgroups, and we must decide precisely which Z 2 we want to orbifold by. We choose the following embeddings:  where the difference here is because of the different R transformations. We see that if we project down to only those states invariant under this transformation, our gauge group dissolves from U (2N ) C down to disconnected pieces U (N ) C × U (N ) C . We see that embedding the Z 2 non-trivially in the R-symmetry group means the daughter theory will be non-supersymmetric. The superpartners of our gauge fields are no longer present, but rather the gauginos have been twisted into bifundamentals under the two gauge factors. In the matter sector, letting a, b = 1..N similarly index the two halves of the flavor indices and writing the fields as matrices in a combined color and flavor space, we havẽ Again we see that we have broken supersymmetry. The flavor group has also broken down It is not too hard to roughly see the magic of how the orbifold-projected theory continues to protect scalar masses. Draw the one-loop the diagrams in the mother theory which would contribute to a calculation of the mass of, say,q Aa , as in Figure 4 .2. In the mother theory we know there are no quadratic corrections by supersymmetry. In the daughter theory, half of each sort of diagram will be eliminated by the orbifold projection, so it's clear that there will still be no quadratic divergences. But because it is different internal states that have been eliminated for different classes of diagrams, the daughter theory has no supersymmetry to speak of! Again, as we saw in the example of the twin Higgs, the magic is in that at one-loop we really only need to get the counting right, and so we can use orbifolding to construct theories which do that in clever ways. The structure of the theory can be succinctly summarized in a 'quiver' or 'moose' diagram where the various symmetry groups correspond to nodes in a graph and the Despite the fact that our daughter theory has no supersymmetry, the orbifold correspondence guarantees that in the N → ∞ limit the correlation functions have full supersymmetric protection. For finite N the supersymmetric relations are broken, but only by 1/N corrections. Going through this explicitly is useful, and pedagogical discussions of it can be found in [306, 298] . The pattern of orbifolding is extremely simple in this example. We write the Higgs field as a matrix of Γ columns and 2Γ rows in blocks of two, with the field in block i, j being H A j where i simply labels the column and A j = 1, 2 is an SU (2) index. Now we can look at how this field transforms under the chosen Z Γ , and we see immediately that the only invariant elements are those on the diagonal. It is simple to repeat this for the Q, u fields to find the same feature. Then in this example there are no off-diagonal fields at all, and the daughter theory consists of Γ SMlike sectors. These sectors are all identical and so have a S Γ rearrangement symmetry, leading to a one-loop quadratic potential of the form That is, just as in the twin Higgs, the SU (2Γ) symmetry of the scalar potential is respected by the one-loop quadratic corrections as a result of the discrete symmetry despite being explicitly broken by the gauge groups. This clearly opens up a much wider space of Neutral Naturalness models where the Higgs is a pseudo-Nambu Goldstone boson and so receives some protection of its mass without new colored particles at the electroweak scale. The general orbifolding approach to constructing models of Neutral Naturalness was fully laid out in [309, 298] , where they in particular explore a 'regular representation' embedding of the discrete group into the continuous symmetries of the mother theory. This approach of orbifolding to find low-energy models with accidental symmetries is useful also because such models come along with guides for how to UV-complete them. When we wrote down the twin Higgs model above, it was perhaps not obvious that there is a nice UV completion of this theory. But now we see that the twin Higgs is an orbifold projection of a SU (6) × SU (4) gauge theory by Z 2 , so we expect we can uplift this to a five-dimensional UV completion where the twin Higgs emerges dynamically at low energies from orbifold boundary conditions. So we can confidently study solely the lowenergy effective theory of the zero-modes we've projected out without worrying explicitly about whether a UV completion exists. Modern particle theorists must now confront a new version of the issue of electroweak naturalness. When originally understood, the pressing problem was understanding what sorts of UV structure could protect a scalar from large mass corrections. Of course this structure needed to be broken to get the SM structure in the IR and there's lots of interesting physics and subtleties on that end as well. But with the structure of the weak scale barely explored, the ways in which this could be done were abundant. Over the ensuing decades we explored electroweak physics with increasing precision, which has provided invaluable guidance for how the SM structure must appear. Gradually the IR dynamics became more and more constrained to the point where now, as we have emphasized, we have enormous constraints on any appearance of new physics with SM quantum numbers up to mass scales that are often many times the electroweak scale. We thus have a qualitative change in the electroweak naturalness issue over the past decades. We term the modern, empirical, low-energy puzzle of electroweak naturalness without visible structure around the weak scale as 'The Loerarchy Problem', for obvious reasons. 22 In this language, the little hierarchy problem is just one approach toward this problem, which assumes that one of the classic solutions is just out of reach and another module is needed to postpone the appearance of SM charged particles. While it's more than worthwhile to continue looking for and exploring those theories, in the face of increasingly powerful LHC data in excellent agreement with the Standard Model it's worth thinking transversely. As intriguing as the Neutral Naturalness models are, these classes of models 22 For readers who do not share my sense of humor, the reasoning is an implied fake etymology for the word 'hierarchy' as 'high + erarchy', and a retcon of the term 'The Hierarchy Problem' as emphasizing the 'high' energy, UV aspects of the issue, by which we are comparatively emphasizing the 'low' energy, IR aspects, suggesting that a natural parallel term would similarly combine 'low + erarchy' to form 'loerarchy', which is not a dictionary word.  There are more things in heaven and earth, Horatio, Than are dreamt of in your philosophy. Hamlet, c. 1600 [310] The line of thought we suggest here is that perhaps the apparent violation of EFT expectations at the weak scale is a sign of the breakdown of EFT itself. Depending on how much background in particle physics one has this statement may seem more or less heretical, but the idea is not as radical as it may at first seem-for one reason, the cosmological constant problem has inspired sporadic reexaminations of the validity of effective field theory in our universe for decades. There have been important attempts to address the cosmological constant problem with a violation of effective field theory, from Coleman's suggestion [311] that nonlocality induced by wormholes may allow the early universe to be sensitive to late-time requirements to Cohen-Kaplan-Nelson's suggestion [312] that the Bekenstein bound demands an infrared cutoff on the validity of any EFT. From one perspective, our suggestion to extend this philosophy to the hierarchy problem appears natural in light of its apparent need in cosmology. We can point to an even-more-general motivation with the realization that gravity necessarily violates EFT. The perturbative quantum field theory of the Einstein-Hilbert Lagrangian [313] is quite clearly an effective field theory of a symmetric two-index tensor field which obeys diffeomorphism invariance and with power counting in 1/M pl [314] , as we can easily see by writing it out and expanding around flat space g µν = η µν + h µν : where we have only given the schematic form of the operators in terms of the number of derivatives and linearized gravitational fields they contain, as the full expressions quickly become complicated [315, 316] . Then we may expect that this effective field theory will be a good approximation to infrared gravitational physics until we get to energies close That quantum gravitational effects will affect infrared particle physics is likewise not a new idea. This has been the core message of the Swampland program [331] , which has been cataloging-to varying degrees of concreteness and certainty-ways in which otherwise allowable EFTs may conjecturally be ruled out by quantum gravitational considerations. These are EFTs which would look perfectly sensible and consistent to an infrared effective field theorist, yet the demand that they be UV-completed to theories which include Einstein gravity reveals a secret inconsistency. While this is powerful information, the extent to which the UV here meddles with the IR is relatively minorjust dictating where one must live in the space of infrared theories. Even so, they have been found to have possible applications to SM puzzles, including the hierarchy problem [97, 98, 99, 100, 101, 102, 103, 104, 11, 5, 105] . Let us review briefly the approach to connect the Weak Gravity Conjecture (WGC) [332] to the hierarchy problem. The WGC is one of the earliest and most well-tested Swampland conjectures, its formulation is relatively easy to understand, and it's em-blematic of the way one might try to connect the hierarchy problem to Swampland conjectures in general. The prime motivation for formulating the WGC was the well-known folklore that quantum gravity does not respect global symmetries. conjecture dictates that a quantum gravitational theory with a U (1) gauge symmetry must be enlarged into a theory allowing magnetic monopoles by a cutoff Λ gM pl , which connects to such a description never being valid in the limit g → 0. The 'electric' form of the WGC is that such a theory must contain a particle which is 'super-extremal'it has charge greater than its mass gqM pl > m. The existence of such a particle would destabilize the extremal, charged black holes, allowing them to decay (though the extent to which this really soothes our entropic worries is unclear). This was suggested in the context of gauged U (1) B−L with very tiny coupling giving an upper bound on the lightest neutrino mass [97] , but the magnetic form of the WGC is difficult to deal with in this context. This can be circumvented by introducing a new dark Abelian gauge group U (1) X and charged states which get (some of) their mass from the Higgs [5] . The way such a model solves the hierarchy problem is by changing the shape of our prior for the electroweak scale, as mentioned in Section 2. Ultimately, our ability to address the hierarchy problem through quantum gravitational violations of EFT is limited by our incomplete understanding of quantum gravity. This motivates finding non-gravitational toy models that violate EFT expectations on their own, providing a calculable playground in which to better understand the potential consequences of UV/IR mixing. In Chapter 7 we pursue the idea that UV/IR mixing may have more direct effects on the SM by considering noncommutative field theory (NCFT) as such a toy model. These theories model physics on spaces where translations do not commute [335, 336] , and have many features amenable to a quantum gravitational interpretation-indeed, noncommutative geometries have been found arising in various limits of string theory [337, 338, 339, 340] . In the beginning the Universe was created. This has made a lot of people very angry and been widely regarded as a bad move. The Restaurant at the End of the Universe (1980) [341]  It is an amazing and serendipitous fact that the universe started off hot. As a result of the initially high energies and densities, the details of microscopic physics greatly affected the large-scale evolution of the universe. Since the speed of light is finite, by looking out in the sky at enormous distances we can not only learn about the history of the universe but we can use this information to learn about particle physics. While cosmology doesn't give us probes of arbitrarily high temperatures, there's still a humongous amount to be learned-in part due to further serendipity. The fact that the universe transitions from radiation domination to matter domination shortly before it becomes transparent to photons means that the cosmic microwave background (CMB) encodes information both about the light, radiation-like degrees of freedom as well as the matter density in the early universe. Had radiation domination ended far before recombination, it would be far more difficult to use the CMB to constrain light degrees of freedom like extra neutrinos. Had radiation domination ended far after recombination, there would be little evidence of dark matter in the CMB, which is the strongest evidence for particle dark matter instead of, say, a modification of gravity at large distances. In fact such a 'cosmic coincidence' also occurs much later, as there is a very long epoch of matter domination before the universe transitions to dark energy domination. Were there just slightly less dark energy, its effects would be essentially invisible thus far in the history of the universe, and it would be very difficult to measure dark energy at all. All that is to say that there is enormous value in collaboration between particle physics and cosmology. In this chapter we investigate this connection for the twin Higgs model in particular, though our findings are relevant for general Neutral Naturalness theories as well. In Section 4.1.1 we noted that the energy frontier does not effectively probe these theories. Since they do not introduce new particles with Standard Model charges, it is only precision electroweak measurements made at colliders that constrain them at all. However, such theories are in fact probed very well by cosmology, as they introduce new light degrees of freedom. Despite the fact that these do not directly interact with normal matter, their gravitational effects still contribute to the evolution of the universe, and so the CMB provides a powerful constraint on new light particles. It is this cosmological effect that provided the biggest obstacle to the original twin Higgs proposal [288] , which became an urgent issue after the null results of run 1 of the LHC and the increased interest in models where the lightest states responsible for Higgs naturalness were SM-neutral. The landmark approach taken in [289] was to pare In fact we will return to this model in Chapter 6 to study a novel collider search strategy to which it lent credence and which turns out to be a broadly useful probe of many theories of BSM physics. Despite the fact that the vast, vast majority of theory papers written in particle physics will not ultimately be the exact right model of the universe, they still contain value. They may guide experimental searches toward interesting classes of signals to look for, or teach us new things about the range of particle phenomenology or quantum field theories. Regardless, we obviously don't know in advance which model will be correct, so exploring all possible directions is crucial. Yet when there is the possibility for a more parsimonious model, it's certainly worth pursuing that option. This is the philosophy that led to my collaborators and me looking into the prospect of attaining a realistic twin Higgs cosmology that respected the Z 2 symmetry. The primary challenge to the mirror Twin Higgs comes not from LHC data, but from cosmology. An exact Z 2 exchange symmetry predicts mirror copies of light Standard Model neutrinos and photons states, which contribute to the energy density of the early universe. In particular, twin neutrinos and a twin photon provide a new source of dark radiation that is strongly constrained by CMB and BBN measurements [342, 343] . While these constraints could be avoided if the two sectors were at radically different tempera- In this work we take an alternative approach and investigate ways in which early universe cosmology can reconcile the mirror Twin Higgs with current CMB and BBN observations. In doing so, we find compelling scenarios that transfer the signatures of electroweak naturalness from high-energy colliders to cosmology. We consider several possibilities in which the energy density of the light particles in the twin sector is diluted by the out-of-equilibrium decay of a new particle after the two sectors have thermally decoupled. Crucially, the new physics in the early universe respects the exact (albeit spontaneously broken) Z 2 exchange symmetry of the mirror Twin Higgs. This symmetry may be used to classify representations of the particle responsible for this dilution. We concentrate on two minimal cases: In the first, the long-lived particle is Z 2 -even and the asymmetry is naturally induced by kinematics. In the second, there is a pair of particles which are exchanged by the Z 2 symmetry and which may be responsible for inflation. 24 Moreover, in these cases the new physics does not merely reconcile the existence of a mirror twin sector with cosmological constraints, but predicts contributions to cosmological observables that may be probed in current and future CMB experiments. This raises the prospect of discovering evidence of electroweak naturalness first through cosmology, rather than colliders, and provides natural targets for future cosmological constraints on minimal realizations of neutral naturalness. The next sections are organized as follows: In Section 5.2.2 we discuss the thermal history of the mirror Twin Higgs, with a particular attention to the interactions keeping the Standard Model and twin sector in thermal equilibrium and the cosmological constraints on light degrees of freedom. In Section 5.2.3 we present a simple model where the out-of-equilibrium decay of a particle with symmetric couplings to the Standard Model and twin sector leads to a temperature difference between the two sectors after they decouple. We turn to inflation in Section 5.2.4, constructing a model of "twinflation" in which the softly broken Z 2 -symmetry extends to the inflationary sector and leads to two periods of inflation. The first primarily reheats the twin sector, while the second primarily reheats the Standard Model sector. We conclude in Section 5.2.5. The primary challenge to the mirror Twin Higgs comes from cosmology, rather than collider physics. The mirror Twin contains not only states responsible for protecting the Higgs against radiative corrections (such as the twin top), but also a plethora of extra states due to the Z 2 symmetry that are irrelevant to naturalness. The lightest of these, namely the twin photon and twin neutrinos, contribute significantly to the energy density of the early universe around the era of matter-radiation equality, since they have a temperature comparable to that of the Standard Model plasma at all times. This is because the same Higgs portal coupling that makes the Higgs natural also keeps the two sectors in thermal equilibrium down to O(GeV) temperatures. Then the identical particle content in the twin and Standard Model sectors guarantees that they remain at comparable temperatures even after they decouple -for every massive Standard Model species that becomes non-relativistic and transfers its entropy to the rest of the plasma, its twin counterpart does the same within a factor of f /v in temperature. In this section we undertake a detailed study of the decoupling between the Standard Model and twin sectors as well as the constraints from precision cosmology. In thermal equilibrium, each relativistic degree of freedom has roughly the same energy density. In general, we express the energy density of the universe ρ during the radiationdominated era as ρ ≡ g π 2 30 T 4 , where we define g through this relation as the effective number of relativistic degrees of freedom and T the temperature of the SM photons. This Likewise, entropy densities for each sector i are defined as s i (T ) = 2π 2 45 g i (T )T 3 . We neglect the small differences between the number of relativistic degrees of freedom defined from energy and entropy densities, which are not significant over the range of temperatures of interest here. In the early universe, the two sectors are thermally linked by interactions mediated by the Higgs, which, through mixing with both h A and h B components, allows for SM fermions and weak bosons to scatter off or annihilate into their twin counterparts. However, once the temperature drops sufficiently for this Higgs-mediated interaction to become rare on the expansion time-scale, the sectors decouple and thereafter thermally evolve independently. More precisely, thermal decoupling will occur once the rate at which energy can be exchanged between SM and twin particles (through the Higgs) falls below Thermal decoupling is traditionally formulated from the Boltzmann equations describing the evolution of single-particle phase space number densities, wherein collisions induce instantaneous changes to the shape of these distributions. When the collisions occur faster than the expansion rate, the phase space probability density functions of the interacting species are expected to relax to an equilibrium distribution (Boltzmann, neglecting quantum statistics, will be applicable to our case). However, once the rate of collisions falls below the expansion rate, collisions become rare on cosmological time scales and the phase space distributions depart from equilibrium. The decoupling temperature is determined as that at which the scattering rate of a participating particle, Γ, drops below the Hubble rate, assuming that this occurs instantaneously across the entire phase space where the number density is significant. This formulation can be used to determine the time at which a particular species of particle will cease to scatter off twin particles on cosmological time scales. In the case of interest here, however, both sectors of particles remain thermalised within themselves while the interactions between sectors freeze-out. This implies that the phase space number densities are still Boltzmann distributions throughout decoupling, with a different temperature for each sector. As it is the twin sector temperature that ultimately determines the impact of the light twin degrees of freedom on the cosmological observables (discussed below in Section 5.2.2), we wish to describe the thermal evolution of the two sectors by that of their entire energy or entropy content and the bulk heat flows between them. They may then be identified as thermally decoupled once the rate at which they exchange energy falls below the expansion rate. If the SM and twin sector plasmas have temperatures T andT respectively, then calling q the net heat flow density from the SM to the twin sector, the rate at which the twin entropy densities s t and s SM evolve is determined by Here, H is the Hubble rate. The heat flow rate has been decomposed into the sum of the energy transferred into and out of the twin sector by collisions in the second equality in each line, where dq in dt and dqout dt are both positive. The rate of heat flow q may be calculated by performing a phase space average of the rate that energy is transferred from the SM to the twin sector through particle interactions. Since the decay rates of top quarks or weak bosons are fast compared to their scattering rate and the Hubble rate, energy transferred to them is instantaneously transferred to the rest of the plasma. Similarly, the scattering rate of lighter fermions off other particles of the same sector (such as photons or gluons) is much faster than their interaction rate with twin fermions. Energy transferred to the lighter fermions therefore quickly diffuses throughout their respective plasmas. The rate of heat flow between sectors may therefore be well approximated by the rate at which energy is transferred from SM particles to twin particles in Higgs mediated interactions. This may occur through elastic scattering of SM particles off twin particles or annihilations of SM particle/antiparticle pairs into twin particles (or the reverse). The energy density transferred to twin particle i from SM particle j in scattering is given by where p is the outgoing 4-momentum of particle i. In the cosmic comoving frame, the phase space number densities f i and f j are just Boltzmann factors, although evaluated at the different temperatures of each sector. The factor g i is the number of internal degrees of freedom of particle i, which here includes colour (the cross section should not be colour averaged, as each colour of quark is present in the plasma in equal abundances and each mediates the exchange of energy, so have their contributions summed). Finally, is the on-shell energy of particle i with momentum k, while dσ ij→ij dΩ is the differential scattering cross section for species i scattering off j per solid angle Ω and v rel is the usual relative speed of the incoming particles. As described in [370] , the factor in the integrand giving the energy transferred per reaction is simply a component of a 4-vector, This may be calculated in the centre-of-mass frame and then boosted back into the cosmic comoving frame where the integrals in (5.4) can be evaluated, similarly to the thermal averaging procedure described in [371] . The integral (5.4) may be decomposed into two terms giving the positive and negative energy changes of the twin particle, which respectively contribute to dq in dt and dqout dt . When evaluated in the centre-of-mass frame, these terms correspond to the cases where the scattering angle of the twin particle is respectively less than and greater than the angle between its initial momentum and the total momentum of the system. However, when T =T , we find the integrals involved in this decomposition substantially more arduous than when they are evaluated together. Energy transferred through annihilations may be similarly calculated as where dσ jj→iī dΩ is now the differential annihilation cross section. This rate may be evaluated as described above and is more directly amenable to the factorisation of the integrals observed in [371] . See also [372] for further details of similar calculations. The first term of (5.6) is the energy transferred from the SM to the twin sector and contributes to dq in dt in (5.2), while the second term is the energy transferred from the twin sector to the SM and contributes to dqout dt . In thermal equilibrium, the rate of energy transferred through collisions into one sector will be balanced by that of energy transferred out of it so that there is negligible net heat flow. This state will be rapidly attained (compared to the age of the universe) if dq in,out dt 3HT s t . However, as the universe expands and the plasma cools, the energy transfer rates fall faster than the Hubble rate. This is demonstrated in the Figure 5 This effective interaction is appropriate for the temperatures of interest here and helps to simplify the integrals of (5.4). In order to further simplify the integrations of (5.4) when it is to be decomposed into terms in which the energy exchange is positive and negative, we calculate dq in dt under the assumption that the sectors have the same temperature (this ensures that the rate dqout dt is identical). This is then combined with the rate of energy transferred from annihilation. A similar calculation of these rates was recently performed in [347] , for cases where the Yukawa couplings do not respect the Z 2 twin symmetry. In Figure 5 .2 we compare the energy transfer rate to the Hubble rate in order to determine when decoupling occurs. As long as the energy exchange rate exceeds the expansion rate, the sectors will be thermalised and have the same temperature. Decoupling then occurs once this rate drops below the Hubble rate. From Figure 5 .2, this occurs at a temperature ∼ 2 GeV. However, even after the energy exchange rate drops below the Hubble rate, the sectors will remain at the same temperature unless some event that either injects or redistributes entropy occurs within a sector (such as the temperature dropping below a mass threshold). As the heavy quark masses roughly coincide with the decoupling temperature, these do cause the twin sector to be mildly reheated with respect to the SM below decoupling. However, the resulting temperature difference is small and the energy exchange rates are expected to continue to be well-approximated by the rates presented in Figure 5 When the twin sector is colder than the SM (which will be important for much of what follows) the heat flow is typically dominated by annihilations of SM into twin particles. However, the energy exchange from elastic scattering can be comparable to that from annihilations, as illustrated in Figure 5 .2. Although the energy exchange in an annihilation will generally exceed that of a scattering because all of the energy involved in the process must be transferred, the annihilation rate also becomes more Boltzmann or threshold suppressed when the temperature drops below the mass of the heavier twin particles. It is therefore not always clear that energy transfer through annihilations dominates. Decoupling is not exactly instantaneous and there is some range of temperatures over which the rate of heat flow freezes-out. The net heat flow rate dq dt is greater for larger temperature differences between sectors. The generation of a potentially large temperature difference within this brief epoch of sector decoupling, such as those discussed below in Twin temperatures colder thanT min will partially thermalise back to this value. As the participating fermions are not non-relativistic, instantaneous decoupling is not as accurate an approximation as it is, for example, for chemical decoupling of a WIMP, although it is still reliable. In Figure 5 .3, we show the minimum temperature that the twin sector may have as a function of SM temperature for heat flow to freeze out, estimated using (5.7). Only annihilations have been included in the determination of the minimum temperature, although we have verified that, for these temperatures, the scatterings contribute only 10% to the heat flow. Note that while the energy exchange rate, such as 1 T dq in dt in (5.2), in scattering processes may be faster, the net energy flow rate, or heat flow ( 1 T dq dt in (5.2)), which is the difference between energy exchange rates into and out of the sector, is actually dominated by annihilations. Generally, we find that decoupling begins at temperatures ∼ 4 GeV. The temperature difference can reach an order of magnitude without relaxing once the SM temperature drops to ∼ 1 GeV. While the extent of thermal decoupling is temperature dependent, the maximum temperature difference that will not relax grows quickly as the SM temperature drops. Then we may describe the two sectors as being decoupled if, in a given cosmology, all events that raise the temperature of one sector relative to the other (such as the crossing of a mass threshold and the resulting entropy redistribution, the most significant of which is the confinement of colour) induce temperature differences that are too small to partially relax. valid. For example, at temperatures below the twin sector QCDPT, which occurs at ∼ 1 + log( f v ) higher temperatures than in the SM, the partonic calculation of twin quark/anti-quark pair production must be replaced by a hadronic one. Furthermore, the growth of the twin strong coupling necessitates that the quark-Higgs Yukawa couplings be RG evolved to the scale of the energy exchanged, which can induce an O(1) change to the cross section, although this has only a relatively small effect on the decoupling temperature. It is nevertheless clear that decoupling is mostly complete by then and that these uncertainties are not large enough to affect this conclusion. In the standard mirror Twin Higgs cosmology, knowing the decoupling temperature tells us how the temperatures of the two sectors will be related at subsequent times. The sectors separately evolve adiabatically after decoupling, though they redshift in the same way and differences in temperature only arise from events that redistribute entropy. Non-minimal cosmological events that could potentially cause the temperatures of each sector to diverge can therefore only be effective if they leave each sector colder than this approximate decoupling temperature. Given that the twin and Standard Model sectors remain in thermal equilibrium to O(GeV) temperatures, the simplest mirror Twin Higgs scenario is cosmologically inviable due to the presence of light twin species (photons and neutrinos) with abundances comparable to those of the SM. The cosmological observables through which evidence of light species may be inferred are typically represented by N eff , the "effective number of neutrino species" in the early universe; their individual masses, which determine their free-streaming distances; and the "effective mass" m eff ν , which parameterises their contribution to the present-day energy density of non-relativistic matter. These observables are probed by both the CMB and large scale structure (LSS). Effective number of neutrinos The parameter N eff describes the amount of radiationlike energy density during the evolution of the CMB anisotropies before photon decoupling. It is defined as the effective number of massless neutrinos with temperature as predicted in the standard cosmology that would give equivalent energy density in radiation: We here review the CMB physics of dark radiation, summarising the discussion in [374] . See also [342] for further review. The angular size and scale of the first acoustic upon Ω c permit (for ΛCDM+N eff , a variation of ∼ 10% in Ω c h 2 is consistent with present CMB+BAO measurements [342] , although these variations must be consistent with other observables). This degeneracy is not expected to be broken by CMB-S4 [375]. Given that a eq is approximately fixed, the utility of N eff arises because, in simple ex-tensions of the ΛCDM model, it approximately corresponds to the suppression of power in the small scale CMB anisotropies that arises from Silk damping. The reason for this is roughly that, although the greater expansion rate induced by the extra radiation reduces the time that CMB photons have to diffuse before decoupling, it also reduces the sound horizon size more severely. As the angular size of the sound horizon is determined by the location of the acoustic peaks and is also well measured, the reduction in the sound horizon must be compensated for by a reduction in the angular diameter distance to the CMB. This effectively raises the angular distance over which photon diffusion proceeds and results in a prediction of smoother temperature anisotropies at small scales. This correspondence with the Silk damping allows N eff to be approximately factorised from other parameters and constrained independently, providing a direct observational avenue for detecting the presence of new, massless fields [374] (see [376] for further implications for model building). This relationship arises because the fixing of a eq implies that N eff effectively determines the energy density of the universe, and hence the Hubble rate, during CMB decoupling. Note, however, that further extensions of ΛCDM may complicate this correspondence, in particular deviations from the standard Big Bang Nucleosynthesis prediction of the primordial helium abundance. The contribution to N eff (or ∆N eff ) in the mirror Twin Higgs arises from two sources: the twin photons, which can be treated as massless dark radiation with an appropriate twin temperature T t eq at the time of matter-radiation equality, and the twin neutrinos, whose non-zero masses may need to be accounted for. For the twin photons, the contribution to N eff is simple; their equation of state is always w = 1/3 and their energy density is given by g π 2 30 T t eq 4 , where g = 2. The twin temperature at matter-radiation equality is found from the SM temperature using comoving entropy conservation, where the two sectors have the same number of thermalized degrees of freedom by this time. Here, T SM eq is the SM photon temperature at matter-radiation equality and T decoup is the sector decoupling temperature. Since neutrinos are massive, their behavior is more complicated. Their equation of state parameter takes on a scale factor dependence which is controlled by their mass. In the Standard Model, this sensitivity is negligible because present CMB bounds imply that neutrinos are ultra-relativistic at a eq to good approximation [342] . However, the factor by which the twin neutrino masses are enhanced may raise them to order T t eq or greater (see as in [377] . Call the quasi-relativistic neutrino energy densityρ(a) with time-evolving equation of state parameter w(a), which is to be balanced against some extra non-relativistic energy density ∆ρ CDM (a) ∝ a −3 to keep a eq the same. This amount of non-relativistic energy where ρ r and ρ m are the energy densities of the radiation and non-relativistic matter. For a perfect fluid, dρ da = −3(1 + w(a))ρ/a (neglecting the anisotropic stress that is expected only to contribute to a weak phase shift in the CMB [378] ), this results in a Hubble parameter of H 2 (a eq ) = 2 3M 2 pl [ρ r (a eq ) + 3w(a eq )ρ(a eq )] . (5.12) This suggests a definition of the effective number of neutrinos, N eff , via where ρ i is the contribution to the energy density from some species i with equation of state parameter w i and ρ th ν,m=0 is the energy density of a massless neutrino with a thermal distribution in the standard cosmology. Then 3w gives the 'relativistic fraction' of the energy density. Note that this is simply a ratio of the pressure exerted by the new fields to that of a massless neutrino. The effectiveness of this approximation was discussed in [379] in the context of thermal axions (while effective at keeping a eq fixed, changes to odd peak heights subsequent to the first are imperfectly cancelled and require further changes to H 0 to compensate -see Section 5.2.2 below). Calling T i ν the temperature at which the neutrinos in sector i freeze-out and a i ν the corresponding scale factor, then assuming instantaneous decoupling, the phase space number density for scale factor a is given by a redshifted Fermi-Dirac distribution [380] f i α (p) for the α neutrino mass eigenstate in the i sector (m i α T i ν , so has been dropped). The energy density and pressure are where g α = 2 is the number of degrees of freedom for a neutrino species. Since the neutrino decoupling temperature depends on the strength of the weak in- Here n t ν is the number density of a relic twin neutrino flavour and n SM ν is that for a SM neutrino. It is assumed that the neutrinos have been thermally produced as hot relics. The relic abundance of a neutrino species is given by its number density when it decoupled, diluted by the factor by which the universe has since expanded. The scale factors at which neutrino decoupling occurs in the two sectors, a SM ν and a t ν can be deter-mined from (5.18), the relative temperatures in the two sectors and comoving entropy conservation, to obtain where the same mass thresholds have been assumed in each sector below their neutrino decoupling temperatures, so that g SM T SM ν = g t (T t ν ). The neutrino number densities are then For f /v from 3 to 10 and using T decoup ∼ 2 − 6 GeV from Section 5.2.2, we find g t (T decoup ) / g SM (T decoup ) ∼ 0.8 and thus arrive at where n = 1 for Dirac masses and n = 2 for Majorana masses. If they are sufficiently light and hot, the twin neutrinos only affect the CMB as dark radiation and their masses may then only be inferred from tests of the matter power spectrum. However, if heavier and colder, they are better described as a hot dark matter component. Their impact on the CMB is discussed in [382] , where the shape of the power spectrum can depend upon the individual neutrino kinetic energies through their characteristic free-streaming lengths. The early Integrated Sachs-Wolfe effect (eISW) is also sensitive to the masses if the neutrinos become non-relativistic during decoupling (thereby affecting the radiation energy density and the growth of inhomogeneities) [381] . There is a significant degeneracy in cosmological fits to the CMB between Ω m and H 0 (the Hubble constant) [383] , where raising the non-relativistic matter fraction, such as with nonrelativistic neutrinos, can be accommodated by a decrease in H 0 (or equivalently, the dark energy density), which keeps the angular diameter distance to the CMB approximately fixed. This degeneracy can be broken by measurements of the baryon acoustic oscillations (BAOs), which are sensitive to the expansion rate of the late universe and provide an independent measurement of Ω m and H 0 . It is through combination with these results that bounds from Planck on neutrino masses are strongest [342] . Bounds The authors are unaware of any specialised analysis of the present and projected future cosmological constraints on scenarios with both massless dark radiation and additional light, semi-relativistic sterile neutrinos. In the absence of this, we use bounds from [342] as a rough indication of the present level of sensitivity to these parameters, which we nevertheless expect to be a reliable indication of the (in)viability of this model. The 95% confidence limits on these parameters are N eff = 3.2 ± 0.5 and eV, again depending on data sets combined (see [385] , [386] ), although these are subject to greater uncertainties in the inference of the power spectra of dark matter halos from galaxies surveys and the Lyα forest. It must also be noted that the shape of the CMB temperature anisotropies depends upon both the mass of individual neutrino components (through their free-streaming distance) and their contribution to the energy density of the nonrelativistic matter that does not cluster on small scales. However, it is not expected that improvements in bounds on the former will be made from improved measurements of the primary CMB itself, differently with f /v). As is plainly evident, the mirror Twin Higgs is ruled out cosmologically, no matter the choices of neutrino masses one makes, if only for the presence of the twin photon. In the standard cosmology, the twin sector will have roughly the same temperature as the SM, giving 4.6 ∆N eff 6.3 for f /v < 10, according to the definition of (5.20). This range depends upon f /v through the twin neutrino decoupling temperature (5.18), which determines the extent to which the twin photons are reheated relative to the twin neutrinos after twin electron/positron annihilations. This is sufficiently large that even the cold dark matter fraction cannot be adjusted to keep matter-radiation equality fixed, resulting inevitably in changes to the height and shape of the first acoustic peak. The energy density in neutrinos is predicted to be above the present observational upper bounds for most neutrino mass configurations, with the exception of the minimal values permitted by neutrino oscillation measurements with f /v 6. We therefore discuss cosmological mechanisms in which the twin radiation is diluted to levels compatible with these observational bounds in the subsequent sections of this paper. We now turn to simple scenarios that reconcile the mirror Twin Higgs with cosmological bounds, while taking care to respect the softly-broken Z 2 symmetry. We begin with the out-of-equilibrium decay of a particle with symmetric couplings to the Standard Model and twin sectors, in which the desired asymmetry is generated kinematically. That is to say, the dimensionless couplings between the decaying particle and the two sectors are equal, and asymmetric energy deposition into the two sectors is a direct consequence of the asymmetric mass scales. In this respect, the scenario is philosophically similar to N naturalness [387] , albeit with a parsimonious N = 2 sectors. See also [388] , [389] and [372] for other recent related ideas of using long-lived particles for the dilution of dark sectors. For simplicity, here we will focus on the case of a real scalar X coupled symmetrically to the A and B sector Higgs doublets. Due to the difference in masses between the sectors after electroweak symmetry breaking, simple kinematic effects give X a larger branching ratio into the Standard Model. This occurs over a range of X masses within a few decades of the weak scale. If X decays out-of-equilibrium below the decoupling temperature of the two sectors, this injects different amounts of energy into the two sectors, effectively suppressing the temperature of the twin sector relative to the Standard Model. This relative cooling suppresses the contribution of the light degrees of freedom of the mirror Twin Higgs to below cosmological bounds. Insofar as the asymmetry is driven entirely by kinematic effects arising from v f , the resulting temperature inequality between the two sectors is proportional to powers of v/f . Model temperature necessitates that the X dominate the cosmology before it decays. Our main discussion will follow the simplest case of an X which dominates absolutely before it decays, comprising all of the energy density of the universe and effectively acting as a 'reheaton'. Afterwards, we will discuss the possibility of a 'thermal history' for X -a scenario where X is in thermal equilibrium with the two sectors, then chemically decouples at some high temperature and grows to dominate the cosmology before it decays. This scheme will result in additional stringent constraints on the viable parameter space. A Z 2 -even scalar X which is a total singlet under the SM and twin gauge groups admits the renormalisable interactions where m X is the mass of X (neglecting corrections from mixing that will be shown below to be tiny), λ x is a dimensionless coupling and x is a dimensionful parameter, which one may imagine identifying as a vacuum expectation value (vev) of X in an UV theory. Note that these interactions preserve the accidental SU (4) symmetry of the Twin Higgs. The X field may additionally possess self-interactions, which we omit here as they do not play a significant role in what follows. The interactions in (5.25) allow X to decay into light states in the Standard Model and twin sectors. If X reheats the universe through out-of-equilibrium decays, the reheating temperatures of the two sectors will be determined by its partial decay widths, assuming that the decay products do not equilibrate. In the instantaneous decay approximation, X decays when the Hubble parameter falls to its decay rate Γ X ∼ H. As we will show in Section 5.2.3, in order to evade cosmological constraints we need the X to decay mostly into the SM, so we may estimate Γ X ∼ Γ(X → SM). Then the energy that was contained in the X is transferred into radiation energy density, with the resulting temperature of the radiation given by (see [390] ) where g is the effective number of relativistic degrees of freedom, as defined in Section 5.2.2, of the particles that are being reheated. Our numerical calculation of the reheating temperature, which will be presented in Section 5.2.3, indicates that the approximation T ∼ 0.1 Γ X M pl reliably reproduces the reheating temperature over the range of interest. As shown in Section 5.2.2, the two sectors thermally decouple when the temperature falls below T decoup ∼ 1 GeV, so reheating must take place to below this temperature. At even lower temperatures, big bang nucleosynthesis (BBN) places strong constraints on energy injected into the SM at temperatures below O(1 − 10) MeV [391] . Requiring that the SM reheating temperature is above ∼ 10 MeV, these constraints on the SM reheating temperature become constraints on the decay rate of the X into the SM, which in the above approximation becomes 5 × 10 −21 GeV Γ X 3 × 10 −16 GeV. (5.27) This then constrains the couplings λ x and x of the X to the Higgs sector. Importantly, it means that X must couple very weakly, in order to be long-lived enough to reheat to a low temperature, as will be shown below. The asymmetry in partial widths arises from different effects depending upon the mass of X. For masses below the SM Higgs threshold, it is predominantly differences in mass mixing with the two Higgs doublets that produces the asymmetry, where the size of the mixing angles determines the effective coupling of X to the SM and twin particles and therefore its branching fractions. For masses below the twin scale, the relative size of the mixing scales inversely with the vevs in each sector. Thus the hierarchy v f already present in the Higgs sector can automatically gives rise to a hierarchy in partial widths. Note that additional threshold effects can enhance the asymmetry further, in particular when X has mass above threshold for a significant decay channel in the SM, but below the corresponding mass threshold in the twin sector. Decays into on-shell Higgses complicate this picture further. In what follows, we first give an analytic calculation of the mass mixing effect, then present a more precise calculation of the decay widths into each sector. To lowest order, X decays via its interactions with the SM and twin Higgs, and only to other fermions and gauge bosons through its mass mixing with the Higgs scalars. Expanding the X potential after the SU (4) is spontaneously broken, the mixing term between X and h A in the scalar mass matrix is The h A and h B components of the X mass eigenstate, which we denote respectively as δ XA and δ XB , can then be determined. The expressions for the mixing angles are in general complicated, but they simplify in limits m X < f and m X f : to lowest order in (v/f ) 2 and κ/λ. The partial width for the decay of X into SM states (excluding the Higgs) is where Γ h (m h = m X ) denotes the decay width of a SM Higgs if it were to have mass m X . Note that the Higgs partial width must be computed using the vev v A ≈ v/ √ 2 to determine the masses and couplings of the SM particles. The partial width of the X into twin states is computed the same way using δ XB and the vev v B ≈ f / √ 2 . From the mixing angles (5.28), it is already apparent over what mass range asymmetric reheating from X decays will work. These give Thus when the mass of X is less than the twin scale, the Standard Model will be reheated to a higher temperature than the twin sector, but in the large mass limit this mechanism works in the opposite direction and would appear to lead to preferential reheating of the twin sector. More precise statements about the relative branching ratios and resulting temperatures require additional care. In addition to decaying through mass mixing, X can decay into the Higgs mass eigenstates themselves if above threshold. As the energy is ultimately transferred to the SM and twin sectors, we then need to consider how these states decay and account for the further mixing of the Higgs mass eigenstates into Higgs gauge eigenstates. For m X > 2m h , decay can occur into the lighter (SM-like) Higgs mass eigenstate h with partial width Similarly, for m X > 2m H , decays can proceed into HH with a similar partial width, but with the h mass replaced with that of the H. Above the intermediate threshold In what follows we will work in the region of parameter space where mixed decays to hH are negligible. The rate of heat flow into each sector may be well approximated by adding the decay rates of X into each channel and weighting these by the fraction of energy transferred into the particular sector. Of course, when X decays into Higgs particles, these in turn decay out of equilibrium into both the Standard Model and twin sectors. As the Higgs decays are almost instantaneous, the fraction of energy transferred into each sector is simply that carried by the Higgs decay products multiplied by their branching fractions for each sector. The total rate at which X particles are transferred into the SM plasma is The corresponding rate for energy deposition into the twin sectors is simply given by the replacement of SM → Twin. The first term is the rate at which X decays directly into the SM through mass mixing with the Higgs. The second is the fraction of X energy that is transferred into lighter Higgs states that subsequently decay into the SM. The third is the analogous term for decays into the heavy Higgs, where cascade decays of the H into the h and subsequently other SM particles must be included. Note that decays of the heavy Higgs into the light Higgs make up a majority of decay width, because of the large quartic coupling required for the twin Higgs potential. Below the hh threshold, it is possible for X to decay via one on-shell and one offshell Higgs boson. The partial width for off-shell Higgs production was calculated for X → hh * → hbb and found to be negligible compared to two-body decays through mass mixing and so we omit three-body decay widths in what follows. Ultimately, the complete partial widths for the decay of X into the Standard Model and twin sectors includes the sum of decays into Higgs bosons h and H and direct decays into the fermions and gauge bosons of the two sectors. We compute the latter to an intended level of accuracy of ∼ 10% (including, e.g., NLO QCD corrections to decays into light-flavor quarks), mostly following [392] . The resulting partial widths into the Standard Model and twin sectors are shown as a function of m X in Figure 5 .5 with the ratio of branching fractions displayed in Figure 5 .6. Over much of the space below the Higgs mass, the branching ratio exhibits the expected (f /v) 2 scaling from the mass mixing. Below ∼ 40 GeV, suppression of the twin partial width arises because the twin bottom quark pair production threshold is crossed. As m X nears m h , the SM branching fraction grows by ∼ 4 orders of magnitude as the W W * , ZZ * , and then W W and ZZ decays go above threshold. Since the analogous thresholds are at much higher energies in the twin sector, the enhancement is not paralleled by decays into the twin sector until m X is close to the twin scale. There is therefore a large range of masses m h m X m H over which the SM branching fraction dominates by several orders of magnitude. Above the X → hh threshold, the ratio of decay widths is roughly constant in mass up to the HH threshold. The twin sector decay rate is dominated by decays of on-shell light Higgs into twin states, Γ(X → Twin) ≈ Γ(X → hh)Br(h → Twin) ∝ 1/m X as in (5.31) . If the SM were also predominantly reheated through this channel, then the ratio of branching fractions would again be approximately δ 2 hA /δ 2 hB ≈ (f /v) 2 . However, The gray bands in Figure 5 .5 highlight regions where our analytic estimates of the partial widths encounter enhanced uncertainties arising from the bottom and charm thresholds in both sectors. Over most of these ranges, we estimate the size of these uncertainties to be either ∼ 10% or confined to very small subregions. The thicknesses of these bands have been chosen conservatively, and ultimately the branching ratios should be accurate to within a factor of ±Λ QCD of the bottom and charm mass thresholds. In particular, the prescription of [393] has been followed for approximating the bottom partial width close to the open flavour threshold. Resonant decay into gluons from bottomonia mixing has been neglected, although these resonant mass ranges are expected to be only ∼ MeV wide at the CP-even, spin-0 bottomonia masses m X = m χ bi (see [393] and [394] ). It should be noted, however, that at temperatures above that of the QCD phase transition, the quark decay products behave differently compared to that expected in a low temperature environment. In particular, for hot enough temperatures, the b or c quarks may not hadronise and the partonic partial widths may more reliable. The applicability of the treatment of the flavour thresholds used here may therefore not be valid if the decay occurs in the hot early universe. However, it is only very close to the threshold itself (within several GeV) that this uncertainty becomes significant. Finally, quark masses have been neglected in the gluon partial width. For m X close to the flavour thresholds, this approximation breaks down, but the gluon branching fraction is only ∼ 10% and so the error does not contribute to the uncertainty of the total width by more than this order (it is this uncertainty that is responsible for most of the extension of the length of the gray bands about the flavour threshold). Close to the charm threshold, the analogous uncertainties are even more poorly under-stood. Below the charm threshold, hadronic decays of a light scalar are highly uncertain (see [395] for discussion). We avoid these regions altogether by restricting our considerations to m X roughly above the twin charm threshold. Note that below the SM charm threshold, the smaller decay rate of a Higgs-like scalar necessitates larger couplings λ X x for X to have a lifetime within the required reheating window. The larger couplings then imply potentially stronger constraints from invisible mesonic decays. See [394, 395, 396] for further discussion and recent analysis of the pertinent experimental constraints. Taken together, the results in Figures 5.5 and 5.6 bear out the expectation that a scalar X with symmetric couplings to the Standard Model and twin sectors may nonetheless inherit a large asymmetry in partial widths from the hierarchy between the scales v and f . Across a wide range of masses m X , the asymmetry is proportional to (or greater than) v 2 /f 2 , tying the reheating of the two sectors to the hierarchy of scales. Before proceeding to our computation of cosmological observables, we comment on an alternative variation on the reheating mechanism presented here that involves having X odd under the twin parity. This permits two renormalisable interactions with the Higgses to give a Higgs potential of the form: If X then acquires a vev at some scale, it may be possible to arrange for the resulting spontaneous breaking of the Z 2 to give that required in the Higgs potential. However, we find that, in order for X to be long-lived and reheat the universe, its couplings to the Higgs must be highly suppressed and therefore that the resulting vev of X required to explain the soft Z 2 -breaking in the Higgs potential must be many orders of magnitude above the twin scale. If this is to be identified with the characteristic mass scale of X, then a UV-completion of the twin Higgs is required for anything further to be said of the prospects of this possibility. However, if such a UV completion has similar structure to the couplings in (5.34), then asymmetric reheating may require a cancellation between the odd and even couplings of X to the Higgs potential in order to suppress its twin-sector branching fraction (because the odd coupling appears with opposite signs in the coupling between X and the h A and h B states). We do not consider this possibility further. For appropriate values of m X , the out-of-equilibrium decay of X reheats the two sectors to different temperatures and effectively dilutes the energy density in the twin sector. We obtain an analytic estimate of the effects of the X decay on the number of light Analytic estimate of N eff If X dominates the energy density of the universe and then decays, depositing energy ρ SM and ρ t into the SM and twin sectors respectively, then the temperature ratio is determined by where T SM reheat and T t reheat are the reheating temperatures for each sector, while g SM and g t are the SM and twin effective number of relativistic degrees of freedom, respectively. We have assumed that the two sectors are cool enough that they have already decoupled. We point out that not only does the number of effective degrees of freedom in each sector need to be evaluated at the temperature of that sector, but that g t and g SM differ as functions of temperature due to the differences in the spectra of the sectors, as seen in  the temperature of the sector at matter-radiation equality, which the CMB probes as explained in Section 5.2.2, and a(T ) is the scale factor as a function of temperature. In the mirror Twin Higgs model, the two sectors have the same number of light degrees of freedom at recombination (three neutrinos and a photon, assuming that the neutrinos are still relativistic), so As our range of reheat temperatures encompasses the QCD phase transitions of both sectors, the factors of g can be important. Given the temperatures of the two sectors after X decays, we can obtain a simple estimate of the contribution to N eff that neglects the impact of masses of the twin neutrinos discussed in Section (5.2.2), In this limit the most recent Planck data give a 2σ bound of ∆N eff 0.40 assuming pure ΛCDM+N eff [342] . This translates into the requirement Of course, as discussed in Section 5.2.2, the twin neutrino masses are relevant at the temperature of matter-radiation equality, so we can obtain a more meaningful estimate of ∆N eff using the results of Section 5.2.2 evaluated at the twin temperature determined above: Numerical Calculation of N eff A more precise study of the effect of X decay on the number of effective neutrino species at recombination may be performed by numerically solving a system of differential equations for the entropy in X and the two sectors as a function of time. Following the analysis of Chapter 5.3 of [390] we have where S i are comoving entropy densities and it has been assumed that X is cold by the time it decays so that ρ X = m X n X with number density n X (this is reliable as we only consider m X > 10 GeV, which is above the decoupling temperature of ∼ 1 GeV). The rate of heat flow from sector j to i per proper volume, dq j→j dt , is defined in (5.6). To account for the temperature-dependence of the effective number of relativistic degrees of freedom in each sector, these equations are solved iteratively in the profiles of g i (T i ). The equations are solved in three stages: before, during and after the decoupling of the SM and twin sectors. The ratio f /v is fixed to 4 for this analysis. Initial conditions were chosen with ρ = 10 −12 ρ X , for combined SM and twin energy densities ρ. However, it is only the requirement that the initial energy density of X dominates over that of the SM and twin sectors that is important for simulating the cosmology over the times of interest here, as the entirety of the latter is then generated by the subsequent decay. The results close to the decoupling and reheating epochs are otherwise insensitive to the initial conditions and ultimately match onto the standard outcome [390] computation of the scattering rates unreliable at temperatures below ∼ 1 GeV, as found in Section 5.2.2 and also in the results below, the sectors decouple above these temperatures. Notably, the impact of X on the expansion rate causes decoupling to occur at slightly hotter temperatures than expected from the analysis of Section 5.2.2 for the decoupling in the standard cosmology. The ratio of energy densities in each sector determines N eff , from (5.39) . A plot of this ratio over time is shown in Figure 5 .7, with the expectation under the approximations of the previous section shown as well. This approximation is reliable as long as the lifetime of X is much longer than the temperature at which decoupling concludes, here ∼ 1 GeV. The larger asymptotic value of the ratio of the blue line arises because the lifetime lies close to the decoupling period, so that a significant fraction of the energy is transferred while the sectors are thermalised or partially thermalised and does not contribute toward asymmetric reheating. Equivalently, as will be discussed below, insufficient time elapses between decoupling and reheating for the twin energy density to dilute and be repopulated by the decays to the level predicted by (5.35) . The subsequent bump represents the period between the reheating of the twin sector by its QCD phase transition followed by that of the SM. The green and orange lines correspond to reheating temperatures that lie between SM and twin QCD phase transitions. In these cases, the reheating of the SM from the subsequent SM QCD phase transition raises its energy density relative to the twin sector above that expected from the ratio of branching fractions. As this occurs after the lifetime of the reheaton, the estimate of the reheating temperatures presented in (5.36) is still good as subsequent changes in the ratio due to the evolution of g are accounted for in our analysis of the reheating scenarios. The steep drop in the energy density ratio corresponds to the brief period during which the energy density of the twin sector present at decoupling dilutes and redshifts, which continues until it reaches a comparable size to the energy density that is being replenished by reheating. If the twin-sector branching fraction is highly suppressed, as can occur in the "valley" region in Figure 5 .6 with m h m X 2m h , then a longer time is required for this to happen, especially close to the decay epoch where the diminishing of the X population also contributes to a reduced reheating rate. These effects can prolong the time required for the energy density ratio to converge to the asymptotic prediction of (5.35). Contour plots of ∆N eff as a function of m X and f /v appear in Figure 5 Also, as discussed in Section 5.2.2, a large temperature difference may partially relax back if reheating occurs close to sector decoupling. However, a reliable calculation of the heat flow at the temperatures of interest here must incorporate non-perturbative effects. We do not perform such a computation, but note that, at a slightly higher SM reheating temperature of 2 GeV where this computation is more reliable, ∆N eff in Figure 5 . We emphasize that, if the lifetime of X is sufficiently close to the time of decoupling, or equivalently, that the reheating temperature is sufficiently close to the decoupling temperature, then the residual twin energy density left-over may be comparable to or greater than that regenerated by reheating. Consequently, the suppression in ∆N eff would be less than that predicted in (5.36) . In this respect, the projection of Figure 5 .8 should be regarded as a lower bound on ∆N eff . In the regions of high suppression, such as the "valley" region, the full asymmetry may not be generated before the complete decay of X when the reheating temperature is of similar order as the decoupling temperature. In particular, for the reheating temperature chosen here of 0.7 GeV and branching fraction Br(X → Twin) ∼ 10 −5 , the numerical calculation of the energy density ratio saturates at ∼ 4 × 10 −5 . We do not include this effect in Figure 5 .8 as its only impact is to mildly shift the unobservably small ∆N eff = 10 −4 contour. Lower reheating temperatures would agree with the prediction of (5.35) were it not for the caveat that the twin neutrinos may be produced out of equilibrium. However, this minimum value at which ∆N eff is saturated can grow significantly with hotter reheating temperatures upon which it is highly dependent. CMB-S4 observations will be able to probe a large portion of the most natural parameter space, save the region m h m X 2m h where decays into the Standard Model dominate well beyond the ratio f 2 /v 2 , as previously discussed. Significantly, precision Higgs coupling measurements at the LHC are unlikely to probe the mirror Twin Higgs model beyond f ∼ 4v, so that the observation of additional dark radiation may be the first signature of a mirror Twin Higgs. Neutrino Masses In addition to the bounds on N eff , we must also respect the bounds on neutrino masses. The analysis remains nearly the same as in Section 5.2.2, but now with the twin neutrinos at a lower temperature, as determined above. As mentioned above, for large enough f /v and SM reheating temperature sufficiently close to the lower bound, the reheating temperature of the twin sector may be below the twin neutrino decoupling temperature and the resulting energy density would be more difficult to compute. For simplicity, we choose λ x x large enough such that the twin reheating temperature is always above the twin neutrino decoupling temperature. The dark blue region is in tension with Planck, while the light blue region will be tested by CMB-S4. Gray regions are where the X mass is below the twin charm threshold and our calculation of the twin sector partial width is unreliable. As before, we compute m eff ν as In relating the scale factors at neutrino decoupling in each sector, we now have to use the above temperature ratio to find, analogously to Section 5.2.2, that  In our discussion up to this point, we have been agnostic about the origin of the cosmic abundance of X and have operated under the assumption that it absolutely dominates the cosmology before it decays. Here, we consider the possibility that X was thermally produced through freeze-out and subsequently dominates the universe as a relic before decaying. This thermal history is viable, but places strong constraints on the mass and couplings of the X. The energy density of relativistic species redshifts as ρ r ∝ a −4 ∝ T 4 , while the energy It has been assumed that Γt Γ SM ∼ (v/f ) 2 . Note however, that, from Figure 5 .8, this scaling of the partial widths holds only for the mass range 50 GeV m X 120 GeV, outside of which the twin partial width is more suppressed and the model is only testable through ∆N eff over a smaller range in f /v. density of non-relativistic, chemically decoupled matter scales as ρ m ∝ a −3 . The energy density contained in the X can therefore only grow relative to the energy density in the thermal bath once it becomes non-relativistic. We found in Section 5.2.3 that by recombination, ρ t /ρ SM 0.09 is needed to evade current bounds on ∆N eff . Thus we need to have the energy density in the X dominate over the SM and twin plasmas by more than this factor when it decays. If X becomes non-relativistic instantaneously at the moment that its temperature reaches some fraction c ∼ O(0.1) of its mass, then, as T ∝ 1/a and ρ X is ∼ 1/g of the total energy density, the mass is required to satisfy m X 10/c × g (T = m X ) T SM Xreheat . Since the SM reheating temperature is strongly constrained to be above BBN, this effectively puts a lower limit on the mass of the X. Importantly, X must freeze-out when relativistic or its energy density will be further Boltzmann suppressed. The lower limit on the mass of the X becomes an upper limit on the X's couplings -if it couples too strongly to the thermal bath, then it won't freeze out early enough to be hot. In fact the situation is somewhat less favorable than the above analysis suggests, because it is relevant operators that must keep X in thermal equilibrium. For an X with the interactions introduced in Section 5.2.3, the annihilations have rates that scale with temperature as Γ ∼ n X σv ∼ T for T m X , m h (where n X is the number density of X and σv is its thermally averaged annihilation cross section). However, in a radiation-dominated universe, H ∼ T 2 . Thus, at high enough temperatures, X is not in thermal equilibrium with the plasma and it is only once the universe cools enough that it may thermalise. Then, as the temperature drops, XX → qq annihilations become suppressed by the Higgs mass and subsequent Boltzmann suppression causes X to freezeout. Note that the rates of these annihilation processes are controlled by the coupling λ x , independently of x, which is unconstrained by itself (other processes mediated by λ x x are found to be subdominant in the ensuing analysis, for the range of λ x over which thermal production is successful). If the coupling is too weak to begin with, then the X never thermalises and thermal production cannot happen. Thermal production therefore requires a careful balancing of parameters -small coupling λ x is preferred for X to freeze-out hot and as early as possible, but the coupling is bounded from below by the requirement that X reach thermal equilibrium. This combination of constraints severely restricts the size of the parameter space over which thermal production is viable to cases in which the coupling is selected so that X enters and departs from thermal equilibrium at close to the same temperature. To obtain numerical predictions for this scenario, the calculation of Section 5.2.3 was modified to account for the time after the freeze-out of X before it becomes nonrelativistic. During this period we use (5.15) and (5.16) for the energy density of the X, approximating decays as being negligible, before switching over to (5.42) when the temperature drops below the mass of the X. The approximation that the X does not decay appreciably while it is relativistic must be good if there is to be sufficient time for it to grow to dominate between becoming non-relativistic and decaying. The decay width of X was fixed to 5 × 10 −21 GeV, corresponding to a reheating temperature close to the ∼ 10 MeV lower limit, in order to maximise the amount of time over which the energy density of X may grow relative to the SM plasma, thereby providing the greatest possible reheating. The predictions for ∆N eff from a thermally produced X are shown in Figure 5 .10 for the small regions of parameter space where this is viable, with f /v = 4. We find that the dominant annihilation channels over this region are XX → tt and XX → bb, mediated by the light Higgs, as well as their twin analogues, mediated by the heavy Higgs. As expected, the primordial energy density in the twin sector is too large compared to that generated by the X for the asymmetric reheating to be effective when m X is too light ( 100 GeV in this case). Similarly, when the coupling is too strong, the X is held in equilibrium for longer and freezes-out underabundant compared to the twin energy density. However, when the coupling is too weak (the gray region), X never thermalises to begin with (close to the boundary with this region, X freezes-out almost immediately after thermalising). The peak in the contours occurs because of the "H-funnel" in which the twin Higgs resonantly enhances annihilations into twin quarks. All of this region will be testable by CMB-S4.  As an alternative to the model presented above of late, out-of-equilibrium decays of a Z 2symmetric scalar, one may imagine that the field driving primordial inflation reheats only the Standard Model to below the decoupling temperature of the two sectors. Production of the twin particles then ceases at some time after the temperature drops below the decoupling temperature during reheating. To make this consistent with a softly-broken Z 2 symmetry, we extend the inflationary sector and introduce a 'twinflaton' that couples solely to the twin sector. The combined inflationary and twinflationary sectors respect the Z 2 symmetry. However, if the two sectors are entirely symmetric then one generally expects both inflationary dynamics to happen coincidentally, which would result in identical reheating. We therefore rely on soft Z 2 -breaking to give an asymmetry between the two sectors that causes the twinflationary sector to dominate the universe first. With the right arrangement we can end up with two distinct periods of inflation -a first caused by the "twinflaton" and a second that then reheats the Standard Model to below the decoupling temperature, having diluted the sources of twin-sector reheating from the first period. One simple mechanism for Z 2 -breaking which is well-suited for introducing asymmetry to inflationary sectors is to introduce an additional Z 2 -odd scalar field η (as was done in [399] ). This admits linear and quadratic interactions to antisymmetric and symmetric combinations of the inflationary sector fields, respectively. When η acquires a vev, this introduces an asymmetry in the fields to which it was coupled, dependent on the combination of its vev and its couplings. If η is coupled to both the inflationary sectors and the Higgs sectors, it could be the sole source of Z 2 -breaking in a twinflationary theory. One may generally imagine that, in some UV completion, the mechanism that softly breaks the symmetry in the Higgs potential could also be the origin of the soft breaking of the inflationary sector. Cosmologically, this possibility may have similar observational signatures as the model discussed in Section 5.2.3, where the amount of twin-sector dark radiation is determined by the partial widths of the inflaton of the second inflationary epoch. If this dominantly couples to the SM, then ∆N eff will be suppressed which, while successfully resolving the cosmological problems of the Mirror Twin Higgs, may also be observationally inaccessible. However, additional, distinctly inflationary signatures may make this potentially testable by other cosmological observations. The mechanism of twinflation completes a catalog of models of asymmetric reheating by late decays, which may be indexed by representations of the twin parity: the case of a Z 2 -even particle, in which a kinematic asymmetry in the partial widths provides the reheating asymmetry, the case of a Z 2 -odd particle, which can also provide the spontaneous Z 2 -breaking required in the Higgs potential, and the case where two distinct, long-lived particles couple to each sector, which may also be related to inflation. As a toy model we here consider 'twinning' the simple ϕ 2 chaotic inflation scenario. The inflationary dynamics in this case are easy to understand and we have the additional benefit that this inflationary model has been considered in the literature before as 'Double Inflation' (see [400] , [401] and [402] ). We furthermore specialize to 'double inflation with a break', where there are two distinct periods of inflation which produces a step in the power spectrum, and we consider the constraints that this places on our model. In this case, it is assumed that each inflaton field couples and therefore decays dominantly into the sector to which it belongs. We will comment briefly on the case without a break and the additional signals one could look for in that case. The potential of the inflationary sector for inflaton ϕ A and twinflaton ϕ B is where m A = m B may arise from soft Z 2 -breaking, perhaps related to the soft Z 2breaking in the Higgs potential. In order for the 'twinflation' to occur first, we require that the energy of the B field initially dominates the energy density of the universe. We take the initial positions of the fields to be the same and m 2 where ϕ c is the critical value at which inflation stops and m B = rm A = rm with n, r > 1. The inflationary dynamics are then those of slowlyrolling scalar fields. At some point in the early universe we imagine that the slow-roll approximation holds for both fields and the inflationary sector dominates the universe. The dominating field then slow-rolls down its potential for n 2 −1 2 e-folds, while the lighter field's velocity is suppressed by approximately ϕ A r 2 ϕ B . Solving the system numerically reveals that the motion of ϕ A during this period can be neglected entirely. After ϕ B reaches the critical value √ 2 M pl , it stops slow-rolling and begins oscillating around the minimum of its potential. For there to be two distinct periods of inflation, there must be a period where these oscillations dominate the universe, which requires that the energy densities of each inflaton ρ A and ρ B satisfy ρ B (ϕ c ) = r 2 m 2 M 2 pl > ρ A (ϕ(0)) = n 2 m 2 M 2 pl and therefore r > n. For a ϕ 2 B potential, the energy in these oscillations redshifts as ρ B ∼ a −3 . Eventually, the energy density in ϕ B drops below that of ϕ A and a new epoch of inflation, driven by ϕ A , begins. This provides a further n 2 −1 2 e-folds of inflation to give n 2 − 1 in total, while the B-sector energy density is diluted away. Note that in order for our toy model to reheat below the decoupling temperature of the two sectors, reheating must occur well after the end of inflation. If, during the coherent 25 Note that merely giving the twin field a much larger initial condition does not instigate twinflation. The dynamics of the subdominant field in this case are such that it will track the dominant field and both will reach the critical value at the same time. This is easily confirmed numerically. oscillation of an inflaton, it becomes the case that the inflaton decay width Γ ∼ H, then reheating will occur and result in temperature T reheat ∼ 0.1 ΓM pl . However, if Γ H when inflation ends, then all of the energy in the inflaton is immediately transferred and we instead have reheating temperature T reheat ∼ 0.1 m α M pl for an inflaton of mass m α . But in order for T reheat 1 GeV, it is required that m α 10 −7 eV, so this possibility that the inflaton is short lived is not viable. The procedure of twinning inflationary potentials may be generalised to other, more realistic models, provided that this constraint upon the reheating temperature can be satisfied. One could always make a twinflationary scenario consistent with observational constraints by letting the second inflationary period of inflation last long enough. In our toy model, this would correspond to setting n high enough that the momentum modes which left the horizon during the first inflation have not yet re-entered the horizon -such a scenario would look exactly like single-field chaotic inflation. Alternatively, we may also allow for n small enough that all the momentum modes that left the horizon during the second inflation are currently sub-horizon. In this case, fluctuations at large enough wavenumbers (equivalently, small enough length scales) are 'processed' (cross the horizon) at a different inflationary energy scale than those that were processed earlier, giving a step in the power spectrum. While Planck has measured the primordial power spectrum for modes with 10 −4 Mpc −1 k 0.3 Mpc −1 (where the lower bound is set by the fact that smaller modes have not yet re-entered the horizon), proposed CMB-S4 experiments will increase this range [375] somewhat, as will be discussed further below. We wish to show that the power spectrum of our toy model is not ruled out and, furthermore, may be observed in the coming decades. The height of the step in the primordial power spectrum is determined by the energy scale of each period of inflation, so modes crossing the horizon in the second inflationary period should be suppressed by a factor of r 2 > n 2 25 compared to those exiting in the first period. This degree of suppression is ruled out by Planck for the range of modes over which it has reconstructed the power spectrum [398] . A computation of the primordial power spectrum for double inflation was given in [401] . It was found that significant damping does not occur for modes which cross outside the horizon during the first inflationary period, re-enter during the inter-inflationary period and again cross the horizon during the second inflationary period. It is only those scales which first cross the horizon during the second inflationary period that are significantly damped (although other features in the shape, such as oscillations, may be present for modes that are subhorizon during the intermediate period). The relation of this characteristic scale to present-day observables is easily done using the framework given in [403] . Let the subscripts a, b, c, d, e respectively correspond to the beginning of the first inflationary period, the end of that period, the beginning of the second inflationary period, the end of that period, and the beginning of radiation domination. During the coherent oscillation periods, the inflaton acts as matter and the energy density falls as ρ ∝ a −3 . Let k i be the momentum whose mode is horizon-size at the i epoch; k i = a i H i . The scales k i can be related using the number of e-folds in each period, which are themselves determined from the first Friedmann equation. Denoting 2 N bc k c and similarly for the other characteristic modes, where, in particular, slow-roll inflation predicts that N ab = N cd = n 2 −1 2 . The evolution of the characteristic momentum scales is shown schematically in Figure 5 .11. Finally, k e can be determined using the conservation of comoving entropy: where T 0 and a 0 are the temperature and scale factor today and T reheat is the reheating temperature (which is sufficiently low that only SM particles are produced). We work explicitly with the convention a 0 = 1. The characteristic modes associated with the break can then be determined. As mentioned above, [401] shows that damping occurs for modes that exit the horizon only during the second inflationary period, so we should take the characteristic damping scale to be the smallest such scale, which here corresponds roughly to k b This can be determined as where k c only differs by the factor of (r/n) 1/3 (which is roughly close to unity). Once again, between k b and k c are oscillatory features, so k b should merely be taken as the rough characteristic scale of the damping. Now the characteristic damping scale is determined by m, n, r, and T reheat . Our observational bound on k b is that Planck has not seen this suppression on momentum scales at which it has been able to reconstruct the primordial power spectrum from the angular temperature anisotropy power spectrum, which is roughly k 0.3 Mpc −1 . We have constraints on the reheating temperature from rethermalization of the twin sector or interrupted big bang nucleosynthesis 10 MeV T reheat 1 GeV, on having a period 214 of intermediate matter domination between the two inflations r > n and on the total number of e-folds n 2 − 1 25 to solve cosmological problems. Note that we require fewer e-folds of inflation than is typically assumed in the standard cosmology. Since the low reheating temperature gives fewer e-folds from reheating up to today, less inflation is needed to explain the large causal horizon and flatness. The normalization of the spectrum provides a further constraint, the most recent measurement of which come from Planck [398] . The scalar power spectrum at k = 0.05 Mpc −1 is measured to be P R (k ) = e 3.094±0.034 × 10 −10 . Then for k < k c (i.e. k having left the horizon during the first period of inflation and not re-entered before the second, so no deviation from single-field inflation would be seen at this scale), the spectrum of [401] yields the constraint The characteristic scale (5.49) depends much more strongly on n than it does on any of the other parameters. In Figure 5 .12, we give a rough idea of the scale as a function of n, having set T reheat = 10 MeV and r = 2n, while m is chosen to satisfy the normalization condition. We also show the constraint on k b set by Planck. Note again that the region described as "observationally single-stage inflation" does still provide a solution to the problem of reconciling cosmology with the mirror Twin Higgs. CMB-S4 will improve the constraint on k b through its improved measurement of polarization anisotropies [375] . With only precision measurements of temperature anisotropies, the un-lensed power spectrum cannot be so easily reconstructed from the lensed spectrum. The effects of gravitational lensing of CMB place an upper limit on the size of primordial temperature anisotropies that can be measured [404] , which Planck has saturated. However, the polarization anisotropy power spectrum allows the removal of lensing noise from the temperature spectrum so that higher primordial modes can be detected. The polarization power spectrum itself also gives us another window into the high-modes of the primordial power spectrum, as the signal does not become dominated by polarized foreground sources until higher scales near ∼ 5000. CMB-S4 is projected to make cosmic variance limited measurements of both the temperature and polarization anisotropy power spectra up to the modes where they become foreground-contaminated and so provide additional information on the shape of the primordial power spectrum [375]. The map from measurements of angular modes to contraints on spatial modes k depends on the evolution of the power spectrum between inflation and the CMB, so forecasting constraints requires careful study. However, these improvements will not test most of the parameter space presented in Figure 5 .12, where the step is predicted on extremely small distance scales. We have discussed a twinflationary model of double inflation with a break for simplicity, but there is a parametric regime where double inflation without a break gives the required amount of asymmetric reheating into the Standard Model. With two periods of inflation, the second period dilutes the energy density of the heavier field sufficiently that there is no observable signal of it produced in reheating. However, even with only one period, inflation can continue for long enough after the inflaton turns the corner in field space such that, at late times, the fraction of the inflaton in the B state relative to the A state is small enough that the expected energy densities that are transferred into each sector satisfy ρ B /ρ A < 0.1. This occurs as long as r 1.2, assuming that the mixing angle of the slow-rolling field with the ϕ A and ϕ B fields entirely determines the fraction of its energy that reheats each sector. There is thus a much larger range of r where this toy model of inflation passes N eff bounds than our above analysis shows. The resulting imprint on the CMB could resemble that of the long-lived decay model of When there is only one period of inflation, the step is smoothed out and less pronounced and it is necessary to locate the feature numerically. Furthermore, having multiple degrees of freedom available allows for non-trivial evolution of momentum modes after they become super-horizon, which does not occur in single-field inflation but may be calculated from the full solution to the field equations [402] . While a twinned potential leading to two periods of inflation generally predicts a step in the power spectrum, when there is no break the predictions, and thus constraints, this prediction become more model-dependent. Therefore we leave detailed predictions in that case for future study using realistic models and merely state that the range of r = 1 to n interpolates between the single field spectrum and that with a step, as one would expect. There are also at least two other detectable effects one might expect in double inflation without a break and in general realistic twinflationary models. Interactions between inflaton fields may produce primordial non-Gaussianities, while the presence of additional oscillating degrees of freedom may produce isocurvature perturbations. These do not appear in our toy model because the heavy field is exponentially damped during the second inflation. CMB-S4 is projected to improve Planck's bounds on non-Gaussianities by a factor of ∼ 2 and on isocurvature perturbations by perhaps an order of magnitude (though model-independent projections have not been made), so may be able to detect or place useful constraints on realistic twinflationary models [375]. We have introduced twinflation as a mirror Twin Higgs model which suppresses the cosmological effects of twin light degrees of freedom. It extends the mirror symmetry to the inflationary sector. The soft Z 2 symmetry-breaking of the Higgs sector may be used in the inflationary sector to cause distinct periods of inflation. There exists a parametric region where this is cosmologically indistinct from single-stage inflation, but also another in which it may be observable. As the direct product of inflation and the Mirror Twin Higgs, this is in some sense a minimal solution. In this section we have considered scenarios in which cosmology provides meaningful insight on solutions to the electroweak hierarchy problem. In particular, we have demon- be realized in complete inflationary models that match the observed spectral index and constraints on the tensor-to-scalar ratio. While we have taken care to ensure that our scenarios respect the well-measured cosmological history beneath T ∼ 1 MeV, we have not addressed the origin of the observed baryon asymmetry. In the case of out-of equilibrium decays, there are a number of possibilities. It is plausible that a somewhat larger baryon asymmetry is generated through various conventional mechanisms and diluted by late decays. Alternatively, the decay mechanism itself may possibly be expanded to generate a baryon asymmetry or some other late decay may generate the baryon asymmetry below ∼ 1 GeV. In the case of twinflation, inflationary dilution of pre-existing baryon asymmetry requires that baryogenesis occur in association with reheating or via another mechanism at temperatures below ∼ 1 GeV. It would be worthwhile to study models for the baryon asymmetry consistent with these scenarios. Steps in this direction have been taken in [362] , which attempted to relate this to asymmetric dark matter in the twin sector. Likewise, any investigation of dark matter, be it related directly to the twin mechanism or otherwise, must also address implications of the dilution. Previous work attempting to construct dark matter candidates in the twin sector [347, 357, 358, 359, 360, 361, 362, 363] ) has relied upon explicit Z 2 -breaking that is not present in the mirror model. Dark matter may alternatively be unrelated to the Twin Higgs mechanism, such as a a WIMP in some minimal extension of the electroweak sector that freezes-out as an overabundant thermal relic and is then diluted to the observed density during reheating. Alternatively, it may be that the dark matter abundance is produced directly during reheating. It would be interesting to study extensions of our scenarios that incorporate dark matter candidates directly related to the mechanism of dilution. Finally, we have only approximately parameterized Planck constraints and the reach of CMB-S4 on twin neutrinos and twin photons. Ultimately, more precise constraints and forecasts may be obtained via numerical CMB codes. This strongly motivates the future study of CMB constraints on scenarios with three sterile neutrinos and additional dark radiation whose temperatures differ from the Standard Model thermal bath. In this work, we build on a MTH framework where 'hard' breaking of the Z 2 is absent. In [13, 405] , it was realized that late-time asymmetric reheating of the two sectors could arise naturally in these models if the spectrum were extended by a single new state. This asymmetric reheating would dilute the twin energy density and so attune the MTH with the cosmological constraints. This dilution of twin energy density to negligible levels would seem to hamper the prospect that twin states might constitute the dark matter, and generating dark matter was left as an open question. This presents a major challenge toward making such cosmologies realistic. However, we show that asymmetric reheating perfectly sets the stage for a MTH realization of the 'freeze-in' mechanism for dark matter production [406, 407, 408, 409, 410, 411, 412, 413] . Freeze-in scenarios are characterized by two assumptions: 1) DM has a negligible density at some early time and 2) DM interacts with the SM so feebly that it never achieves thermal equilibrium with the SM bath. 26 This second assumption is motivated in part by the continued non-observation of non-gravitational DM-SM interactions. Both assumptions stand in stark contrast to freeze-out scenarios. Freeze-twin dark matter is a particularly interesting freeze-in scenario because both assumptions are fulfilled for reasons orthogonal to dark matter considerations: 1) the negligible initial dark matter abundance is predicted by the asymmetric reheating already necessary to resolve the MTH cosmology, and 2) the kinetic mixing necessary to achieve the correct relic abundance is of the order expected from infrared contributions in the MTH. To allow the frozen-in twin electrons and positrons to be DM, we need only break the Z 2 by a relevant operator to give a Stueckelberg mass to twin hypercharge. Additionally, the twin photon masses we consider can lead to dark matter self-interactions at the level relevant for small-scale structure problems [414] . The next sections are organized as follows: In Section 5.3.2, we review the MTH and its cosmology in models with asymmetric reheating, and in Section 5.3.3 we introduce our extension. In Section 5.3.4, we calculate the freeze-in yield for twin electrons and discuss the parameter space to generate dark matter and constraints thereon. We discuss future directions and conclude in Section 5.3.5. For the interested reader, we include some discussion of the irreducible IR contributions to kinetic mixing in the MTH in Appendix A. The mirror twin Higgs framework [288] introduces a twin sector B, which is a 'mirror' copy of the Standard Model sector A, related by a Z 2 symmetry. Upgrading the SU (2) A × SU (2) B gauge symmetry of the scalar potential to an SU (4) global symmetry adds a Higgs-portal interaction between the A and B sectors: We refer to twin particles by their SM counterparts primed with a superscript ', and we refer the reader to [288, 289] for further discussion of the twin Higgs mechanism. The thermal bath history in the conventional MTH is fully dictated by the Higgs portal in Eq. (5.51) which keeps the SM and twin sectors in thermal equilibrium down to temperatures O(GeV). A detailed calculation of the decoupling process was performed in [13] by tracking the bulk heat flow between the two sectors as a function of SM temperature. It was found that for the benchmark of f /v = 4, decoupling begins at a SM temperature of T ∼ 4 GeV and by ∼ 1 GeV, the ratio of twin-to-SM temperatures may reach 0.1 without rebounding. While heat flow rates become less precise below ∼ 1 GeV due to uncertainties in hadronic scattering rates, especially close to colorconfinement, decoupling between the two sectors is complete by then for f /v 4. For larger f /v, the decoupling begins and ends at higher temperatures. As mentioned above, one class of solutions to this N eff problem uses hard breaking of the Z 2 at the level of the spectra [289, 347, 348, 349, 350] while keeping a standard cosmology. An alternative proposal is to modify the cosmology with asymmetric reheating to dilute the energy density of twin states. For example, [405] uses late, out-of-equilibrium decays of right-handed neutrinos, while [13] uses those of a scalar singlet. These new particles respect the Z 2 , but dominantly decay to SM states due to the already-present soft Z 2 -breaking in the scalar sector. In [405] , this is solely due to extra suppression by f /v-heavier mediators, while in [13] , the scalar also preferentially mass-mixes with the heavier twin Higgs. [13] also presented a toy model of 'Twinflation', where a softly-broken Z 2 -symmetric scalar sector may lead to inflationary reheating which asymmetrically reheats the two sectors to different temperatures. In any of these scenarios, the twin sector We will stay agnostic about the particular mechanism at play, and merely assume that by T ∼ 1 GeV, the Higgs portal interactions have become inefficient and some mechanism of asymmetric reheating has occurred such that the energy density in the twin sector has been largely depleted, ρ twin ≈ 0. 27 This is consistent with the results of the decoupling calculation in [13] given the uncertainties in the rates at low temperatures, and will certainly be true once one gets down to few × 10 2 MeV. One may be concerned that there will be vestigial model-dependence from irrelevant operators induced by the asymmetric reheating mechanism which connect the two sectors. However, these operators will generally be suppressed by scales above the reheating scale, as in the example studied in [13] . Prior to asymmetric reheating, the two sectors are in thermal equilibrium anyway, so these have little effect. After the energy density in twin states has been diluted relative to that in the SM states, the temperature is far below the heavy masses suppressing such irrelevant operators, and thus their effects are negligible. So we may indeed stay largely agnostic of the cosmological evolution before asymmetric reheating as well as the details of how this reheating takes place. We take the absence of twin energy density as an initial condition, but emphasize that there are external, well-motivated reasons for this to hold in twin Higgs models, as well as concrete models that predict this occurrence naturally. In order to arrange for freeze-in, we add to the MTH kinetic mixing between the SM and twin hypercharges and a Stueckelberg mass for twin hypercharge. At low energies, these reduce to such terms for the photons instead, parametrized as This gives each SM particle of electric charge Q an effective twin electric charge Q. 28 The twin photon thus gives rise to a 'kinetic mixing portal' through which the SM bath may freeze-in light twin fermions in the early universe. The Stueckelberg mass constitutes soft Z 2 -breaking, 29 but has no implications for the fine-tuning of the Higgs mass since hypercharge corrections are already consistent with naturalness [289] . We will require m γ > m e , to prevent frozen-in twin electron/positron annihilations, and m γ > 2m e , to ensure that resonant production through the twin photon is kinematically accessible. Resonant production will allow a much smaller kinetic mixing to generate the correct relic abundance, thus avoiding indirect bounds from supernova cooling. We note that while taking m γ f does bear explanation, the parameter is technically natural. On the other hand, mixing of the twin and SM U (1)s preserves the symmetries of the MTH EFT, so quite generally one might expect it to be larger than that needed for freeze-in. However, it is known that in the MTH a nonzero is not generated through three loops [288] . While such a suppressed mixing is phenomenologically irrelevant for most purposes, here it plays a central role. In Appendix A, we discuss at some length the vanishing of infrared contributions to kinetic mixing through few loop order. If nonzero 28 Note that twin charged states do not couple to the SM photon. Their coupling to the SM Z boson has no impact on freeze-in at the temperatures under consideration. Furthermore, the miniscule kinetic mixing necessary for freeze-in has negligible effects at collider experiments. See Ref. [418] for details. 29 While we are breaking the Z 2 symmetry by a relevant operator, the extent to which a Stueckelberg mass is truly soft breaking is not clear. Taking solely Eq. (5.52), we would have more degrees of freedom in the twin sector than in the SM, and in a given UV completion it may be difficult to isolate this Z 2asymmetry from the Higgs potential. One possible fix may be to add an extremely tiny, experimentally allowed Stueckelberg mass for the SM photon as well [419] , though we note this may be in violation of quantum gravity [420, 421] or simply be difficult to realize in UV completions without extreme finetuning. We will remain agnostic about this UV issue and continue to refer to this as 'soft breaking', following [418] . contributions appear at the first loop order where they are not known to vanish, kinetic mixing of the order ∼ 10 −13 − 10 −10 is expected. The diagrams which generate kinetic mixing will likely also generate higher-dimensional operators. These will be suppressed by (twin) electroweak scales and so, as discussed above for the irrelevant operators generated by the model-dependent reheating mechanism, freeze-in contributions from these operators are negligible. As we are in the regime where freeze-in proceeds while the temperature sweeps over the mass scales in the problem, it is not precisely correct to categorize this into either "UV freeze-in" or "IR freeze-in". Above the mass of the twin photon, freeze-in proceeds through the marginal kinetic mixing operator, and so a naive classification would say this is IR dominated. However, below the mass of the twin photon, the clearest approach is to integrate out the twin photon, to find that freeze-in then proceeds through an irrelevant, dimension-six, four-Fermi operator which is suppressed by the twin photon mass. Thus, at temperatures T SM m γ , this freeze-in looks UV dominated. This leads to the conclusion that the freeze-in rate is largest at temperatures around the mass of the twin photon. Indeed, this is generally true of freeze-in -production occurs mainly at temperatures near the largest relevant scale in the process, whether that be the largest mass out of the bath particles, mediator, and dark matter, or the starting thermal bath temperature itself in the case that one of the preceding masses is even higher. As just argued, freeze-in production of dark matter occurs predominantly at and somewhat before T ∼ m γ . We require m γ 1 GeV so that most of the freeze-in yield comes from when T < 1 GeV, which allows us to retain 'UV-independence' in that we need not care about how asymmetric reheating has occurred past providing negligible density of twin states at T = 1 GeV. Specifically, we limit ourselves to m γ < 2m π 0 , both for this reason and to avoid uncertainties in the behavior of thermal pions during the epoch of the QCD phase transition. However, we emphasize that freeze-in will remain a viable option for producing a twin DM abundance for even heavier dark photons. But the fact that the freeze-in abundance will be generated simultaneously with asymmetric reheating demands that each sort of asymmetric reheating scenario must then be treated separately. Despite the additional difficulty involved in predicting the abundance for larger twin photon masses, it would be interesting to explore this part of parameter space. In particular, it would be interesting to consider concrete scenarios with twin photons in the range of tens of GeV [422] . To calculate the relic abundance of twin electrons and positrons, we use the Boltzmann equation for the number density of e : n e + 3Hn e = k,l − σv e ē →kl (n e nē − n eq e n eq e ) , (5.53) where σv e ē →kl is the thermally averaged cross section for the process e ē → kl, the sum runs over all processes with SM particles in the final states and e ē in the initial state, and n eq e is the equilibrium number density evaluated at temperature T . As we are in the parametric regime in which resonant production of twin electrons through the twin photon is allowed, 2 → 2 annihilation processesf f → γ →ē e , with f a charged SM fermion, entirely dominate the yield. In accordance with the freeze-in mechanism, n e remains negligibly smaller than its equilibrium number density throughout the freeze-in process, and so that term is ignored. It is useful to reparametrize the abundance of e in terms of its yield, Y e = n e /s where s = 2π 2 45 g * s T 3 is the entropy density in the SM bath. Integrating the Boltzmann equation using standard methods, we then find the yield of e today to be where T i = 1 GeV is the initial temperature of the SM bath at which freeze-in begins in our setup, g * (T ) is the number of degrees of freedom in the bath, and M P l is the reduced Planck mass. We will calculate this to an intended accuracy of 50%. To this level of accuracy, we may assume Maxwell-Boltzmann statistics to vastly simplify the calculation [423] . As a further simplification, we observe that the ∂ T g s term is negligible compared to 1/T except possibly during the QCDPT -where uncertainties on its temperature dependence remain [369] -and so we ignore that term. The general expression for the thermally averaged cross section of the process 12 → 34 is then σv n eq 1 n eq 2 = , and |M| 2 12→34 is the matrix element squared summed (not averaged) over all degrees of freedom. To very good approximation, the yield results entirely from resonant production, and so we may analytically simplify the matrix element squared forf f →ē e using the narrow-width approximation Γ γ is the total decay rate of the twin photon. For the range of m γ we consider, the twin photon can only decay to twin electron and positron pairs. Thus, its total decay rate is Its partial widths to SM fermion pairs are suppressed by 2 , and so contribute negligibly to its total width. The final yield of twin electrons is then where T f = Λ QCD for quarks, T f = 0 for leptons, Γ γ →f f is the partial decay width of the twin photon to ff , and the sum is over all SM fermion-antifermion pairs for which Since we have approximated the yield as being due entirely to on-shell production and decay of twin photons, the analytical expression for the yield in Eq. (5.58) exactly agrees with the yield from freezing-in γ via 'inverse decays'f f → γ , as derived in [410] . We have validated our numerical implementation of the freeze-in calculation by successfully reproducing the yield in similar cases found in [423, 424] . We have furthermore checked that reprocessing of the frozen-in dark matter [411, 425] through e ē → e ē e ē is negligible here, 30 as is the depletion from e ē → ν ν . An equal number of twin positrons are produced as twin electrons from the freeze-in processes. Requiring that reproduce the observed DM abundance today, we find where Ω χ h 2 ≈ 0.12, ρ crit /h 2 ≈ 1.1 × 10 −5 GeV/cm 3 , and s 0 ≈ 2900/cm 3 [48] .Ỹ e is the total yield with the overall factor of 2 removed. This requisite kinetic mixing appears 30 To be conservative, we calculate the rate assuming all interactions take place at the maximum √ s m γ and find that it is still far below Hubble. We perform the calculation of the cross section using MadGraph [426] with a model implemented in Feynrules [427] . Interestingly, self-interactions of this order have been suggested to fix small-scale issues, and some claimed detections have been made as well. We refer the reader to [414] for a recent review of these issues. As mentioned above and discussed further in Appendix A, the level of kinetic mixing required for freeze-in is roughly of the same order as is expected from infrared contributions in the MTH. It would be interesting to develop the technology to calculate the high-loop-order diagrams at which it may be generated. In the context of a complete model of the MTH where kinetic mixing is absent in the UV, is fully calculable and depends solely on the scale at which kinetic mixing is first allowed by the symmetries. Calculating would then predict a minimal model at some m γ to achieve the right dark matter relic abundance, making this effectively a zero-parameter extension of MTH models with asymmetric reheating. Importantly, even if is above those shown in Fig. 5 .13, that would simply point to a larger value of m γ which would suggest that the parameter point depends in more detail on the mechanism of asymmetric reheating. We note that in the case that the infrared contributions to are below those needed here, the required kinetic mixing may instead be provided by UV contributions and the scenario is unaffected. The mirror twin Higgs is perhaps the simplest avatar of the Neutral Naturalness program, which aims to address the increasingly severe little hierarchy problem. Understanding a consistent cosmological history for this model is therefore crucial, and an important step was taken in [13, 405] . As opposed to prior work, the cosmology of the MTH was remedied without hard breaking of the Z 2 symmetry by utilizing asymmetric reheating to dilute the twin energy density. Keeping the Z 2 as a good symmetry should simplify the task of writing high energy completions of these theories, but low-scale reheating may slightly complicate cosmology at early times. These works left as open questions how to set up cosmological epochs such as dark matter generation and baryogenesis in such models. We have here found that at least one of these questions has a natural answer. In this work, we have shown that twin electrons and positrons may be frozen-in as dark matter following asymmetric reheating in twin Higgs models. This requires extending the mirror twin Higgs minimally with a single free parameter: the twin photon mass. Freezing-in the observed DM abundance pins the required kinetic mixing to a level expected from infrared contributions in MTH models. In fact, the prospect of calculating the kinetic mixing generated in the MTH could make this an effectively parameter-free extension of the MTH. Compared to generic freeze-in scenarios, it is interesting in this case that the "just so" stories of feeble coupling and negligible initial density were already present for reasons entirely orthogonal to dark matter. This minimalism in freeze-twin dark matter correlates disparate signals which would allow this model to be triangulated with relatively few indirect hints of new physics. If deviations in Higgs couplings are observed at the HL-LHC or a future lepton collider, this would determine f /v [433, 434, 12, 10] , which would set the dark matter mass. An observation of anomalous cooling of a future supernova through the measurement of the neutrino 'light curve' might allow us to directly probe the m γ , curve [428, 429] , though this would rely on an improved understanding of the SM prediction for neutrino production. 31 Further astrophysical evidence of dark matter self-interactions would point to a combination of f /v and m γ . All of this complementarity underscores the value of a robust experimental particle physics program whereby new physics is pursued via every imaginable channel. 31 We thank Jae Hyeok Chang for a discussion on this point. There is nothing like looking, if you want to find something. You certainly usually find something, if you look, but it is not always quite the something you were after. The Hobbit, 1937 [435] Long-Lived Particles When we introduced models of Neutral Naturalness in Section 4.1.2, we were motivated by the lack of signals of new, SM-charged particles at colliders. However, experimentalists are very clever, and it is in fact possible to observe the effects of such particles if you know where to look. In particular, Neutral Naturalness models usually exhibit a 'hidden valley' type phenomenology, wherein a dark sector is connected to the SM only through some heavy states. At a collider, a high energy collision can transfer energy from our sector to these heavy dark sector states, which may then decay just as heavy SM particles do. If the dark sector contains absolutely stable particles, then the energy may cascade into those states, which simply leave the detector. But without a symmetry dictating that stability, dark sector states may be destabilized by the effects of that same high-energy link to the SM. That connection may induce higher-dimensional operators which allow the decay of the dark sector states back into SM states. Since this decay channel comes from interactions of heavy states, the width may be highly suppressed, leaving to a macroscopically long lifetime, even when the scale set by the particle's mass is microscopically small. This leads to the appearance of SM particles out of nowhere inside a detector a macroscopic distance away from the interaction point in a collider. In fact there are many reasons an unstable particle may be long-lived, and we see a to QCD, this may lead to the production of many states in a dark shower. If (some of) the lowest-lying states in the dark sector may decay back to SM particles, these will generically be displaced decays because they must go through the higher-dimensional operator. In the Twin Higgs, this may be production of either Higgs leading to dark showers with many twin glueballs, some of which mix with the Higgs and decay into SM states. Figure adapted from [436] . its leading decay mode proceeds through the small bottom quark Yukawas. Given the genericity of long-lived particles in our sector, it's entirely reasonable to imagine that some dark sector will have similar phenomenology. So even aside from our Neutral Naturalness motivation, such searches are generically useful and interesting things to look for. In this section we forecast how well an electron-positron collider will be able to probe Higgs decays to long-lived particles, using the parameters for some machines which have been proposed by the community and search strategies of our own design. Such forecasting is crucially important at the present time, as the community is still discussing what the next collider is that we will build 32 . It's clearly necessary to know what sort of physics program we expect we can carry out before we build a machine to do it. And these studies are used to motivate different types of detectors, their detailed design features and how trigger bandwidth is allocated. Following the discovery of the Higgs boson in 2012 [437, 438] , the precision study of its properties has rapidly become one of the centerpieces of the physics program at the LHC. The expansion of this program beyond the LHC has become one of the key motivators for proposed future accelerators, including lepton colliders such as CEPC [439, 440] , FCCee [441] , the ILC [442, 443] , and CLIC [444, 445] that would operate in part as Higgs factories. The for prompt exotic decay modes in the context of both the LHC (see e.g. [446, 447] ) and future Higgs factories [448] . However, an equally compelling possibility is for new physics to manifest itself in exotic decays of the Higgs boson to long-lived particles (LLPs). Such signals were first considered in the context of Hidden Valleys [449, 450, 451] and subsequently found to arise in a variety of motivated scenarios for physics beyond the Standard Model, including solutions to the electroweak hierarchy problem [289] and models of baryogenesis [452] ; for an excellent recent overview, see [453] . The search for exotic Higgs decays into LLPs necessarily involves strategies outside the scope of typical analyses. The non-standard nature of these signatures raises the compelling possibility of discovering new physics that has been heretofore concealed primarily by the novelty of its appearance. There is a rich and rapidly growing program of LLP searches at the LHC. A variety of existing searches by the ATLAS, CMS, and LHCb collaborations (e.g. [454, 455, 456] ; for a recent review see [457] ) constrain Higgs decays into LLPs at roughly the percent level across a range of LLP lifetimes. Significant improvements in sensitivity are possible in future LHC runs with potential advances in timing [458] , triggers [459, 460, 461] , and analysis strategies [353, 352] . Most notable among these is the possible implementation of a track trigger [459, 460] , which would significantly lower the trigger threshold for Higgs decays into LLPs and potentially allow sensitivity to branching ratios on the order of 10 −6 in zero-background scenarios. While studies of prompt exotic Higgs decays at future colliders [448] have demonstrated the potential for significantly improved reach over the LHC, comparatively little has been said about the prospects for constraining exotic Higgs decays to long-lived particles at the same facilities. 33 In this work we take the first steps towards filling this gap by studying the sensitivity of e + e − Higgs factories to hadronically-decaying new particles produced in exotic Higgs decays with decay lengths ranging from microns to meters. For the sake of definiteness we restrict our attention to circular Higgs factories operating at or near the peak rate for the Higgsstrahlung process e + e − → hZ, namely CEPC and FCC-ee, while also sketching the corresponding sensitivity for the √ s = 250 GeV stage of the ILC. While essentially all elements of general-purpose detectors may be brought to bear in the search for long-lived particles, the distribution of decay lengths for a given average lifetime make it advantageous to exploit detector elements close to the primary interaction point. We thus focus on signatures that can be identified in the tracker. In order to provide a faithful forecast accounting for realistic acceptance and background 33 A notable exception is CLIC, for which a study of tracker-based searches for Higgs decays to LLPs has been recently performed [462] . For preliminary studies of other non-Higgs LLP signatures at future lepton colliders, see e.g. [463] . For studies of LLP signatures at future electron-proton colliders, see [464] . discrimination, we employ a realistic (at least at the level of theory forecasting) approach to the reconstruction and isolation of secondary vertices. A key question is the extent to which future Higgs factories can improve on the LHC sensitivity to Higgs decays to LLPs, insofar as the number of Higgs bosons produced at the LHC will outstrip that of proposed Higgs factories by more than two orders of magnitude. Higgs decays to LLPs are sufficiently exotic that appropriate trigger and analysis strategies at the LHC should compensate for the higher background rate and messier detector environment. As we will see, there are two natural avenues for improved sensitivity at future lepton colliders: improved vertex resolution potentially increases sensitivity to LLPs with relatively short lifetimes, while lower backgrounds and a cleaner detector environment improves sensitivity to Higgs decays into lighter LLPs whose decay products are collimated. This chapter is organized as follows: In Section 2 we present a simplified signal model for Higgs decays into pairs of long-lived particles, which in turn travel a macroscopic distance before decaying to quark pairs. We further detail the components of our simulation pipeline and lay out an analysis strategy aimed at eliminating the majority of Standard Model backgrounds. In Section 3 we translate this analysis strategy into the sensitivity of future lepton colliders to long-lived particles produced in Higgs decays as a function of the exotic Higgs branching ratio and the mass and decay length of the LLP. While these forecasts are generally applicable to any model giving rise to the signal topology, we additionally interpret the forecasts in terms of the parameter space of several motivated models in Section 4. We summarize our conclusions and highlight avenues for future development in Section 5. Our aim here is to be representative, rather than comprehensive, as each production and decay mode for a long-lived particle is likely to require a dedicated search strategy. For the purposes of this study, we adopt a simplified signal model in which the Higgs decays to a pair of long-lived scalar particles X of mass m X , which each decay in turn to pairs of quarks at an average "proper decay length" cτ . 34 Both the mass m X and proper decay length cτ are treated as free parameters, though they may be related in models that give rise to this topology. For the sake of definiteness, for m X > 10 GeV we take a branching ratio of 0.8 to bb and equal branching ratios of 0.05 to each of uū, dd, ss, cc, though the precise flavor composition is not instrumental to our analysis. For m X ≤ 10 GeV we take equal branching ratios into each of the lighter quarks. We further restrict our attention to Higgs factories operating near the peak of the e + e − → hZ cross section, for which the dominant production process will be e + e − → hZ followed by h → XX. The associated Z boson provides an additional invaluable handle for background discrimination. Here we develop the conservative approach of focusing on leptonic decays of the Z, though added sensitivity may be obtained by incorporating hadronic decays. Given the signal, there are a variety of possible analysis strategies sensitive to Higgs decays to long-lived particles, exploiting various parts of a general-purpose detector. Tracker-based searches are optimal for decay lengths below one meter, with sensitivity to shorter LLP decay lengths all the way down to the tracker resolution. Timing information using timing layers between the tracker and electromagnetic calorimeter offers optimal coverage for slightly longer decay lengths, while searches for isolated energy deposition in the electromagnetic calorimeter, hadronic calorimeter, and muon chambers provides sensitivity to decay lengths on the order of meters to tens of meters. In principle, instrumenting the exterior of a general-purpose detector with large volumes of scintillator may lend additional sensitivity to even longer lifetimes. In this work we will focus on tracker-based searches at future lepton colliders, as these may be simulated relatively faithfully and ultimately are among the searches likely to achieve zero background while retaining high signal efficiency. We define our signal model in FeynRules [427] and generate the signal e + e − → hZ → XX + ¯ at √ s = 240 GeV using MadGraph 5 [426] . Where appropriate, we will also discuss prospects for Higgs factories operating at √ s = 250 GeV (potentially with polarized beams) such as the ILC by rescaling rates with the appropriate leading-order cross section ratios. In order to correctly simulate displaced secondary vertices, the decay of the LLP X and all unstable Standard Model particles is then performed in Pythia 8 [465] . In addition to the signal, we consider some of the leading backgrounds to our signal process and develop selection cuts aimed at achieving a zero-background signal region. Correctly emulating the detector response to LLPs using publicly-available fast simulation tools is notoriously challenging. In particular, we have found that the default clustering algorithms in the detector simulator Delphes [466] tends to cluster calorimeter hits from different secondary vertices into the same jets, significantly complicating the realistic reconstruction of secondary vertices. As such, we develop an analysis strategy using only ingredients from the Pythia output, although we do further run events through Delphes and utilize ROOT [467] for analysis. We implement two distinct tracker-based analyses with complementary signal parameter space coverage, which we denote as the 'large mass' and 'long lifetime' pipelines. We shall eventually see that the former will be effective for m X 10 GeV down to proper decay lengths cτ 1µm, while the latter is able to push down in m X by a factor of a few though is only fully effective for cτ 1 cm. Full cut tables for both irreducible backgrounds and a variety of representative signal parameter points appear in Tables 6.1 and 6.2, respectively. We next identify candidate secondary vertices using a depth-first 'clustering' algorithm, which roughly emulates that performed in the CMS search [468] . We perform this clustering using all particles in the event because at later points in the analysis we need this truth-level assignment of neutral particles to clusters, but we expect that this (admittedly unrealistic) inclusion does not significantly modify the performance of this algorithm. Beginning with a single particle as the 'seed' particle for our algorithm, we look through all other particles in the event and create a 'cluster' of particles consisting of the seed particle and any others whose origins are within cluster = 7 µm (the projected tracker resolution of CEPC [440] ) of the seed particle. We then add to that cluster any particles whose origins are within cluster of any origins of particles in the cluster, and do this step iteratively until no further particles are added to the cluster. We then choose a new seed particle which has not yet been assigned to a cluster and begin this clustering process again. We repeat this process until all particles in the event have been assigned to clusters. We assign to each cluster a location d cluster which is the average of the origins of all charged particles in the cluster. To ensure that our events contain displaced vertices, we impose a minimum bound on the displacement from the interaction point | d cluster | > d min , and clusters satisfying this requirement constitute candidate secondary vertices. For our 'large mass' analysis we set d min to be the impact parameter resolution ( 5 µm for both CEPC and FCC-ee [440] ), and so retain sensitivity to very short X lifetimes. For our 'long lifetime' analysis we set d min = 3 cm, which removes the vast majority of clusters coming from B hadron decays in background events, as seen in we are limited to Pythia objects, but to mock up the (small) penalty to signal of such a selection we implement a selection on the total invariant mass of the clusters M 2 cluster ≡ ( i∈ cluster p µ i ) 2 . Since this is truth-level information, to turn it into an analog 246 for the dijet invariant mass we apply a Gaussian smearing with a standard deviation of 10 GeV to account for the dijet resolution. We then select only candidate secondary vertices with M cluster < m h /2. This has no effect on background in our simulation pipeline as the background candidate secondary vertices are the result of hadronic decays, so the invariant masses of these clusters are not analogs for dijet invariant masses. As emphasized above, the imposition of this cut is strictly to account for possible selections that might appear in a more realistic experimental analysis. While the total invariant mass of the clusters is not an experimental observable, the invariant mass of charged particles in the clusters M charged = ( i charged p µ i ) 2 is experimentally accessible. For our 'large mass' analysis we select candidate secondary vertices with M charged > 6 GeV, which gets rid of nearly all clusters from hadronic decays, as seen in Table 6 .1. For the 'long lifetime' analysis, while the increased displacement requirement removes b hadrons it still allows c, s hadrons, and so we select M charged > 2 GeV to address this, which Table 6 .2 shows is again very effective. Next we select the cluster closest to the beamline which passes the above selection requirements as our secondary vertex for the event. Choosing the closest one preferentially selects X decay clusters over hadronic decay clusters in the jets to which the X decays, though this can be fooled by a non-zero fraction of 'back-flowing' quarks in X decays (quarks with momenta pointing toward the beamline). To remove displaced vertices coming from the decays of charged b hadrons we implement a 'pointer track' cut in both analyses as follows. For the cluster selected as the secondary vertex, we consider a sphere of radius r = 0.5 mm around the position d cluster . We look for any charged particles whose origins are outside this sphere and whose momenta (at the point at which they were created) point into it, and veto the event if there are any such particles. The main effect of this cut is to remove clusters which were produced from the decay of a charged hadron. The sphere size has been chosen to maxi-mize this effect, though this allows a small effect on signal due to geometric coincidence. Since this cut is only on charged particles, roughly ∼ 30% of background clusters are unaffected. For this cut we ignore the effect of the magnetic field in the tracker, which should not highly impact the trajectories on short scales. For the 'long lifetime' analysis we further implement an 'isolation' cut to remove neutral hadronic background decays. Given the cluster selected as the secondary vertex, we consider the plane perpendicular to the sum of momenta of charged particles in the cluster which passes through d cluster . We project the paths of prompt (vertex within 3 µm of the primary vertex, the planned CEPC vertex resolution [440] ) charged particles onto this plane (again ignoring the magnetic field) and veto the event if any come within R = 10 cm of the position of the secondary vertex. This radius was chosen to maximally reduce background, and does have a deleterious effect on short decay lengths 10 mm, as can be seen in Table 6 .2. This cut is not perfectly effective at rejecting background due to the non-negligible presence of jets whose prompt components have neutral fraction 1. To confirm that our analysis pipelines put us in the zero-background regime we run both the 'long lifetime' and the 'large mass' analyses on 500k e + e − → Z(bb)Z( ¯ ) events and 100k e + e − → h(bb)Z( ¯ ) background events. For both pipelines we find that zero e + e − → Z(bb)Z( ¯ ) events remain, while for e + e − → h(bb)Z( ¯ ) we find efficiencies of 5 × 10 −5 and 1 × 10 −5 respectively. We then run each analysis on 5k signal events to get acceptance × efficiencies for each (m X , cτ ) point, for a selection of points with m X = 2.5 from GeV to 50 GeV and cτ from 1 µm to 50 m. In Table 6.2 we give a cut table for both backgrounds and some representative signal parameter points for the 'long lifetime' analysis, and in Table 6 .1 we do the same for the 'large mass' analysis. In the zero-background regime, Poisson statistics rules out model points which predict 3 or more signal events to 95% confidence (or better) if no signal is detected. We may then find a projected 95% upper limit on branching ratio as with N sig = 3 and A × ε the result of our simulations. For both the CEPC and FCC-ee, the most recent integrated luminosity projections [440, 469] give L × σ(e + e − → hZ) = 1.1 × 10 6 Higgses produced. In Figure 6 .2 we show projected 95% upper limits on Br(h → XX) as a function of X mass and proper decay length. While we plot separate lines for both CEPC and FCC-ee, we only use one set of signal events generated at √ s = 240 GeV and only account for the difference in tracker radii, so these overlap entirely at smaller lifetimes. Approximate limits for the ILC can be obtained by multiplying the above branching ratio limits by a factor of ∼ 1.8 (i.e. weakening the limit) to account for the leading order differences in center-of-mass energy, polarization, and integrated luminosity at the √ s = 250 GeV ILC run, assuming comparable acceptance and efficiency. The ILC limits weaken slightly further for large decay lengths, as its proposed tracker radius is 1.25 m. Of course, adding the higher-energy ILC runs should significantly improve sensitivity given analyses suitable for the W W fusion production relevant at those energies. For small masses we are only able to use the 'long lifetime' analysis, which requires large displacement from the beamline to cut out the SM b hadron background. As a result we only retain good sensitivity to X decay lengths comparable with the tracker size, though the fact that we only require one displaced vertex (out of two Xs per signal event) significantly broadens our sensitivity range. This fact also helps us retain efficiency at low masses, as we are able to get down to a projected branching ratio limit of 1 × 10 −4 for m X = 2.5 GeV despite our 2 GeV cut on charged invariant mass of the decay cluster. For larger masses this cut has less effect, which allows it to push down to even lower branching ratios ∼ 5 × 10 −5 . The 'large mass' analysis begins working well for masses not far above the 6 GeV charged invariant mass cut and provides sensitivity to far shorter decay lengths, reaching all the way down to the impact parameter resolution and below. For m X = 10 GeV, where we are aided by the boost factor, we project a limit of 1 × 10 −4 for a proper decay length of 1 micron. The sensitivity to extremely small decay lengths drops for larger masses, but at m X = 50 GeV we cross below the 10 −4 threshold by 7.5 µm. For X masses high enough that the charged invariant mass cut does not remove a large amount of signal events, this analysis projects a branching ratio limit of ∼ 5×10 −5 across roughly the entire range of decay lengths corresponding to the geometric volume of the detector. There is a slight dip in sensitivity for cτ ∼ 1 mm, where the pair of dijets from the two X decays are most likely to overlap and trigger the cut on 'pointer' tracks. The notable region of this parameter space to which our analyses do not provide good sensitivity is the low mass (m X 6 GeV) and short proper decay length (cτ 1 cm) regime. The difficulty is that, from the perspective of the tracker, the X here looks more and more like a neutral SM hadron. An analysis making use of the impact parameter distribution of particles in clusters may help here [468] , but we leave this to future work. Taking advantage of calorimeter data to distinguish between clusters in single jets versus dijets is also likely to provide good sensitivity, but we again leave this to future exploration. Broadly speaking, our results suggest a peak sensitivity of Br(h → XX) ∼ 5 × 10 −5 , weakening to ∼ 10 −4 for lower-mass LLPs. Significant additional improvement could be expected with the inclusion of hadronic Z decays, but this requires further study to ensure the control of corresponding Standard Model backgrounds. These limits are competitive with LHC forecasts based on conventional Higgs triggers [353, 352] , noting that these latter forecasts assume zero background. However, the lepton collider limits are potentially superseded by an efficient CMS track trigger [459, 460] for higher-mass LLPs, again assuming zero background is achievable with high signal efficiency across a range of lifetimes. In this respect, the primary strengths of the Higgs factories in searching for exotic Higgs decays to LLPs are the potential to push down to shorter decay lengths and lighter LLPs. In particular, the relatively clean and low-background environment of lepton colliders should enable efficient LLP searches even when the LLP decay products become collimated, which remains a weakness of the corresponding LHC searches.  While the bounds presented in the previous section apply to any scenario in which the Higgs decays into pairs of long-lived particles which in turn decay (at least in part) into pairs of quarks, it is also useful to interpret these bounds in the context of specific models  As a general proxy model for Higgs decays into LLPs, we first consider the archetypal Higgs portal Hidden Valley [450] . This entails the extension of the Standard Model by an additional real singlet scalar φ, which couples to the Standard Model through the Higgs portal [470, 471, 472] via If φ respects a Z 2 symmetry under which φ → −φ, this additionally sets µ = A = 0, such that the singlet scalar only couples to the Standard Model via the quartic interaction |H| 2 φ 2 . After electroweak symmetry breaking, in unitary gauge H = 0, 1 √ 2 (h + v) , but the CP-even scalars h and φ do not mix. Nonetheless, the quartic interaction nonetheless provides a significant portal for the production of φ, as φ may be pair produced via the decay h → φφ for m φ < m h /2. Of course, φ is stable if the Z 2 symmetry is exact, rendering it a potential (albeit highly constrained) dark matter candidate [473, 474, 475] . This model gives rise to long-lived particle signatures [450] if the Z 2 is broken by a small amount, such that A = 0 but e.g. A 2 /M 2 κ. The relative smallness of A is technically natural, as the Z 2 symmetry is restored when A → 0. This then leads to mass mixing between the CP even scalars. As long as A is small compared to M and v, the mass eigenstates consist of an SM-like Higgs h SM and a mostly-singlet scalar s, related to the gauge eigenstates by where θ 1 is the mixing angle. There are now two parametrically distinct processes: pair production of the scalar s via Higgs decays, governed by the size of the Z 2 -preserving coupling κ, and decay of the s scalar back to the Standard Model, governed by the size of the Z 2 -breaking coupling A. In the limit of small mixing, the former process is of order where we are neglecting subleading corrections proportional to λ H sin 2 θ. The latter process proceeds into whatever Standard Model states Y are kinematically available, with partial widths LLPs (with the scalar s playing the role of the LLP), but also by precision Higgs coupling measurements. Higgs coupling deviations in this scenario arise from two parametrically distinct effects: tree-level deviations proportional to θ 2 due to Higgs-singlet mixing, and one-loop deviations proportional to κ due to s loops. Both effects result in a universal modification of Higgs couplings, which is best constrained at lepton colliders via the precision measurement of the e + e − → hZ cross section [476, 477] . The net deviation in the e + e − → hZ cross section due to these effects in the limit of small mixing is where the radiative correction [477] dM hh dp 2 s is approximated at θ = 0. Either effect can dominate depending on the relative size of A/M and κ. Constraints from a direct search for Higgs decays to LLPs and precision Higgs measurements as a function of the underlying parameters θ and κ are shown in Figure 6 .3 for the illustrative benchmarks m s = 2.5, 10, and 50 GeV. Unsurprisingly, in the regime where s is long-lived, the bounds from precision Higgs coupling measurements are modest and direct searches provide the leading sensitivity. Higgs decays to LLPs are also motivated by naturalness considerations, arising frequently in models of neutral naturalness that address the hierarchy problem with SM-neutral degrees of freedom [288, 298] . In these models, partially or entirely SM-neutral partner These decays occur on length scales ranging from microns to kilometers, making them a motivated target for LLP searches at colliders [289, 352] . For simplicity, here we will restrict our focus to scenarios with the sharpest predictions for the Higgs branching ratio to LLPs. In these cases, the LLPs in question are typically glueballs of the additional QCD-like sector, of which the J P C = 0 ++ is typically the lightest. The coupling of the SM-like Higgs to these LLPs is predominantly due to top partner loops, for which the scales and couplings are directly related to the naturalness of the parameter space. In the Fraternal Twin Higgs [289] , to the branching ratio h → γγ. Finally, in the Hyperbolic Higgs [303] (see also [304] ), the scalar top partners are entirely SM-neutral, and induce couplings to 0 ++ glueballs that are generically the lightest states in the hyperbolic QCD spectrum. As with the Fraternal Twin Higgs, however, there are also tree-level Higgs coupling deviations due to mass mixing among CP-even neutral scalars. In each of these scenarios, the branching ratio of the SM-like Higgs can be parameterized as follows: Here α s denotes the coupling of the additional QCD-like sector (whether twin, folded, or hyperbolic), which is necessarily of the same order as the SM QCD coupling α s , and m 0 is the mass of the glueball, which is determined in terms of the QCD-like confinement scale. Adopting the schematic notation of [352] , the parameter y 2 M 2 encodes the modeldependence of the Higgs coupling to pairs of gluons in the QCD-like sector, with In each case, the 0 ++ glueballs of the additional QCD-like sector decay back to the Standard Model by mixing with the SM-like Higgs, with a partial width to pairs of SM particles Y given by  Γ(0 ++ → Y Y ) = 1 12π 2 y 2 M 2 v m 2 h − m 2 0 2 4πα B s F S 0 ++ 2 ×Γ(h SM [m 0 ] → Y Y ),(6. The exploration of exotic Higgs decays is an integral part of the physics motivation for future lepton colliders. New states produced in these exotic Higgs decays may themselves decay on a variety of length scales, necessitating a range of search strategies. While considerable attention has been devoted to the reach of future lepton colliders for promptly-decaying states produced in exotic Higgs decays, the reach for long-lived particles is relatively unexplored. In this paper we have made a first attempt to study the reach of proposed circular More broadly, it is an ideal time to study the potential sensitivity of future Higgs factories to long-lived particles, as the results are likely to inform the design of detectors for these proposed colliders. This is a necessary step in motivating the physics case of future Higgs factories and ensuring that they enjoy optimal coverage of possible physics beyond the Standard Model. At its heart, the electroweak hierarchy problem is a question of how an infrared (IR) scale can emerge from an ultraviolet (UV) scale without fine-tuning of UV parameters. Given the sensitivity of the Standard Model Higgs mass to UV scales, the expectation of effective field theory (EFT) is that the two should coincide. Conventional solutions to the hierarchy problem introduce both symmetries that control UV contributions to the Higgs potential and dynamics that generate IR contributions, leading to considerable structure at the weak scale and correspondingly sharp experimental tests. Ongoing exploration of the weak scale has given no evidence for these solutions, despite their theoretical soundness. In the face of increasingly powerful LHC data in excellent agreement with the Standard Model, it's worth taking seriously the possibility that Nature may be leading us to the conclusion that there is no new physics at the weak scale. While this is often taken to suggest the existence of considerable fine-tuning in the Higgs potential, here we pursue an alternative idea. Perhaps the apparent violation of EFT expectations at the weak scale is a sign of the breakdown of EFT itself. We'll use the broad term 'UV/IR mixing' to denote any effects that the UV has on low-energy physics which goes past that expected in EFT. In this work we pursue the idea that such UV/IR mixing may have more direct effects on the SM by considering noncommutative field theory (NCFT) as a toy model. These theories model physics on spaces where translations do not commute [335, 336] , and have many features amenable to a quantum gravitational interpretation-indeed, noncommutative geometries have been found arising in various limits of string theory [337, 338, 339, 340] . 35 35 Noncommutative branes arising in gauge theory matrix models have also been found to contain This noncommutativity bears out the general expectation that the general-relativistic notion of spacetime should break down in a theory of quantum gravity [490] . Its realization here leads directly both to UV/IR mixing in the form of a violation of decoupling and to nonlocal effects in interactions. This gives rise to many interesting effects, but particularly fascinating for our purposes is that UV divergences present in the S-matrix elements of QFTs on commutative spaces can be transmogrified into new infrared poles in the corresponding field theory on noncommutative space [491] . An effective field theorist living in a noncommutative space would have no way to understand the appearance of this infrared scale; its existence is intrinsically linked to the geometry of spacetime and to the far UV of the theory. Such an effective field theorist would see a surprising lack of new physics accompanying this pole to explain its presence. It is clear from the outset that the direct application of NCFT to understand the hierarchy problem is immediately hindered by the Lorentz invariance violation which is inherent to these theories. Precisely how fatal this might be is not entirely clear; results regarding the extent to which 'generic' Lorentz violation is empirically ruled out [492] are partly circumvented here by the fact that the Lorentz violation is not generic, but comes as part of some larger structure. In this case the novel effects of UV/IR mixing in fact only appear in nonplanar loop diagrams [493] and care is required when interpreting EFT constraints on Lorentz violation-a point we will emphasize in Section 7.2. Even so, it is difficult to imagine that observed properties of the weak scale and the wide range of constraints on Lorentz violation leave room for NCFT to be directly relevant to puzzles of the Standard Model. Thus we make no claim about having solved the hierarchy problem. The value of this work is in the exploration of this toy model of UV/IR mixing, which possesses emergent gravitational effects, and so have been suggested as novel quantum theories of gravity [479, 480, 481, 482, 483, 484, 485, 486, 487, 488] . We do not pursue this perspective here, but refer the reader to [489] for a review of this approach. the intriguing feature that ultraviolet dynamics generate a scale whose lightness would be baffling to an effective field theorist. As this is the only model (of which we are aware) with this feature-and this feature, at the level of words, increasingly matches the experimental situation with the Higgs-it's worth understanding its appearance in as much detail as possible. To make this work self-contained for the contemporary particle theorist, we begin with an extensive introduction. In Section 7.2, we review quantum field theory on noncommutative spaces with an emphasis on the violation of EFT expectations. In Section 7.3 we use this technology to go over the classic result of [491] which first identified this emergent infrared pole in a Euclidean φ 4 theory. We compute also the effect in dimensional regularization to evince the regularization-independence of the UV/IR mixing effects. In Section 7.4 we ask how general the effect of UV/IR mixing is within NCFT, which leads us to study noncommutative Yukawa theory in detail. We find that the scalar propagator again develops a new infrared pole at one loop, in contrast with previous work. Intriguingly, the pole in this case is accessible in s-channel scattering in the Lorentzian theory, making Yukawa theory a promising setting for probing phenomenological consequences of UV/IR mixing. In Section 7.5 we upgrade our model to the softly-broken Wess-Zumino model to study the interplay between UV-finiteness and UV/IR mixing effects. When the fermion is kept in the spectrum of the theory below the cutoff, the lack of UV sensitivity of the field theory removes the light pole. As the fermion is taken above the cutoff, an effective theorist again sees effects past those observed in Wilsonian EFT. These results are expected, but this model affords us a concrete demonstration that UV/IR mixing can only have interesting low-energy effects if the field theory is UV sensitive, and puts this naturalness strategy in stark contrast to conventional approaches. Of course, this also makes addressing the hierarchy problem with UV/IR mixing a potentially Pyrrhic victory: to generate an IR scale, the field theory alone cannot be fully predictive. Finally, in Section 7.6 we examine the appearance of the emergent light pole in NCFT from more general arguments, so as to ascertain the relative importance of nonlocality and Lorentz-violation for these effects. The conclusion is inevitably that in this case the two are inexorably linked, and no strong conclusion about the possibility of finding a light pole in a theory with only one or the other is available. However, we provide some direction toward future explorations into both of these possibilities. We wrap up in Section 7.7. In this section we review the salient features of the formulation of noncommutative field theories and the standard formalism for studying their perturbative physics. Useful general references for this background include [494, 495] . Readers familiar with NCFT may wish to skip to Section 7.3, but we emphasize that our interest is necessarily nonperturbative in the parameter controlling the noncommutativity, unlike much of the earlier phenomenological literature. Physics on noncommutative spaces involves the introduction of a nonzero commutator between position operators where we will refer to θ µν = −θ νµ as the noncommutativity tensor, and we emphasize that it is covariant under Lorentz transformations. So while it does break Lorentz invariance, it only does so in the way that turning on a magnetic field in your lab chooses a preferred frame, and it can indeed be thought of as simply a background field. This basic definition is reminiscent of the introduction of a nonzero commutator in passing from classical mechanics to quantum mechanics. Indeed much of the structure is precisely analogous, including importantly the construction of noncommutative versions of familiar commutative theories via a quantization map. At an even more basic level, the above nonzero commutator induces an uncertainty relation which immediately makes apparent the presence of UV/IR mixing in this theory. If you attempt to create a wavepacket which is very small in one direction it will necessarily be elongated in another, and so we see already the non-trivial mixing of UV and IR modes. This clearly violates the separation of scales which is baked in to EFT. Thus purely from the defining relation of noncommutative geometry, we see already an indication that noncommutative theories should violate EFT expectations. Field theories on this space may be conveniently formulated in terms of fields that are functions of commuting coordinates imbued with a new field product, known as a Groenewold-Moyal product (or star-product), with position-space representation We derive this procedure in Appendix B. It is important to observe that this is a nonlocal product, since it contains an infinite series of derivative operators. So we see again that one of the tenets of EFT has been violated by our basic definition of field theory on noncommutative spaces. With this in hand we may now write down noncommutative versions of familiar theories in terms of commuting coordinates, which will then allow us to use normal QFT methods to analyze them. First note that this noncommutative quantization will not affect the quadratic part of the tree-level action due to momentum conservation and the antisymmetry of the noncommutativity tensor. For the interacting part of the action the effects of noncommutative quantization are not so trivial, but are easy to analyze classically. As an example, for a simple φ n theory we find Note, importantly, that the star-product has endowed our vertices with a notion of ordering, as it is only cyclically invariant. If we now Fourier transform the action to momentum space, we find that we can account for the effects of quantization on the tree-level action with a simple modification of the momentum-space vertex factor: V (k 1 , . . . , k n ) = δ (k 1 + · · · + k n ) exp i 2 n i<j k µ i k ν j θ µν . (7.5) A word of caution is in order. We can now express the action in momentum space as and so-as good effective field theorists-we may be tempted to expand the exponential for small momenta ∼ |k 2 | |θ| 1. Indeed, doing so would give us a series of irrelevant operators which would correct the leading interaction. However, once the theory is truncated at some finite order in θ, we are left with a perfectly local EFT. In other scenarios where an infinite series of operators appears, this is a valid approximation procedure and allows one to calculate the leading corrections a theory predicts. But here our definition of NCFT introduces UV/IR mixing which we expect to violate EFT expectations. Truncating the series removes these effects entirely, and a theory so defined no longer has anything to do with NCFT-at least not in the effects we will be interested in, which are nonperturbative in θ as we shall see explicitly in the following sections. There has been much work expended on these 'noncommutative-inspired' theories, but they do not contain UV/IR mixing, and do not capture the most striking and most interesting features of physics on a noncommutative space, from our perspective. 36 With that in mind, we may now proceed to do perturbative quantum field theory calculations, but we must worry about keeping track of all the phases from each of the vertices. In fact there is another simplification that occurs, as found by Filk [493] , which allows us to simplify the process of finding the phase factor for a diagram to a graphtopological statement. Filk proved two simple rules for the phase factors: 1. An internal line which ends on two different vertices can be contracted while keeping the ordering of the other lines fixed. V (k 1 , . . . , k n 1 , p)Ṽ (−p, k n 1 +1 , . . . , k n 2 ) =Ṽ (k 1 , . . . , k n 2 ) δ(k 1 + · · · + k n 1 + p) (7.7) 2. A loop which doesn't cross any lines can be eliminated. Note that the fixed ordering of the lines at a vertex means that we can now meaningfully speak of lines which do or don't cross each other. V (k 1 , . . . , k n 1 , p, k n 1 +1 , . . . , k n 2 , −p) =Ṽ (k 1 , . . . , k n 1 , k n 1 +1 , . . . , k n 2 ) if The proof of these facts relies only on the antisymmetry of θ µν and the fact that each vertex contains a momentum-conserving delta function. We may make use of this to simply find the phase factor of any Feynman diagram. Using the first rule, we can reduce any diagram to a single vertex, which is a rosette of the external lines and closed loops. The second rule allows us to eliminate loops which don't cross other lines. If the graph was planar (including, importantly, any tree-level graph), then by definition all loops can be eliminated. So all contributions to phase factors from internal lines cancel, and we're only left with an overall phase corresponding to the ordering of the external lines, which has remained fixed throughout the reduction process. For a nonplanar graph, in this representation it is easy to see that we only pick up phase factors from lines which cross. The loop gives vertex legs with ±p µ , and for an external line which doesn't cross this loop, both loop legs will be on the same side of it in the cyclic ordering, and so the two terms will cancel in the sum. Only for an external line which crosses it are the ±p on different sides, and so the antisymmetry of θ will make the two negative signs cancel to give a coherent phase for this vertex. Thus we define I ij , the intersection matrix of an oriented graph: Then for any graph G, the contribution Γ(G) of the phase factors is just In what follows we will omit the overall external phase when evaluating diagrams, as it will not be important for our purposes. We have now simplified perturbative field theory on noncommutative spaces down to the simple task of marking line-crossings, at least at the level of writing down integrands of amplitudes. The triviality of this task for tree-level graphs leads to the interesting feature that tree-level amplitudes on noncommutative spaces are the same as on commutative manifolds, and it is only at looplevel that we find deviations. We will see in the next section that the loop integration will bring surprising features. An important issue for the interpretation of NCFTs is that of their unitarity. There is no problem in Euclidean space, but for Lorentzian spacetimes with noncommutativity in the time directions ('timelike' or 'space-time' noncommutativity when −k µ θ µρ θ ρν k ν ≡ k • k < 0 is allowed), one may find a breakdown of unitarity by taking cuts of one-loop diagrams [499, 500] . 37 This may be interpreted physically as being due to the production of tachyonic states, which if added to the Fock space of the theory result in a formal restoration of the cutting relations whilst making the nonunitarity explicit [503] . This failure of unitarity is well-understood from the stringy perspective. Spatial noncommutativity appears from a background magnetic field and the field theory limit to a spacelike NCFT is smooth [339] . In the case of timelike noncommutativity, however, approaching the field theory limit forces an electric field to supercritical values whence pair-production of charged strings destabilizes the vacuum [504] . Within the realm of field theory, there have long been suggestions that this difficulty is pointing to the need for a modified definition of quantum field theories on timelike noncommutative spaces (for some early references, see [520, 521, 522, 523, 524, 525, 526, 527] ). From this perspective, the issue is that such field theories are non-local in time, which renders nonsensical the normal time-ordering involved in the perturbative Dyson series (at the least). That is, our effective definition of these theories above via the diagrammatic expansion may be too naïve. An interesting line of work is to formulate a modification of the standard quantum field theory machinery to non-local-in-time theories which avoids the unitarity issue by construction. We note that the same UV/IR mixing effects of interest in the two-point function have been seen to persist in at least some of these approaches (e.g. [522] ). For some recent work on the formulation and properties of nonlocal field theories, see e.g. [528, 529, 530, 498, 531, 532] . Below we will begin in Euclidean space, where k • k ≥ 0 is guaranteed for any θ µν , but will then venture into Lorentzian signature. All of our calculations and the general features we find, including finding new infrared poles, will hold robustly in spacelike noncommutative theories. However we will comment also on how these features are modified when timelike noncommutativity is turned on, taking license from the aforementioned hints that unitary completions/reformulations of timelike NCFT may retain the UV/IR mixing exhibited in the naïve approach. In this section we review the perturbative physics of the noncommutative real scalar φ 4 theory at one loop, which was first studied in detail by Minwalla, Van Raamsdonk, and Seiberg in [491] . 38 In four Euclidean dimensions the action on noncommutative space becomes where we have already used the fact that the quadratic part of the noncommutative action is the same as the commutative theory to eliminate the star product there. Our object of interest will be the one-loop correction to the two-point function. In the commutative theory this is given by a single Feynman diagram, but the noncommutative theory contains both a planar diagram and a nonplanar diagram. The expressions for these two diagrams now differ-not only in symmetry factor but also due to the phase in the integrand. We find 1,nonplanar = g 2 6 (2π) 4 d 4 k k 2 + m 2 e ik µ θµν p ν . We may already see that something interesting should happen, as in the nonplanar diagram the phase mixes the internal and external momenta. One may intuit that the rapidly oscillating phase in the UV of the loop integration will dampen the would-be 38 Some early results in this model may also be found in [533, 534] . divergence, and indeed we will see that nonplanar diagrams are finite. However, unlike in the case where the vertex factor vanishes rapidly for large Euclidean momenta and so ensures UV-finiteness [532] , here the damping is in some sense 'marginal'. This fact will be responsible for the interesting feature we will find presently. The simplest method to evaluate noncommutative diagrams is to use Schwinger parameters, recalling the identity 1 . The presence of the phase in the nonplanar diagram means we must complete the square before going to spherical coordinates to get a Gaussian integral. This means that after the momentum integrals we end up with where again p • q = −p µ θ 2 µν q ν . Moving to Schwinger space trades large-k divergences for small-α divergences, which we now smoothly regulate by multiplying the integrands by exp (−1/(Λ 2 α)) so that the small α region will be driven to zero. Note that a term of this form already exists in the expression for the nonplanar diagram. After introducing the regulator, we can evaluate the integrals to find where we've defined which is the effective cutoff of the nonplanar diagram. The first thing to note is that it seems the UV divergence of the nonplanar diagram has disappeared-the graph is finite in the limit Λ → ∞, and so appears to have been regulated by the noncommutativity of spacetime. In fact the effect is more subtle, as alluded to earlier, and now the UV and IR limits of this amplitude do not commute. If we first take an infrared limit p • p → 0 we find that Λ eff → Λ and the ultraviolet divergence of the commutative theory reappears. If we take the UV limit Λ → ∞ first we find an IR divergence 1 p•p , so the noncommutativity has transmogrified the UV divergence into an IR one. 39 Turning to the question of renormalizability, one may naïvely ask if we can absorb all UV divergences into a finite number of counterterms. Under this criterion, it is clear that this procedure works in the noncommutative theory at least when the commutative version is renormalizable. In the current case, we may absorb the UV divergences of this correction to the two-point function into a redefinition of the physical mass, M 2 = m 2 + g 2 Λ 2 48π 2 − g 2 m 2 48π 2 log Λ 2 m 2 , and so write down a one-particle irreducible quadratic effective action which has a finite Λ → ∞ limit: However, in the Λ → ∞ limit one finds that at one loop the propagator now has two poles. The first is a standard radiative correction to the free pole, but the second has appeared ex nihilo at one loop: (7.17) 39 We note here that the failure of a 'correspondence principle' between commutative and noncommutative theories as θ µν → 0 is clearly intrinsically linked to the appearance of UV/IR mixing. This failure doesn't violate Kontsevich's proof of the existence of deformation quantization for any symplectic manifold [535] , as that is confined solely to 'formal' deformation quantization-that is, the production of a formal power series expansion of the algebra of observables in terms of the deformation parameter. As was noted in Section 7.2 and is now on prime display, the physics of the theory with nonperturbative θ-dependence is starkly different from that of any truncation. where we have assumed that θ µν is full rank. The former is to be interpreted as the on-shell propagation of the particles associated to our fundamental field φ. If θ µν has only one eigenvalue 1/Λ 2 θ -with Λ θ thought of as the scale associated with the breakdown of classical geometry-we have p • p = p 2 Λ 4 θ . We see that the new pole appears at p 2 ∝ g 2 Λ 4 θ m 2 , and so if our field φ lives in the deep UV of the theory, our new pole appears at parametrically low energy scales. To the extent that poles are particles, we appear to have generated a new light particle from ultraviolet dynamics. The interpretation of the new pole can be sharpened by considering more carefully the criteria for renormalizability in Wilsonian EFT. In a Wilsonian picture, we upgrade our Lagrangian parameters to running parameters, and define our theory at the scale Λ as It is immediately apparent from the above calculation that we cannot write the action at a lower scale Λ 0 < Λ in this same form by choosing appropriate definitions for Z(Λ), m(Λ), g(Λ)-there's nowhere to put the 1 p•p term! 40 Stated more precisely, for Wilsonian renormalizability we require that we can define the running couplings such that correlation functions computed from this action converge uniformly to their Λ → ∞ limits. However, this requirement is flatly violated by the noncommutation of the UV and IR limits of the diagrams. For any finite value of Λ, 40 There has been much work on understanding renormalizability of NCFTs, especially with an eye toward finding a mathematically well-defined four-dimensional quantum field theory with a non-trivial continuum limit. Renormalizability has been proven for modifications of NCFTs where the free action is supplemented by an additional term which adjusts its long-distance behavior. Such an action is manufactured either by requiring it manifest 'Langman-Szabo' duality [536] p µ ↔ 2(θ −1 ) µν x ν [537, 538] or by adding a 1/p•p term to the free Lagrangian [539] , the latter of which directly has the interpretation of adding 'somewhere to put the 1/p • p counterterm'. For recent reviews of these and related efforts we refer the reader to [540, 541] . It would be interesting to understand fully the extent to which the physics of these schemes agrees with the interpretation of the IR effects as coming from auxiliary fields [491, 515] . the effective action of Equation 7.16 differs significantly from its limiting value for small momenta p • p 1 Λ 2 . This is the precise sense in which the violation of Wilsonian EFT appears in this one-loop correction. This brings up the question of how an effective field theorist would describe the universe if they unknowingly lived on a noncommutative space. A consistent Wilsonian interpretation can be regained by including a degree of freedom which can absorb the new infrared dynamics of the quadratic effective action. Since we need this to involve the φ momentum, this new particle must mix linearly with the φ field. We manufacture its tree-level Lagrangian such that the problematic inverse p • p term in the quadratic effective action of φ is replaced with its Λ → ∞ value for all values of Λ, to satisfy our precise condition for Wilsonian renormalizability. To see how this works, we add to our tree level Wilsonian action Since χ appears quadratically, we may integrate it out exactly at tree level to find a contribution to the effective action This precisely subtracts off the problematic term in the original 1PI quadratic effective action and adds back its Λ → ∞ limit, as we had wanted. Ignoring the logarithmic term, 41 we are left with an effective action which is manifestly independent of the cutoff Λ, and so satisfies our criterion for Wilsonian renormalizability. 42 We discuss the generalization of this procedure in Appendix C. 41 Discussion of the interpretation of logarithmic singularities as being due to auxiliary fields propagating in extra dimensions may be found in [515] . 42 In Equation 7 .19, the four-derivative quadratic action of the auxiliary field can be rewritten as two fields with two-derivative actions, one of which is of negative norm and may be thought of as the 'Lee-Wick partner' of the positive norm state [542] , viz. Now while we have written down an action which identifies the new observed IR pole with a field and in doing so gives our effective action a Wilsonian interpretation, the extent to which χ can be taken seriously as a fundamental degree of freedom is unclear. 43 The new pole is inaccessible in Euclidean space-so one does not immediately conclude there is a tachyonic instability-and relatedly, when we naïvely analytically continue this result to Lorentzian spacetime this new pole is inaccessible in the s-channel. 44 However, its presence is still enough to break unitarity for this theory [499] , and in fact may still be interpreted as being due to the presence of tachyons [503] . As discussed in Section 7.2, it is possible this may be resolved if analytical continuation is adjusted for nonlocal-in-time theories, or it may be that a UV theory cures this apparent violation. Separately, it is not obvious much has been gained by attributing the new pole to a new, independent field, past acting as a formal tool to regain a notion of renormalizability. Since the only interaction of χ above is linear mixing, its action is not renormalized-any One may then wonder if the lightness of the new IR pole may be understood through the regularization performed by the Lee-Wick field, as is done for the Higgs in the 'Lee-Wick standard model' [543] . However, in that theory the Higgs is kept light because every particle comes with a Lee-Wick partner, and so all diagrams contributing to corrections to the Higgs mass are made finite. The presence of the Higgs' Lee-Wick partner alone is not enough to keep it light. Here, the lightness of χ can be understood diagrammatically as being simply due to the fact that its only interaction is linear mixing with φ, and so any correction to its two-point function is absorbed into that of the two-point function of φ. A further issue with the Lee-Wick rewriting is that the seeming perturbative unitary of the theory is normally guaranteed by the Lee-Wick partner being heavy and unstable. But as we take the Λ → ∞ limit in our Wilsonian action, we see that the Lee-Wick partner becomes massless as well, in accordance with the result that this theory is non-unitary [499] . 43 We note that in matrix models containing dynamical noncommutative geometries it has been argued that emergent infrared singularities should be associated with the dynamics of the geometry (see e.g. [544, 489] ). As our field theories are formulated on fixed noncommutative backgrounds, this interpretation is unavailable to us. 44 Note that this peculiar connection regarding (in)accessibility is due to the Lorentz violation. While the normal pole which is inaccessible in Euclidean signature becomes accessible for timelike momenta in Lorentzian signature, the Wick rotation affects the noncommutative momentum contraction differently. When taking x 4 → −ix 0 , one also rotates θ 4ν → −iθ 0ν such that Equation 7.1 continues to hold for the same numerical θ µν . For the simplest configuration of full-rank noncommutativity with θ µν block-off-diagonal and only one eigenvalue 1/Λ 2 θ , the Euclidean p • p = p 2 /Λ 4 θ becomes a Lorentzian p • p = (p 2 0 − p 2 1 + p 2 2 + p 2 3 )/Λ 4 θ . So a noncommutative pole which is inaccessible in the Euclidean theory becomes accessible in the Lorentzian theory for spacelike momenta, while a noncommutative pole which can be accessed in the Euclidean theory becomes accessible in the s-channel in Lorentzian signature. divergences are instead absorbed into the running of φ parameters-and so no interactions are generated. Furthermore one is obstructed from integrating out the heavy field φ to come up with an effective action of χ at low energies by the fact that the kinetic terms of χ are non-standard, which prevents diagonalization of the quadratic terms in the Lagrangian. Thus it seems it is intrinsically linked with the heavy scalar which begat it. There are further obstructions to asking that this specific mechanism be responsible for the lightness of an observed particle such as the Higgs. Prime among these is the modified dispersion relation of the new field, p • p = O(g 2 ), which means that the free propagation of this field would be Lorentz violating. 45 We will explore these issues further in the next sections, as in the Yukawa theory of Section 7.4 the new pole will appear with the opposite sign and so will offer the prospect of appearing as an s-channel pole. We emphasize that a new infrared scale whose lightness is unexplained in the context of Wilsonian effective field theory is an exciting feature that makes further exploration of UV/IR mixing an interesting pursuit. The fact that it here appears as the scale of a pole in a propagator makes the connection to the hierarchy problem captivating, but asking that this toy model-where Lorentz violation is at the fore-literally solve the problem for us would be too much. We proceed without further hindrance in exploring NCFT so as to learn more about the appearance and effects of UV/IR mixing here. A good question to ask is whether, or to what extent, these effects are an artifact of our choice of regularization. To demonstrate their physicality, we repeat the calculation of the one-loop correction to the two-point function now in dimensional regularization. We set up our integral in d = 4 − dimensions, having defined g 2 =g 2μ , and we again go to Schwinger space: 1,nonplanar =g 2μ 6 (2π) d d d k dα e −α(k 2 +m 2 )+ik µ θµν p ν . After completing the square in the nonplanar integral, the momentum integral and the Schwinger integral may then be performed analytically, with the results: If we expand the planar graph in the limit → 0, which should be thought of as probing the ultraviolet, we recover where in MS we would subtract off the pole and find the renormalization group evolution of m from the logarithmic term, as usual. The question of dimensional regularization for the nonplanar diagram is a subtle one [545] . If we first take the → 0 limit of Equation 7.23, we see this manifestly has no divergences, and we are simply left with the finite, 0 term  1,nonplanar = −g 2 m 2 6(4π) 2 2 + ln µ 2 m 2 , (7.27) and now we recover the UV divergence that was present in the commutative theory, so that once again we find the UV and IR limits don't commute. The key to understanding clearly this seemingly ambiguous dimensional regularization procedure is that while Γ We observed in our first example that the UV divergences of the real φ 4 commutative theory are transmogrified into infrared poles in the noncommutative theory. 46 It is natural to ask whether this "strong UV/IR duality" [546] is a common feature of all noncommutative theories. The answer is no, and the simplest counterexample is provided in the case of a complex scalar field with global U (1) symmetry and self-interaction [546] . In the quantization of the scalar potential we have two quartic terms which are noncommutatively-inequivalent due to the ordering non-invariance, so the general noncommutative potential is where λ 1 and λ 2 are now different couplings. By doodling some directed graphs, one sees simply that the one-loop correction to the scalar two-point function contains planar graphs with each of the λ 1 , λ 2 vertices, but the only nonplanar graph has a λ 2 vertex. There is thus no necessary connection of the ensuing nonplanar IR singularity to the UV divergence in the θ → 0 limit, as the coefficients are unrelated (and in particular, we are free to turn off the IR singularity at one loop by setting λ 2 = 0). Another important counterexample is that of charged scalars, the simplest example of which is noncommutative scalar QED, which was first constructed in [547] . There is a very rich and interesting structure of gauge theories on noncommutative spaces, a full discussion of which is far beyond the scope of this paper. We refer the reader 46 While we only presented the calculation of the one-loop correction to the two-point function, [491] goes through corrections to the two-and n-point functions for φ n with n = 3, 4 and finds the same features in all cases. to [548, 549, 550, 551, 552, 497, 553] for discussions of some features relevant to SM model-building. We here satisfy ourselves with the simplest case, for which we have the noncommutative Lagrangian 47 where even though we're quantizing U (1) we have to the noncommutativity, where [· * , ·] is the commutator in our noncommutative algebra. is an element of the noncommutative U (1) group, which consists of functions U (x) = e iθ(x) , which is the exponential constructed via power series with the star-product. The potential and the covariant derivative both depend on the representation we choose for the scalar. In contrast to commutative U (1) gauge theory, where we merely assign φ a charge, our only choices now are to put φ in either the fundamental or the adjoint of the gauge group. Note that an adjoint field smoothly becomes uncharged in the commutative limit. Such a field φ transforms as φ → U φ U † . The covariant The gauge-invariant potential then includes both quartic terms in Equation 7.28, in addition to others such as φ * φ φ φ, since the adjoint complex scalar is uncharged at the level of the global part of the gauge symmetry. Strong UV/IR duality then should not hold here either. The situation is even worse if φ is in the fundamental, where it transforms as φ → U φ It is easy to see in this case that the λ 2 interaction term is no longer gauge invariant, and a charged scalar may only self-interact through V = λ 1 φ * φ φ * φ. Purely from gauge invariance we thus see that a fundamental scalar has no nonplanar self-interaction diagrams in the one-loop 47 It is important to note that many fundamental concepts which one normally thinks of as depending upon Lorentz invariance still hold on noncommutative spaces, due to a 'twisted Poincaré symmetry' [554, 555, 556, 557] . This includes the unitary irreducible representations, so it is sensible to speak of a vector field. correction to its two-point function, and so there is no remnant of strong UV/IR duality to speak of. 48 The question is then whether there are other examples where this strong UV/IR duality does occur, or whether it is perhaps a peculiar feature of real φ n theories on noncommutative spaces. To answer this, we will study in detail another case of especial phenomenological significance: Yukawa theory. Noncommutative Yukawa theory was first studied in [560] . 49 Our result on the presence of strong UV/IR mixing differs, for reasons we will explain henceforth. For reasons that will soon become clear, we will now work directly in Minkowski space, and begin with a commutative theory of a real scalar ϕ and a Dirac fermion ψ with Yukawa interaction: When constructing a noncommutative version of this theory, the quadratic part of the action does not change. However, ordering ambiguities appear for the interaction term, and we in fact find two noncommutatively-inequivalent interaction terms which generically appear: These terms are inequivalent because the star product is only cyclically invariant. In 48 Noncommutative QED also has strange behavior in the gauge sector that runs counter to strong UV/IR duality-the photon self-energy correction gains an infrared singularity from nonplanar one-loop diagrams, even though the commutative quadratic power-counting divergence is forbidden by gaugeinvariance. The theory is constructed in detail in [558] , while more physical interpretation is given in [559] , and the possible relation to geometric dynamics in the context of matrix models is discussed in [544] . 49 Aspects of noncommutative Yukawa theory have also been studied recently in d=3 in [561] , and with a modified form of noncommuativity in [562] . the analysis of [560] , only the g 2 interaction was included. As a result, it was concluded that this theory contains no nonplanar diagrams at one loop, and the first appear at two loops as in Figure 7 .1. This immediately tells us that the one-loop quadratic divergence of the scalar self-energy will not appear with a one-loop IR singularity, and so rules out the putative strong UV/IR duality of the theory they studied. However, we must ask whether we actually have the freedom to choose g 1 and g 2 independently. To address that question, we must understand the role of discrete symmetries in noncommutative theories. For ease of reference we here repeat our definition of the noncommutativity parameter It is manifest that the noncommutativity tensor does not transform homogeneously under either parity or time-reversal, but only under their product: P T : x µ → −x µ ⇒ P T : θ µν → θ µν . So while any Lagrangian with full-rank noncommutativity unavoidably violates both P and T , it may preserve P T . Since both ϕ and the scalar fermion bilinear are invariant under all discrete symmetries, these symmetries naïvely play no further role in this theory. However, the time-reversal operator is anti-unitary, and thus negates the phase in the star-product: Armed with this, we may now apply CPT to our interaction Lagrangian, to find Comparing with Equation 7.31, we see that our interactions have been re-cycled! Requiring that our interactions preserve CPT amounts to imposing And so the theory of [560] appears to violate CPT. 50 When we instead include both orderings of interactions the nonplanar diagrams now occur at the first loop order. Furthermore, with both couplings set equal the planar and nonplanar diagrams will have the same coefficients, which reopens the question of strong UV/IR duality for this theory. In the following we will keep g 1 and g 2 distinguished merely to evince how the different vertices appear, but in drawing conclusions about the theory we will set them equal. 51 50 We note that while the CPT theorem has only been proven in NCFT without space-time noncommutativity [563, 564, 565, 566] , the difficulty in the general case is related to the issues with unitarity discussed in Section 7.2, and we expect it should hold in a sensible formulation of the space-time case as well. 51 We should note that in the construction of noncommutative QED it has been argued that it is sensible to assign θ the anomalous charge conjugation transformation C : θ µν → −θ µν ([567] and many others since). The argument is that charged particles in noncommutative space act in some senses like dipoles whose dipole moment is proportional to θ, and so charge conjugation should naturally reverse these dipole moments. Here, however, our particles are uncharged, and thus we have no basis for arguing in this manner. Furthermore, such an anomalous transformation makes charge conjugation relate theories on different noncommutative spaces M θ → M −θ . The heuristic picture of the CPT theorem (that is, the reason we care about CPT being a symmetry of our physical theories) is that after Wick rotating to Euclidean space, such a transformation belongs to the connected component of the Euclidean rotation group [568] , and so is effectively a symmetry of spacetime. So it is at the least not clear that defining a CPT transformation that takes one to a different space accords with the reason CPT should be satisfied in the first place. The 'symmetrization' of the momenta of the internal propagators is an important calculational simplification. This calculation is textbook save for our Schwinger-space regularization, so we will be brief and merely point out the salient features. The sum of these diagrams gives . To evaluate this, we must now introduce two Schwinger parameters α 1 , α 2 and then switch to 'lightcone Schwinger coordinates' which effects the change Each now has one g 1 vertex and one g 2 vertex, which makes it clear why the analysis of [560] found no such diagrams. The two diagrams will come with opposite phase factors, e ip∧k and e ik∧p , so we can compute one and then find the other by taking p → −p. In this case it's obvious that after completing the square we will only be left with terms which are quadratic in p, and so the two diagrams give the same contribution. We can thus compute both terms at the same time. The phase factor in the integrand will modify our change of variables, as it did in the φ 4 case, to give again an effective cutoff for this diagram due to the noncommutativity. We find Γ (2),s,np 1 (p) = g 1 g 2 π 2 dqdα 1 dα 2 q 3 M 2 − q 2 + α 1 α 2 (α 1 + α 2 ) 2 p 2 + p • p 4(α 1 + α 2 ) 2 × e −(α 1 +α 2 )(q 2 +M 2 )− α 1 α 2 α 1 +α 2 p 2 − p•p 4(α 1 +α 2 ) . (7.37) We can now follow the same steps to regulate and integrate this, and again find a closed-form expression for the pieces which contain divergences. Note that unlike the φ 4 calculation, we can already see that the nonplanar expression will not merely be given by Λ → Λ eff , as the change of variables has here modified the numerator of the integrand to give an extra piece to the momentum polynomial multiplying the exponential. And so integration gives us Γ (2),s,np 1 (p) = g 1 g 2 1920π 2 3 640M 2 + p 4 p • p + 40(4M 2 + p 2 )p • pΛ 2 eff K 0 4M 2 + p 2 Λ eff + 20 4M 2 + p 2 Λ eff −96 + p 2 p • p + 12p • pΛ 2 eff K 1 4M 2 + p 2 Λ eff . (7.38) We must now think slightly more carefully about what we want to add to the quadratic effective action to find a Wilsonian interpretation of this theory. We may isolate the IR divergence that appears when the cutoff is removed by first taking the limit Λ → ∞ with p • p held fixed, and then expanding around p • p = 0. We may then ask that this same divergence appears at any value of Λ. To account for this IR divergence, we must add to our effective action ∆S 1PI (Λ) = − 1 2 which can easily be done through the addition of an auxiliary scalar field as was done in Section 7.3 and is discussed in more generality in Appendix C. After having added this to our action, for small p•p the scalar two-point function now behaves as Γ s 1 (p) = − 2g 1 g 2 π 2 p•p +. . . for any value of Λ. The new pole in this case has the opposite sign as that in 7.19, and so will be accessible in Euclidean signature, clearly signaling a tachyonic instability. While this puts the violation of unitarity in this theory on prime display, it also means that this pole will be accessible in the s-channel in the Lorentzian theory if we allow for timelike noncommutativity. We emphasize that any conclusions about the Lorentzian theory with timelike noncommutativity are speculative and dependent upon a solid theoretical understanding of a unitary formulation of the field theory, and in principle such a formulation could find radically different IR effects than this naïve approach. However, it was found in [522] that a modification of time-ordering to explicitly make the theory unitary (at the expense of microcausality violation) leaves the one-loop correction to the self-energy unchanged in φ 4 theory, and the same might be expected to hold true for Yukawa theory. This makes it worthwhile to at least briefly consider the potential phenomenological consequences of the new pole. At low energies, the propagator is here modified to m 2 + (p i + p j ) 2 − 2g 1 g 2 π 2 1 (p i +p j )•(p i +p j ) . If we consider scattering of fermions through an s-channel ϕ and take the simple case of a noncommutativity tensor which in the lab frame has one eigenvalue 1/Λ 2 θ with m 2 Λ 2 θ , then the emergent pole appears at s = 2g 1 g 2 π 2 1−β 2 1+β 2 Λ 4 θ m 2 . Here s = −(p i + p j ) 2 is the invariant momentum routed through the propagator, and β is the boost of the (p i +p j ) system with respect to the lab frame. The Lorentz-violation here then has the novel effect of smearing out the resonance corresponding to the light pole for a particle which is produced at a variety of boosts. This is in contrast to the pole at m 2 , which gives a conventional resonance at leading order. Of course, we have not constructed a fully realistic theory in to M − / p α 2 α 1 +α 2 + 1 2 pµθ µν γν α 1 +α 2 , so the would-be divergence in pθ will cancel manifestly between the two diagrams. After this everything proceeds as before, and we find We see that with g 1 = g 2 ≡ g, the fermion quadratic effective action also behaves as expected from 'strong UV/IR duality'. The logarithmic divergence of the commutative theory has been transmogrified in the nonplanar diagrams into IR dynamics via the simple replacement Λ → Λ eff , and so a p • p → 0 pole will emerge when we remove the cutoff. We discuss the use of an auxiliary field to restore a Wilsonian interpretation here in Appendix C.2. The correction to the vertex function constitutes further theoretical data toward the Wilsonian interpretation of the noncommutative corrections. We calculate the one-loop correction in this section and delay the discussion of the use of auxiliary fields to account for them until Appendix C.3. We will find that while we can use the same fields to account for the modifications to both the propagators and the vertices, the physical interpretation of such fields is unclear. We can compute corrections for each fixed ordering of external lines separately since they're coming from different operators. For simplicity we'll compute the g 1 ordering, which we will denote Γ ϕψψ 3 (r, p, ). There are four diagrams in total: one planar diagram with two insertions of the g 2 vertex, one nonplanar diagram with two insertions of the g 1 vertex, and two nonplanar diagrams with one insertion of each. It is easy to see by looking at the diagrams that the same expressions with g 1 ↔ g 2 compute the correction to the other ordering, Γ ψϕψ 3 (r, p, l). The new feature of this computation is that we now need three Schwinger parameters, and this presents a problem for our previous computational approach. We won't be able to perform the two finite integrals before expanding in a variable which isolates the divergences when α 1 + α 2 + α 3 → 0, analogously to what we did in 2d Schwinger space. Instead we slice 3d Schwinger space such that we can perform the integral which isolates the leading divergences first, and then-as long as we're content only to understand this divergence-we can discard the rest without having to worry about performing the other two integrals. The planar diagram is (k + p 2 + 2 ) 2 + M 2 (k − p 2 − 2 ) 2 + M 2 (k + p 2 − 2 ) 2 + m 2 . After moving to Schwinger space, integrating over the loop momentum, and introducing a cutoff exp (−1/ (Λ 2 (α 1 + α 2 + α 3 ))), we switch variables to α 1 = ξ 1 η, α 2 = ξ 2 η, α 3 = (1 − ξ 1 − ξ 2 )η, (7.43) under which Performing the momentum integral transfers the divergence for large k to a divergence in small α 1 +α 2 +α 3 = η. This will allow us to find the leading divergent behavior immediately by carrying out the η integral and then expanding in Λ → ∞. This yields Γ ϕψψ 3,p (p, ) = g 1 g 2 2 16π 2 log Λ 2 + finite, (7.44) where we are unable to determine the IR cutoff of the logarithm, but this suffices for our purposes. We now turn our attention to the softly-broken noncommutative Wess-Zumino model as a controllable example of the interplay between UV/IR mixing and the finiteness of the field theory. We will restrict ourselves to calculating the one-loop correction to the scalar two-point function. Since the new poles appearing in the quadratic effective action in the scalar and Yukawa theories are intimately related to the quadratic divergences of the commutative theories, we will not be surprised to find that this feature will disappear when both the scalar and the fermion are present in the EFT below the cutoff. By studying the softly-broken theory we can take the fermion above or below the cutoff to smoothly see the relation between the finiteness of the field theory and the effects of UV/IR mixing. The exactly supersymmetric noncommutative Wess-Zumino model was first discussed in detail in [573] , and the absence of an infrared pole in a softly-broken theory was first noted in [559] . The softly-broken Wess-Zumino model was first considered in [496] . 52 The noncommutative Wess-Zumino theory can be suitably formulated in off-shell superspace as where Φ is a chiral superfield and we have included a wavefunction renormalization factor in the Kähler potential Z = 1 + O(y 2 ). We can introduce soft supersymmetry breaking by promoting this factor to a spurion Z = 1 + (|M | 2 − m 2 )θ 2 θ †2 , the only effect of which is to modify the scalar mass spectrum. 52 Our one-loop results agree with those of [496] save for their claim that logarithmic IR divergences are absent in the exactly supersymmetric theory, which contradicts [573] . We will below find a logarithmic IR divergence in the wavefunction renormalization which is independent of the soft-breaking, which is consistent with the expectations of strong UV/IR duality. Formulating the noncommutative theory including the auxiliary F fields makes it manifest that we have preserved supersymmetry off-shell. This procedure is in fact precisely the same as quantizing after integrating out F , and so we end up with a starproduct version of the familiar Lagrangian: where φ is a complex scalar and ψ is a Weyl fermion. Of course, now that we've introduced supersymmetry breaking we expect to find that there is further renormalization beyond that associated with Z, but keeping the manifest factors of Z will allow us to easily compare to our expectations for the supersymmetric limit. The calculation of the one-loop correction to the two-point function goes much as the previously-demonstrated examples. The presence of the three-scalar interaction gives a new class of diagrams, whose evaluation is routine. The two-component fermions yield slightly different factors than did the Dirac fermions [119] . Finally, it is important to note that the results for the diagrams computed in Section 7.3 cannot be used here, as we must here regulate uniformly using exp(−1/(Λ 2 (α 1 + α 2 ))) like we did in Section 7.4. This may be easily accommodated by writing the integrand in the quartic diagrams as 1 k 2 +m 2 k 2 +m 2 k 2 +m 2 . Adding up all these diagrams and taking the limit where Λ, Λ eff are large, we find that the one-loop scalar two-point function may be organized as Γ (2),s ≡ Zp 2 + Z −1 (m 2 + δm 2 ) (7.50) Z = 1 + y 2 32π 2 log ΛΛ eff M 2 + . . . (7.51) δm 2 = y 2 32π 2 M 2 − m 2 log ΛΛ eff M 2 + . . . , (7.52) where we make manifest the presence of supersymmetric nonrenormalization in the limit m → M , which acts as a non-trivial check. As expected, the absence of the quadratic UV divergence in the Wess-Zumino model has led to the absence of an infrared pole from the noncommutativity, even as the fermion is made arbitrarily heavy relative to the scalar. However, logarithmic UV/IR mixing still occurs. We may repeat this calculation using dimensional regularization and taking note of the issues which arose in Section 7.3.1. Using the same parametrization of the one-loop two-point function as above, the planar diagrams contribute Z planar = 1 + y 2 64π 2 2 + log µ 2 M 2 + . . . 2 + log µ 2 M 2 + . . . , (7.54) as expected. The full form of the nonplanar diagrams is unenlightening, but if we take the IR limit p • p → 0 first, they give precisely the same contribution as the planar diagrams, since the diagram degeneracies are all the same in this case. Taking the UV limit → 0 first (and staying in d < 2), we instead find Z nonplanar = 1 + y 2 64π 2 log 4 M 2 p • p + . . . which has precisely the same correspondence with the Schwinger-space regularization as we saw for the φ 4 case. We thus see clearly the conflict between supersymmetry and the use of UV/IR mixing to explain low-energy puzzles. UV/IR mixing transmogrified UV momentum dependence into IR momentum dependence, and so depended crucially on the sensitivity of our field theory to UV modes. For a theory which is finite as a field theory, the dependence on the UV physics has been removed, and so we see no interesting IR effects. Of course, in the presence of a cutoff Λ it is also possible to study the behavior of the scalar two-point function when M 2 Λ 2 |M 2 − m 2 | as the fermion is taken above the cutoff while keeping the scalar light. This corresponds to taking M/Λ, M/Λ eff > 1 and then expanding in the limit where Λ, Λ eff are large. This gets rid of the nonplanar Yukawa-type diagrams and, as one might expect, results in a return of UV sensitivity in the scalar EFT below the cutoff, foreshadowing a return of the UV/IR mixing effects. The scalar mass-squared in this limit becomes δm 2 = y 2 256π 2 6M 2 + 16Λ 2 + 8Λ 2 eff + . . . . (7.57) and UV/IR mixing reappears at the quadratic level. So our EFT intuition isn't totally out the window; it's been broken in a controlled way, and we can smoothly interpolate between theories with and without UV/IR mixing by taking the states responsible for finiteness above the cutoff. This sharpens the sense in which UV/IR mixing can do something interesting in the IR as long as the field-theoretic description of our universe is never finite. Ultimately, this highlights a central challenge for approaching the hierarchy problem via UV/IR mixing. The hierarchy problem is particularly sharp when the full theory is finite and scale separation is large, in which case the sensitivity of the Higgs mass to underlying scales is unambiguous. But UV/IR mixing effects potentially relevant to the hierarchy problem are absent in this case, and emerge only when finiteness is lost. This tension is not necessarily fatal to UV/IR approaches to the hierarchy problem-ultimately the UV sensitive degrees of freedom are not the ones we would wish to identify with the Higgs-but it bears emphasizing. Moreover, there is a possible loophole in the general argument that finiteness must be surrendered in order to generate a scale from UV/IR mixing. The presence of interesting effects in the IR here depends solely on the UV sensitivity of the nonplanar diagrams. The 'orbifold correspondence' [308, 307, 306] provides non-supersymmetric field theories constructed via orbifold truncation of N > 0 theories whose planar diagrams agree with those of the supersymmetric theory and so are finite. A noncommutative orbifold field theory [574] may then provide a theory which is fully predictive, yet which still generates an infrared scale via UV/IR mixing. Generally, it may be possible that UV/IR mixing appears in such a way that it is the sole effect sensitive to short distances. To attempt to formulate a realistic theory which uses UV/IR mixing to solve extant theoretical puzzles, it would be useful to have an understanding of which features of NCFT were responsible for the curious infrared effects discussed above. This would be helpful whether one wishes to test out these ideas in any of the many proposed modifications of NCFT, or to write down other toy models which share some features of NCFT but are based upon different principles. Qualitatively, the two unusual features involved in the formulation of NCFT are Lorentz invariance violation and nonlocality. However, it is obvious that one may have theories with one or both of these features without the interesting effects we have seen. The answer then is not so simple as pointing to one axiom or another of EFT which has been broken, but depends sensitively on the way in which they are broken. We briefly explore two ways we may better understand the interplay here between nonlocality and Lorentz-violation and how they come together to cause surprising low-energy effects. We first give a general argument based on the way nonlocality appears to postdict the form of the violation of EFT expectations. We then phenomenologically examine the loop integration appearing in our NCFT calculations to diagnose what caused the appearance of the IR pole. This will lead us to discuss an avenue toward investigating (or manufacturing) such effects in nonlocal, Lorentz-invariant theories. To see how EFT expectations may be violated, consider the peculiar way in which the noncommutative effects in the one-loop action (e.g. Equation 7.16) induce nonlocality. In Wilsonian EFT, integrating out momentum modes p Λ produces a nonlocal theory at those scales, or equivalently on distances x 1/Λ. However, particles on a noncommutative space can be thought of as rods of size L ∼ pθ [507, 508, 509, 510, 511] . This tells us that in a NCFT we should expect nonlocality to be present for scales x pθ. Comparing the two scales, we see that we should find nonlocal effects past those expected in Wilsonian EFT for 1 Λ < pθ. Here this momentum-dependent nonlocality occurs in a Lorentz-violating way. This expectation was exactly borne out in the examples above, where we saw that the one-loop effective action in momentum space is nonlocal for p • p 1/Λ 2 [491] . Purely from this analysis of the form of nonlocality, we may conclude there will be a breakdown of Wilsonian renormalization. After we remove the cutoff, the theory should be nonlocal on all scales p•p > 0. But if we compute a correlation function at a large-butfinite Λ, the theory will still be local for momenta p • p < 1/Λ 2 , and so will greatly differ from the continuum result. So our surprising discovery of the non-uniform convergence of correlation functions in the examples above is understood easily from this picture. While this sort of momentum-dependent nonlocality may seem ad hoc, it has been suggested previously for separate purposes. It has been argued [575] that quantum gravity should obey a 'Generalized Uncertainty Principle' ∆x ∆p + 2 p ∆p, with p the Planck length, based on the use of Hawking radiation to measure the horizon area of a black hole. This gives precisely the same sort of momentum-dependent nonlocality as we saw above. We refer the reader to [576] for a review of the Generalized Uncertainty Principle, [577, 578] for similar conclusions within string theory, and [579] for a more general review of the appearance of an effective minimal length in quantum gravity. It would be interesting to investigate other field theories which obey such uncertainty principles and determine whether UV/IR mixing causes similar features as appear in NCFT. For theories which violate Lorentz invariance, care must be taken to avoid arguments that even Planck-scale Lorentz violation is empirically ruled out [492, 580] . We may also attempt to phenomenologically diagnose what caused the appearance of the IR pole from the form of the loop integration. The presence of an exponential of momenta was clearly crucial, and this implies a necessity of nonlocality. It's also clear that the modification of the cutoff in the nonplanar diagrams Λ → Λ eff , which rendered the diagrams UV finite in a way that brought UV/IR mixing, was a result of the contraction between the loop momentum and the external momentum. Less obviously, one may see that any quadratic term in loop momentum in the exponential would have erased this feature, as after momentum integration one would find an integrand ∼ 1 1+α + , and any divergence will have disappeared. Heuristically, the quadratic suppression in loop momentum is too strong and regulates the UV divergence entirely independently of the cutoff, so no UV/IR mixing appears. NCFT disallows such terms as a result of momentum contractions being performed with an antisymmetric tensor, and this particular mechanism seems to imply the necessity of Lorentz invariance violation. However, this argument only considers small deviations from the form of the integral in NCFT. Further discussions of the form of loop integrals with generalizations of the star-product may be found in [581, 582] . Likely a better approach to understand the prospect for finding features similar to that of NCFT in a Lorentz invariant theory is to back up and study formulations of Lorentz invariant extensions of NCFT. This is accomplished by upgrading the noncommutativity tensor θ µν from a c-number to an operator. This was proposed already by Snyder in 1947 [335] , and this approach has been revived a number of times more recently (e.g. [583, 584, 585, 586, 587] ). Schematically, this results in an action containing an integral over θ µν S = d 4 x d 6 θ W (θ) L(φ, ∂φ), (7.58) where W (θ) is a 'weighting function', and the Lagrangian is still defined using the starproduct. The challenge in this approach for our purposes is in devising a method for nonperturbative calculations in θ, which as we saw above was necessary to preserve the features of UV/IR mixing. Searching more generally for Lorentz invariant theories which contain UV/IR mixing will likely allow more promising phenomenological applications. That such theories should exist can be broadly motivated by quantum gravity, as any gravitational theory is expected both to be nonlocal and to have UV/IR mixing. That Lorentz violation should be present is less clear. A particularly interesting line of development is to then understand in detail the class of nonlocal theories that would have UV/IR mixing of a sort similar to that discussed here. Recent work toward placing nonlocal quantum field theories on solid theoretical ground [498, 532] is clearly of sharp interest here, though the larger goal is quite distinct. The nonlocality studied in these works is designed to render the field theory UV-finite, and so the nonlocal vertex kernels are chosen precisely to avoid the introduction of new poles by ensuring these are momentum-space entire functions which vanish rapidly in Euclidean directions. The nonlocal vertices of NCFT manage to introduce new poles by oscillating as p → ∞, which presumably allows for the appearance of new 'endpoint singularities' [588, 589] , though a full examination of the Landau equations in NCFT has not (to our knowledge) been performed. Our interest is thus in a disjoint class of nonlocal theories, where new poles can appear in interesting ways. Classifying the space of such theories and developing an approach to systematically understand their unitarity properties seems well motivated. The lack of evidence for conventional solutions to the hierarchy problem has placed particle physics at a crossroads. While it is possible that the answer ultimately lies further down the well-trodden path of existing paradigms, the appeal of less-travelled paths grows greater with every inverse femtobarn of LHC data. In this work we have ventured to take seriously the apparent failure of expectations from Wilsonian effective field theory regarding the hierarchy problem by investigating a concrete framework-noncommutative field theory-in which Wilsonian EFT itself breaks down. Not only does noncommutative field theory violate Wilsonian expectations, it provides a sharp instance of UV/IR mixing: ultraviolet modes of noncommutative theories can generate an infrared scale whose origin is opaque to effective field theory. To the extent that UV/IR mixing has any relevance to the hierarchy problem, the emergence of an infrared scale seems to be among the most promising effects. Although the realworld applicability of these theories is likely limited by their Lorentz violation, they nonetheless provide valuable toy models for exploring the potential relevance of UV/IR mixing to problems of the Standard Model. To this end, we have surveyed existing results on noncommutative theories with an eye towards 'strong UV/IR duality'-the transmogrification of UV divergences into infrared poles at the same order. This led us to a detailed analysis of noncommutative Yukawa theory, perhaps the most useful toy model for thinking about the hierarchy problem (insofar as the Yukawa sector of the Standard Model is responsible for the largest UV sensitivity of the Higgs mass, and highlights the relative UV insensitivity of the fermion masses). In the noncommutative theory, the presence of both inequivalent Yukawa couplings implies the same strong UV/IR duality exhibited by real φ 4 theory: a quadratic divergence in the one-loop correction to the scalar mass from fermion loops gives rise to a simple IR pole, while a logarithmic UV divergence in the one-loop correction to the fermion mass from scalar loops give rise to only a logarithmic IR divergence. Intriguingly, the infrared pole in the scalar two-point function appears accessible in the s-channel in the Lorentzian theory, a feature which gives it particular phenomenological relevance. We then introduced softly-broken supersymmetry as a way to explore the interplay between (in)finiteness and UV/IR mixing. Choosing soft terms in order to keep the scalar light as the fermion mass is varied concretely illustrates several expected features. Strong UV/IR duality is preserved in the sense that both UV and IR divergences are absent at quadratic order (and persist at logarithmic order) when both the scalar and the fermion are in the spectrum. However, infrared structure reappears as the fermion mass is raised above a fixed cutoff and (quadratic) finiteness is lost. This underlines the sense in which UV/IR mixing may only ever play an interesting role when the field theory is quadratically UV sensitive at all scales, a scenario in which the hierarchy problem is less concrete. Finally, building on the lessons from the toy models considered here, we have highlighted a variety of interesting lines of exploration in theories featuring nonlocality with or without Lorentz violation that may be of relevance to the hierarchy problem. While the prospect that UV/IR mixing will solve outstanding theoretical problems in the low-energy universe is possibly fanciful, now is the time for such reveries. The paradigms of the past few decades of particle theory are under considerable empirical pressure, and innovative approaches are needed. At the very least, by pushing the limits of EFT we stand to learn more about the broad spectrum of phenomena possible within quantum field theory. Chapter 8 Scientists are baffled: What's up with the universe? The Washington Post Headline November 1, 2019 [590] We end the way we began: Declaring it to be an exciting time in particle physics. The picture we have painted above on the state of the field is one of uncertaintyand indeed we have barely even touched on many of the important problems of the Standard Model. Dark matter and neutrino masses, while having had canonical, obvious, beautiful solutions in the context of supersymmetric grand unified theories, are also as yet mysterious. These fields have likewise turned their focus toward alternative mechanisms in the past few years as a result of the lack of observational evidence for their standard solutions. But these facts all make the universe a more exciting place to study. Imagine if we had found weak-scale supersymmetry at the LHC, and our job now was simply to interpret the data in terms of which of the supersymmetric extensions proposed and well-studied in the past decades were correct. Or even worse, if technicolor had really been the answer and we had to watch Nature repeat the same trick she used at the strong scale again at the weak scale. How dreadfully boring! Yes, yes, this attitude is selfish and a bit flippant, but what we now have is the chance to learn more about the universe and about the spectrum of possibilities in physics, and to explore new, radical ideas. Let me end with a reminder of another, prior era in which theoretical physicists had thought they had everything figured out, recalled by no less than Max Planck in a 1924 talk at the University of Munich, and bring to your mind the outcome of those predictions: Philipp von Jolly for advice regarding the conditions and prospects of my chosen field of study. He described physics to me as a highly developed, nearly fully matured science, that through the crowning achievement of the discovery of the principle of conservation of energy it will arguably soon take its final stable form. It may yet keep going in one corner or another, scrutinizing or putting in order a jot here and a tittle there, but the system as a whole is secured, and theoretical physics is noticeably approaching its completion to the same degree as geometry did centuries ago. That was the view fifty years ago of a respected physicist at the time. 53 As translated in Wells (2016) [591] from Planck (1933) [592] May the universe continue to surprise us. 53 In fairness to von Jolly (1809-1884), he really was a respected experimental physicist in his dayenough so to have been knighted-and earlier in his life made important contributions to the understanding of gravity and of osmosis [591] . This attitude was not rare at the time, and he wouldn't be remembered for it were it not for a student of his having played a role in revolutionizing physics. Since kinetic mixing plays a central role in freeze-twin dark matter, we discuss here at some length the order at which it is expected in the low-energy EFT. Of course, there may always be UV contributions which set to the value needed for freeze-in. However, if the UV completion of the MTH disallows such terms -for example, via supersymmetry, an absence of fields charged under both sectors, and eventually grand unification in each sector (see e.g. [593, 594, 292, 293, 595, 596] )-then the natural expectation is for mixing of order these irreducible IR contributions. To be concrete, we imagine that = 0 at the UV cutoff of the MTH, Λ 4πf . To find the kinetic mixing in the regime of relevance, at momenta µ 1 GeV, we must run down to this scale. As we do not have the technology to easily calculate high-loop-order diagrams, our analysis is limited to whether we can prove diagrams at some loop order are vanishing or finite, and so do not generate mixing. Thus our conclusions are strictly always 'we know no argument that kinetic mixing of this order is not generated', and there is always the possibility that further hidden cancellations appear. With that caveat divulged, we proceed and consider diagrammatic arguments in both the unbroken and broken phases of electroweak symmetry. Starting in the unbroken phase, we compute the mixing between the hypercharge gauge bosons. Two-and three-loop diagrams with Higgs loops containing one gauge vertex and one quartic insertion vanish. By charge conjugation in scalar QED, the threeleg amplitude of a gauge boson and a complex scalar pair must be antisymmetric under exchange of the scalars. However, the quartic coupling of the external legs ensures that their momenta enter symmetrically. As this holds off-shell, the presence of a loop which looks like causes the diagram to vanish. However, at four loops the following diagram can be drawn which avoids this issue: where the two hypercharges are connected by charged fermion loops in their respective sectors and the Higgs doublets' quartic interaction. This diagram contributes at least from the MTH cutoff Λ 4πf down to f , the scale at which twin and electroweak symmetries are broken. We have no argument that this vanishes nor that its unitarity cuts vanish. We thus expect a contribution to kinetic mixing of ∼ g 2 1 c 2 W /(4π) 8 , with g 1 the twin and SM hypercharge coupling and c W = cos θ W appearing as the contribution to the photon mixing operator. In this estimate we have omitted any logarithmic dependence on mass scales, as it is subleading. In the broken phase, we find it easiest to perform this analysis in unitary gauge. The Higgs radial modes now mass-mix, but the emergent charge conjugation symmetries in the two QED sectors allow us to argue vanishing to higher-loop order. The implications of the formal statement of charge conjugation symmetry are subtle because we have two QED sectors, so whether charge conjugation violation is required in both sectors seems unclear. However, similarly to the above case, there is a symmetry argument which holds off-shell. The result we rely on here is that in a vector-like gauge theory, diagrams with any fermion loops with an odd number of gauge bosons cancel pairwise. Thus, each fermion loop must be sensitive to the chiral nature of the theory, so the first non-vanishing contribution is at five loops as in: where the crosses indicate mass-mixing insertions between the two Higgs radial modes which each contribute ∼ v/f . Thus, both the running down to low energies and the finite contributions are five-loop suppressed. From such diagrams, one expects a contribution ∼ e 2 g 2 A g 2 V (v/f ) 2 /(4π) 10 , where with g V and g A we denote the vector and axial-vector couplings of the Z, respectively. We note there are other five loop diagrams in which Higgses couple to massive vectors which are of similar size or smaller. Depending on the relative sizes of these contributions, one then naturally expects kinetic mixing of order ∼ 10 −13 − 10 −10 . If is indeed generated at these loop-levels, then mixing on the smaller end of this range likely requires that it becomes disallowed not far above the scale f . However, we note that our ability to argue for higher-loop order vanishing in the broken versus unbroken phase is suggestive of the possibility that there may be further cancellations. We note also the possibility that these diagrams, even if nonzero, generate only higher-dimensional operators. Further investigation of the generation of kinetic mixing through a scalar portal is certainly warranted. define as the eigenfunctions of appropriately-defined derivatives on the noncommutative space, and which look familiar e ip·x . To can get a sense for this algebra it is useful to carry out the simple exercise of multiplying two plane waves by simply applying Baker-Campbell-Hausdorff e ik·x · e ik ·x = exp ik ·x + ik ·x − 1 2 k µ k ν [x µ ,x ν ] = e − i 2 θ µν kµk ν e i(k+k )·x . (B.2) As in quantum mechanics, we will wish to study noncommutative versions of familiar commutative theories, and so it will be useful to view R d θ as a 'deformation' of R d . We then wish to construct a map from our commutative algebra to our noncommutative one which returns smoothly to the identity as θ µν → 0. The standard such choice is the Weyl-Wigner mapŴ, which one may roughly think of as merely replacing xs withxs. The procedure is simply to Fourier transform from commutative space to momenta, and then inverse Fourier transform to noncommutative space. Given a commutative space Schwartz function f , we may compose the two operations and writê Note that this is an injective map of Schwartz functions on R d to those on R d θ which respects the vector space structure but not the structure of the algebra. This property is familiar from quantum mechanics. We may now construct noncommutative versions of field variables, but we still don't know how to do physics on these spaces. That is, we can write down the Lagrangian for noncommutative φ 4 theory, and we could even determine an action after we formulate a notion of an integral over a noncommutative space. But our familiar results about how to go from the action of a field theory to a calculation for a physical observable most certainly depended implicitly on living on a commutative space, and so it seems we must re-formulate physics from the bottom up. Fortunately, such a drastic measure may not be necessary, as one may formulate QFT on noncommutative spaces as a simple modification of our normal field theory structure. The core idea is to find an algebra of functions on R d which is isomorphic to Alg R d θ [x], · by pushing the noncommutativity into a new field product, known as a Groenewold-Moyal product (or star-product). We diagram the structure we wish to look for in Figure B .1. W an isomorphism of algebras In particular, we may do this by demanding that our quantization mapŴ is upgraded to an isomorphism between Alg R d θ [x], · and an algebra on the vector space of functions of commutative Euclidean space, with a multiplication operation which is chosen to preserve the algebraic structure. That is, we must satisfŷ where the θ subscript merely tells us the star-product will depend on the noncommutativity tensor, and this will henceforth be dropped. This gives a position-space representation of the star-product, The general procedure to construct a noncommutative field theory from a commutative one is then by application of the Weyl-Wigner map. As an example, for a simple φ n theory we find  It is simple to generalize the procedure discussed in Section 7.3 to add to the quadratic effective action of φ any function we wish through judicious choice of the two-point function for an auxiliary field σ which linearly mixes with it. In position space, if we wish to add to our effective Lagrangian where f (−i∂) is any function of momenta, and c is a coupling we've taken out for convenience, then we simply add to our tree-level Lagrangian where f −1 is the operator inverse of f . It should be obvious that this procedure is entirely general. As applied to the Euclidean φ 4 model, we may use this procedure to add a second auxiliary field to account for the logarithmic term in the quadratic effective action as where we point out that the argument of the log is just 4/(Λ 2 eff p • p) in position space. We may then try to interpret σ also as a new particle. As discussed in [515] , its logarithmic propagator may be interpreted as propagation in an additional dimension of spacetime. Alternatively, we may simply add a single auxiliary field which accounts for both the quadratic and logarithmic IR singularities by formally applying the above procedure. But having assigned them an exotic propagator, it then becomes all the more difficult to interpret such particles as quanta of elementary fields. To account for the IR structure in the fermion two-point function, we must add an auxiliary fermion ξ. If we wish to find a contribution to our effective Lagrangian of where O is any operator on Dirac fields, then we should add to our tree-level Lagrangian  we again find a one-loop quadratic effective Lagrangian which is equal to the Λ → ∞ value of the original, but now for any value of Λ. We may further generalize the procedure for introducing auxiliary fields to account for IR poles to the case of poles in the three-point effective action. It's clear from the form of the IR divergences in Equation ? ? that they 'belong' to each leg, and so naïvely one might think this means that the divergences we've already found in the two point functions already fix them. However those corrections only appear in the internal lines and were already proportional to g 2 , and so they will be higher order corrections. Instead we must generate a correction to the vertex function itself which only corrects one of the legs. To do this we must introduce auxiliary fields connecting each possible partition of the interaction operator. However, while an auxiliary scalar χ coupled as χϕ + χψψ would generate a contribution to the vertex which includes the χ propagator with the ϕ momentum flowing through it, it would also generate a new (ψψ) 2 contact operator, which we don't want. To avoid this we introduce two auxiliary fields with off-diagonal two-point functions, a trick used for similar purposes in [515] . By abandoning minimality, we can essentially use an auxiliary sector to surgically introduce insertions of functions of momenta wherever we want them. We can first see how this works on the scalar leg. We add to our tree-level Lagrangian Now to integrate out the auxiliary fields we note that for a three point vertex, one may use momentum conservation to put all the noncommutativity between two of the fields. That is, χ 2 (x) ψ(x) ψ(x) = χ 2 (x)(ψ(x) ψ(x)) = (ψ(x) ψ(x))χ 2 (x) as long as this is not being multiplied by any other functions of x. So we may use this form of the interaction to simply integrate out the auxiliary fields. We end up with ∆L eff = κ 1 κ 2 ψ ψ f (−i∂)ϕ (C. 8) which is exactly of the right form to account for an IR divergence in the three-point function which only depends on the ϕ momentum. For the fermionic legs, we need to add fermionic auxiliary fields which split the Yukawa operator in the other possible ways. We introduce Dirac fields ξ, ξ and a differential operator on such fields O −1 (−i∂). Then if we add to the Lagrangian ∆L = −ξO −1 ξ −ξ O −1 ξ+c 1 (ξ ψ ϕ+ψ ξ ϕ)+c 2 (ξ ϕ ψ+ψ ϕ ξ)+c 3 (ξ ψ+ψξ ), (C.9) we now end up with a contribution to the effective Lagrangian (C.10) where we have abused notation and now the argument of O specifies which fields it acts on. These terms have the right form to correct both vertex orderings. Now that we've introduced interactions between auxiliary fields and our original fields, the obvious question to ask is whether we can utilize the same auxiliary fields to correct both the two-point and three-point actions. In fact, using two auxiliary fields with offdiagonal propagators per particle we may insert any corrections we wish. The new trick is to endow the auxiliary field interactions with extra momentum dependence. For a first example with a scalar, consider differential operators f , Φ, and add to the Lagrangian ∆L = −χ 1 f −1 (−i∂)χ 2 + κ 1 χ 1 ϕ + κ 2 χ 2 ψ ψ + gϕΦ(−i∂)χ 2 . (C.11) We may now integrate out the auxiliary fields and find ∆L eff = gκ 1 ϕf (Φ(ϕ)) + κ 1 κ 2 ψ ψ f (ϕ) (C.12) where we've assumed that f and Φ commute. If we take Φ = 1 then we have the interpretation of merely inserting the χ two-point function in both the two-and threepoint functions. But we are also free to use some nontrivial Φ, and thus to make the corrections to the two-and three-point functions have whatever momentum dependence we wish. It should be obvious how to generalize this to insert momentum dependence into the scalar lines of arbitrary n−point functions. The case of a fermion is no more challenging in principle. where the generalization to n-points is again clear. Note that in the fermionic case it's crucial that we be allowed to insert different momentum dependence in the corrections to the two-and three-point functions, as these have different Lorentz structures. Now we cannot quite implement this for the two-and three-point functions calculated in Section 7.4, for the simple reason that we regulated these quantities differently. That is, we have abused notation and the symbol 'Λ' means different things in the results for the two-and three-point functions. In order to carry out this procedure, we could simply regulate the two-point functions in 3d Schwinger space, though we run into the technical obstruction that the integration method above only calculates the leading divergence, which is not good enough for the scalar case. 322 
