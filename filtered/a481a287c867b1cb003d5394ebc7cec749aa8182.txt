a481a287c867b1cb003d5394ebc7cec749aa8182
P001 Reduced cellular respiration and ATP production in an in vitro model of sepsis
V  Herwanto Y  Wang M  Shojaei B  Tang A S Mclean T  Shimazui T  Nakada L  Fujimura A  Sakamoto M  Hatano S  Oda 

University of Sydney University of Sydney University of Sydney Nepean Hospital University of Sydney Nepean Hospital University of Sydney University of Sydney University of Sydney Nepean Hospital University of Sydney Nepean Hospital University of Sydney Nepean Hospital University of Sydney University of Sydney 
Critical Care 2018, 22(Suppl 1):P001 Introduction: Leukocyte dysfunction may play a role in sepsis pathogenesis. Established evidence showed that leukocyte dysfunction leads to reduced immune response and consequently an increased sepsis-related mortality. Impaired metabolism has been recently proposed as one possible mechanisms underpinning leukocyte dysfunction in sepsis. In this study, we investigated the global changes in leukocyte metabolism in sepsis, using an established in vitro model of lipopolysaccharide (LPS) stimulation. Methods: Peripheral blood mononuclear cells (PBMC) were isolated from healthy volunteers (n=4) and incubated with 62.5 ng/mL LPS. Mitochondrial respiration was measured using Agilent Seahorse XF Analyzer (Cell Mito Stress Test Kit). Total cellular oxidative stress was measured using DCFDA Cellular Reactive Oxygen Species (ROS) Detection Assay Kit (Abcam) and mitochondrial superoxide was measured using MitoSOXTM (Life Technology). Apoptosis was measured by Annexin V-FITC Apoptosis Detection Kit (Abcam). Evaluation of oxidative stress and apoptosis were performed using BD FACSCanto flow cytometer and flow cytometry data was analyzed using FlowJo Software V10. Results: LPS stimulation of PBMC from healthy volunteers showed a trend of decrease in both oxidative phosphorylation and cellular respiration (Fig. 1) . This decrease in cellular metabolism was accompanied by a trend towards an increase in cell death in the stimulated leukocytes (Fig. 2) . The increase in cell death was associated with an increase in oxidative stress (total and mitochondria) (Fig. 2) , suggesting that the adverse effect of LPS on cellular metabolism may be mediated by an imbalance in redox potential. Conclusions: The LPS stimulation model could provide a useful approach to study the effect of sepsis on leukocyte metabolism. Further study is required to better understand the mechanism of reduced leukocyte metabolism, including the possible role of oxidative stress in reducing cellular respiration and causing leukocyte cell death. Noninvasive technique using fluorescence imaging to assess vascular permeability in sepsis Critical Care 2018, 22(Suppl 1):P002 Fig. 1 (abstract P001) . Cellular metabolism as measured in oxygen consumption rate (OCR) (n = 4). Basal denotes energetic demand of the cell under baseline condition; spare respiratory capacity denotes the capability of the cell to respond to energetic demand; proton leak denotes remaining basal respiration not coupled to ATP production, can be a sign of mitochondrial damage; ATP production shows ATP produced by the mitochondria to meet the energetic need of the cell. Fig. 2 (abstract P001). Number of apoptotic cells, total cellular ROS, and mitochondrial superoxide as measured by Annexin V, DCFDA, and MitoSOX (n = 4). Critical Care 2018, 22(Suppl 1):82 The relationship between systemic glycocalyx degradation markers and regional glycocalyx thickness in non-septic critically ill patients is unclear. Conjunctival sidestream dark fieldimaging for the purpose of glycocalyx thickness estimation has never been performed. We aimed to investigate whether changes in glycocalyx thickness in conjunctival and sublingual mucosa are associated with global glycocalyx shedding markers. Methods: In this single-centre prospective observational study, using techniques for direct in-vivo observation of the microcirculation, we performed a single measurement of glycocalyx thickness in both ocular conjunctiva and sublingual mucosa in mixed cardio surgical (n=18) and neurocritical patients (n=27) and compared these data with age-matched healthy controls (n=20). In addition we measured systemic syndecan-1 levels Results: In the sublingual and conjunctival region we observed a significant increase of the perfused boundary region (PBR) in both neuro critical and cardiac surgical ICU patients, compared to controls (2.20 7 ], p<0,05). We detected a weak correlation between syndecan-1 and sublingual PBR(r=0.40, p=0.002) but no correlations between global glycocalyx damage markers and conjuctival glycocalyx thickness. Conclusions: Conjunctival glycocalyx thickness evaluation using SDF videomicroscopy is suitable and is impaired in non-septic ICU patients but only measurements in sublingual mucosa are correlating with systemic glycocalyx shedding markers. Global glycocalyx damage is more severe in cardiac comparing to neuro critical patients. Introduction: Endothelial dysfunction plays a major role in the sepsis related organ dysfunction, and is featured by vascular leakage. AMP-activated protein kinase (AMPK) is known to regulate actin cytoskeleton organization and interendothelial junctions (IEJs), contributing to endothelial barrier integrity. We have already demonstrated its role in defence against sepsis induced hyperpermeability [1] , but the underlying mechanisms remain unknown. This project aims to identify molecular targets involved in the beneficial action of AMPK against endothelial barrier dysfunction. Methods: Experiments have been performed in human microvascular dermal endothelial cells. α1AMPK activity has been modulated via the use of a specific siRNA or treatment by two pharmacological AMPK activators (AICAr, 991). We have investigated the effect of this modulation on the expression/phosphorylation of Connexin 43 (Cx43) and Heat shock protein 27 (HSP27), two proteins playing a key role in maintenance of IEJs and actin dynamics respectively. Results: We show that α1AMPK is required to sustain the level of Cx43 expression as it was drastically reduced in cells transfected with a siRNA targeting specifically α1AMPK. Regarding HSP27, its expression level was not affected by α1AMPK deletion. However, both AMPK activators increased its phosphorylation on Ser82, in a α1AMPKdependent manner, while they had no effect on Cx43. Our results also reveal that HSP27 phosphorylation concurred with the appearance of actin stress fibers at the periphery of cells, suggesting a beneficial role for pHSP27 as well as F-actin stress fibers in vascular barrier function through reinforcing the endothelial tethering. Conclusions: Our work identifies the regulation of Cx43 expression and HSP27 phosphorylation as potential protective responses underlying the beneficial action of AMPK against endothelial barrier dysfunction. AMPK could consequently represent a new therapeutic target during sepsis. Introduction: Sepsis Induced Cardiomyopathy (SIC) is a serious condition during sepsis with a mortality rate up to 70% (1) . SIC is clinically manifested with left ventricle impaired contractility (2) . Melusin is a muscle-specific protein involved in sustaining cardiomyocyte survival thorough the activation of AKT signaling pathways (3) . PI3K-AKT signaling pathway plays a pivotal role in regulating calcium channel activity (4) . We hypothesized that Melusin overexpression could exert a protective effect on cardiac function during septic injury. Methods: Animals were treated with an intraperitoneal injection of lipopolysaccharide (LPS) at 12 mg/kg. SV129 strain Knockout mice (KO) for Melusin gene and FVB strain with cardiac-specific overexpression (OV) of Melusin were compared. Each group was studied together with a control group (WT). Hemocardiac parameters were studied at 0 hour and 6 hours through echocardiography. Another cohort of animals was sacrificed 6 hours after 20 mg/kg LPS treatment and cardiac tissues and blood sample were harvested for Wb analysis to quantify the expression of AKT, P-AKT and CACNA1C and Elisa analysis for Troponin levels. Results: SV129 WT, KO Melusin and FVB WT mice groups, fractional shortening (FS) was significantly impaired after LPS challenge and was associated with compensatory tachycardia (Fig. 1) . FVB OV mice group didn't show decrease in FS. Consistent with the increased AKT phosphorylation observed in OV mice, the expression of CACNA1C was also significantly higher both at basal levels and after LPS treatment in OV mice compared to WT mice (Fig. 2) . Troponin levels didn't differ between mice groups after LPS treatment Conclusion: Melusin has protective role in LPS induced cardiomyopathy, likely through Akt phosphorylation controlling the CACNA1C protein density. Introduction: Liver dysfunction is frequent in sepsis, but its pathophysiology remains incompletely understood. Since altered liver function has also been described in ICU patients without sepsis [1, 2] , the influence of sepsis may be overestimated. We hypothesized that sedation and prolonged mechanical ventilation after abdominal surgery is associated with impaired liver function independent of sepsis. Methods: Sedated and mechanically ventilated pigs underwent abdominal surgery for regional hemodynamic monitoring and were subsequently randomized to fecal peritonitis and controls, respectively (n=4, each), followed by 80 h observation. Indocyanine green (ICG) retention rate 15 minutes after injection of 0.25mg/kg ICG (ICG R15) was determined at baseline, and 11, 32 and 80 h after sepsis induction (SI), and at the same time points in controls. Concurrent with ICG R15, plasma volume, total hepatic perfusion (ultrasound transit time), and bilirubin and liver enzymes were measured. ANOVA for non-parametric repeated measurements was performed in both groups separately. Results: ICG R15 increased over time without significant differences between groups (Table 1 ). There was a parallel increase in bilirubin in septic but not control animals. The other measured parameters were similar in both groups at the end of the experiment. Conclusion: Liver function was impaired under sedation and prolonged mechanical ventilation after abdominal surgery, even in animals without sepsis. The underlying reasons should be further explored. Introduction: Previous work has shown the cytoprotective properties of antithrombin-affinity depleted heparin (AADH), by neutralization of cytotoxic extracellular histones [1] , major mediators of death in sepsis [2, 3] . AADH was produced from clinical grade heparin, resulting in preparations that have lost >99,5% of their anticoagulant activity. To gain insight into the mechanisms and the basic pharmacological aspects of AADH protective properties, we performed a systematic analysis of how AADH is tolerated in mice and ascertained its effects in three different in vivo models of inflammation and infection. Methods: Dose ranging studies, short term and medium term, were performed in C57BL/6 mice. The effects of i.v. administration of extracellular histones in the presence or absence of AADH were assessed in mice. We further analysed the effect of AADH in models of Concanavalin A-and MRSA-mediated lethality. In all studies we assessed clinical signs, lab parameters and histology. Results: AADH was well tolerated in both short term and intermediate term (till 7 days) experiments in mice, in the absence of any signs of tissue bleeding. AADH was able to revert the cytotoxic properties of i.v. administered histones. In a Concanavalin A mediated model of sterile inflammation, we confirmed that AADH has protective properties that counteract the cytotoxic effects of extracellular histones. In an in vivo lethal MRSA model, for the first time, AADH was shown to induce a survivalbenefit. Conclusions: We conclude that AADH contributes to the overall increased survival by means of neutralization of extracellular histones and represents a promising product for further development into a drug for the treatment of inflammatory diseases and sepsis. Introduction: Urokinase (UK) and tissue plasminogen activator (tPA) mediate thrombolytic actions by activating endogenous plasminogen. Thrombomodulin (TM) complexes with thrombin to activate Protein C and thrombin activatable fibrinolysis inhibitor (TAFI). Activated Protein C (APC) modulates coagulation by digesting factors V and VIII and activates fibrinolysis by decreasing PAI-1 functionality. Methods: The purpose of this study is to compare the effects of rTM and APC on urokinase and tPA mediated thrombolysis utilizing thromboelastography. Results: Native whole blood was activated using a diluted intrinsic activator (APTT reagent, Triniclot). The modulation of thrombolysis by tPA and UK (Abbott, Chicago, USA) was studied by supplementing these agents to whole blood and monitoring TEG profiles. APC (Haematologic Technologies, VT, USA) and rTM (Asahi Kasai Pharma, Tokyo, Japan) were supplemented to the activated blood at 0.02 -3.0 ug/ml. The modulation of tPA and UK induced thrombolysis by APC and rTM was studied in terms of thromboelastograph patterns. The effect of both APC and rTM on plasma based systems supplemented with tPA was also investigated. Patients treated with antibiotic therapy were eligible for inclusion. The plausibility of infection (definite, probable, possible, none) was determined based on the Centers for Diseases Control (CDC) criteria. Patients with sepsis (definite/probable/possible infection and a SOFA score increase of >=2) were screened for death within 60 days and secondary infections 48 h to 60 days after ICU admission, using the CDC criteria. HLA-DRA and CD74 mRNA expressions were determined by reverse transcription quantitative PCR. Results: Among 579 ICU admissions, a blood sample for RNA analysis was collected in 551 cases. Two hundred fifty-seven patients met the inclusion criteria and provided written informed consent. Sepsis was noted in 134 patients. The sepsis patients experienced death in 36 cases (27%), secondary infection in 32 cases (24%), and death and/or secondary infection in 60 cases (45%). Table 1 shows the results of HLA-DRA and CD74 expression related to death and secondary infections. Conclusions: The mRNA expression of HLA-DRA on ICU admission was significantly decreased in patients with sepsis who died or contracted secondary infections within 60 days. CD74 expression was not significantly decreased in patients with negative outcome. Introduction: Acid-base disturbances are common in patients with infection admitted to the intensive care unit (ICU). More attention is given to hyperlactatemia in this patient population as a prognostic factor, although other acid-base disturbances may also have an impact on patient outcomes. Our objective is to describe the acid-base profile of this patient population and determine the association between different acid-base abnormalities and ICU mortality. Methods: Retrospective cohort of patients admitted with infection to an intensive care unit. Patients were stratified according to pH (<7.35; 7.35 -7.45; > 7.45) and, then, according to the standard base excess (SBE) (< -2; -2 -+2; > +2). In each of these strata and the whole population, the proportions of acid-base disturbances were quantified during the first 24 hours of ICU admission. To assess the association between acid-base disturbances and outcome, a logistic regression model was fit, adjusting for age, sex and SAPS 3 score. Results: 605 patients were analysed. 304 (50%) patients were acidemic and 244 (40%) presented with a normal pH. Metabolic acidosis (as assessed by SBE) was observed in all subgroups, regardless of pH levels (pH < 7 ). Lactic acidosis was observed in 71% of the whole population; SIG (Strong ion gap) acidosis, in 75%; SID (hyperchloremic) acidosis, in 58%; metabolic alkalosis, in 7%; and respiratory acidosis, in 13% of the patients. Introduction: Sepsis-induced brain dysfunction has been neglected until recently due to the absence of specific clinical or biological markers. There is increasing evidence that sepsis may pose substantial risks for long term cognitive impairment. Methods: To find out clinical and inflammatory factors associated with acute sepsis-induced brain dysfunction (SIBD) serum levels of cytokines, complement breakdown products and neurodegeneration markers were measured by ELISA in sera of 86 SIBD patients and 33 healthy controls. Association between these biological markers and cognitive test results was investigated. Results: SIBD patients showed significantly increased IL-6, IL-8, IL-10 and C4d levels and decreased TNF-α, IL-12, C5a and iC3b levels than healthy controls. No significant alteration was observed in neuronal loss and neurodegeneration marker (neuron specific enolase (NSE), amyloid β, tau) levels. Increased IL-1β, IL-6, IL-8, IL-10, TNF-α and decreased C4d, C5a and iC3b levels were associated with septic shock, coma and mortality. Transient mild cognitive impairment was observed in 7 of 21 patients who underwent neuropsychological assessment. Cognitive dysfunction and neuronal loss were associated with increased duration of septic shock and delirium but not baseline serum levels of inflammation and neurodegeneration markers. Conclusions: Increased cytokine levels, decreased complement activity and increased neuronal loss are indicators of poor prognosis and adverse events in SIBD. Cognitive dysfunction and neuronal destruction in SIBD do not seem to be associated with systemic inflammation factors and Alzheimer disease-type neurodegeneration but rather with increased duration of neuronal dysfunction and enhanced exposure of the brain to sepsisinducing pathogens. Introduction: High levels of some aromatic microbial metabolites (AMM) in serum are related to the severity and mortality of critically ill patients [1] . Several studies have discussed the imbalance and loss of the diversity of gut microbiota but there are practically no data on the gut microbial metabolites in critical conditions, only a little -in healthy people [2, 3] . The aim of this work is to analyze the connection between serum and fecal levels of AMM in ICU patients. Methods: 13 simultaneously serum and fecal samples (SFS) from ICU patients with nosocomial pneumonia (group I), 21 SFS from ICU neurorehabilitation patients (group II) and 5 SFS from healthy people were taken for GC/MS analyses. The following AMM were measured: phenylpropionic (PhPA), phenyllactic (PhLA), p-hydroxybenzoic (p-HBA), p-hydroxyphenyllactic (p-HPhLA), p-hydroxyphenylacetic (HPhAA), p-hydroxyphenylpropionic (p-HPhPA) and homovanillic (HVA) acids. Data were presented as medians with interquartile range (IR, 25-75%) using STATISTICA 10. Results: The sum of the level of 4 most relevant metabolites (4AMM) -PhLA, p-HPhLA, p-HPhAA, and HVA -in serum samples from group I and group II were equal to 0.9 (0.6-9.6) μ M and 0.7 (0.5-1.0) μ M, respectively, and were higher than in healthy people -0.4 (0.4-0.6) μ M (p<0.05). We suppose the presence of the correlation of AMM profile in blood and intestine. Particularly, SFS of healthy people are characterized by the prevalence of PhPA; AMM are not detected in feces of non-survivors but only HVA dominates in their serum in the absence of other (Fig. 1) . Conclusions: The AMM profiles in gut and serum are interrelated; AMM in serum probably reflect the violation and loss of biodiversity of the gut microbiota in critically ill patients. Introduction: Since nitrogen oxide (NO) is an essential component of the immune system, the dynamics of plasma NO concentration was studied in order to predict the development of sepsis [1, 2] . Methods: With the permission of the Ethics Committee included the 200 full-term newborns with respiratory diseases on a ventilator, retrospectively divided into two groups (I, n=46 -sepsis 4-5 days; II, n=154 without sepsis), at 1, 3-5, 20 days was studied by ELISA the plasma concentration of NO, NOS-2, NOS-3, ADMA (Multilabel Coulter Victor-21420, Finland). To select points "Cut-Off" used the method of ROC-Lines. Results: The statistical power of the study was 86.7% (Î±<0.05). At admission in patients of groups I and II decrease the concentration of NO and increased ADMA in plasma (p<0.05) relative to healthy newborns. After 3-5 days, relatively in patients of groups Introduction: Sepsis-associated disseminated intravascular coagulation (SAC) is associated with decreased platelet counts and formation. The widespread activation of platelets contribute to vascular occlusions, fibrin deposition, multi-organ dysfunction, contributing to a two-fold increase in mortality. The purpose was to measure markers of platelet function in the plasma of patients with clinically established SAC and to determine association to disease severity and outcome. Methods: Plasma samples from 103 adult intensive care unit (ICU) patients with sepsis and suspected SAC were collected at baseline and on days 4 and 8. DIC scores were calculated using platelet count, D-Dimer, INR, and fibrinogen. Patients were categorized as having no DIC, non-overt DIC, or overt DIC. Plasma levels of CD40L, von Willebrand Factor (vWF), platelet factor-4 (PF-4), and microparticles (MP) were quantified using commercially available ELISA methods. Results: Markers of platelet activation were significantly elevated in patients with sepsis alone and with suspected DIC compared to normal healthy individuals on ICU day 0 (p<0.001). Levels of platelet-associated biomarkers were compared between survivors and non-survivors. PF-4 was significantly decreased in non-survivors compared to survivors (p = 0.0156). Patients were stratified based on platelet count and levels of markers were compared between groups. CD40L, vWF, PF4, and MP showed significant variation based on platelet count, with all markers exhibiting stepwise elevation with increasing platelet count. Conclusions: Markers of platelet activation were significantly elevated in patients with SAC compared to healthy individuals. PF4 levels showed significant difference based on DIC score or mortality, and differentiated the non-survivors compared to survivors. CD40L, vWF, PF4, and MP showed significant association with platelet count, increasing in a stepwise manner with increases in platelet count (Table 1) . Prognostic value of mean platelet volume in septic patients: a prospective study A Chaari King Hamad University Hospital, Bussaiteen, Bahrain Critical Care 2018, 22(Suppl 1):P024 Introduction: Mean Platelet Volume (MPV) has been reported as a valuable marker of inflammatory diseases. The aim of the current study is to assess the prognostic value of MPV in septic patients. Methods: Prospective study including all patients admitted to the intensive care unit (ICU) with sepsis or septic shock. Demographic, clinical and laboratory data were collected. The MPV was checked on admission and on day 3. Two groups were compared: Survivors and non-survivors. [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] days in survivors and 8.5 [3.5-12] days in non-survivors (p=0.623). Conclusions: The decrease of the platelet count but not the increase of the MPV was associated with increased mortality in critically-ill septic patients. Endotoxin activity assay levels measured within 24 hours after ICU admission affected patients' severity assessments A Kodaira 1 , T Ikeda 2 , S Ono 2 , S Suda 2 , T Nagura 2 1 Tokyo Medical University, Tokyo, Japan, 2 Introduction: Sepsis profoundly alters immune homeostasis by inducing first a systemic pro-inflammatory, then an anti-inflammatory state. We evaluate the prognostic value of ex vivo lipopolysaccharide (LPS) stimulation of whole blood in septic patients, at day 1 and 7 after intensive care unit (ICU) admission. Methods: This prospective cohort study included patients with severe sepsis or septic shock admitted to a surgical ICU of a university hospital. Blood was drawn on day 1 and day 7, and stimulated ex vivo with LPS for 24 hours. Tumor necrosis factor alpha (TNF), interleukin (IL) 1, IL6 and IL10 were measured. Twenty-three healthy adults served as controls. Outcomes were ventilator and ICU-free days, SOFA score at day 1 and 7, and need for dialysis during the course of sepsis. Results: Forty-nine patients were included (mean age 62 ± 15 years). The blood of septic patients was less responsive to ex vivo stimulation with LPS than that of healthy controls, as demonstrated by lower TNF, IL1, IL6 and IL10 release ( Fig. 1 ). At day 1, patients above the 50th percentile of IL10 release had significantly fewer ventilator and ICU-free days than those in the lower 50th percentile (Fig. 2) . In contrast, patients in whom IL10 release increased between day 1 and day 7 had significantly lower SOFA scores at day 1 and 7 and need for dialysis, and more ICU-free days than patients in whom IL10 release decreased (Table 1) . Conclusions: Greater LPS-stimulated IL10 release in septic patients at day 1 was associated with poorer clinical outcomes and may reflect the severity of the forthcoming immunoparalysis. However, an increase in IL10 release between day 1 and day 7 was associated with favorable outcomes, perhaps signaling immune restoration. Introduction: Hyperthermic Intraperitoneal Chemotherapy with Cytoreductive Surgery (HIPEC-CRS) is a curative treatment modality for peritoneal carcinomatosis. Extensive debulking surgery, peritoneal stripping and multiple visceral resections followed by intraperitoneal installation of heated high-dose chemotherapeutic agents, a process leads to a 'high-inflammatory' syndrome. Serum procalcitonin (PCT), a biomarker for bacterial sepsis, in the heightened inflammatory state after HIPEC-CRS might be of limited utility. Our aim is to determine the trends of PCT in the early postoperative phase of HIPEC-CRS and to identify trends in patients with and without bacterial sepsis Methods: In a case-control design, we reviewed all patients undergoing HIPEC-CRS over a 24-month period (2015) (2016) (2017) . Patients were divided into 2 groups based on whether they developed bacterial sepsis in the first 5 days after surgery (infected v/s non-infected). Summary data are expressed as medians and ranges. Two-tailed nonparametric tests were performed and considered significant at p values of less than 0.05 Results: 82 patients' data was analyzed. Infections developed in 16% (13 patients) with Escherichia coli as the predominant pathogen isolated (36% isolates). PCT levels (ngm/ml) were elevated postoperatively in both infected and non-infected patients; Day 1 infected 0.97 (IQR 0. 5 Introduction: Early outcome in cardiac surgery has been an area of growing interest where the given risks raise several predictive models for assessment of postoperative outcome [1] . Procacitonin (PCT) emerges as a possible predictive tool in cardiothoracic intensive care unit (CTICU).We aim at testing the predictive power of PCT for early morbidity, prolonged ventilation, ICU and hospital stay, in patients developing early fever after cardiac surgery Methods: A retrospective descriptive study done in tertiary cardiac center, enrolling patients who stayed for more than 24 hours post-operatively in the CTICU Risk stratification included additive Euro score and PCT immunoluminometricaly prior to surgery and every 48 hours in response to onset of fever. Results: We screened 501 consecutive patients who underwent open heart cardiac, of which 119 patients were enrolled in the study. Patients were divided into two groups based on the level of PCT, those with value > 2 ng/ml (Group 1) and those with level < 2 ng/ml (Group 2). Patients in group 1 as compared to Group 2, over the postoperative course was associated with prolonged ICU stay (P=0.04), length of mechanical ventilation (P=0.05), length of hospitalization (p=0.05), acute kidney injury (P=0.04) and culture positivity (P=0.02). Multivariate analysis showed that PCT >2ng/ml was was significantly associated with positive cultures. (p=0.023) Conclusions: A rise of serum PCT carries the signals of early ICU morbidity and lengths of ventilation, ICU stay and hospital stay Methods: 42 patients aged 153 (44-252) days (4-360 days) underwent cardiac surgery with cardiopulmonary bypass for severe congenital heart disease. In the dynamics levels of PCT, MR-proADM, CT-proAVP and MR-proANP were measured before surgery and on the 1, 2, 3 and 6 days after the operation with the Kryptor compact plus analyzer. Data are presented as medians with interquartile range. The Mann-Whitney U-test was used to compare the data. Values of p <0.05 were statistically significant. Results: 24 patients (57%) required ALV for more than 72 hours. In this group statistically significant higher levels of PCT, MR-proADM and MR-proANP were found throughout the period ( Table 1 ). The level of CT-proAVP had increased to statistical significance since the 3 day after the operation. 23 patients were in the ICU for more than 168 hours. In this group statistically significant higher levels of PCT, MR-proADM were found throughout the whole period ( Table 2 ). The higher level of MR-proANP was statistically significant on the 1st and 6th days after surgery, MR-proANP had a tendency of increasing values on 2nd and 3rd days. CT-proAVP increased to statistical significance since the 2nd day after the operation and persisted throughout the studied period. Conclusions: PCT, MR-proADM and MR-proANP can be used as predictors of prolonged ALV for children of the first year of life after cardiac surgery with cardiopulmonary bypass. The level of CT-proAVP can be considered since the 3 day after surgery. PCT and MR-proADM may be used to predict the LOS in the ICU. MR-proANP and CT-proAVP can be considered since the 1 and 2 days after surgery respectively. Introduction: Early prediction of the risk of death among patients admitted at the Emergency Department (ED) remains an unmet need. The prognostic performance of HBP that is secreted by neutrophils was prospectively validated in a series of sequential ED admissions. Methods: HBP and elements of qSOFA were analyzed prospectively in 310 serial ED admissions (main reasons for admission: acute abdominal pain 28.4%; fever 24.5%; vomiting/diarrhea 23.9%; dyspnea 22.3%; neurologic signs 11.3%; non-specific complaints 38.1%; most patients admitted for more than one reasons). Upon ED admission patients were scored as low-risk, intermediate-risk and high-risk at the discretion of the physician. HBP was measured in blood samples upon admission by an enzyme immunosorbent assay. Results: HBP was significantly greater among patients who died very early (Fig. 1 ). In five out of six of patients dying early HBP was greater than 15 ng/ml. We combined HBP more than 15 ng/ml and the presence of one sign of qSOFA into a new score; this had 82.4% sensitivity to predict 30-day mortality. The respective sensitivity of two signs of qSOFA was 23.5% (p: 0.002). The use of this new score allowed better stratification of patients originally considered at the triage as low-risk into high-risk (Fig. 2) . Conclusions: We propose HBP more than 15 ng/ml and one qSOFA sign as an early score for 30-day mortality at the ED. Introduction: Despite of our growing knowledge in pathophysiology of septic shock still remain one of the most important factors of hospital mortality. It is thought that early diagnosis and treatment at early stage of septic shock would decrease its mortality. There have been on-going studies in recent years which research the usability of Heparin Binding Protein (HBP) in early diagnosis of sepsis [1] . To seek the usability of C-reactive protein (C-RP), procalcitonin (PCT) and HBP biomarker combination in early diagnosis of septic shock. Methods: 30 patients, who have the diagnosis of septic shock, that are expected to stay in intensive care unit more than 24 hours, and aged between 22-75 are included in the study. Data are collected from the patients' blood samples that are drawn on admission, on the 24th hour, and on the day of discharge or death. Results: It has been found in our study that, best "cut-off" value 124 ng/mL, specificity 0.82 and sensitivity 0.77 for HBP. Compared with other biomarkers, HBP was the best predictor of progression to organ dysfunction (area under the receiver operating characteristic curve (AUC) = 0.801). Conclusions: Although there have been many biomarkers for early diagnose of septic shock, C-RP and PCT are the most common used markers in nowadays' clinical practice. The usability of HBP in early diagnosis of sepsis is still being researched. We concluded that PCT, C-RP and HBP biomarker combination is usable to diagnose septic shock at the end of our study. Introduction: Reduced ADAMTS-13 and increased von Willebrand Factor (vWF)/ADAMTS-13 ratio have been observed in sepsis and are associated with the severity of the disease [1, 2] . However, their change during the septic episode and in the event of a change in the clinical status of the septic patients has not been investigated. The aim of the study was to assess the variation of these hemostatic parameters in critically ill patients during the course of a septic episode. Methods: We monitored 34 septic patients admitted in the Intensive Care Unit (ICU). 23 improved (group A) while 11 deteriorated (group B). We assessed vWF, ADAMTS-13 and the vWF/ADAMTS-13 ratio on admission in ICU (time point 0) and at the time of a change in patients' clinical condition (remission or deterioration, time point 1). Results: In group A, ADAMTS-13 and the vWF/ADAMTS-13 ratio did not significantly change (567.0±296.0 vs 670.7±534. 5 Conclusions: Hemostatic disorders, as assessed by vWF and ADAMTS-13 levels were detected in septic patients, while their changes differed according to the evolution of the septic episode. ADAMTS-13 changes may be associated with outcome. Methods: 100 adult patients with at least one sign of qSOFA and infection or acute pancreatitis or after operation were prospectively followed-up. Blood was sampled the first 24 hours; those with HIV infection, neutropenia and multiple injuries were excluded. Sepsis was diagnosed using the Sepsis-3 criteria. Soluble urokinase plasminogen activator receptor (suPAR) was measured by an enzyme immunoassay. Results: Sixty patients were classified with sepsis using the Sepsis-3 definitions. Presence of at least two signs of qSOFA had 56.7% sensitivity, 95.0% specificity, 92.8% positive predictive value and 38.0% negative predictive value for the diagnosis of sepsis. The integration of qSOFA signs and suPAR improved the diagnostic performance ( Fig. 1) . Conclusions: Conclusions Two signs of qSOFA have significant positive prognostic value for sepsis but low sensitivity. This is improved after integration with suPAR. The INTELLIGENCE-1 study is supported by the European Commission through the Seventh Framework Programme (FP7) HemoSpec. Introduction: Sepsis is a frequent reason for admission in the Emergency Department (ED) and its prognostic mainly relies on early diagnosis. In addition, no validated prognostic tool is currently available. Therefore, identification of patients at high risk of worsening in the ED is key. The TRIAGE objective was to assess the prognostic value of a blood marker panel to predict early clinical worsening of patients admitted in the ED with suspected sepsis. Methods: TRIAGE was a prospective, multicenter (11 sites in France and Belgium) study on biological samples conducted in partnership with bioMerieux S.A. Patients admitted in the ED with suspected or confirmed community-acquired infection for less than 72h were included. Exclusion criteria were: admission in the ED for more than 12 hours, septic shock at admission, immunodepression, sepsis syndrome 30 days prior to admission. The protocol included 5 clinical and biological time points (H0, H6, H24, H72, D28). Patients were classified in 3 groups at admission (infection, sepsis, severe sepsis) and divided into 2 evolution/prognosis groups depending on worsening or not from their initial condition to severe sepsis or septic shock and SOFA score's evolution. The evolution criteria were centrally evaluated by an independent adjudication committee of sepsis experts including emergency physicians and intensivists. Patients were followed up to day 28 for mortality. Results: The study duration was 3 years with 600 patients included (102 excluded). The centralized analysis is in progress to select the combination of biomarkers with the best prognostic performance comparing both evolution/prognosis groups. Currently, 125 patients have been classified as worsening and some results will be available in 2018. Conclusions: TRIAGE is the largest prospective multicenter study assessing the prognostic value of a panel of blood markers in EDs which could help identification of septic patient at risk of worsening at time of admission in the ED and develop specific management. Introduction: Immune status characterization in Intensive Care Unit (ICU) patients presents a major challenge due to the heterogeneity of response. In this study, the FilmArray® system was used with customized gene assays to assess the immune profile of critically-ill ICU patients compared to healthy volunteers; from within the REALISM cohort. Methods: A customized FilmArray® pouch containing 24 assays was designed; 16 target and 8 reference genes. Detection and semiquantification of assays from whole blood collected in PAXgene tubes occurs in the device within 1 hour. A total of 20 subjects from the REALISM cohort were tested in duplicates: 1 trauma, 5 septic shock and 5 surgery patients, along with 9 healthy volunteers. The patients' selection was based on HLA-DR expression on monocytes, and PHA-(Phytohaemagglutinin) stimulated T-cell proliferation assay, to have various immune profiles. Results: Quantification cycle values of the target genes were normalized by the geometrical mean of reference genes to account for the different cell counts among specimens. The number of the CD3+ cells and HLA-DR, determined by flow cytometry, showed good correlation to CD3D and CD74 gene expression, respectively. Seven genes showed significant differences in expression levels between the healthy volunteers and patient groups: CD3D, CD74, CTLA4 & CX3CR1 were down-regulated, while IL-10, IL1RN and S100A9 were up-regulated in the patient populations. The use of relative quantitative difference of some markers was able to distinguish and Introduction: Early, rapid diagnosis is integral to the efficient effective treatment of sepsis; however, there is no gold standard for diagnosis, and biochemical surrogates are of limited and controversial utility. The CytoVale system measures biophysical properties of cells by imaging thousands of single cells per second as they are hydrodynamically stretched in a microfluidic channel. This platform has been shown to measure dozens of mechanical, morphological, and cell surface biomarkers of WBC activation simultaneously [1, 2] . In this study, we show the performance of the CytoVale system in measuring biophysical markers for sepsis detection in the emergency department (ED). Methods: We conducted an IRB-approved prospective cohort study of Emergency Department (ED) patients with 2+ SIRS criteria and evidence of organ dysfunction. 307 patients were included for analysis. Blood samples for the Cytovale assay were collected in the ED, and the diagnosis of sepsis was adjudicated by blinded clinician review of the medical record. Captured imaging data were analyzed using computer vision to quantify mechanical parameters per cell, and a logistic model was trained to discriminate patients who had sepsis from those who did not. Results: We found substantial biophysical differences between cells from septic and non-septic patients as observed at both the single cell level (Fig. 1) and when looking at the overall leukocyte populations (Fig. 2) . A multiparameter classification algorithm to discriminate septic from non-septic patients based on biophysical markers currently yields a sensitivity of 88% with a negative predictive value of 95%. Conclusions: In patients presenting to the ED with 2 of 4 SIRS criteria and evidence of organ dysfunction, the CytoVale system provides a potentially viable means for the early diagnosis of sepsis via the quantification of biophysical properties of leukocytes. Oxidative stress and other biomarkers to predict the presence of sepsis in ICU patients V Tsolaki, M Karapetsa, G Ganeli, E Zakynthinos ICU, Larissa, Greece Critical Care 2018, 22(Suppl 1):P040 Introduction: Early identification of sepsis adds a survival benefit in ICU patients. Several biomarkers have been evaluated, yet an optimal marker is still lacking [1] . Methods: We prospectively determined oxidative status in patients admitted in a general Intensive Care Unit of the University Hospital of Larisa. Oxidative status was determined measuring the novel static (sORP) and capacity (cORP) oxidation-reduction potential markers. Other biomarkers (BNP, presepsin, CRP) were measured, and the discriminative properties for the detection of sepsis were evaluated. Results: Oxidative status was evaluated in a hundred and fifty two consecutive patients. Patients with severe sepsis and septic shock had significantly higher sORP values than patients without sepsis ( Introduction: C-reactive protein (CRP), is reported to be an effective marker for the assessment of vascular inflammation activity and acute coronary events prediction [1] .We hypothesized that preoperative CRP elevation is related to the occurrence of postoperative adverse cardiovascular outcomes. Methods: We prospectively included patients scheduled to undergo different vascular surgeries from december 2016 to september 2017. we assessed demographic data, comorbidities, revised cardiac risk index (RCRI) and biomarkers (CRP, cardiac troponin high sensitive Ths, creatinine and urea) in the preoperative period. we also noted type and duration of surgery, intraoperative blood loss, ICU stay and mortality. we evaluated CRP as a predictive marker of major cardiovascular events defined as chest pain, Ths elevation, electrocardiogram changes, arrhythmia, pulmonary embolism, stroke occuring within postoperative 3 months. Results: During our study, 30 patients were scheduled to undergo vascular surgeries. From the 30 patients, 66% developed adverse cardiac events (Table 1) . We showed the predictive value of CRP in major cardiovascular event in a ROC analysis (Fig. 1) . The cuttoff value of CPR was 54 giving 85% of sensitivity and 82% of specificity. Conclusions: Our study pointed out that CRP preoperative elevation could have a very strong predictive value of post-operative cardiovascular events in vascular surgery, this is in line with results showed by previous studies [1] . Introduction: Elderly are particularly susceptible to bacterial infections and sepsis, and they comprise an increasing proportion of intensive care unit (ICU) admissions. Our aim was to evaluate the impact of age on critically ill infected patients. Methods: We performed a post-hoc analysis of all infected patients admitted to ICU enrolled in a 1-year prospective, observational, multicenter study involving 14 ICUs. Patients aged <65, 65-74 and >=75 years were compared (group A, B, and C). Multidrug-resistance (MDR) was defined as acquired non-susceptibility to at least one agent within three or more antimicrobial categories. Results: Of the 3766 patients analyzed, 1652 (43.9%) were infected on ICU admission. Of these, 828 (50%) belonged to group A, 434 (23%) to group B and 440 (27%) to group C. Group C were more dependent, had higher SAPS II and Charlson scores (p<0.05). ICU and hospital length of stay did not differ between groups. Microorganism isolation and bacteremia were higher in group B (53% and 24%, respectively) than groups A (45% and 19%, respectively) and C (47% and 17%, respectively; p<0.05). Septic shock was present in 58% of patients and was more frequent in groups B (55%) and C (55%) than group A (48%). The most common sources of infections were respiratory and intra-abdominal. Isolation of gram-negative bacteria was significantly increased in group B and C (p=0.034). The most common isolated bacteria were Escherichia coli (17%), Staphylococcus aureus (15%) and Pseudomonas aeruginosa (8%) for all groups. In total, 151 isolates (22%) corresponded to MDR bacteria, of which 57% were Staphylococcus aureus. Age was not a risk factor for infection by MDR. All-cause mortality in ICU and hospital was: 23% and 30%; 29% and 40%; 36% and 53% -respectively for groups A, B, and C (p < 0.001). Conclusions: Old patients (65-74 years) were more prone to present with bacteremia, which could account for the increased severity of sepsis and higher all-cause mortality. Age was not a risk factor for MDR infection. Introduction: The rapid identification of pathogens using patient samples is crucial. Delays in this can potentially have serious implications for patients and infection prevention/control [1] . The aim of this project was to identify the number of microbiology samples sent, the number rejected and reasons for rejection, with the intention to reduce such instances. Methods: Data was collected retrospectively on ICU admissions from January-June 2017 to a university hospital in the UK. Patients were identified and data collected using the Intensive Care National Audit and Research Centre (ICNARC) database and from electronic patient records. Data collected included: demographics, length of stay, microbiology samples sent and details on the rejected samples. Results: 530 patients were identified with a total of 4725 (median: 4 samples/patient) samples sent to microbiology. 144 were rejected (3%). 100 (18%) patients had at least 1 sample rejected. The median number of samples rejected per patient was 1 (Range: 1-10). The Fig. 1 (abstract P041). The area under the curve for CRP elevation is 0.891 most common samples rejected were urine (22%), blood (20%), faeces (19%) and sputum (8%). 69 (48%) of the samples were resent for testing (median 1 day; range 0-20). Reasons for sample rejection are shown in Table 1 . Most rejections occurred within 48-hours of admission ( Fig. 1) . Conclusions: This study confirms a high number of samples are sent to microbiology. Although a few are rejected, overall this represents a large number, with most occurring during the first days of admission. Reasons for sample rejection are remedial through improved training and vigilance. A bespoke guide to sample collection for microbiology coupled with a training program for healthcare professionals has been introduced with the aim to reduce sample rejections from 3% to 0.5%. Introduction: Careful hand hygiene of health-care workers (HCWs) is recommended to reduce transmission of pathogenic microorganisms to patients [1] . Mobile phones are commonly used during work shifts and may act as vehicles of pathogens [2, 3] . The purpose of this study was to assess the colonization rate of ICU HCWs' mobile phones before and after work shifts. Methods: Prospective observational study conducted in an academic, tertiary-level ICU. HCWs (including medical and nursing staff) had their mobile phones sampled for microbiology before and after work shifts on 6 different days. Samples were taken with eSwab in a standardized modality and seeded on Columbia Agar plus 5% sheep blood. A semiquantitative growth evaluation was performed at 24 and 48 hours after incubation at 35°C. Results: Fifty HCWs participated in the study (91% of department staff). One hundred swabs were taken from 50 mobile phones. Fortythree HCWs (86%) reported a habitual use of their phones during the work shift, and 38 of them (88.4%) usually kept their mobiles in the uniform pocket. All phones (100%) were positive for bacteria. The most frequently isolated bacteria were Coagulase Negative Staphylococcus, Bacillus sp. and MRSA (97%, 56%, 17%, respectively). No patient admitted to the ICU during the study period was positive for bacteria found of HCWs' mobile phones. No difference in bacteria types and burden was found between the beginning and the end of work shifts. Conclusions: HCWs' mobile phones are always colonized mainly by flora resident on HCW's hands, even before the work shift and irrespective of the microbiological patients' flora. Further studies are warranted to investigate the role of mobile phones' bacterial colonization in the ICU setting and to determine whether routine cleaning of HCWs' mobile phones may reduce the rate of infection transmission in critical patients. Methods: Sixty samples were collected from AICU (n=25), PICU (n=15) and OR (n=20) during August to September 2017. Samples were randomly selected and taken at the end of the HCWs duty with a sterile swab covering all MP surfaces. The inoculation was made into blood sheep and eosyn methilene blue agar for culture. Isolated bacteria were identified according to standard microbiological techniques. Antibiotic sensitivity testing was performed using disc diffusion method. Results: Overall MP bacterial colonization rate was 95%. Main results are detailed in Table 1 . Most common non pathogenic bacteria was Staphylococcus epidermidis n=18 (90%). Isolated pathogenic bacteria Conclusions: We found high rates of MP colonization with pathogenic bacteria. An educational program is necessary to reduce the contamination and transmission of these high risk microorganisms. Introduction: The objective of this study was to evaluate the variability in the dynamics and levels of airborne contamination within a hospital Intensive Care Unit in order to establish an improved understanding of the extent to which airborne bioburden contributes to cross-infection of patients. Microorganisms from the respiratory tract or skin can become airborne by coughing, sneezing and periods of increased activity such as bed changes and staff rounds. Current knowledge of the clinical microflora is limited however it is estimated that 10-33% of nosocomial infections are transmitted via air. Methods: Environmental air monitoring was conducted in Glasgow Royal Infirmary ICU, in the open ward and in patient isolation rooms. A sieve impactor air sampler was used to collect 500 L air samples every 15 minutes over 10 hour (08:00-18:00 h) and 24 hour (08:00-08:00 h) periods. Samples were collected, room activity logged and the bacterial contamination levels were recorded as CFU/m 3 of air. Results: A high degree of variability in levels of airborne contamination was observed over the course of a 10 hour day and a 24 period in a hospital ICU. Counts ranged from 12-510 CFU/m 3 over 24 hours in an isolation room occupied for 10 days by a patient with C. difficile infection. Contamination levels were found to be lowest during the night and in unoccupied rooms, with an average value of 20 CFU/m 3 . Peaks in airborne contamination showed a direct relation to increased room activity. Conclusions: This study demonstrates the degree of airborne contamination that can occur in an ICU over a 24 hour period. Numerous factors were found to contribute to microbial air contamination and consideration should be given to potential improved infection control strategies and decontamination technologies which could be deployed within the clinical environment to reduce the airborne contamination levels, with the ultimate aim of reducing healthcareassociated infections from environmental sources. New practice of fixing the venous catheter of the jugular on the thorax and its impact on the infection F Goldstein, C Carius, A Coscia QuintaD'or, Rio de Janeiro, Brazil Critical Care 2018, 22(Suppl 1):P047 Introduction: Central Line-associated Bloodstream Infection (CLABSI) is an important concern in the ICU, mainly in those with a high density of use of central venous catheter. Any measures that may have an impact on the reduction of CLABSI are important in reducing morbidity and mortality of hospitalized patients. Therefore we present a retrospective study comparing the fixation site (neck vs. thorax) of the catheters implanted in the jugular vein, guided by ultrasonography and evaluating its impact on the incidence of CLABSI. The purpose of our study was to identify if there is any positive impact on the reduction of CLABSI when the catheter is fixated on the thorax. Methods: A retrospective unicentric study comparing the infection rates between the year of 2012, when the traditional technique of catheter fixation on the neck was used, and 2015, when 100% of the catheters were fixated on the thoracic region. The criteria for CLABSI were defined by the Infection Commission of QuintaD`or Hospital and the data on CLABSI were provided by the same commission. During this period there were no changes in the team of our unit and the patient's profile was the same. No deep vein catheter impregnated with antibiotics were used in the patients included in the study. The comparison used Fisheŕs test as a tool. All the patients hospitalized in the intensive care unit with indication of the central venous catheter of short permanence in the internal jugular vein were included. Patients with the central venous catheter of short permanence in other topographies, patients with hemodialysis catheter or with PICC were excluded. Results: During the year of 2012, 98 internal jugular vein catheters were installed in our unit using the traditional technique, fixing the catheter on the neck. In this period, 6 cases of CLABSI were detected. On the other hand, in the year of 2015, 127 internal jugular vein catheters were installed in the same unit, all of them, using the thorax as the point of fixation. Although the number of catheters installed this year was higher, there was no case of CLABSI. It appears that this position, provides a better fixation of the catheter, avoiding that the bandage gets uncovered. Conclusions: During the year of 2015, though there were more patients using deep vein catheters of short permanence, we had less CLABSI events on our unity compared to the year of 2012. Fisher's exact test identified a p-value of this association of 0.476. Fixation of the internal jugular vein catheter in the thorax seems to contribute to the prevention of CLABSI. Further prospective and randomized studies are required to evaluate the contribution of fixation of the jugular vein catheter in the thorax in the CLABSI prevention. Introduction: The oral cavity of a patient who has been hospitalized presents a different flora from normal healthy people. After 48h hours of hospital stay, the flora presents a bigger number of microorganisms that can be responsible for secondary infections, like pneumonia, because of their growth and proliferation. The objective of our study was to assess the dental plaque index on patients on admission to an Intensive Care Unit, and reassess 7 days later, to evaluate the efficacy of oral hygiene. Methods: Prospective, descriptive and observational study in an Intensive Care Unit of the CHP. Demographic, admission motive, hospital length of stay, feeding protocol, respiratory support need and oral hygiene protocol data was collected. The Greene & Vermillion Simplified Oral Hygiene Index (IHO-S) was used as the assessment tool on the first 24h and on 7th day. Results: 74 patients were evaluated, 42 of which were excluded for not meeting the minimal dentition. 32 patients had a mean age of 60,53 ± 14,44 years, 53,1% were males and most of medical and surgical scope (37,5% each). Mean hospital length of stay was 15,69 ±6,69 days. The majority of patients were sedated (75%), under ventilator support (81,3%) and with enteric nutritional support, under nasogastric tube feeding. Initial IHO-S score was 0,67±0,45, rising to 1,04±0,51 (p<0,05) 7 days later. Conclusions: Various studies have proven the importance of a good oral hygiene to avoid bacterial growth and reduce the risk for nosocomial infections. In this study, we've observed a significant worsening of oral hygiene one week after admission. Although this could be unimportant for a one week staying patient, it could indicate an increased risk for nosocomial infections for longer staying patients, which could benefit from a more efficient oral hygiene protocol. Positive pocket cultures and infection risk after cardiac electronic device implantation-a retrospective observational single-center cohort study P Pekić 1 Methods: We performed a retrospective observational single-center cohort study on 251 patients who received de novo implantation of pacemaker, cardioverter-defibrillator or cardiac resynchronization therapy device in a two-year period. Each patient was implanted using standard aseptic procedure according to local protocol and antibiotic (cefazolin) prophylaxis before the procedure. Pocket aspirate was taken after irrigating the wound with normal saline just before device placement. Results: We analyzed 251 patients (58.6% male, 41.4% female). The most often implanted device was a DDD pacemaker followed by a VVI pacemaker. Mean length of hospital stay was 12.02±8.34 days. There were 54 (21.5%) positive cultures with overall 3 (1.19%) clinically apparent infections which required prolonged iv antibiotics, removal of device and reimplantation after infection resolution. In regard to microbiology, S. epidermidis (48.2%) and coagulase negative Staphylococcus (29.6%) were the most often finding which is in contrast to the cultures described in the literature. The only statistically significant risk factor for positive pocket culture was male sex and presence of a urinary catheter. Invasive vascular devices, previous intrahospital infection, and diabetes were not found to increase the likelihood of positive pocket culture. Conclusions: Positive pocket cultures after CIED implant are a frequent finding mostly due to contamination and colonisation. The risk factors for such a finding differ from the usual and expected clinical circumstances. Our results are consistent with those in the literature. It turns out that the most important preventive measure in CIED implantation is strict aseptic procedure. Introduction: Intensive care patients are in constant risk of contamination due to suppression of their immune system, use of invasive procedures and medical equipment and health associated infections (HAI). Chlorhexidine Gluconate (CHG) is an antiseptic and disinfectant product. In medical research it has been found that daily CHG bathing is affective in reducing levels of skin and central line related infections (Climo, 2013) . It is also referred to in the recommendations of the ministry of health "prevention of septicemia due to central lines" (2011). Methods: Unit guide lines for patient Dry Bathing were written in May 2015 and thereafter began the implementation and instruction of nursing staff. Quality control was inspected by observation. There was a 15 phase questioner that included several categories such as: preparation of the CHG solution, staff protection actions, infusions and surgical wound dressings, bathing performance and documentation. Results: A gradual rise of 97%was observed in theperformance ofdry bathing according to the unit guidelines Conclusions: 97% of observed dry baths where performed according to the guide lines. Points for improvement: Correct care of infusions and surgical wound dressing and verify use of separate wipes for each body part. Next we will examine the correlation between the use of dry baths and theextent of infections in the unit. Dry Baths are nowconsidered an integralpart of the daily nursing routine. They have no substantial costs, help prevent complications from infection and add to the patient's safety. Introduction: Despite reductions in mortality reported with SDD, concerns about bacterial resistance and alteration of microbiome limit use. A retrospective observational study was conducted into the effect of local SDD protocols on VAP rates and resistance patterns. Over a 2-year period, 2 regimens were used dependent on drug availability and hospital antibiotic stewardship concerns. The study was designed to review practice and identify any risks of partial implementation. Methods: Patients ventilated on a general intensive care were identified via clinical information systems. Three periods were reviewed for adherence to SDD protocols, Pre SDD (Jan -Feb 14), Full (July -Sept 15) and Partial (July -Sept 16). High-risk patients during both SDD periods also received IV antibiotics for 96 hours. Patients admitted with pneumonia or tuberculosis were excluded from VAP analysis. Remaining patients' records were reviewed and the Clinical Pulmonary Infection Score (CPIS) calculated for each ventilated day to identify VAP rates. Positive respiratory microbiological results for all patients admitted to the ICU during each time period were reviewed to assess for wider changes in local resistance patterns. Results: Protocol adherence was assessed in 71 patients during the full SDD period and 70 during the partial ( Table 1 ). The number of patients included for analysis of VAP rates during each period was 38 pre SDD, 50 during full SDD and 37 during partial SDD. There were no significant changes in resistance patterns or Clostridium difficule rates (Table 2) . Conclusions: Compliance with the available enteral antibiotics was reasonable but with IV antibiotics was poor. It is accepted that alterations and non-adherence to protocols risk development of resistant bacterial strains. Within our unit no decrease in VAP rates was seen but reassuringly no increased rates of extended bacterial resistance were identified during the treatment periods. Introduction: Arterial catheters are commonly used in Intensive Care Units (ICU) and are among the most frequently manipulated vascular access devices. Our aim was to evaluate the rate of arterial catheterrelated bloodstream infection and colonization. Methods: This was a 12-month, prospective and monocentric cohort study, performed in a multipurpose ICU. All arterial catheters, inserted in or presented to the ICU, were cultured and assessed for colonization or catheter-related bloodstream infection (CRBI). Results: We enrolled 119 patients (63.8% males, average age 59±17 years, SAPS 2 42±21) of whom a total of 141 arterial catheters were analyzed for a total of 1552 catheter-days. Radial arterial catheters were inserted in 88.7% (n=125), femoral arterial catheters in 7.8% (n=11) and other arterial catheters in 3.5% (n=5). Signs of dysfunction were found in 28.8% and 45.5%, respectively. Radial arterial catheters colonization (n=5) and CRBI (n=1) occurred at a rate of 3.0 and 0.8/ 1000 catheter-days. Femoral arterial catheters colonization (n=2) and CRBI (n=1) occurred at a rate of 10.8 and 5.4/1000 catheter-days, respectively. Mean catheter time insertion was significantly higher in colonized catheters/CRBI (21±8 days; 95% CI: 14-28) when compared to arterial catheters with negative cultures (10±8 days; 95% CI: 9-12); p = 0.002). Colonized lines showed Acinetobacter baumannii (n=3), Staphylococcus epidermidis (n=1), Enterococcus spp (n=1) and Pseudomonas aeruginosa (n=1). CRBI were caused by Staphylococcus epidermidis (n=1) and Staphylococcus haemolyticus (n=1). Conclusions: The incidence of radial arterial catheters colonization and CRBI were lower than reported rates in literature. Colonization and CRBI rates were higher in femoral catheters. Femoral catheters showed dysfunction more frequently. Prolonged catheterization was associated with colonization and CRBI. A multimodality approach to decreasing ICU infections by hydrogen peroxide, silver cations and compartmentalization and applying acinetobacter as infection marker Introduction: Nosocomial infections at the intensive care unit (ICU) represent a substantial health threat [1, 2] . ICU infections are mainly attributed to the extended hospital delay which results in high morbidities and mortalities. Methods: A cross sectional study was conducted at the Intensive Care Unit, Aseer Central Hospital, Saudi Arabia over 13 months period (2014) (2015) . The intervention program included the application of mist of hydrogen peroxide and silver cations, physical separation and compartmentalization of the intensive care unit. The GLOSAIR™ 400 System was used to deliver a mist of hydrogen peroxide and silver cations. Hydrogen peroxide is an oxidizing agent, which kills microorganisms. Results: A total of 103 strains of Acinetobacter species were identified from the patients over the 13 months period (Fig. 1) . The mean infection rates decreased from 14.3 in the first three months of the program to 4 in the last three month after continuous. Conclusions: The program using the three procedures offered a significant decrease in infections at the ICU as measured by Acinetobacter count, which is one of the most hazardous nosocomial pathogens. Introduction: The efficacy of ß lactam antibiotics is related to the time above MIC. Continuous or extended infusions can be used to increase the time above MIC, especially in patients with normal or increased drug clearance. Administering antibiotics by continuous infusion is not a new concept. A review in 1992 looks at the outcomes of continuous infusions [1] . More recently an improvement in mortality has been demonstrated [2] . Our perception was that uptake of this low cost intervention was not common, so we undertook a survey to determine how commonly continuous infusions are used in England. Methods: A telephone survey of all intensive care units in England was undertaken. Questions included: -Are you using continuous or extended antibiotic infusions? -Which antibiotics are you using for continuous or extended infusions? -If not currently using has it been considered? Data was collected over a week in June 2017. Results: There was an 87% response rate. 73 (44.5%) of the units continuously infuse some antibiotics, however 71.2% of those only infuse vancomycin and not ß lactams. Only 21 of the total responders (12.8%) infuse antibiotics other than vancomycin (i.e. ß lactams). Conclusions: The theoretical advantage of continuous infusion of ß lactam antibiotics has been described for over 20 years. There is now evidence that this may improve survival. Despite this, uptake in England has been slow. Introduction: Infections contribute to a significant proportion of morbidity and mortality worldwide. While many infections are successfully managed with antimicrobial therapy, rates of antimicrobial resistance (AMR) are increasing. Certain patient populations such as those admitted to intensive care units (ICU) are at high risk. Methods: We conducted a retrospective, observational study of all ICU patients at a tertiary referral hospital in Rwanda from January 2015 through December 2016 We collected data on diagnosis, ICU length of stay, mortality and hospital length of stay, as well as microorganism, site of culture, AMR and antibiotics prescribe. Results: Overall, 331 patients were admitted to the ICU. Most patients were admitted from the main operating theater (n=150, 45%).The most common admitting diagnoses were sepsis (n=113, 34%), head trauma (n= 90, 27%). A total of 268 samples were collected from 331 patients. The samples were from blood (n=110, 33%), tracheal aspirate (n=22, 7%),. The most common organisms isolated were Klebsiella (n=30, 29%), Acinetobacter (n=20, 19%), E.coli (n=16, 15%), Proteus (n=15, 14%), Citrobacter (n=8, 8%), S aureus (n=7, 7%), Pseudomonas (n=5, 5%), and other (n=9, 9%). Of Klebsiella isolates, 100% and 76% were resistant to ceftriaxone and cefotaxime, respectively. Of E.coli isolates, 86% and 71% were resistant to ceftriaxone and cefotaxime, respectively. All Acinetobacter isolates were resistant to ceftriaxone and cefotaxime. Conclusions: There is an alarming rate of antimicrobial resistance to commonly used antibiotics in the ICU. Expanding antibiotic options and strengthening antimicrobial stewardship are critical for patient care. The last three days G Latten 1 , P Stassen 2 1 Zuyderland MC, Sittard-Geleen, Netherlands, 2 Introduction: This study provides an overview of the prehospital course of patients with a (suspected) infection in the emergency department (ED). Most research on serious infections and sepsis has focused on the hospital environment, while potentially most delay, and therefore possibly the best opportunity to improve treatment, lies in the prehospital setting. Methods: Patients were included in this prospective observational study during a 4 week period in 2017. All patients aged 18 years or older with a suspected or proven infection were included. Prehospital, ED and outcomes were registered. Results: In total, 2452 patients visited the ED during the study period, of whom 440 (17.9%) patients had a (suspected) infection. (Fig. 1 ) Median duration of symptoms before ED visit was 3 days (IQR 1-7 days), with 23.9% of patients using antibiotics before arrival in the ED. Most patients (83%) had been referred by a general practicioner (GP), while 41.1% of patients had visited their GP previously during the current disease episode. Twenty-two patients (5.0%) experienced an adverse outcome (ICU admission and/or 30-day all-cause mortality): these patients were less often referred by a general practicioner (GP) (59.1 vs. 84.2%, p=0.001) and were considered more urgent both by EMS and in the ED. Conclusions: The prehospital phase of patients with an infection provides a window of opportunity for improvement of care. Patients become ill 3 days before the ED visit and 41.1% already visited their GP previously during the current disease episode, while 23.9% is currently using antibiotics. Future research should focus on quality improvement programs in the prehospital setting, targeting patients and/or primary care professionals. Introduction: Worldwide, the prevalence of tetanus has decreased.-However, even if progress has been made in the combat to eradicate tetanus it may be a cause of admission to intensive care.The objectives of our study are to determine epidemiological,clinical and prognostic characteristics for severe tetanus in our unit. Methods: We conducted a retrospective study in the medical intensive care unit of Ibn Rushd hospital in Casablanca in Morocco from 2010 to 2016.We studied the epidemiological,clinical and prognostic characteristics of the patients who were admitted for severe tetanus. Results: The incidence of severe tetanus was 2.04% affecting male in 100%.41.9% were aged between 31 and 40 years old. In 85.7% there were a integumentary portal of entry. Contractures were present in 69%of the cases. At intensive care unit admission, 21.4% of the patients were sedated. The anti-tetanus vaccination was never updated. According to the Dakar score 28.6% of the patients were listed Dakar 1, 54.8% Dakar 2 and 16.6% Dakar 3. For the Mollaret score, the crude form was found in 44.2%, the acute generalized form was found in 32.6% and the severe form in 20.9% of the cases.Mechanical ventilation was necessary in 83.3%. Diazepam and baclofen were used in 92.9%, phenobarbital in76.2% and propofol in 42.85%. A serotherapy was used for all the patients and a preliminary vaccination dose for 26.9%. All the patients received antibiotics, penicillin G 33.33% and metronidazole 76.2%. The mortality was 61.9%. The length of intensive care stay was significantly higher. The need for an intubation,its duration and the occurrence of autonomic dysfunction have significantly influenced the mortality. Conclusions: To improve the prognosis in these serious forms of tetanus,it is highly important to identify the warning signs and refer patients in intensive care for early and appropriate management in intensive care. Introduction: Bloodstream infections (BSIs) are associated with increased mortality in the ICU. The aim of the study was to evaluate the epidemiology and resistance patterns during the period 2013 to 2017. Methods: Bacteria and fungi isolated from the blood of patients hospitalized in a mixed ICU during the study period were retrospectively analyzed. Sensitivity testing was performed with disk diffusion (Kirby-Bauer) and Microscan Walkaway 96 plus for minimal inhibitory concentrations. Results: During the study period 1198 patients were hospitalized in the ICU. BSIs were diagnosed in 284 cases (23.7%). The isolated microorganisms were Acinetobacter baumannii (29%), Klebsiella pneumoniae (15%), other Enterobacteriaceae (8%), Pseudomonas aeruginosa (6%), Stenotrophomonas maltophilia (1%), enterococci (20%), staphylococci (8%) and Candida spp. (13%). Of the A. baumannii isolates, 97% were resistant to carbapenems, 9.6% to colistin, and 31% to tigecycline. Of the K. pneumoniae isolates 80% were resistant to carbapenems, 70% to colistin, and 4.5% to tigecycline. Of the P. aeruginosa species 44% were resistant to carbapenems and they were all susceptible to colistin. The rate of resistance to vancomycin was 56% for the E. faecium isolates, 5.5% for the E. faecalis, while the resistance to methicillin of the coagulase negative staphylococci was 90%. The most commonly isolate species of Candida was C. albicans. Conclusions: Multi-drug resistant isolates, especially A. baumannii and Enterobacteriaceae, are a serious problem in our ICU. Gram positive bacteria are less common, but the resistance of enterococci to vancomycin is significant. Antibiotic stewardship and infection control measures should be applied in a more strict way. Nosocomial sinusitis in intensive care unit patients I Titov 1 Introduction: Nosocomial sinusitis (NS) is a complication of critically ill patients which develops 48-72 h after admission and is mostly linked but not limited to such invasive procedures as nasotracheal intubation and nasogastric tube placement. NS is often overlooked as a source of pyrexia of unknown origin, meningeal manifestations, sepsis and ventilator associated pneumonia in ICU patients. CT scanning and sinus puncture are used to confirm the inflammatory process and identify the pathogen behind it. Methods: A retrospective case study of 6.479 ICU patients for a period of 2012-2016 was performed. We have analysed data from the CT scans of paranasal sinuses and bacteriological findings of samples obtained from sinus puncture. Results: 644 (9.9%) patients were suspected of NS on the 5-7th day of stay in the ICU. The CT scan confirmed pathological changes in 464 patients (7.1%). Hemisinusitis was detected in 422 patients (90.9%) and pansinusitis in 41 patients (8.8%). There was also an isolated case of maxillary sinusitis in 1 patient (0.2%). The pathogenic culture was identified only in 297 (64%) samples, 34.6% of which revealed isolated bacteria and 65.4% a polymicrobial association. Gram positive bacteria were detected in 16.1% of cases and Gram negative in 49.5%. Most cases revealed multiple antibiotic resistance. Conclusions: 1. NS has proved to be largely caused by Gram negative bacteria and polymicrobial associations. The use of broad spectrum antibiotics in ICU may justify the presence of sterile cultures. 2.Early identification of risk patients in ICU as well as the use of screening CT scan may benefit timely diagnosis and adequate treatment of patients. 3 .Preventive considerations include: patient's bed head elevation, the use of oral gastric tube in sedated and coma patients on ventilation, nasotracheal intubation only if indicated, removal of nasogastric tube at night, proper hygiene. Conclusions: Only 78 of 4,011 TB patients (2%) required critical care intervention (Table 1) . Those admitted to ICU were older and more likely to have pulmonary, CNS, miliary or abdominal TB (Table 2) . Mortality was high despite critical care input in a unit familiar with managing TB, and 24 hour access to Infectious Diseases advice within the trust, likely due to overwhelming organ dysfunction, patient frailty and advanced TB infection. Rates of drug resistant TB were low and comparable to UK-wide rates over that period (5% mono-drug resistant, 2% MDR) thus less likely a contributory factor to the majority of deaths. Short term antibiotics prevent early VAP in patients treated with mild therapeutic hypothermia after cardiac arrest T Daix 1 , A Cariou 2 , F Meziani 3 , PF Dequin 4 , C Guitton 5 , N Deye 6 , G Plantefève 7 , JP Quenot 8 , A Desachy 9 , T Kamel 10 , S Bedon-Carte 11 , JL Diehl 12 , N Chudeau 13 , E Karam 14 , F Renon-Carron 1 , A Hernandez Padilla 1 , P Vignon 1 , A Le Gouge 4 Introduction: Patients treated with mild therapeutic hypothermia after cardiac arrests with shockable rhythm are at high risk of ventilator-associated pneumonia (VAP) [1] . Despite retrospective trials suggesting a benefit of short-term (48h) antibiotics in this setting [2] , it is not recommended. The primary objective was to demonstrate that systematic antibiotic prophylaxis can reduce incidence of early VAP (<7 days). The impact on incidence of late VAP and on Day 28 mortality was also assessed. Methods: Multicenter, placebo-controlled, double-blinded, randomized trial. ICU patients >18 years, mechanically ventilated after out-of-hospital resuscitated cardiac arrest related to initial shockable rhythm and treated with mild therapeutic hypothermia were included. Moribund patients and those requiring extracorporeal life supports, with ongoing antibiotic therapy, known chronic colonization with multiresistant bacteria or known allergy to beta-lactam antibiotics were excluded. Either IV injection of amoxicillin-clavulanic acid (1g/200mg) or placebo was administered 3 times a day for 2 days. All pulmonary infections were recorded and blindly confirmed by an adjudication committee. Results: In intention to treat analysis, 196 patients were analyzed, (treatment group n=99; mean age 60.5±14.4 years, sex ratio=4, SOFA score 8.7±3.1). Global characteristics of cardiac arrest were similar (no flow= 3.5min vs 3.8min, low-flow= 21.8min vs 18.2min). 60 VAP were confirmed incl. 51 early VAP, 19 in treatment group vs 32 in placebo group (HR=0.546; IC 95%=[0.315; 0.946]) (Fig. 1) . Occurrence of late VAP (4% vs 5.1%) and Day 28 mortality (41.4% vs 37.5%) was not affected by the study procedure. Conclusions: Short-term antibiotic prophylaxis significantly decreases incidence of early VAP in patients treated with mild therapeutic hypothermia after out-of-hospital cardiac arrest related to shockable rhythm and should be recommended. Introduction: Antibiotics are the most commonly prescribed drugs in ICU.In the era of antibiotic resistance it is difficult to choose antibiotics during septic episode.The choice antibiotics mainly depends on clinical diagnosis,culture sensitivity and local flora. Whether severity of illness really maters is not well known. To study antibiotic prescription pattern and whether the choice of antibiotic varies according to hemodynamic stability in patients admitted in ICU.To study of microbiological isolates and their variability according to hamodynamic stability in ICU patients. Methods: All ICU patients of more than 18 years age who received antibiotics and where cultures had been sent were included in the study.Patients discharged against medical advice and where treatment had been withdrawn were excluded in this study. This prospective observational study was conducted between July 2016 to March 2017.Patients were divided into stable and unstable group according to hemodynamic parameter and usage of antibiotics and microbiological isolated were correlated. ICU mortality and length of stay were correlated between hemodynamically stable and unstable group. Results: 786 sepsis episode were analysed. Mean age was 65 years, male predominant, and average APACHE IV score was 58(SD25). We had 444 patients in unstable group of which 71% patients got discharged and 86% of patients got discharged in stable group. Antibiotic combination therapy was used more in hemodynamically unstsble patients(p 0.3). BLBLI was used more in stable group. Drug resistance in microbiological isolates did not reveal any statistically significant difference among stable or unstable group. Conclusions: There is a tendency to administer combination antibiotics in sicker group of patients with hemodynamic instability. Prevalence of microbial flora did not show any statistical difference. outcome is worse in hemodynamically unstable patients. The clinical significance of Candida score in critically ill patients with candida infection H Al-Dorzi 1 , R Khan 1 , T Aldabbagh 1 , A Toledo 1 , S Al Johani 1 , A Almutairi 2 , S Khalil 2 , F Siddiqui 2 , Y Arabi 3 1 King Abdulaziz Medical City, Riyadh, Saudi Arabia, 2 MSD, Riyadh, Saudi Arabia, 3 King Saud bin Abdulaziz University for Health Sciences, Riyadh, Saudi Arabia Critical Care 2018, 22(Suppl 1):P065 Introduction: Candida score (CS) is used to identify patients with invasive candidiasis in the ICU, but its clinical use has not become widespread. Our objective was to evaluate the clinical significance of CS in a mixed population of ICU patients. Methods: This was a prospective observational study of critically ill patients who had Candida species growth during their stay in any of six different ICUs of a tertiary-care center. Two intensivists classified patients as having Candida colonization or invasive candidiasis according to predefined criteria. CS was calculated for each patient on the day of Candida species growth as follows: 1 . See text for description point for parenteral nutrition + 1 point for surgery + 1 point for multifocal Candida colonization + 2 points for severe sepsis. The Receiver Operating Characteristic (ROC) curve was plotted to assess CS ability to discriminate between invasive candidiasis and Candida colonization. Results: CS was 1.6±0.9 in patients with Candida colonization (N=261) and 2.4±0.9 in those with invasive candidiasis (N=120) (p<0.001). However, only 38.7% of invasive candidiasis cases had CS >= 3 (compared with 8.0% of Candida colonization cases; p<0.001). The ROC curve (Fig. 1) showed that CS had fair ability to discriminate between invasive candidiasis and Candida colonization (area under the curve 0.71, 95% confidence interval 0.65 to 0.77; p<0.001). In patients with invasive candidiasis, CS was similar in hospital survivors and nonsurvivors (2.2±0.9 and 2.5±0.8, respectively; p=0.13). CS did not discriminate between survivors and nonsurvivors (area under the ROC curve 0.61, 95% confidence interval 0.46 to 0.75; p<0.15). Conclusions: CS was higher in patients with invasive candidiasis than those with Candida Colonization. However, its ability to discriminate between these patients was only fair. CS was not associated with hospital mortality. Poor reliability of creatinine clearance estimates in predicting fluconazole exposure in liver transplant patients M Lugano, P Cojutti, F Pea ASUIUD, Udine, Italy Critical Care 2018, 22(Suppl 1):P066 Introduction: Invasive candidiasis (IC) is a frequent complication in liver transplant (LT) recipients, especially during the first 1-3 months after LT. Fluconazole is a triazole antifungal used for prophylaxis and treatment of IC. Due to its renal elimination, dose adjustments are usually based on estimated creatinine clearance (eCrCL). However, the reliability of eCrCL in predicting fluconazole clearance has never been investigated in this population. The aim of this study was to conduct a population pharmacokinetic (popPK) analysis in a cohort of LT patients who underwent therapeutic drug monitoring (TDM) in order to find out which covariates may influence fluconazole pharmacokinetics (PKs). Methods: This retrospective study included LT patients who were admitted to the intensive care unit of our University Hospital between December 2007 and May 2016, and who were treated with intravenous fluconazole in the first months after LT. TDM of fluconazole was performed with the intent of attaining the efficacy pharmacodynamic target (AUC24h/MIC > 55.2). The tested covariates were: age, gender, CKD-EPI eCrCL, time from LT, serum albumin and transaminases, SAPS II score. PopPK was carried out with Pmetrics software. Results: Nineteen patients (mean±SD age, weight and serum creatinine of 60±8.4 years, 75±16.8 kg, 1.0±0.62 mg/dL, respectively) with a total of 89 fluconazole trough plasma concentrations were included in the popPK analysis. Mean±SD fluconazole distribution volume (Vd) and clearance (CL) were 27.02±10.78 L and 0.55±0.19 L/h. Age and time from LT were the only clinical covariates significantly correlated with fluconazole Vd and CL, respectively. Conversely, CKD-EPI eCLCr was unable to predict fluconazole CL. Conclusions: CKD-EPI eCLCr is unreliable in predicting fluconazole exposure in LT recipients. Consistently, in this population adaptation of fluconazole dose should be based on measured CrCL, and TDM may be helpful in optimizing drug exposure. Outcomes of a candidiasis screening protocol in a medical ICU M Boujelbèn 1 , I Fathallah 1 , H Kallel 1 , D Sakis 1 , M Tobich 1 , S Habacha 1 , N Ben Salah 1 , M Bouchekoua 2 , S Trabelsi 2 , S Khaled 2 , N Kouraichi 1 Introduction: The aim is to determine the incidence, characteristics and risk factors of invasive candidiasis (IC) in critically ill patients by using a weekly screening protocol. Methods: A 9 months' prospective study was conducted in a 6-bed MICU. The candidiasis screening consisted of the culture of plastic swabs (from different body sites), urine and respiratory tract samples.It was conducted upon admission and on weekly basis for all the patients. Decision to treat was based on clinical and microbiological features. Results: 97 patients were included. The colonization rate with Candida spp was 28.8%(n=28). 415 screening samples were collected with a positivity rate at 27.9%(n=118). Table 1 describes the isolated Candida species by site. Antifungal resistance was tested in 72(62%) species. The resistance rate to fluconazole was 13.8%(n=10). The antifungal resistance of Candida albicans is detailed in Table 2 . 14(14.4%) patients presented an IC with a mean age and mean SAPS II at 54.3 ± 18 years and 48±18.7 respectively. 7(50%) presented acute renal failure upon admission. 85.7% (n=12) of the patients needed mechanical ventilation. The median length of stay was 29 days [18.5-62.5] and the mortality rate was 42.9%(n=6). The mean SOFA score upon infection was 8.5±2.79. The candida score was >= 2.5 and the colonization index was >= 0.5 in Fig. 1 (abstract P065). ROC curve for Candida score discrimintaing between invasive candidiasis and Candida colonization 92.8%(n=13) and 78.5%(n=11) of the patients respectively. Only one patient had a positive blood culture. Mannan antigen and anti-mannan antibodies were screened only in five patients with a positivity rate at 100%(n=5). The most isolated species was: Candida albicans 64.3%(n=9). Multivariate analysis showed that prior use of Imipinem more than 6 days was a risk factor for IC (OR=9.37, CI95[1.15 ; 75.8], p=0.03). Conclusions: This study showed the ecology and epidemiology of Candida species in our MICU with an increased IC rate and high mortality. Prior Imipinem use was a risk factor for IC. Introduction: ICU-acquired infection is as high as 42.7 episodes per 1000 patient-days in lower-middle income countries like India (WHO). Almost three times higher than in high-income countries [1] . Candida Infection is the 3rd most commonly acquired nosocomial infection in India burdening the debilitated patient with longer ICU stay [2] . There are no definite guidelines on whether & when to start antifungal treatment, specific to India where IFI risk is high and diagnostic facilities are limited. Currently, the Intensivists across India are using antifungals, according to their clinical experience and selective application of international guidelines leading to non-uniformity of patient outcomes. In an endeavour to synchronize anti-fungal therapy and educate intensivists from small cities of India, 2 Intensivists and 1 Infectious Disease specialist of international repute were approached to design a module on 'Invasive Fungal Infections -When to Start Anti-fungals in ICU [ Fig. 1 ]. The IFI in India was summarised into a compact 1 hour session for dissemination of knowledge using IDSA 2016 as a reference guideline. 12 Intensivists from across India were trained on the module by our faculty. The module was rolled out to Intensivists and Pulmonologists focussing particularly on the tier-2 & tier -3 cities where avenues for learning are limited [ Fig. 2 Introduction: Trichosporon species are fungi found in nature and human normal flora but they can be an opportunistic pathogen, Introduction: This study assessed whether empiric combination antibiotic therapy directed against Gram-negative bacteria is associated with lower intensive care unit (ICU) mortality compared to single antibiotic therapy. Methods: Retrospective cohort study on prospectively collected data conducted in the ICU of a tertiary care hospital in India between July2016 to March2017. All consecutive infection episodes treated with empiric antibiotic therapy and with subsequent positive culture for Gram-negative bacteria were included. Primary and secondary outcomes were all cause ICU mortality and ICU length of stay (LOS). Outcomes were compared between infection episodes treated with single vs.combination antibiotic therapy. Results: Of total 214 episodes of gram-negative infections 66.4% received combination-antibiotic therapy. Baseline demographic and clinical characteristics between single vs. combination therapy groups were similar (mean age: p=0.07; sex: p=0.3; mean APACHE IV score: p=0.07). Overall ICU mortality did not significantly differ between single and combination antibiotic groups (30.2% vs. 27%; p=0.7). In single antibiotic group, ICU mortality was significantly higher for antibiotic-resistant compared to antibiotic-sensitive bacteria (77.8% vs. 18.5%, p=0.0002). In combination group, significantly lower ICU mortality was noted if bacteria was sensitive to even one antibiotic compared to pan-resistant bacteria (21.4% vs. 63.6%, p=0.0001). ICU LOS was similar between antibiotic-sensitive bacteria and antibiotic-resistant bacteria, both in single and combination therapy groups (single, antibiotic-sensitive vs. antibiotic-resistant: mean LOS±SD 14.6±12.7 vs.12.8±11days; p=0.6; combination, antibioticsensitive vs. antibiotic-resistant: 15.5±13.3 vs.11.2 days; p=0.1). Conclusions: Irrespective of the number of antibiotics prescribed as empiric therapy, outcome of patients solely depends on the sensitivity pattern of the bacteria isolated. Pharmacokinetics of trimethoprim and sulfametrole in critically ill patients on continuous haemofiltration R Welte 1 , J Hotter 1 , T Gasperetti 1 , R Beyer 1 , R Introduction: The combination of trimethoprim and sulfametrole (TMP-SMT, Rokiprim®) is active against multi-drug resistant bacteria and Pneumocystis jirovecii. In critically ill patients undergoing continuous veno-venous haemofiltration (CVVH), however, its use is limited because of lacking pharmacokinetic data. Methods: Pharmacokinetics of both drugs were determined after standard doses in patients on CVVH and in critically ill patients with approximately normal renal function. Quantification of TMP and SMT was done by high pressure liquid chromatography (HPLC) and UV detection after pre-purification by solid phase extraction. The total clearance (CLtot) was estimated from arterial plasma levels and the haemofilter clearance (CLHF) from plasma and ultrafiltrate concentrations. Results: Six patients on CVVH (3 after the first dose, 3 at steady state) and nine patients off CVVH have been enrolled (4 after first dose, 7 at steady state). After a single dose, CLtot of SMT was 3.5 (1.8-3.8, median [range]) and 1.7 (1.1-2.7) L/h on and off CVVH, respectively. At steady state, we observed a CLtot of 1.0 (0.5-1.0) and 0.3 (0.2-0.9) L/h, respectively, on and off CVVH. Steady state trough levels (Cmin) of SMT amounted to 52-113 mg/L in patients on CVVH and 18-145 in patients off CVVH. CLtot of TMP was 4.4 (2.5-5. 3) L/h on CVVH and 5.4 (3.2-9.9) L/h off CVVH after the first dose. At steady state, its CLtot amounted to 0.8 (0.4-0.8) and 1.0 (0.6-1.9) L/h on and off CVVH, respectively. Cmin was 4-12 mg/L on CVVH and 3-9 mg/L in patients off CVVH. CLHF accounted for 22-68% of CLtot of SMT and 28-72% of CLtot TMP. Conclusions: Exposure to both antimicrobial agents is highly variable, but comparable in patients on and off CVVH. As considerable amounts of SMT and TMP are eliminated by CVVH, no excessive accumulation appears to take place during treatment with standard doses. The positive impact of meropenem stewardship intervention at a Brazilian intensive care unit W Freitas 1 Introduction: Loss of colistin as a clinical option has profound public health implications. Widespread use of colistin in agriculture and humans has seen the emergence of Mcr-1 mediated resistance amongst South African patients [1] . We sought to describe the trends of colistin minimum inhibitory concentrations (MIC) over two years using data collected by SMART. Methods: SMART monitors the in vitro susceptibility of clinical aerobic and facultative gram-negative bacterial isolates to selected antimicrobials of importance, enabling longitudinal analyses to determine changes over time. The dataset comprised bacterial isolates from four different South African private pathology laboratories and one public sector pathology laboratory from 2015 -2016. The methods used in the study have been described elsewhere [2] . Isolate proportions between years were compared using the chisquared test with Yates' continuity correction. (2) (3) (4) (5) days]; 4 patients underwent renal replacement therapy. The median treatment duration (IQR) was 14 (13-14) days. In 41.6% of cases, antibiotic-therapy therapy combination (phosphomycin and colistin) was chosen. All the patients experienced a clinical response by 72/96 hours from the ceftazidime/avibactam commencing. In 8/ 9 bacteraemic patients negativization of blood culture occurred by 96 hours as well as of the rectal swab in 5/14 patients. A (B) recurred and a second treatment was given. 11/14 (78.5%) patients survived, whereas death was caused by multi-organ failure. The susceptibility test of strains showed sensitivity to ceftazidime/avibactam, whereas 100% of resistance to carbapenems, quinolones and III/IV generation cephalosporin, tigecycline and piperacillin/tazobactam; 62.5% of susceptibility to fosfomycin and colistin; (v) less than 50% of suceptibility to aminoglicosides. Conclusions: The 14 strains of KP-cp were susceptible to ceftazidimeavibactam despite the high carbapenem-resistance recorded in our ICU, because od rare identification of KP-cp VIM/NDL +. The preliminary data seems to confirm the efficacy and clinical utility of this antibiotic for the critically ill patients. Introduction: Multidrug resistant bacteria (MDR) are an increasing problem on intensive care units. Lung infections caused by Acinetobacter baumannii are frequently difficult to treat. Phages have regained attention as treatment option for bacterial infections due to their specificity and effectivity in lysis. The aim of this preclinical study was to determine efficacy and safety of a novel phage preparation in mice. Methods: Mice were transnasally infected with a MDR A. baumannii strain [1] and 12 hours later treated intratracheally with a specific phage or solvent. Phage Acibel004 [2] was produced as suspension including efficient depletion of endotoxins. At defined time points, clinical parameters, bacterial burden in lung and bronchoalveolar lavage fluid (BALF) and cell influx were determined. Further, lung permeability and cytokine release were quantified and histopathological examination was performed. Results: Mice treated with phages recovered faster from infectionassociated hypothermia. 48 hours after infection, phage treatment led to a reduction in bacterial loads in lungs and BALF. In addition, lung permeability and cytokine production were reduced in phagetreated mice. Histopathological examination of the lungs showed less spreading of bacteria to the periphery in phage-treated mice, whereas cellular recruitment into the lung was unaffected. No adverse effects were observed. Conclusions: For the first time a highly purified phage against A. baumannii was successfully used in vivo. The current preclinical data support the concept of a phage-based therapy against pulmonary A. baumannii infections. Introduction: VAP is common in critically ill patients and associated with high morbidity and mortality, especially when caused by antibiotic resistant bacteria. Recently, phage therapy has emerged as a promising non-antibiotic based treatment of antibiotic resistant bacterial infections. However, proof-of-concept experimental and clinical studies are missing before its wider use in clinical medicine. The goal of this experimental study was to compare the efficacy of phage therapy versus antibiotics for the treatment of MRSA in a rat model of VAP. Methods: Four hours after intubation and protective ventilation, rats were inoculated via the endotracheal tube with 6-9 x 10 9 CFU (LD100) of the MRSA clinical isolate AW7. The animals were subsequently extubated. Two hours after bacterial challenge, rats were randomised to receive intravenously either teicoplanin (n= 8), a cocktail of 4 lytic anti-S. aureus bacteriophages (n=9) or combination of both (n=6). 5 animals served as control (no treatment). Survival by 96 hours was the primary outcome. Secondary outcomes were bacterial count in lungs, spleen and blood. Kaplan-Meier estimates of survival were done and multiple comparisons of survival rates performed using the Holm-Sidak method. Results: Treatment with either phages, antibiotics or combination of both significantly increased survival (66%, 50%, 50% respectively, compared to 0% survival for controls, p<0.05). There were no statistical differences in survival rates between either forms of treatment ( Fig. 1) . Treatments hinder the systemic extension of the infection into the blood and spleen without impacting bacterial counts within the lungs, but the numbers are too small to perform statistical tests (Table 1) Introduction: The aim of the study was comparative evaluation of the clinical and microbiological efficacy of combination of amikacin thru nebuliser Aeroneb Pro and standard antimicrobal therapy (AMTcomb) with standard antimicrobal therapy (AMTst) in treatment of ventilator-associated pneumonia (VAP) and ventilator-associated tracheobronchitis (VAT) caused by multi-drug resistant gram-negative bacteria. Methods: In prospective two-center study with retrospective control included patients with VAP and VAT. In AMTst group (retrospective, n=25) we used combination of meropenem 1 g every 8h iv as continuous infusion, cefoperazon/sulbactam 4 g every 12 h iv as continuous infusion and amikacin 1 g iv every 24 h. In AMTcomb group (prospective, n=25) we used combination of AMTst and amikacin inhalation 500 mg every 12 h thru nebuliser Aeroneb Pro. Results: In AMTcomb clinical cure rate was 84%, while in AMTst 29.2% (p<0.001), Clinical Pulmonary Infection Score (CPIS) on day 7 was 6 (4-7) points in AMTst and 2 (0-4) points in AMTcomb (p<0.001). Recurrence of VAP/VAT was 29.2% in AMTst and 12.5% in AMTcomb (p=0.008). On day 7 infectious agent titer in tracheal aspirate was 10 7 (10 3 -10 8 ) CFU/ml in AMTst group, while 10 3 (no growth-10 6 ) CFU/ml in AMTcomb (p=0.016). Microbiological eradication observed in 13 patients in AMTcomb vs in 1 patient in AMTst and microbiological persistance observed in 6 patients in AMTcomb vs 17 patients in AMTst (p=0.002). In AMTcomb on 3rd day sputum was less purulent (p=0.016). Amikacin nebulisation didn't led to deterioration of organ dysfunction: on day 7 there was no difference in platelet count, creatinine and bilirubin levels as compared to day 0 (p=0.102; p=0.297, p=0.532, respectively). Conclusions: Addition of amikacin inhalation 500 mg every 12 h thru Aeroneb Pro nebuliser in patients with VAP and VAT was more efficacious than intravenous standard antimicrobal treatment with comparable safety profile. Introduction: The aim of the study was to assess the effectiveness of inhaled colistin (IC) as an adjunct to systemic antibiotics in the treatment of ventilator-associated pneumonia (VAP). Methods: 110 ICU patients with VAP were enrolled in this observational study. Resolution of VAP was assessed as primary endpoint; eradication of pathogens in sputum, weaning time, duration of ICU stay and mortality were assessed as secondary outcomes. Patients were split into 2 groups: Gr.1 (n = 60) -addition of IC to systemic antibiotics without changing the basic regimen; Gr. 2 (n = 50) -change in systemic antibiotics according to sensitivity. Groups were comparable. IC was administered in a dose of 2 million IU TID (Xselia Pharmaceuticals ApS, Denmark). Statistical analysis was performed using Statistica 7.0 (M, σ, Newman-Keuls test; p <0.05). Results: VAP resolution rate was 77% in Gr.1 (vs. 50% in Gr. 2, p = 0.0295); eradication of pathogens from sputum by the 7th day. treatment was achieved in 80% of Gr. 1 and 60% in the Gr. 2 (n = 12) (p> 0.05); in Gr. 1 weaning from ventilation was possible earlier than in Gr. 2 -7.8±1.3 days. in Gr. 1 vs. 10.9±4.5 days. in Gr. 2 (p = 0.0000); in Gr. 1 duration of ICU stay was shorter than in Gr. 2 -11.5±3.2 days vs. 17.1±2.3 days. in Gr. 2 (p = 0.0000). No mortality differences were detected. Conclusions: Administration of inhaled colistin 2 million IU TID is effective as an adjunct to systemic antibiotics in the treatment of VAP. This modified treatment promotes a more rapid resolution of VAP, earlier weaning from ventilator, reduction of the duration of ICU stay, with no impact on mortality. The addition of IC to systemic antibiotics should be considered as second-line regimen in VAP patients. Factors associated with no de-escalation of empirical antimicrobial therapy in ICU settings with high rate of multi-drug resistant bacteria C Routsi 1 Introduction: De-escalation is recommended in the management of antimicrobial therapy in ICU patients [1] . However, this strategy has not been adequately evaluated in the presence of increased prevalence of multidrug-resistant (MDR) bacteria. The aim of this study was to identify factors associated with no de-escalation in ICUs with high rate of MDR bacteria [2] . Methods: Prospective, multicenter study conducted in 12 Greek ICUs over a 1-year period. Patients with laboratory confirmed infections were included. SOFA score on admission, on septic episode and thereafter every 24 h over 14 days, infection site(s), culture results, antimicrobial therapy, and mortality were recorded. Only the first septic episode was analyzed. In order to assess the factors associated with no de-escalation, a multivariate analysis was performed. Results: A total of 211 patients (admission SOFA score 10±3) were analyzed. 43% of those had septic episode on ICU admission; 57% patients had an ICU-acquired. De-escalation was applied to 44 (21%) patients whereas it was not feasible in 75 patients (44 %) due to the recovery of MDR pathogens or it was not applied, although the microbiology results allowed it, in 92 patients (56 %). Septic shock on the day of septic episode was present in 67% and 79% of patients with and without de-escalation, respectively, p=0.072). Compared to no de-escalation, de-escalation strategy was associated with a shorter duration of shock (4 ±5 vs. 9±7 days, p<0.001) and all-cause mortality (15.4% vs. 46.4%, p<0.001). Multivariate analysis showed that the variables associated with no de-escalation were: a deteriorating clinical course as indicated by an increasing SOFA score (OR 14.7, p<0.001) and a lack of de-escalation possibility due to recovery of MDR pathogens (OR 27.3, p=0.008). Conclusions: Deteriorating clinical course and MDR pathogens are independently associated with no de-escalation strategy in critically ill patients. Conclusions: The qSOFA scale in the prognosis of sepsis does not differ significantly from the SIRS criteria, but in the prognosis of mortality is significantly better than SIRS. qSOFA significantly worse in the prognosis of sepsis and death than the SOFA scale. The international task force of Sepsis-3 introduced the quick Sequential Failure Assessment (qSOFA) score to supersede the systemic inflammatory response syndrome (SIRS) score as the screen tool for sepsis. The objective of this study is to prospectively access the diagnostic value of qSOFA and SIRS among patients with infection in general wards. Methods: A prospective cohort study conducted in ten general wards of a tertiary teaching hospital. For a half-year period, consecutive patients who were admitted with infection or developed infection during hospital stay were included. Demographic data and all variables for qSOFA, SIRS and SOFA scores were collected. We recorded daily qSOFA, SIRS and SOFA scores until hospital discharge, death, or day 28, whichever occurred earlier. The primary outcome was sepsis at 28 days. Discrimination was assessed using the area under the receiver operating characteristic curve (AUROC) and sensitivities or specificities with a conventional cutoff value of 2. Results: Of 409 patients (median age, 55 years [IQR, 40-67]; male, 225[55%]; most common diagnosis pneumonia, 234[57%]) who were identified with infection in general wards, 229(56%) developed sepsis at a median of 0 (IQR, 0-1) day, 146 patients (36%) and 371 patients (91%) met qSOFA and SIRS criteria at a median of 1 (IQR, 0-5) and 0 (IQR, 0-0) day, respectively. The qSOFA performed better than SIRS in diagnosing sepsis, with an AUROC of 0.75 (95% CI, 0.71-0.79) vs 0.69(95% CI, 0.64-0.74). With a conventional cutoff value of 2, qSOFA had lower sensitivity (53% [95% CI, 47%-60%] vs. 98% [95% CI, 95%-99%], p < 0.001) and higher specificity (87% [95% CI, 81%-91%] vs. 18% [95% CI, 13%-23%], p < 0.001) than SIRS (Table 1) . Conclusions: Among patients with infection in general wards, the use of qSOFA resulted in greater diagnostic accuracy for sepsis than SIRS during hospitalization. qSOFA and SIRS scores can predict the occurrence of sepsis with high specificity and high sensitivity, respectively. Prognostic accuracy of quick sequential organ failure assessment (qSOFA) score for mortality: systematic review and meta-analysis Introduction: The purpose of this study was to summarize the evidence assessing the qSOFA [1] , calculated in admission of the patient in emergency department (ED) or intensive care unit (ICU), as a predictor of mortality. The hypothesis was that this tool had a good prediction performance. Methods: Systematic review and meta-analysis of studies assessing qSOFA as prediction tool for mortality found on PubMed, OVID, EMBASE, SCOPUS and EBSCO database from inception until November 2017. The primary outcomes were mortality (ICU mortality, inhospital mortality, 30 and 90-day mortality). Studies reporting sensitivity and specificity of the qSOFA making it possible to create a 2x2 table were included. The diagnostic odds ratio (LnDOR) was summarized following the approach of DerSimonian and Laird using the software R ('mada' package). The summary ROC curve was created using the Reistma model (bivariate model). The RevMan 5 software was used to organize the data. Results: The search strategy yielded 266 citations. Of 134 unique citations, 48 met the inclusion criteria (426,618 patients). The sensitivity and specificity from each study are shown in Fig. 1 . The meta-analysis of the DOR was 4.838 (95% confidence interval (CI): 3.808 -6.146) and of the LnDOR was 1.576 (95% IC: 1.337 -1.816) (Fig. 2) . The pooled area under the summary receiver operating characteristic (SROC) curve was 0.717. The summary estimative of the sensitivity was 0.55 and the false positive rate was 0.209, by bivariate diagnostic random-effects metaanalysis. The Chi-square goodness of fit test rejects the assumption of homogeneity, and the fit of the model for heterogeneity was better (p-value = 0.3661). Conclusions: The qSOFA has a poor performance to predict mortality in patients admitted to the ED or ICU. Introduction: Sepsis and septic shock patients are the most common cause of death in intensive care units. [1] The aim of this study is to quantify the relationship between 72 hours sequential organ failure assessment (SOFA) scores change and in-hospital mortality as a treatment outcome in sepsis and septic shock patients. Introduction: An Outreach Team, akin to a Rapid Response Team, is made up of healthcare professionals assembled together for quick and effective reviews in managing of rapidly deteriorating or gravely deteriorated patients [1] . This study aimed to look at the variety of patient referrals in terms of their severity, patient dynamics, reasons for referral and their subsequent dispositions. Methods: 258 patient records were randomly reviewed retrospectively from July to October 2017. Data were collated in an excel spreadsheet for comparison and then sorted in accordance with the clinical questions and percentages calculated. Results: From the 258 referrals, the severity criteria was done by calculating the National Early Warning Score (NEWS). It was found that 51% patients had a score of 0-4, 23% had a score of 5-6, and 26% scored more or equal to 7. 50% of patients were in the age range 61-70 years old. 78% referrals came from the Emergency Department (ED) where a consultant was involved in the decision of the referral; of this, 46% were referred during office hours of 8AM to 5PM where there was greater manpower to aid management. 19% referrals came from inpatients on the General Wards; 32% were done during office hours. 65% of referrals were transferred to IC/HD upon review; 35% were not, from whom 9 died and 7 were later admitted after procedures (2%) or because they deteriorated further (1%). For reasons for referrals and disposition decisions, see Fig. 1 . Conclusions: Despite having no set criteria for Outreach Team referrals, the accuracy rate was nearly 65% admissions to IC/HD based on clinician concerns. There was only 1% re-admission rate having been re-reviewed when the patients had not been deemed suitable for IC/HD admission initially. Therefore referrals were done accurately and safely with the protocol of clinician referral openness directly to IC consultants. Introduction: Prompt recognition of patient deterioration allows early initiation of medical intervention with reduction in morbidity and mortality. This digital era provides an opportunity to harness the power of machine learning algorithms to process and analyze big data, automatically acquired from the electronic medical records. The results can be implemented in real-time. Intensix (Netanya, Israel) has developed a novel predictive model that detects early signs of patient deterioration and alerts physicians. In this study we prospectively validated the ability of the model to detect patient deterioration in real time. Methods: The model was developed and validated using a retrospective cohort of 9246 consecutive patients admitted to the Intensive Care Unit in the Tel-Aviv Sourasky medical centera tertiary care facility in Israel, between January 2007 and December 2015. In this study, we tested model performance in real time, on a cohort of 333 patients admitted to the same ICU between June 2016 and August 2017. Significant events that lead to major interventions (e.g. intubation, initiation of treatment for sepsis or shock, etc.) were tagged upon medical case review by a senior intensivist, blinded to model alerts. These tags were then compared with model alerts. [3] [4] [5] [6] . 1240 reviews occurred despite 'low NEWS' (Fig. 1) . RRT review led to CC admission in 315 (31.2%) cases; median [IQR] NEWS 5 [3] [4] [5] [6] [7] [8] . Probability of admission increased with higher NEWS (Fig. 1 ), however 82 admissions had 'low NEWS'. Of these 51 were excluded due to high NEWS trigger in the preceding 24hrs or post-operative status. The remaining 31 (9.8%) represented genuine low NEWS cases; age 54 [36-64], 50% male, admission APACHE II 12 [8-17] and day 1 SOFA 2 [1] [2] [3] [4] [5] . Admission source was emergency department 29%, medical 42%, surgical 29%. Diagnoses are shown in Table 1 . No low NEWS patients with sepsis were qSOFA positive. CC length of stay was 2 [1] [2] [3] [4] days and ICU mortality was 9.6%. Conclusions: A high proportion of RRT activity occurs at low levels of abnormal physiology. Despite an association between NEWS and CC admission, NEWS fails to trigger for approximately one in ten admitted cases. Clinical concern remains an important component of the escalation of acutely ill patients. Meanwhile, novel markers of deterioration should be sought and validated. Introduction: Although rapid response systems are known to reduce in-hospital cardiac arrest rate, their effect on mortality remains debated. The Rapid Response Call (RRC) is a system designed to escalate care to a specialised team in response to the detection of patient deterioration. There are diurnal variations in hospital staffing levels that can influence the performance of rapid response systems and patient outcomes. The objective of this study was to examine the relationship between the time of RRC activations and patient outcome. Methods: Review of retrospectively collected, linked clinical and administrative datasets, at a private hospital during a 34-month period. All patients with medical emergency team activation were included. Rapid response calls occurring between 18:00-07:59 were defined as 'out of hours'. Results: Between January 2015 and October 2017 there were 209 RRC. The trigger for RRCs activation was nurse concern (101; 38.3%), modified early warning score (80; 28.3%) and cardiac arrest (28; 13.4%). 44 RRCs were "out of hours" being the main activation trigger a modified warning score > 5. "Out of hours" patients had higher ICU admissions (31.7% versus 20%) and were more likely to have an inhospital cardiopulmonary arrest (OR=1.4, p<0.002). Conclusions: The diurnal timing of RRCs appears to have significant implications for patient outcomes. Out of hours calls are associated to a poorer outcome. This finding has implications for staffing and resource allocation. and septic shock) and severe sepsis (incl. septic shock) using ICD-10 codes coded as primary and secondary discharge diagnoses and procedural OPS codes. We assessed incidences and discharge disposition incl. mortality. Results: Incidences, mortalities and discharge disposition comparing 2010 and 2015 and the mean annual increase in incidence rates are reported in Tables 1 and 2 . Conclusions: The annual increase in standardized sepsis incidence rates is greater than in infections, but similar to the increase in infectious disease patients with organ dysfunction, which are less prone to coding incentives than sepsis codes. An increasing number of patients is discharged to nursing homes and hospice. Given the alarming increase in sepsis cases and deaths, this analysis confirms sepsis as a key priority for health care systems. Introduction: Patients with urgent admissions to the hospital on weekends may be subjected to a higher risk of worse outcomes, which may be due to differences in compliance to established processes. Because delays to antibiotic administration is an important measure of sepsis protocol efficiency and has been associated to worse outcomes, we aimed to assess the association of the weekend effect (admissions on weekend) with timing to antibiotic administration. Methods: Patients included in the sepsis protocol in the emergency department (ED) of Hospital Sao Rafael, from January 2016 to July 2017 were retrospectively evaluated. Sepsis protocol is supposed to be activated to every patient with a suspected sepsis diagnosis in the ED. We evaluated the association of weekend (saturday or sunday) admission with timing to antibiotic administration. Introduction: Current sepsis guidelines emphasize resuscitation of hypotension to a mean arterial pressure (MAP) of at least 65 mmHg [1] . A MAP less than 90 mmHg appears to be associated with poor outcomes in postoperative patients in the intensive care unit (ICU) [2] . However, extent of hypotension in critically ill septic patients during ICU stay and its relationship with adverse outcomes is poorly defined. We determined the magnitude of hypotension in ICU patients with a diagnosis of sepsis and its association with major complications. Conclusions: Reduced mortality may be supposed to be correlated to a quicker recovery of organ damage sepsis related. PCRTs should be warranted in the future to corroborate these preliminary data. Introduction: The PD-1/PD-L1 immune checkpoint pathway is involved in sepsis-associated immunopathy. We assessed the safety of anti-PD-L1 (BMS-936559, Bristol-Myers Squibb) and its effect on immune biomarkers and exploratory clinical outcomes in participants with sepsis-associated immunopathy. Methods: Participants with sepsis/septic shock and absolute lymphocyte count <=1100 cells/μ L received BMS-936559 i.v. (10-900mg; n=20) or placebo (PBO; n=4) + standard of care and were followed for 90d. Primary endpoints were death and adverse events (AEs); secondary endpoints were monocyte (m)HLA-DR levels and clinical outcomes. Methods: This observational study was performed using a prospective, multi-center registry of septic shock. We compared the 28-day mortality between patients who were excluded from the new definition (defined as <2 mmol/L after volume resuscitation) and those who were not (lactate level >=2 mmol/L after volume resuscitation), from among a cohort of patients with refractory hypotension, and requiring the use of vasopressors. Results: Of 567 patients with refractory hypotension, requiring the use of vasopressors, 435 had elevated lactate levels, while 83 did not have elevated lactate levels (neither initially nor after volume resuscitation), and 49 (8.2%) had elevated lactate levels initially, which normalized after fluid resuscitation (Fig. 1 ). Thus, these 49 patients were excluded by the new definition of septic shock. Significantly lower 28-day mortality was observed in these patients than in those who had not been excluded (8.2% vs 25.5%, p=0.02). Conclusions: It seems reasonable for septic shock to be defined by the lactate levels after volume resuscitation, however due to small sample size further large scale study is needed. Results: Significant downregulation (p<0.01) of about 30 pro-and anti-inflammatory cytokines, including IL-6, IP-10, TNF-a, MIP-1a, MIP-1ß, IL-10, was documented. IFN-g effect on macrophages and dendritic cells was inhibited at the level of phosphorylated STAT1. IFN-ginduced expression of CXCL10 and CXCL9 in macrophages was reduced. Patients treated in vivo with higher dosages of apoptotic cells had lower cytokine/chemokine levels compared to those treated with lower levels, and in inverse correlation to aGVHD staging. In vitro binding of apoptotic cells to LPS was documented. Conclusions: The cytokine storm is significantly modified towards homeostasis following apoptotic cell treatment. The mechanism is multifactorial and was shown to include TAM receptor triggering, NFkb inhibition, and LPS binding. These results together with previous studies showing significantly higher murine survival in sepsis models of LPS and cecal ligation puncture suggest that apoptotic cells may be used to treat patients with sepsis. A multicenter clinical trial in septic patients is planned in 2018. Moreover, the urine output significantly increased in survival group. Conclusions: The present study suggests that cytokine-oriented critical care using PMMA-CHDF might be effective the treatment of sepsis and ARDS, particularly,in the treatment of ARDS associated with aspiration pneumonia in elderly patients. The polymyxin b immobilized fiber column direct hemoperfusion has an effect for septic shock but has no effect on sepsis: a cohort study and propensity-matched analysis K Hoshino 1 Introduction: Overwhelming cytokine release often referred to as "cytokine storm" is a common feature of septic shock, resulting in multiple organ dysfunction and early death. Attenuating this cytokine storm early by eliminating cytokines may have some pathophysiological rationale. Our aim was to investigate the effects of extracorporeal cytokine removal (CytoSorb) therapy on organ dysfunction and inflammatory response within the first 48 hours from the onset of septic shock. Methods: Patients with: sepsis of medical origin, on mechanical ventilation, noradrenaline >10mg/min, procalcitonin >3ng/mL and no need for renal replacement therapy, were randomized into CytoSorb and Control groups. CytoSorb therapy lasted for 24 hours. In addition to detailed clinical data collection, blood samples were taken to determine IL-1, IL-1ra, IL-6, IL-8, IL-10, TNF-α, PCT, CRP levels. Introduction: Blind pericardiocentesis leading to low success rate and high complication rates such as ventricular wall or oesophageal perforations, pneumothorax or upper abdominal organ injury.Real time needle visualisation is allowing us to avoid this major complication [1] . Methods: We presented 2 cases of acute traumatic cardiac tamponade secondary to severe chest injury. Both patients presented with haemodynamic instability and echocardiographic features of pericardial tamponade. Pericardiocentesis under ultrasound guidance at left parasternal area with needle directed from medial to lateral technique were performed (Fig. 1) . Real time needle tip visualisation done throughout the procedure (Fig. 2a) . Needle placement in pericardial space was confirmed with agitated saline and guidewire visualisation (Fig. 2b) . Pigtail catheter was inserted and blood was aspirated until the patient were haemodynamically improved. Repeated ultrasound was done to confirm the absence of ultrasonographic features of tamponade and complications. Results: We demonstrated a successful real time needle visualisation ultrasound guided pericardiocentesis in 2 cases acute traumatic pericardial tamponade. Procedural time (time from needle piercing the skin to time needle entering the pericardium) in both cases were less than 1 minute. Post procedural ultrasound confirmed no major complications. Conclusions: The real time needle visualisation using ultrasound was important to reduce major complications during pericardiocentesis. The safety of the highly invasive procedure can be improved with real time needle visualisation. Osman A et al. Eur J Emerg Med (in press), 2017 Introduction: Diagnosis of cardiac tamponade post continuous-flow left ventricle assist devices (cf-LVADs) is challenging due to missing pulsatility. Recent case study of sublingually microcirculation with incident dark-field imaging (IDF) provide a new improved imaging for clinical assessment of cardiac tamponade in a patient with cf-LVAD. We sought to examine the changes in microvascular flow index (MFI) as a sign of cardiac tamponade following LVAD implantation. Methods: Off-site quantitative analysis of sublingual microcirculation clips with Automated Vascular Analyses software (AVA; MicroVision Medical©), and the velocity distributions followed during admission till discharge in patients with end-stage heart failure treated with cf-LVAD complicated by cardiac tamponade. Results: Eleven out of thirty LVAD implantations, 9 males, mean age 58 ± 10 years, April 2015 to January 2017, ((8 Heart Mate 3 (HM 3) and 3 HeartMate II (HM II) (Thoratec Corp., CA)), were complicated by rethoracotomy due to early postoperative cardiac tamponade within 1 week. There sublingual microcirculation was examined by a novel incident dark-field imaging (IDF) before and daily post-LVAD implantation. Pre-LVAD microcirculation was typical for heart failure, characterized by slowly, sludging movement of red blood cells (RBCs), (Fig. 1A arrows) . Directly after implantation, a normal microcirculatory flow was seen with a high RBCs velocity (Fig. 1B) . On the day of tamponade the patients were stable except for severe failure of microcirculation as reflected by drop in MFI (Fig. 1C ) and congestion in venules (* in Fig. 1C ). In 8 out of 11 patients there was a significant drop in MFI before tamponade was clinically recognized (p<0.05). Shortly after rethoracotomy a quick restoration of microcirculatory flow has been found. Conclusions: Sublingual microcirculation imaging is a simple and sensitive non-invasive tool in early detection of cardiac tamponade. Survey on the use of cardiovascular drugs in shock (ucards) - Results: A total of 827 physicians responded. As detailed in Table 1 , the respondents think that dobutamine is first-line inotrope to increase cardiac pump function (N=695, 84%) and should be started when signs of hypoperfusion or hyperlactatemia despite adequate use of fluids and vasopressors in the context of low left ventricular ejection fraction are present (N=359, 43%). The most accepted target was an adequate cardiac output (N=369, 45%). The combination of noradrenaline and dobutamine was preferred to single treatment with adrenaline mainly due to possibility to titrate individually (N=366, 44%). The main reason for adding another inotrope was to use synergistic effects of two different mechanisms of action (N=229, 27%). According to respondents, phosphodiesterase-inhibitors should be used in the treatment of predominant right heart failure because of prominent vasodilatory effect on the pulmonary circulation (N=360, 44%). They also believe levosimendan is the only inotrope that does not increase myocardial oxygen demand (N=350, 42%). Vasodilators are used in cardiogenic shock to decrease left ventricular afterload (N=244, 30%). There is no experience or no opinion about the use of ß-blockers in shock states (N=268, 32%). Conclusions: This web-based survey provided latest trends on inotrope use in shock states which showed considerable diversity among respondents in opinions about its use. Introduction: Recent literature data clearly indicated that in patients with shock the resuscitation of macro-circulation often does not match with microcirculation and tissue perfusion improvement. Unfortunately, the bed-side assessment of regional perfusion remains difficult, particulary in critically ill patients. In the last years thermography has been used in different medical fields but no studies have been performed on the use of this technique in critically ill patients. The aim of this study was to evaluate whether thermography is feasible and may provide useful data during resuscitation of patients with septic shock. Methods: In 4 patients with septic shock we collected central systemic temperature and infrared images (FLIR-T640 digital camera) of limbs at 0, 3, 6 and 24 hours after shock occurrence. Thermal pattern distribution of the limbs was obtained by a specific analysis of the images (ThermaCAM™Researcher P). A systemic to peripheral temperature gradient called "Δ systemic-limb temperature" was calculated for each single temperature data collected. Results: Macrocirculatory and perfusion parameters improved in all the patients throughout the study period: mean values of noradrenaline dose decreased from 0.21 to 0.13 γ/kg/min, mean MAP increased from 65 to 81 mmHg and mean blood lactate decreased from 6.6 to 4.2 mMol/L. The "Δ systemic-limb temperature" pattern showed an heterogenous time course in the 4 patients with a mean overall increase at 6 and 24 hours (Fig. 1) . Conclusions: As expected, the regional data obtained by thermography did not match with macrocirculatory and systemic perfusion parameters. The significance and the relationship between treatments and data observed will be investigated by appropriate studies. Regional differences in the treatment of refractory septic shockan analysis of the ATHOS-3 data Introduction: Vasodilatory shock is a common syndrome with high mortality. Despite established care protocols, regional differences in treatment remain. We sought to characterize these differences using data from the recently published ATHOS-3 study [1] . Methods: Individual patient data were analyzed at baseline and at 48h for regional differences in demographics, clinical characteristics, and treatment patterns, and grouped according to four geographical areas: the United States (US), Canada (CA), Europe (EU) and Australasia (AU). P-values were calculated by Kruskal-Wallis tests for continuous data and chi-square tests for categorical data. Subsequent temporal analysis compared changes in the treatment of shock, indexed by changes in patient acuity level. Results: Regional differences existed with respect to BMI (p=0.0076), albumin (p<0.0001), CVP (p=0.0383), MELD score (p=0.0191), APACHE II score (p=0.0007) and SOFA score (p=0.0076). Baseline norepinephrine (NE) and NE equivalent doses were significantly higher in EU (p<0.0001 and p=0.0494, respectively), and utilization of vasopressin was correspondingly lower (p<0.0001). At baseline, stress dose steroids were utilized to a greater extent in the US and CA (p=0.0011). Temporal analysis revealed differences in the utilization of vasopressin and steroids with changes in patient acuity: in EU, increasing acuity was associated with a lower utilization of vasopressin, and in CA, increased acuity was associated with a lower utilization of steroids. Steroid utilization was higher with increased level of acuity in AU and the US. Conclusions: Significant differences in the treatment of vasodilitory shock exist globally, with important implications: (a) there are Introduction: Levosimendan is a calcium sensitizer and KATP-channel opener exerting sustained hemodynamic and symptomatic effects. In the past fifteen years, levosimendan has been used in clinical practice also to stabilize at-risk patients undergoing cardiac surgery. Recently, the three randomized, placebo-controlled, multicenter studies LICORN [1] , CHEETAH [2] and LEVO-CTS [3] have been testing the peri-operative use of levosimendan in patients with compromised cardiac ventricular function. Over 40 smaller trials conducted in the past [4] suggested beneficial outcomes with levosimendan in peri-operative settings. In contrast, the latest three studies were neutral or inconclusive. We aim to understand the reasons for such dissimilarity. Methods: We re-analyzed the results of the latest trials in the light of the previous literature to find sub-settings in which levosimendan can be demonstrated harmful or beneficious. Results: None of the three latest studies raised any safety concern, which is consistent with the findings of the previous smaller studies. In LEVO-CTS, mortality was significantly lower in the levosimendan arm than in the placebo arm in the subgroup of isolated CABG patients ( Fig. 1 ) [3] . The trend towards both hemodynamic and long term mortality benefits is maintained in recent meta-analyses [5, 6] including the three larger recent studies. Conclusions: Despite the fact that the null hypothesis could not be ruled out in the recent trials, we conclude that levosimendan can still Results: 27 patients were included in levosimendan group and 36 in control group. In the whole population, weaning failure incidence and mortality was comparable between the 2 groups (respectively 24% vs 20%, Pr 0, 34 and 36% vs 38%, Pr=0,6). Higher assistance duration, longer stay under mechanical ventilation and longer duration of stay in critical care unit were observed in Levosimendan group. In the post-cardiotomy sub-group (Table 1) , weaning failure was lower in levosimendan group (12% vs 29%, Pr 0,9) and levosimendan was an independent protective factor from weaning failure (OR 0,073, Pr 0,92). Positive impact of levosimendan may be explained in part by his calcium sensitizer effect and by facilitating recovery of myocardial calcium homeostasis in postcardiotomy cardiac stunning. Conclusions: Levosimendan failed to reduce the incidence of ECMO weaning failure, except for post-cardiotomy population. Renal outcomes of vasopressin and its analogues in distributive shock: a systematic review and meta-analysis of randomized trials Introduction: Venous return (VR) is driven by the difference between mean systemic filling pressure (MSFP) and right atrial pressure (RAP) and determines the maximum ECMO flow. MSFP depends on stressed volume and vascular compliance. It can be modified by absolute blood volume changes and shifts between stressed and unstressed volume. Norepinephrine (NE) may increase stressed volume by constriction of venous capacitance and at the same time increase the resistance to systemic flow. We therefore studied the effects of NE on MSFP, maximum ECMO flow and the ECMO pressure head (MAP-RAP). Methods: MSFP was measured with blood volume at Euvolemia and NE 1 to 3 (0.05, 0.125 and 0.2μg/kg/h) in a closed-chest porcine VA-ECMO model (n=9, central cannulation with left atrial vent and avshunt) in ventricular fibrillation. The responses of RAP and VR (measured as ECMO flow, QECMO) were studied at variable pump speeds including maximum possible speed without clinically apparent vessel collapse at constant airway pressure. Results: The ECMO pump speed and QECMO showed a strictly linear relationship (r 2 0.95 to 0.995, range over all conditions) despite increased pressure head, indicating that the maximum QECMO was determined by VR alone. NE led to both increases in MSFP and QECMO in a dose dependent way, indicating a rightward shift in the VR plot ( Fig. 1 ) via recruitment of stressed from unstressed volume ( Table 1 , Fig. 2 ). This resulted in an increased MSFP during NE despite decreased absolute blood volume (3.9±0.4 L vs. 3.3±0.3L, p=0.009). The reduced blood volume was associated with hemoconcentration suggesting plasma leakage. Conclusions: NE shifts the VR curve to the right, allowing a higher maximum ECMO flow. The NE induced increase in MSFP results from recruitment of unstressed volume to stressed volume, which may be modified by changes in vascular compliance. The effects on pump afterload were not limiting. Introduction: To locate vessels for percutaneous central venous catheterizations, it may be helpful to apply not only real-time ultrasound (US) guidance but also US-assistance vein prelocation. The aim of this study was to evaluate the superiority of two US methods compared to surface landmark methods by reviewing randomized control trials (RCTs). Methods: As updating an earlier systematic review [1] , we searched PubMed and CENTRAL in November 2017. We included RCTs which compared the failure rates of internal jugular or femoral venous cannulations among 1) real-time US guidance, 2) US-assistance vein prelocation and 3) surface landmark methods. A frequentist network meta-analysis was conducted using the netmeta package on R. Results: Out of 1395 citations, 11 RCTs (935 patients) were eligible. The number of studies comparing outcomes between real-time US guidance vs. surface landmark methods, US-assistance vein prelocation vs surface landmark methods and real-time US guidance vs US-assistance vein prelocation was 7, 3 and 1. Regarding cannulation failure rate, network meta-analysis in a fix-effect model showed that a p-score was lower in the real-time US guidance than US-assistance vein prelocation (0.61 vs. 0.88), by reference to surface landmark methods, and also regarding arterial punctures, a p-score was lower in the real-time US guidance than US-assistance vein prelocation (0.64 vs. 0.83). Conclusions: Based on the present network meta-analysis of RCTs, pscores of cannulation failure and arterial puncture were lower in the real-time US guidance, suggesting that the US-assistance vein prelocation is superior than the real-time US guidance, both of which achieve lower rates of failure and arterial puncture compared to the landmark methods. We speculates that the inferiority of real-time guidance is associated with difficulties in manipulating the needle together with an echo probe in targeting relatively smaller veins in children. Introduction: We present a case report of 'Shoshin beriberi' in a young female who was 'fussy with food' that developed an acutely progressive metabolic acidosis and multi-organ failure requiring intensive care support. Methods: Our patient was a 36-year-old British woman who presented to the emergency department (ED) with a ten-day history of diarrhea, vomiting and increasing fatigue. She had a past medical history of gastroparesis, polycystic ovary syndrome (on metformin), laparoscopic cholecystectomy and hysteropexy. She lived with her husband and two children who had viral gastroenteritis two weeks previously. Results: The patient had a metabolic acidosis (pH 6.9) with raised lactate (>16) on initial blood gas in the ED. A 1.26% sodium bicarbonate infusion and hemofiltration were commenced overnight. The patient's pH and lactate remained static with an increasing work of breathing over this period. By morning she developed flash pulmonary oedema and hypotension, the first signs of acute cardiac failure. An echocardiogram displayed severely impaired left ventricular function with ejection fraction of 17%. The patient was intubated and inotropic support was commenced. It was thought that a micronutrient deficiency may have caused a rapid onset cardiac failure. Pabrinex (containing 250ml of Thiamine Hydrochloride) was commenced and within 9 hours the patient's metabolic acidosis markedly improved ( Fig. 1 ). Complete reversal of the cardiac failure occurred over 96 hours. Conclusions: Shoshin is a rare clinical manifestation of thiamine deficiency [1] . It is an important differential diagnosis to bear in mind after excluding more common aetiologies of heart failure. Especially in this case as our patient had no obvious risk factors at the time of presentation. We suggest empiric use of thiamine should be considered in treatment algorithms for young patients presenting with acute cardiac failure. The pateint had provided informed consent for publication. Introduction: Takotsubo syndrome (TS) is known to be an acute transient cardiac condition accompanied with acute heart failure. TS is often triggered by critical illness but that has been rarely studied in ICU practice.Therefore, it is known, that the use of catecholamines can directly induce TS, worsen LVOT obstruction, and delay spontaneous recovery in TS patients, it is nearly impossible to avoid their administration in critically ill [1] . Methods: We have analyzed medical records from 23 patients with TS, that were revealed during year 2017 in our hospital. TS was defined due to Mayo criteria, including transient regional wall motion abnormalities, mildly elevated troponin level and no signs of obstructive CAD on coronary angiography. Results: Out of 23 patients who developed TS in ICU or ICCU, hemodynamic instability occurred in acute phase of TS in 12 (52%) cases. 9 (39%) of patients were admitted to ICU in due to septic shock (2 patients), major bleeding (1), cerebral mass lesion (1) and ARDS (2) and required treatment with catecholamines. General mortality rate in TS patients was 7 (30%), and 5 (55%) in critically ill TS patients. Mean duration of noradrenalin infusion was 7,2 days, dobutamine infusion 4,3 days. Patients with TS needed more ICU resources and longer ICU-stay. Mortality rate was higher in TS patients (55%) vs the ICU-population (28%), p = 0.02. Conclusions: TS seems to be an often cause of LV dysfunction and acute heart failure in critically ill. It seems that TS could be a predictor of worse prognosis in critically ill patients. Although catecholamine administration may worsen the patient prognosis and induce further AHF in critically ill patients it rearely can be avoided. Introduction: Previous studies on readmission following LVAD implantation have focused on hospital readmission after dismissal from the index hospitalization. Since there are very little data existing, the purpose of this study was to examine intensive care unit (ICU) readmission in patients during their initial hospitalization for LVAD implantation to determine reasons for, factors associated with, and mortality following ICU readmission. Methods: This was a retrospective, single center, cohort study in an academic tertiary referral center. All patients at our institution undergoing first time LVAD implantation from February 2007 to March 2015 were included. Patients dismissed from the ICU who then required ICU readmission prior to hospital dismissal were compared to those not requiring ICU readmission prior to hospital dismissal. Results: Among 266 LVAD patients, 45 (16.9%) required ICU readmission. The most common reasons for admission were bleeding and respiratory failure (Fig. 1) . Factors found to be significantly associated with ICU readmission were preoperative hemoglobin level of less than 10 g/dL, preoperative estimated glomerular filtration rate <35mL/min/1.73m2, preoperative atrial fibrillation, preoperative dialysis, longer cardiopulmonary bypass times, and higher intraoperative allogeneic blood transfusion requirements. Mortality at 1 year was 30.2% in patients requiring ICU readmission vs. 11.9% in those not requiring ICU readmission (age-adjusted OR=3.0, 95% CI 1.4 to 6.6, p=0.005). Conclusions: ICU readmission following LVAD implantation occurred relatively frequently and was associated with significant one-year mortality. These data can be used to identify LVAD patients at risk for ICU readmission and implement practice changes to mitigate ICU readmission. Future larger and prospective studies are warranted. Atrial fibrillation and infection among acute patients in the emergency department: a multicentre cohort study of prevalence and prognosis T Graversgaard Odense University Hospital, Odense, Denmark Critical Care 2018, 22(Suppl 1):P140 Introduction: Patients with infection presenting with atrial fibrillation (AF) are frequent in emergency departments (ED). This combination is probably related to a poor prognosis compared to lone AF or infection, but existing data are scarce. Aim: to describe the prevalence and prognosis for AF and infection individually and concomitantly in an ED setting. Introduction: Its afterload reducing effects make PEEP the treatment of choice for cardiogenic pulmonary edema. Studies indicate that PEEP may lower coronary blood flow. Its effects on left ventricular contractility is unclear. Most of the surrogate measures for cardiac contractility are dependent on afterload and contractility assessment under PEEP may therefore be biased. We have investigated cardiac contractility under PEEP with the endsystolic pressure volume relationship (ESPVR) as a load-independent measure of contractility. Methods: 23 patients scheduled for coronary angiography were ventilated with CPAP and a full face mask at three levels of PEEP (0, 5 and 10 cmH2O) in random order. Structural cardiac pathologies were excluded with echocardiography. At every PEEP level, left ventricular pressure volume loops (Millar conductance catheter with INCA System, Leycom, Netherlands) were obtained. The endsystolic elastance was derived from a PV-loop family under preload reduction with an Amplatzer sizing balloon in the inferior caval vein. All participants gave written informed consent. The study was approved by the Bernese ethics committee. Results: 5 women and 18 men with an age 59±6 years were studied. Ejection fraction was 70±8 % at baseline. Mean ESPVR at PEEP levels of 0, 5 and 10 were 2.64±1.3, 2.56± 1.18 and 2.33±0.88 mmHg/mL (p = 0.318, repeated measurements ANOVA). dP/dt and ejection fraction did not differ between the PEEP levels (p=0.138 and 0.48). Conclusions: Moderate levels of PEEP did not influence endsystolic elastance. Higher PEEP and patients in cardiogenic shock should be investigated. Introduction: We sought to assess the feasibility of 3D volumetric analysis with transthoracic echocardiography in critically ill patients. We choose a cohort typical of ICU where accurate volumetric analysis is important: hypoxic, mechanically ventilated patients. 3D analysis is enticing in simplicity and wealth of data available. It is accurate in cardiology patients [1] but has not been assessed in the ICU. Methods: Patients were imaged within 24 hours of admission. Inclusion criteria: adult, hypoxic (P:F <300), mechanically ventilated, Doppler stroke volume (SV) assessment possible. Echocardiography: Seimens SC2000 real-time volumetric analysis with standard B-mode and Doppler assessment. Images unacceptable if >2 segments unable to be seen in 2 volumetric planes. 3D Left ventricle (LV) and right ventricle (RV) analysis with Tomtec Imaging and Seimens Acuson respectively and compared to Doppler derived SV. 30% limit of agreement considered clinically acceptable [2] . Imaging was optimised for volumetric analysis (20-45 vols/sec). Results: 92 patients, 83 in sinus, 9 in AF. No significant difference seen between Doppler vs 2D Simpson's biplane, 3D LV or 3D RV SV estimation. Feasibility, SV values and bias are reported in Table 1 and Fig. 1 . Limit of agreement for corrected Doppler vs LV 3D SV = -48% to 55%; RV 3D SV = -62.7% to 84.3%. Conclusions: 3D LV and RV volumetric analysis is feasible in majority of patients requiring mechanical ventilation, however lacks agreement with Doppler derived stroke volume assessment. Although images may appear sufficient, the semi-automated software appears to underestimate stroke volume. Further larger studies using thermodilution are warranted. Introduction: Body position changes such as leg raising are used to determine fluid responsiveness. We hypothesized that the Trendelenburg position increases resistance to venous return. Together with abolishment of the hepatic vascular waterfall, this may limit the increase in regional blood flow. Methods: Inferior vena cava (IVC), portal vein (PV), hepatic, superior mesenteric (SMA) and carotid artery blood flows and arterial, right atrial (RA) and hepatic (HV) and portal venous blood pressures were measured in anesthetized and mechanically ventilated pigs in supine and 30°Trendelenburg positions. All hemodynamic parameters were measured during end-expiration at 5 cmH2O PEEP, and at inspiratory hold with increasing airway pressures (AWP) of 15, 20, 25 and 30 cmH2O, respectively. Paired t test was used to compare pressures and flows in different positions during end-expiration. Repeated measures ANOVA was performed to evaluate the effects of AWP on hemodynamic parameters. Results: Trendelenburg position significantly increased RA, HV and PV blood pressures at end-expiration, while Qpv and Qsma remained unchanged, Qha increased and Qivc showed a trend to decrease (Table  1 ). In both positions, all blood flows decreased with increasing AWP, and the difference between Ppv and Qsma became smaller, indicating splanchnic blood pooling ( Table 2 ). In the Trendelenburg position, splanchnic blood pooling was less severe compared to supine position. Conclusions: Trendelenburg position tended to decrease venous return from inferior vena cava. Further increases in RAP by augmenting AWP led to a decrease in all flows and signs of abolished hepatic vascular waterfall. Passive manoeuvers to assess fluid responsiveness evoke complex hemodynamic reactions which are not fully understood. Introduction: Despite of preventive measures, the incidence of deep venous thrombosis (DVT) in ICU patients is estimated to range from 5-31%. While clinical diagnostics is unreliable, ultrasound compression test (UCT) has proven to be a highly sensitive and specific modality for the recognition of lower extremity DVT [1] . Delegating this competence to ICU nurses can increase UCT availability and enable preventive DVT screening. Therefore, we decided to conduct a clinical study to evaluate the sensitivity and specificity of UCT performed by general ICU nurse in ICU patients compared to an investigation by ICU physician certified in ultrasound. Methods: Prior to the study, each nurse participating in the study completed one-hour training in UCT and examined 5 patients under supervision. Then, ICU patients without known DVT underwent UCT in the femoral and popliteal region of both lower extremities performed by trained general ICU nurse. On the same day, the examination was repeated by an ICU physician. The results of the examinations of each patient were blinded to each other for both investigators until both tests were performed. In case of a positive test, the nurse immediately reported the result to the ICU physician. The sensitivity and specificity of the test performed by general nurse was calculated in comparison with the examination by a specialist. Results: A total of 80 patients were examined. Both lower extremities were examined in all patients. The prevalence of DVT of 11,25% has been found. The overall sensitivity of the examination performed by general nurse was 90.0%, the specificity 100% with negative predictive value of 98.61%, positive predictive value of 100% and accuracy of 98.77%. The results of our study have shown that general ICU nurses are able to perform bedside screening of DVT by compression ultrasound test with a high degree of reliability after a brief training. Methods: A Cytosorb® (Cytosorbents, New Jersey, USA) HA device was inserted within the CPB circuit in ten patients undergoing elective cardiac surgery. One hour after CPB onset, the activity of coagulation factors (Antithrombin (AT), von Willebrand Factor (vWF), factors II, V, VIII, IX, XI, and XII) were measured before and after the device. Pre and post device measurements were compared using student ttest, a p value <0.05 was considered statistically significant. Results: Patients' mean age was 60.6 ± 21.4 years, 20% were female, the mean EuroSCORE II was 6.2 ± 8.1. Procedures were: coronary artery bypass graft (CABG) (2/10), aortic root replacement (6/10) and CABG combined with aortic valve replacement (2/10). Mean CPB duration was 161.8 ± 52.3 min. Pre and post HA measurements of coagulation factors activity are presented in Fig. 1 . Post-device AT and FII activity was significantly lower (respectively from 70.4 to 66.6, p=0.01 and from 61.5 to 57.1, p=0.03) compared to predevice measurement. There was no statistically significant difference between pre-and post-HA measurements for all other coagulation parameters Conclusions: Pre and post HA Cytosorb® measurements for coagulation factor activity were not different except for a small decrease in AT and FII activity. This might be related with intra-device consumption or adsorption. Further analyses accounting for CPB fluid balance, the entire study population and timepoints are pending. Introduction: The aim of this study is to evaluate changes in hemodynamics and microvascular perfusion during extracorporeal blood purification with Cytosorb in patients with septic shock requiring renal replacement therapy. Methods: Eight adult patients with septic shock requiring continuous renal replacement therapy for acute renal failure were enrolled and underwent a 24-hour treatment with the emodasorption cartridge Cytosorb. Measurements were taken at baseline before starting Cytosorb, after 6h (t1) and 24h (t2) and included: blood gases, macrohemodynamic parameters (Picco2), vasopressor and inotropic dose, plasma levels of cytokines (interleukin [IL]-1, IL6, IL8, IL10, tumor necrosis factor alpha) and parameters of microvascular density and perfusion (sublingual sidestream dark field videomicroscopy). Procalcitonin was measured at baseline and after 24h of treatment. Results: A non-significant decrease in plasma levels of cytokines was observed over time. Hemodynamic parameters and vasopressor requirement remained stable. The microvascular flow index increased significantly at t2, total vessel density and perfused vessel density increased at t1 and t2 ( Introduction: Objective renal replacement therapy (RRT) with the OXIRIS filter is used in sepsis septic shock with AKI, but few clinical studies compare the adsorbing effect of Oxiris filter on the inflammatory mediators to RRT. The aim of this study is 1-to confirm whether oxiris decreases cytokines and procalcitonin in sepsis septic shock.2-This effect is superior to RRT.3-This translates in a better cardio renal response. Methods: A coohort study and a propensity-matched analysis included 73 patients admitted to three Intensive Care (Aurelia Hospital, European Hospital, Tor Vergata -Rome) with a diagnosis of septic shock.50 patients were submitted to RRT with oxiris filter and 23 patients to RRT.Il 6, Procalcitonin, the cardiorenal indices and SOFA score were compared before (T0) and at the end of the treatments (T1). All data are expressed as mean±sd. ANOVA one way was used to compare the changes of the variables in the time. P< 0.05 was considered statistically significant. Results: Of 50 patients submitted to RRT with the oXiris filter 32 could be matched to 22 septic patients who received RRT. IL6 and Procalcitonin decreased in the Oxiris group (p< 0.01) but not in the RRT group.-MAP increased (p< 0.01) and noradrenaline dosage decreased in oxiris group (p< 0.01), but non in RRT group. Also PaO2/FIO2 ratio, diuresis, SOFA improved only in the in the oxiris group (p<0.05). Conclusions: In sepsis/septic shock patients with AKI, IL6 and procalcitonin decrease more in the oXirs group then in the RRT group.This is associated with an improvement of the cardio -renal function and the clinical condition.The study confirms that RRT with oXiris filter may be useful in sepsis/septic shock when other convective/diffusive techinques fail. Introduction: ADVOS (Hepa Wash GmbH, Munich, Germany) is a recently developed CE-certified albumin-based hemodialysis procedure for the treatment of critically ill patients. In addition to the removal of water-soluble and albumin-bound substances, acid-base imbalances can be corrected thanks to an automatically regulated dialysate pH ranging 7.2 to 9.5. Methods: Patients treated with the ADVOS procedure between in the Department of Intensive Care Medicine of the University Medical Center Hamburg-Eppendorf were retrospectively analyzed. Overall 102 treatments in 34 critically ill patients (Mean SOFA Score 16) were evaluated. Additionally, subgroup analysis for hyperbilirubinemia, respiratory acidosis and non-respiratory acidosis were conducted. Results: Severe hyperbilirubinemia (>6 mg/dl) was present in 60 treatments, while 26 and 14 treatments were performed to treat respiratory (PaCO2>45 mmHg) and non-respiratory (PaCO2<45 mmHg) acidosis (pH<7.35), respectively. Mean treatment duration was 16 h. ADVOS procedure was able to correct acidosis and reduce bilirubin, BUN and creatinine levels significantly. The subgroup analysis shows an average bilirubin reduction of 21% per ADVOS multi treatment in the hyperbilirubinemia group (15.24mg/dL vs 11.77mg/dL, p<0.05). Moreover, pH (7.23 vs. 7.35, p<0.001) and PaCO2 (65.88 vs. 53.61 mmHg, p<0.001) were corrected in the respiratory acidosis group, while in the non-respiratory acidosis group, an improvement in pH (7.19 vs. 7.37, p<0.001), HCO3 (15.21 vs. 20.48, p=0.002) and base excess (-12.69 vs. -5.10, p=0.004) could be observed. There were no treatment-related adverse events during therapy. Conclusions: ADVOS is a safe and effective hemodialysis procedure, which is able to remove water soluble and protein bound markers and correct severe acidosis in critically ill patients. Score for timely prescribing (STOP) renal replacement therapy in intensive care unit -preliminary study of a mneumonic approach Introduction: The moment of initiation of renal replacement therapy (RRT) in critically ill patients and a reason for debate, without having objective criteria that indicate it. The objective of this study was to propose a score to help identify the ideal time for the initiation of RRT, and if there is correlation between this score and Intensive Care Unit length of stay and mortality. Methods: patients admitted to the Intensive Care Unit, > 18-yearsold, to whom RRT were indicated by the intensivist. The study protocol was approved by the Hospital das Forças Armadas Ethical Committe, and written informed consent was obtained from all patients. The STOP was assigned according to the presence or not of each of the items (Fig. 1 ). They were classified into groups A and B according to Fig. 2 , and the group change was recorded. Results: 80 patients admitted to ICU in the period, 2 excluded for limitation of therapeutic efforts. 78 were admitted to the study, with the mean age of 75.2 years; 64,1% males (n=50). Distribution among the groups: A1 (n=1, 1.2%), A2 (31, 39.7%), A3 (5, 6.4%), B1 (6, 7.6%), B2 (35, 44.8%) e B3 (no patients). There were statistically significant correlation between group change and mortality (p 0.02), and between the STOP and nephrologist agreement (p 0.01). There was no correlation between STOP value and ICU LOS (p 0,75) or STOP and mortality (p 0.8). Conclusions: The STOP value is correlated with hemodialysis indication agreement between intensivists and nephrologists, and not correlated with ICU LOS or mortality. The group change was correlated to increased mortality, in the study population. The significance of STOP as a tool in determining the moment of initiation of renal replacement therapy remains a work in progress. Introduction: Liver transplant (LT) in patients with renal dysfunction presents intraoperative challenges and portends postoperative morbidity. Continuous renal replacement therapy (CRRT) is increasingly used for intraoperative support; however, there is a paucity of data to support this practice. Methods: Pilot randomized open-label controlled trial in adults receiving cadaveric LT with a Modification of End-Stage Liver Disease (MELD) score >=25 and preoperative acute kidney injury (KDIGO stage 1) and/or estimated glomerular filtration rate <60 mL/min/1.73m2. Patients were randomized to intraoperative CRRT (iCRRT) or standard of care. Primary endpoints were feasibility and adverse events. Secondary endpoints were changes in intraoperative fluid balance, complications, and hospital mortality. Analysis was intention-to-treat. Results: Sixty patients were enrolled, 32 (53%) were randomized (17 to iCRRT; 15 to control). Mean (SD) was age 49 (13) years, MELD was 36 (8), 75% (n=24) had cirrhosis; 63% (n=20) received preoperative RRT; and 66% (n=21) were transplanted from ICU. One patient allocated to iCRRT did not receive LT. Seven (41%) allocated to control crossed over intraoperatively iCRRT ( (137) min, with only 3 interruptions (all due to access). iCRRT fluid removal was 2.8L (range 0-14.5). Fluid balance was 5.3L (2.9) for iCRRT vs. 4.3L (6.1) for control (p=0.57). Postoperative CRRT was similar (77% vs. 50%, p=0.25). There were no differences in reexploration (p=0.36), mechanical ventilation time (p=0.87), reintubation (p=0.18), sepsis (p=0.56), or mortality (p=0.16). Conclusions: In this pilot trial of high acuity LT patients, iCRRT was feasible and safe. These data will inform the design of a large trial to define the role of iCRRT during LT. ClinicalTrials.gov: NCT01575015. The uptake of citrate anticoagulation for continuous renal replacement therapy in intensive care units across the Introduction: The purpose of this descriptive study is to report the trend of citrate anticoagulation uptake, used for continuous renal replacement therapy (CRRT), in intensive care units (ICUs) across the United Kingdom (UK). Citrate anticoagulation has been used in the UK since 2008, but its uptake since then is unknown [1] . Methods: A survey questionnaire targeted pharmacists working in UK adult ICUs providing CRRT. Invitations to participate were distributed utilising the United Kingdom Clinical Pharmacy Association online forum as a platform for access. Survey administration was by self-completion and submissions were accessible over a total of six weeks. Basic demographic data, ICU specifications, the citrate system in use and implementation details were sought. A descriptive statistical analysis ensued. Results: 70 responses were received of which 67 were analysed after duplication removal. 45 trusts, encompassing a total 67 units, in the UK confirmed use of citrate anticoagulation for CRRT. Units reported a mean of 71 days to implement a citrate system (range 0 to 645 days). Prismaflex® (Baxter) and Multifiltrate (Fresenius) were reported as the most commonly used citrate systems; 32 (47.8%) and 28 (41.8%) units respectively. Conclusions: There are 279 ICUs in the UK [2] . We conclude that a minimum of 67 units (24%) use citrate anticoagulation for CRRT in UK critical care centres. Citrate systems of anticoagulation are becoming an increasing popular choice for regional anticoagulation, falling in line with international guidance [3] . These guidelines were introduced in 2012 which corresponds to increase national uptake. Introduction: Patients requiring renal replacement therapy (RRT) whilst on significant doses of vasoactive medications have often been deemed unsuitable to undergo ultrafiltration (UF). However with better understanding of the pathophysiology of renal injury [1] in intensive care patients we hypothesise that vasopressor/inotrope requirement will not significantly increase with UF or with a more negative fluid balance (FB). Methods: Data was retrospectively collected in a general ICU/HDU of adult patients requiring acute RRT for acute kidney injury. Patients on chronic dialysis were excluded. Percentage change in vasopressor index and mean arterial pressure were combined to form the Combined Percentage Change (CPC) which we used as an index of patient stability. Results: 38 patients were assessed undergoing a total of 206 RRT sessions. The mean age was 57 with 23 females and 15 males. Mean FB for the 24 hours from start of RRT was +651mls (range -2317 to +14850mls). Using a model to correct for significant covariates and plotting 24 hour FB against CPC we found no significant effect of FB on stability p=0.98 (Fig. 1 ). Mean UF volume was 880mls (range 0-3009mls). There was a non linear relationship between UF and stability with moderate volumes improving but larger volumes worsening stability (Fig. 2 ). This did not reach statistical significance (p=0.074) so may be due to chance but is likely due to a lack of power. Conclusions: Fluid balance has no effect on cardiovascular stability during RRT in our cohort but there may be a varying effect of UF depending on volume. Introduction: Exposure of blood to a foreign surface such as a continuous renal replacement therapy (CRRT) filter could lead to activation of platelets (plt) and fibrinogen (fib) trapping. Thrombocytopenia has been reported in adults on CRRT but data in pediatrics are scarce. Our institution uses regional citrate anticoagulation (RCA) as standard of care with prefilter hemodilution and HF1000 filters (polysulfone, surface area (SA) 1.1 m2) regardless of patients' (pts) age and size. As filter SA is relatively larger in younger pts, we aimed to investigate the impact of CRRT filter change on hemostasis parameters in infants on CRRT in up to first three filter changes. Methods: Retrospective chart review Results: 30 patients < 10 kg were included, age 4.3 (0.5-8) months, weight 5.4+2.4 kg, with 88 filters. Metabolic disease was the most common principal diagnosis (7/30, 23%), liver failure (LF) was the most common comorbidity (12/30, 40%). All patients received prefilter continuous venovenous hemodiafiltration with minimum dose of 2000 ml/1.73m2/h. Thrombocytopenia was common at CRRT start (28/30, 93%). Plts decreased in 74% filter changes (65/88) by 15+70% (pre vs post plt 71 (44-111) vs 50(30-83), p<0.001). Fibrinogen also decreased from 201 (152-261) to 170 (134-210), p<0.001; there was no change in PTT, PT, or INR values before and after filter changes. Bleeding events were seen in 13/30 (43%) of pts (8/12 of LF pts vs 5/ 18 others, p=0.04), but were not more common in pts who had decrease in plts or fib with filter changes (41% with drop in plts vs 57% without, p=0.66; 47% with drop in fib vs 75% without, p=0.58). Conclusions: Thrombocytopenia is common in infants on CRRT. Further decreases in plt and fibrinogen can be seen in with CRRT filter changes if the filters are relatively large compared to patient size. Bleeding events seems more related to underlying comorbidity, and less to changes in hemostatis parameters observed with filter change but would need to be confirmed with further studies. Intensive monitoring of post filter ionized calcium concentrations during CVVHD with regional citrate anticoagulation: is it still required? Introduction: The aim of the present study was to evaluate the role of postfilter calcium concentrations (pfCa) in terms of safety and efficacy in large retrospective cohort of patients treated with CVVHD and regional citrate anticoagulation. Methods: Retrospective, observational study at a university hospital with 6 ICUs. All patients treated with RCA-CRRT were included in the study. Results: Among 1070 patients treated with RCA-CVVH pfCa at the start of the CVVHD was available in 987 pts. The pfCa concentrations were in target range (0.25-0.35 mmol/L) in the majority of patients (70%), whereas 17% and 13% of patients had the pfCa below or above the target range, respectively. In the further 72h of CVVHD treatment the propotion of patients with targeted pfCa increased to 86% and remained stable. At the start of the RCA-CVVHD there was a significant but weak correlation between the pfCa and ionized systemic Ca (iCa) with a Spearman rank-order correlation coefficient (rho) of 0.374 (p < 0.001). The coefficient of variation of pfCa concentraions was significantly higher if compared to the coefficient of variation of iCa concentration. Using per protocol adaptations the incidence of a severe hypocalcemia (<0.9 mmol/L) was low and present only at first 12 hours of therapy: 4% and 2% of patients with pfCa below the target range and 0.7% and 0.4% of patients with pfCa in target range, at 0h and 12h respectively (p<0.001). There was no correlation between pfCa concentrations and filter lifetime. The results of the present study support the previous reports about higher measurements variation of pfCa compared to systemic iCa (1). Nevertheless due to the weak correlation of iCa and pfCa as well as a low number of patients with a severe metabolic complication, the results of our study question the necessity of intensive pfCa monitoring during RCA-CRRT. Present results need to be validated in further trials. Introduction: In critically ill patients, occurrence of pain is frequent and usually correlates with worse outcomes, such as prolonged ICU length of stay (LOS) and mechanical ventilation. In this regard, pain leads to sympathetic activation, inflammatory mediators and therefore, potentially to organic dysfunction. The aim of this study is to evaluate the relationship between acute pain in critically ill patients and their association with acute kidney injury (AKI). Methods: Retrospective cohort with 6345 adults patients admitted between June 2013 and June 2016, from the ICU of Hospital Sírio Libanês Hospital in Sao Paulo (Brazil). Main exclusion criteria were: length of stay < 48h, coma and previous AKI. The predictor pain was obtained through daily electronic records according to numerical verbal scale (0-10). The outcome was defined as serum creatinine elevation equal to or greater than 0.3mg/dl and/or greater than 50% increase at any time after the first 48 hours in the ICU. The multivariate analysis was performed by Binary Logistic Regression through distinct groups of early or late predictive factors in relation to AKI. Results: After the exclusion of 3220 patients, the incidence of pain with numerical verbal scale equal to or greater than 3 points was 23.6%. The outcome occurred in 31.7% of the cohort. In the binary regression, using the more early predictive factors, sex and pain presented independent relation with the outcome -adjusted OR 1.24 (1.12-1.36) and 1.63 (1.34-1.98), respectively (p <0.001). In the analysis Conclusions: Poor management of ICU pain is associated to worse outcomes, including increased risk to AKI. The search for a better pain management strategy in the ICU scenario should therefore be reinforced. Introduction: Acute Kidney Injury (AKI) is a common complication in hospitalised patients, strongly associated with adverse outcomes [1] . A lack of baseline incidence and outcome data limits our ability to assess local strategies aimed at improving AKI care. Methods: In an audit in three linked inner London hospitals we interrogated our electronic patient data warehouse (Cerner Millennium power insight electronic data warehouse) with a specially written query to identify cases of AKI, defined by KDIGO creatinine criteria, in patients aged over 18y admitted for >24h during January to June 2016. We excluded palliative care and obstetric patients. In the absence of premorbid baseline (median 7-365d pre-admission) the admission creatinine value was used. End stage renal disease (ESRD) and primary sepsis diagnosis was obtained from ICD10 coding. Results: Of 28872 admissions, we excluded 1052 with pre-existing ESRD (Hospital mortality 6.0%) and 8833 with fewer than one creatinine result who could not be assigned AKI status (mortality 1.1%). Of the remaining 18987 there were 3145 with AKI (16.6%), with mortality increasing from No AKI group (2.4%), to AKI stage 1 (12.6%), and a further increase to AKI stages 2-3 (22.4%) (p<0.001) ( Table 1) . Patients with AKI were older (p<0.001), more likely to be medical than surgical (p<0.001), more likely to have a primary sepsis diagnosis (p<0.001) and had higher baseline creatinine (median 91 vs 79 p<0.001). No known baseline was found in 29.7% of patients with AKI, but their mortality did not significantly differ to those with a baseline (14.2% vs 16.6%, p=0.093). Conclusions: An electronic query identified the local burden of AKI and it's associated hospital-mortality; such baseline data is essential to assess the effect of Quality Improvement interventions in AKI prevention and care. Introduction: Acute kidney injury (AKI) is a common condition in critically ill patients [1, 2] . Loop diuretics are generally used as first line treatment. However, controlled trials show controversial results. We ought to search systematically and realize a metaanalysis on the matter. Methods: An electronic search of randomized clinical trials in adult patient treated with diuretics for AKI compared with standard treatment or a control group was conducted. The primary objective of the analysis was to assess recovery of renal function. Secondary endpoints included time to recovery of renal function, need for Renal Replacement Therapy (RRT), mortality in the Intensive Care Unit (ICU) and complications. Introduction: Increased venous pressure is one of the mechanism leading to acute kidney injury (AKI) after cardiac surgery. Portal flow pulsatility and discontinuous intra-renal venous flow are potential ultrasound markers of the impact of venous hypertension on organs. The main objective of this study was to describe these signs after cardiac surgery and to determine if they are associated with AKI. Methods: This single center prospective cohort study (NCT02831907) recruited adult patients able to give consent. Ultrasound studies were performed before cardiac surgery and repeated on post-operative day (POD) 0, 1, 2 and 3. Abnormal portal and renal venous flow patterns are defined in Fig. 1 . The association between the studied markers and the risk of new onset of AKI in the following 24 hours period following an assessment was tested using logistic regression with a 95% confidence interval. Clinical variables associated with the detection of the signs were tested using generalized estimating equation models. This study was approved by the local ethics committee. Results: During the study period, 145 patients were included. The presence of the studied ultrasound signs is presented in Fig. 2 . During the week following cardiac surgery, 49 patients (33.8%) developed AKI, most often on POD 1 (71.4%). The detection of portal flow pulsatility and severe alterations in renal venous flow (Pattern 3) at ICU admission (POD 0) were associated with AKI in the subsequent 24 hours period and was independently associated with AKI in multivariable models including EUROSCORE II and baseline creatinine ( Table 1 ). The variables associated with the detection of abnormal portal and renal patterns were associated with lower perfusion pressure, higher NT-pro-BNP and inferior vena cava measurements (Table 2) . Conclusions: Abnormal portal and intra-renal venous patterns are associated with early AKI after cardiac surgery. These Doppler features must be further studied as potential treatment targets to personalize management. Introduction: Acute kidney injury (AKI) is very prevalent after cardiac surgery in children, and associated with poor outcomes [1] . The present study is a preplanned sub-analysis of a prospective blinded observational study on the clinical value of the Foresight nearinfrared spectroscopy (NIRS) monitor [2] . The purpose of this subanalysis was to develop a clinical prediction model for severe AKI (sAKI) in the first week of PICU stay. Methods: sAKI was defined as serum creatinine (SCr) >/= 2 times the baseline, or urine output < 0.5 ml/kg/h for >/= 12h. Predictive models were built using multivariable logistic regression. Data collected during surgery, upon PICU admission, as well as monitoring and lab data until 6h before sAKI onset, were used as predictors. Relevant predictors with a univariate association with sAKI, were included in the models. Accuracy of the models was tested using bootstraps, by AUROC and decision curves. Results: 177 children were enrolled, admitted to the PICU of the Leuven University Hospitals after cardiac surgery, between October 2012 and November 2015. 5 patients were excluded. 70 children (40.7%) developed sAKI in the first week of PICU stay. A multivariate model with 5 admission parameters (maximum lactate during surgery, duration of CPB, baseline sCr, RACHS1 and PIM2 scores), and 4 postoperative measurements (average heart rate, average blood pressure, hemoglobin, lactate), was most predictive for sAKI ( Fig. 1) . Conclusions: The risk of sAKI in children after congenital cardiac surgery could be predicted with high accuracy. Future models will also include medication data. These models will be compared against and combined with NIRS oximetry data to investigate the independent and added predictive value of the Foresight monitor. Introduction: Acute kidney injury (AKI) occurs in over 50% of the patients in the intensive care unit (ICU). The predominantly ethiology of AKI is septic shock, the most common diagnosis in the ICU. AKI significantly increases the risk of both morbidity and mortality [1] . Methods: 8 ICU patients with septic shock was studied within 24 hrs from admission. 58 patients after cardiac surgery served as control group. All patients were sedated and mechanically ventilated. Renal blood flow (RBF) and glomerular filtration rate (GFR) were obtained by the infusion clearance of paraaminohippuric acid (PAH) and by extraction of 51Cr-ethylenediamine (51Cr-EDTA). N-acetyl-β -D-glucosaminidase (NAG), was measured. Results: RBF was 19% lower, renal vascular resistance 19% higher and the relation of RBF to cardiac index was 29% lower in patients with septic shock compared to the control group. GFR (32%, p=0.006) and renal oxygen delivery (RDO2) (24%) where both significantly lower in the study group (Table 1) . There was no difference between the groups in renal oxygen consumption (RVO2) but Renal oxygen delivery was almost 30% lower in septic shock patients. Renal oxygen extraction was significantly higher in the study group than in the control group. In the study group, NAG was 5.4 ± 3.4 units/mikromol creatinine more, i.e 5 times the value in patients undergoing cardiac surgery [2] . Conclusions: Sepsis related AKI is caused by a renal afferent vasoconstriction resulting in a reduced RBF and lowered RDO2 In combination with an anchanged RVO2, this results in a renal oxygen supply/ demand mismatch. Introduction: The primary aim was to determine if the addition of daily creatine kinase (CK) measurement was usefully guiding decision making in intensive care units within Greater Glasgow and Clyde. Methods: After a change to the daily blood ordering schedule to include CK, a retrospective audit was carried out covering a 5-month period within 3 intensive care units. All patients with CK >870 units/ litre were included. Basic demographics, APACHE 2 score and admitting diagnosis were recorded. Utility of CK was assessed by determining the associated diagnosis and whether the diagnosis was first considered (diagnostic trigger) due to CK level, clinical suspicion or haematuria. Additionally, it was determined if and what actions had been taken based on the raised CK and associated diagnoses. Results: Data was collected from 01/08/2016 to 31/12/2016. 276 patients were captured with CK >870 units/litre from an average combined admission rate of 200 patients/month [1] . Total male patients 191 (69.2%) and female 85 (30.8%). Age range 17 to 95 years (mean 54.7). APACHE 2 score range 0 to 45 (mean 20.9) with estimated mean mortality of 36.7%. 176 patients (63.8%) had associated diagnoses with elevated CK including: burns 2 (0.7%), compartment syndrome 7 (2.5%), myocardial infarction 20 (7.2%), myositis/myocarditis 2 (0.7%), neuroleptic malignant syndrome 1 (0.4%), rhabdomyolysis 61 (22.1%), serotonin syndrome 7 (2.5%), surgical procedure 76 (27.5%). As outlined in Fig. 1 the diagnostic trigger was the routine CK measurement in 65 patients (23.6%), prior clinical suspicion 108 (39.1%), haematuria 1 (0.4%) and unclear in 102 (36.9%). Action was The correlation analysis showed the eGFRs from every formula could all to some extent reflect the glomerular function or GFR accurately. The GFR (Scys) formula was a quickly and accurate method for estimating GFR and may apply clinically in critically ill patients. Perioperative chloride levels and acute kidney injury after liver transplantation: a retrospective observational study S Choi 1 Introduction: The risk of developing acute kidney injury (AKI) after liver transplantation in the immediate postoperative period ranges between 17 to 95%. Most studies in critically ill and surgical patients evaluated the link between chloride-rich resuscitation fluids, not serum chloride levels, and the incidence of AKI. The association between preoperative chloride level or difference in perioperative chloride levels and the incidence of postoperative AKI after liver transplantation were evaluated. Methods: Adult patients (>=18 years old) who underwent liver transplantation at Seoul National University Hospital between 2004 and 2015 were included in the retrospective analysis. The difference between preoperative serum chloride level and the immediate postoperative serum chloride level was defined as intraoperative chloride loading. Postoperative AKI within 7 days of liver transplantation was diagnosed according to the RIFLE criteria. Patients were divided into normochloremia group (96-106 mEq/L), hypochloremia group (<96 mEq/L), or hyperchloremia group (>106 mEq/L) according to their preoperative chloride level. Intraoperative chloride loading was defined as the difference between preoperative serum chloride level and immediate postoperative serum chloride level. .03) compared to patients with preoperative normochloremia. MELD scores > 11 and age >56 years were also associated with increased risk of AKI. Intraoperative chloride loading was not a significant risk factor for AKI after liver transplantation. Conclusions: Preoperative hyperchloremia and hypochloremia were both associated with an increased risk of developing AKI in the immediate postoperative period after liver transplantation. Introduction: Perioperative Acute Kidney Injury (AKI) is associated with significant morbidity and mortality [1] . Certain urinary biochemical parameters seem to have a standardized behavior during AKI development and may act as surrogates of decreased glomerular filtration rate (GFR) aiding in early AKI diagnosis [2] . Aim of this prospective observational study was the evaluation of urinary biochemical parameters as early indicators of AKI in a cohort of major surgery patients. Methods: 68 patients were studied. AKI was defined according to AKIN criteria within 48 hrs after surgery [3] . At pre-defined time points (preoperatively, recovery room [RR] and on postoperative days [POD] 1 to 3) simultaneous serum and urine samples were analyzed Additional studies must confirm these findings and reevaluate these simple parameters as potential AKI monitoring tools. Urinary liver-type fatty acid-binding protein is the novel biomarker for diagnosis of acute kidney injury secondary to sepsis T Komuro, T Ota Shonan Kamakura General Hospital, Kamakura, Kanagawa, Japan Critical Care 2018, 22(Suppl 1):P168 Introduction: Acute kidney injury (AKI) is the predictor of poor prognosis for the patient with sepsis and septic shock. Several diagnostic criteria for AKI is used on clinical settings, but useful biomarker is not known yet. Urinary liver-type fatty acid-binding protein(L-FABP) is associated with kidney function and AKI [1] , But that is not still discussed about AKI secondary to sepsis. Thus, we conducted the study of the association between urine L-FABP and AKI with secondary to sepsis. (Fig. 1) . The cut-off line of L-FABP was 95.71μg/g Cr. Conclusions: L-FABP can be the novel biomarker for diagnosis of AKI. Further investigation need for diagnostic value of L-FABP and usefulness of early intervention for AKI used by L-FABP. Introduction: Biotransformation of 25-hydroxyvitamin D to active 1,25(OH) 2 D occurs primarily in the kidney. Our aim was to explore whether this process was altered in patients with acute kidney injury (AKI). Methods: Consecutive patients admitted to critical care at a tertiary hospital were recruited. The AKI group comprised patients with KDIGO stage II or stage III AKI; the non-AKI group were patients requiring cardiovascular or respiratory support, but with no AKI. Vitamin D metabolite concentrations were measured on days 0, 2 and 5. Statistical analysis included comparison between groups at each time point, and longitudinal profiles of vitamin D metabolites. Results: Interim analysis of 55 participants (44% of the recruitment target) showed that 1,25(OH) 2 D concentrations were significantly lower in patients with AKI at day 2 and day 5. Considering longitudinal changes, 25-hydroxyvitamin D profiles were not different between the groups ( Fig. 1 ) but there was a trend towards a longitudinal increase in 1,25(OH) 2 D in patients without AKI, which was not seen in AKI patients (Fig. 2) . Conclusions: Interim analysis indicates significant differences in concentrations of 1,25(OH) 2 D, but not 25(OH)D, in critically ill patients with AKI. Recruitment is ongoing and further results are awaited. Introduction: Acute renal failure affects from 1% to 25% of patients in the intensive care units (ICUs)1 and it is associated with excess mortality. Hydratation is a useful preventive measure but it is often controindicated in critically ill patients who, on the contrary, often benefit by a strictly conservative strategy of fluid management. Fenoldopam, a selective dopamine 1-receptor agonist, increases renal blood flow and glomerular filtration rate by vasodilating selectively the afferent arteriole of renal glomerulus. The aim of our study is to compare renal effects of fenoldopam and placebo in critically ill patients undergoing a restrictive fluid management. Methods: We enrolled 130 patients admitted to our ICU. Patients were assigned by randomization to study groups: fenoldopam (n=64) and placebo (n=66). Fenoldopam was infused continuously at 0,1 mcg/Kg/ min and equivalent volume for placebo during a period of seven days. Creatinine, cystatin C and creatinine clearance were daily measured as markers of renal function. The incidence of AKI according to RIFLE criteria (Risk, Injury, Failure, Loss, End Stage kidney disease) was also calculated. Results: Patients with a negative fluid balance at the end of the week (~-5000 ml, p=0,0001) were included in the analysis, 32 in the placebo group and 38 in the fenoldopam group. There were not significant differences in the trend of creatinine, creatinine clearance, cystatin C and in the incidence of AKI between the groups during the week of infusion. Conclusions: A continuous infusion of fenoldopam at 0,1 mcg/kg/ min does not improve renal function and does not prevent AKI in critically ill patients undergoing a strictly conservative strategy of fluid management. Introduction: This study aims to evaluate the efficacy of a protocol implemented for dysphagia risk factors [1] in hospitalized patients in a CICU (Coronary Intensive Care Unit). Methods: Patients hospitalized in the CICU of a medium-sized hospital in Presidente Prudente, SP, Brazil, were subjected to a survey that screened for dysphagia during the period from January of 2016 to September of 2017. Patients with at least one risk factor for dysphagia were evaluated by a phonoaudiologist and are the subject of this study. The information was statistically analyzed using EPI INFO, version 7.2.2.2 software. Considering significant P <0.05 two-tailed, for logistic regressions multivariate estimated in the sample. Results: For this study 1018 patients were selected, of which 57.41% were male and the mean age was 71.77 ± 10.96 years. A higher incidence of dysphagia was observed among patients who had at least one of the following risk factors: stroke (Odds Ratio 9.58 p<0.001); brain tumor (OR 4.49 p=0.0013); chronic obstructive pulmonary disease (COPD) (OR 3.45 p=0.023); degenerative diseases (OR 16.76 p<0.001); lower level of consciousness (OR 13.62 p<0.001); ataxic respiration (OR 2.24 p<0.001); aspiration pneumonia (OR 7.04 p<0.001); orotracheal intubation >48h (OR 13.35 p<0.001); tracheostomy (OR 12.99 p<0.001); airway secretion (OR 24.91 p<0.001); nasoenteral tube (OR 14.9 p<0.001); gastrostomy (OR 4.58 p=0.030). There was no statistical significance for age >60, traumatic brain injury, oropharyngeal surgery and unfavorable dentition. Four factors appeared less than 3 times and could not be analyzed (chagas disease, human immunodeficiency virus (HIV), orofacial burn and excess saliva). Conclusions: We concluded that the dysphagia triage protocol insertion was effective to identify dysphagic patients and can be used as an additional tool in the intensive care risk management. physiological bases of this age old concept, more recently applied to endotracheal intubation, have never been confirmed by current methods. We therefore decided to study the effects of an apnea oxygenation period under HFNC oxygen therapy by means of a novel modelization of the respiratory system. Methods: Firstly, an airway model was built with anatomical, physical and physiological attributes similar to that of a healthy subject (Fig. 1) . This system reproduces the physiological evolution of intrapulmonary gases during apnea by progressively increasing CO 2 levels after having cut off previous O 2 supplies (FIO 2 21%). Secondly, the effects of a HFNC apnea oxygenation of 50l/min with an FIO 2 of 100% were analyzed by collecting intrapulmonary gas samples at regular intervals (Fig. 2) . Results: After 1 minute of apnea oxygenation, intrapulmonary oxygen levels remain stable at 21%. After 5 minutes, oxygen fraction reaches 33%, and increases up to 45% in 10 minutes. Regarding CO 2 levels, no significant modifications were observed. Conclusions: A novel experimental and physiological model of the respiratory system has been developed and confirms the existence of an alveolar oxygen supply as well as the lack of a CO 2 washout during HFNC apnea oxygenation. However, these effects are only observed after a delay of about 1.5 to 2 minutes. Therefore, the clinical interests of this technique to reduce apnea-induced desaturation during intubation of a hypoxemic patient in the ICU seem limited without adequate preoxygenation. Combination of both preoxygenation and apnea oxygenation by HFNC can most likely explain positive results observed in other clinical studies. Effect of 4% nebulized lignocaine versus 2% nebulized lignocaine for awake fibreoptic nasotracheal intubation in maxillofacial injuries in emergency department H Abbas, L Kumar King George's Medical University,Lucknow,India, Lucknow, India Critical Care 2018, 22(Suppl 1):P173 Introduction: Topical lignocaine is most commonly used pharmacological agent for anaesthetizing upper airway during fibreoptic bronchoscopy. We compare the effectiveness of two different concentrations, 2% lignocaine and 4% lignocaine, in nebulised form for airway anaesthesia during awake fibreoptic nasotracheal intubation in terms of patient's comfort and optimal intubating conditions, intubation time. Methods: Institutional Ethics Committee approved the study and written informed consent obtained; patients of either sex, between 18-55 years age with anticipated difficult airway planned for intubation were included for this study. Patients were randomly allocated into two groups (A and B) based on sealed envelope method; patients and observers were blinded by using prefilled syringes of lignocaine.One group was nebulized with 10ml of 4% lignocaine(Group A) and other with 10 ml of 2% lignocaine(Group B) in coded syringes via ultrasonic nebuliser for 10 minutes followed by Inj midazolam 0.05 mg/kg IV and Inj Fentanyl 1 microgram/kg IV just before the procedure. The fibreoptic broncoscope was introduced via nostril and the other nostril was used for oxygen insufflation (3-4 L/min). The fibroscope was introduced through the glottic opening and visualising tracheal rings and carina.The endotracheal tube railroaded over the fiberscope and cuff inflated. Results: The primary outcome measure was patient's comfort during awake fibreoptic nasotracheal intubation. The mean patient comfort Puchner scale score of Group A was 1.30 ± 0.08 and of Group B was 2.23 ± 0.12. The mean value of Puchner scale of Group B was significantly higher.The mean procedural time of Group B was significantly higher (15.1%) as compared to Group A (p<0.001). The no of intubations attempts did not differ between the two groups. Conclusions: 4% nebulised lidocaine provided adequate airway anaesthesia and optimal intubating conditions, patient comfort, stable hemodynamics. Introduction: This systematic review and meta-analysis aims to investigate whether video laryngoscopy (VL) improves the success of orotracheal intubation, when compared with direct laryngoscopy (DL). Methods: A systematic search of Pubmed, Embase, and CENTRAL databases was performed to identify studies comparing VL and DL for emergency orotracheal intubations outside the operating room. The primary outcome was rate of first pass intubation. Subgroup analyses by location, device used, clinician experience, and clinical scenario were performed. The secondary outcome was rate of complications. Results: The search identified 32 studies with 15,064 emergency intubations. There was no overall difference in first-pass intubation with VL compared to DL. Subgroup analysis showed first-pass intubations were increased with VL in the intensive care unit (ICU) (2.02 (1.43-2.85); p<0.01), but not in the emergency department or pre-hospital setting. Rate of first-pass intubations were similar with Glidescope® and DL, but improved with the CMAC® (1.32(1.08-1.62); p=0.007). There was greater first-pass intubation with VL than DL among novice/trainee clinicians (OR=1.95 (1.45-2.64); p<0.001), but not among experienced clinicians or paramedics/nurses. There was no difference in first-pass intubation with VL and DL during cardiopulmonary resuscitation or trauma. VL was associated with fewer oesophageal intubations than DL (OR=0.31 (0.14-0.69); p=0.004), but more arterial hypotension (OR=1.49 (1.00-2.23); p=0.05). Conclusions: In summary, compared to DL, VL is associated with greater first-pass emergency intubation in the ICU and among less experienced clinicians. VL is associated with reduced oesophageal intubations but a greater incidence of arterial hypotension. Compared success rate between direct laryngoscope and video laryngoscope for emergency intubation, in Emergency Department: Randomized Control Trial P Sanguanwit, N Laowattana Ramathibodi Hospital, Bangkok, Thailand Critical Care 2018, 22(Suppl 1):P175 Introduction: Video Laryngoscope was used as an alternative to intubate in the Emergency room, designed for tracheal intubation more success [1, 2] . Methods: We performed a prospective randomized controlled trial study of 158 patients who had sign of respiratory failure or met indication for intubation from July 2015 to June 2016. Patients were randomly by SNOSE technique; assigned to Video laryngoscope first or Direct laryngoscope first. We collect the Demographics, Difficult Intubation Predictor, Rapid Sequence Intubation, attempt, Cormack-Lehane view and immediate complication. Primary outcome was first attempt success rate of intubation. Results: First attempt success rate of Video laryngoscope was 73.1% trend to better than Direct laryngoscope was 58.8%, (P=0.06), Good Glottic view (Cormack-Lehane view 1-2) of Video laryngoscope was 88.5% better than Direct laryngoscope 71.3%, and statistically significant (P=0.03), no statistical significant in immediate serious complication between Direct laryngoscope or Video laryngoscope. Conclusions: Compared to the success rate between using Video laryngoscope or Direct laryngoscope for intubation, Video laryngoscope trend to better success rate, and better glottic view. 10-year cohort of prehospital intubations and rescue airway techniques by helicopter emergency medical service physicians: a retrospective database study P De Jong, C Slagt, N Hoogerwerf Radboudumc, Nijmegen, Netherlands Critical Care 2018, 22(Suppl 1):P176 Introduction: In the Netherlands the pre-hospital Helicopter Emergency Medical Service (HEMS) is physician based and an adjunct to ambulance services. All four HEMS stations together cover 24/7 specialist medical care in the Netherlands. In many dispatches the added value is airway related [1] . As part of our quality control cycle, all airway related procedures were analysed. High quality airway management is characterized by high overall and first pass endotracheal intubation (ETI) success [2] . Methods: The HEMS database was analysed for all patients in whom prehospital advanced airway management was performed in the period 2007-2017. Balloon/mask ventilation, supraglottic airway (SGA) devices, total intubation attempts, Cormack & Lehane (C&L) intubation grades, successful ETI, primary and rescue surgical airway procedures and professional background were reviewed. Results: In the 10-year period, there were 17075 dispatch calls. In total 8127 patients were treated in the prehospital setting by our HEMS. Of those, 3233 required a secured airway. ETI was successful in 3078 of 3148 (97.8%). In the remaining 70 patients ( Fig. 1 ) an alternative airway was needed. Rescue surgical airway was performed in 1.4%, 0.5 % received a rescue SGA, rescue balloon/mask ventilation was applied in 0.2% of cases, 1 was allowed to regain spontaneous ventilation and in 0.1% of patients all airway management failed. HEMS physicians, ambulance paramedics, HEMS paramedics and others (e.g. German emergency physicians) had ETI first pass success rates of 83.4%, 59.6%, 62.4% and 84.5% respectively (Fig. 2) . Difficult laryngoscopy (no epiglottis visible) was reported in 2.2% of patients (Table 1) . Conclusions: Our data show that airway management performed by a physician based HEMS operation is safe and has a high overall ETI success rate of 97.8%. The total success rate is accompanied by a high first pass ETI success rate. Introduction: Incidences associated with endotracheal tubes are frequent during mechanical ventilation (MV) of intensive care unit (ICU) patients and can be associated with poor outcomes for patients and detrimental effects on health care facilities. Here, we aimed to identify factors associated with Event occurrence due to Unsafe Management of Endotracheal Tubes (E-UMET). Methods: A retrospective observational study was conducted in three ICUs: one surgical ICU, one stroke ICU, and one emergency department, at a tertiary hospital in Japan from 1 April 2016 to 31 March 2017. Patients requiring MV and oral intubation during their ICU stay were included. The primary finding was the incidence rate of E-UMET (biting, unplanned extubations, and/or displacement of the endotracheal tube). The patients were divided into two groups: with or without E-UMET. To investigate E-UMET, potential factors possibly related to its occurrence were obtained from electronic medical records. We conducted univariable and multivariable analyses to investigate E-UMET factors. Results: Of 410 patients, E-UMET occurred in 112 (27.3%). The mean and standard deviation for age and Acute Physiology and Chronic Health Evaluation (APACHE) II score were 66 (17) and 25 (7), respectively. According to a multivariate logistic-regression analysis, significant risk factors associated with E-UMET included patients of neurosurgery (odds ratio (OR) 3.3; 95% CI, 1.51-7.46; p=0.003), sedative administration (OR 2.9; 95% CI, 1.63-5.32; p<0.001), and higher Richmond Agitation-Sedation Scale (RASS) scores (OR 1.4; 95% CI, 1.24-1.77; p<0.001). The use of a restraint (OR 0.4; 95% CI, 0.22-0.95; p=0.003) was an independent factor associated with a lower probability of E-UMET. Conclusions: This study suggests that risk factors associated with E-UMET include neurosurgery, higher RASS scores, and the administration of sedatives. Patients with these factors and longer oral intubation periods might require extra care. Introduction: The use of nasal high flow (NHF) as a respiratory support therapy post-extubation has become increasingly more common. NHF has been shown to be non-inferior to NIV and reduces escalation needs compared to conventional oxygen therapy. Clinical outcomes using NHF in patients with Type II Respiratory Failure (RF) is less well understood. Our aim was to determine if NHF can be used successfully when extubating Type II RF patients compared to Type I RF. Methods: We conducted a retrospective observational study on the use of NHF as an extubation respiratory support in 56 (n=56) consecutive patients in ICU over a 12-month period. Primary outcome was the need for escalation in therapy (NIV, Intubation and Palliation) post extubation. Patients were categorised as high risk if they scored >=1 from: Age>=75 years, BMI>=30 and >=1 medical comorbidity. Results: Analysis was conducted on all fifty-six (n=56) patients. Type I RF group was composed of 25 (n=25) patients with a mean age of 62.7 (±SD) years. Type II RF group had 31 (n=31) patients with a mean age of 65.5 (±SD) years. In Type I RF 22 patients (88%) were successfully extubated with NHF compared to 21 patients (67.7%) in Type II. In Type II RF the outcomes were more variable with a greater requirement for NIV. Of these patients 16% required NIV, 3.2% required intubation and 12.9% received NHF therapy for palliation. A higher average BMI (30.32 vs 27.16 kg/m2) was found in unsuccessfully vs successfully extubated patients in Type II RF. In Type I RF escalation of therapy was equally distributed with 4% in each category. Conclusions: The use of NHF for respiratory support post-extubation may become standard practice for Type I RF in critical care settings. Our data suggests that NHF can be used but with caution in Type II RF and clinicians should risk stratify patients to identify those at risk of re-intubation and post-extubation respiratory failure. Introduction: Pathogenesis of ventilator-associated pneumonia (VAP) relies on colonization and microaspiration. Oral topical decontamination reduced the VAP incidence from 18 to 13% [1] . The persistence of antiseptic effect in the oral cavity is questionable; we hypothesize that continuous oral antiseptic infusion may offer a better decontamination. Aim of the work: We developed endotracheal tube that allows continuous oral infusion of chlorhexidine (CHX), and we want to test the technique versus the conventional on bacterial colonization. (Provisional patent: 62359944) Methods: A two identical bio models for the upper airways were manufactured by (3DX Diagnostics, USA) to adapt the modified and the ordinary endotracheal tubes (ETT). The two techniques tested were using six hourly disinfection with CHX (group A) versus disinfection through the 24 hours infusion technique (Group B). Five microorganisms plus mixed bacteria were used and each was tested for five times. Normal saline was used constantly to irrigate the biomodels and Ten ml aliquot was collected by the procedure end. Culturing of the aliquots from decanted broth pre and post disinfection was performed. The time to apply CHX by practitioner was also compared. Results: There was a trend towards lower bacterial growth in group A in 5 experiments which reach statistical significance only with Pseudomonas aeruginosa (p=0.045). In one experiment the growth was lower in group B (Fig. 1) . Additionally there was time saving advantage in group B (15±3.3 versus 5±1.2 min, p=0.01). Conclusions: The novel technique got at least non inferior results, plus time saving advantage. These results may warrant future clinical trial. Monitoring airways non invasive online analysing different particle flow from the airways is never done before. In the present study we use a new technology for airway monitoring using mass spectrometric analysis of particle flow and their size distribution (PExA Particles in Expired Air). The exhaled particles are collected onto a substrate and possible for subsequent chemical analysis for biomarkers. Our hypothesis was that by analysing the particle flow online, we could optimise the mechanical ventilation. Our hypothesis was that a small particle flow would probably be more gentle for the lung than a large particle flow when the lung is squeezed out and the majority of all small airways are open. Methods: In the present study we analyse the particle flow from the airways in vivo, post mortem and during ex vivo lung perfusion using different ventilation modes; Volume Controlled Ventilation (VCV) and Pressure Controlled Ventilation (PCV) comparing small tidal volumes(1) versus big tidal volumes(2) at different PEEP (Positive End-Expiratory Pressure) and after distribution of different drugs in six domestic pigs. Results: We found that VCV resulted in a significant lower particle flow than PCV in vivo but in ex vivo settings the opposite was found (Fig. 1 ). In both in vivo and ex vivo settings we found that big tidal volume resulted in a larger particle flow than small tidal volumes.air. The opening and the closure of the small airways reflect the particle flow from the airways. We found that different ventilation modes resulted in different particle flow from the airways. We believe this technology will be useful for monitoring mechanical ventilated patients to optimise ventilation and preserve the lung quality and has a high potential to detect new biomarkers in exhaled air. Introduction: Malaria is a common problem in underdeveloped countries, with an estimated mortality of more than one million people per year. Pulmonary involvement is one of the most serious manifestations of Plasmodium falciparum malaria. Non-invasive ventilation (NIV) decreases muscular works and improves gas exchange by recruitment of hypoventilated alveolus. In this context, we analyze the impact of the use of non-invasive ventilation in malaria with pulmonary dysfunction. Methods: It's a retrospective cohort study. We analyzed electronic records of patients who were diagnosed with malaria, with acute respiratory failure, who underwent respiratory therapy with NIV between 2015-2016 within the intensive care unit (ICU). The study variables were: ICU mortality, length of hospital stay, NIV time and outcome groups. Statistical analysis was performed with the Pearson correlation coefficient, with significance level of p <0.01. The statistics were performed using the BioEstat 3.0 program. Results: Thirty-one patients were included in the study. Four results were analyzed according to Table 1 and Fig. 1 . 94% of the patients were discharged from the hospital. Pearson's correlation coefficient analysis showed statistical significance in the Group (NIV/Discharge) in the analysis of patients hospitalized versus NIV (95% CI = 0.24 to 0.83 <(p) = 0.0036). Conclusions: The use of NIV was positive in patients using this resource as first-line treatment of malaria in the fight against respiratory decompensation, with improvement of symptoms. Introduction: CPAP is used to improve oxygenation in patient with ARF. We aimed to determine non-inferiority (NI) of helmet CPAP to facemask in ARF based on physiological (heart rate (HR) and respiratory rate (RR)) and blood gas parameters (PaO2 and PaCO2). We also compared patients' perception in dyspnea improvement after CPAP using dyspnea scale (visual analogue scale (VAS)) and Likert score. Methods: We randomized 123 patients to helmet (n=64) and facemask (n=59) with 71.7% of ARF was due to acute pulmonary edema. CPAP was applied for 60 minutes. Patients' physiological and blood gas parameters were recorded before and after intervention. Patients then marked on dyspnea scale and Likert score. NI of helmet would be declared if confidence interval (CI) of mean difference between groups (helmet's mean minus facemask's mean) in improving physiological, blood gas parameters and dyspnea scale was no worse than predetermined non-inferiority margin (NIM). Secondary outcome was to compare incidence of discomfort and mucosal dryness between groups. Methods: This is a single center retrospective study performed in the ICU of Tel Aviv Medical Center, Israel, a tertiary academic referral hospital. Using the electronic medical record system and Intensix predictive critical care system for analysis, all patients admitted to the ICU between 1.2007 and 12.2014 were assessed. Respiratory deterioration in MV patients was defined as acute adjustment of FiO2 increase >20% or PEEP increase > 5 cmH2O that persisted for at least 2 hours. The primary outcome was ICU mortality. Secondary outcome was length of ICU stay (LOS). A Chi square test for trends was used for the significance of mortality data and a one way ANOVA test for LOS. Results: 5376 MV patients were admitted to the ICU with an overall mortality of 16.5%. Mortality and LOS were tripled in patients who experienced at least one respiratory deterioration when compared to no events (33.8% vs. 9.9 %, p<0.0001 and 10.7 vs. 2.2 days, p<0.0001 respectively) (Fig. 1) . Increased events of respiratory deteriorations showed significant trend of increased mortality (p<0.0001). Conclusions: In MV patients, a single respiratory deterioration event carries a 3 times higher mortality rate and Length Of Stay (LOS). Any additional event further increases both parameters. Association of lung ultrasound score with mortality in mechanically ventilated patients J Taculod, JT Sahagun, Y Tan, V Ong, K See National University Hospital Singapore, Singapore, Singapore Critical Care 2018, 22(Suppl 1):P191 Introduction: Lung ultrasound is an important part of the evaluation of critically ill patients. It has been shown to predict recruitability in acute respiratory distress syndrome. However, little is known about the application of lung ultrasound in predicting mortality in mechanically ventilated patients. Methods: Observational study of mechanically ventilated patients admitted to the medical intensive care unit (ICU) of a tertiary hospital (National University Hospital, Singapore) in 2015 and 2016. Only the first ICU admissions of these patients were studied. Lung ultrasound was done at six points per hemithorax and scored according to Soummer (Crit Care Med 2012): normal aeration = 0; multiple, well-defined B lines =1; multiple coalescent B lines = 2; lung consolidation = 3. The Lung Ultrasound (LUS) score was calculated as the sum of points (score range 0-36). We analysed the association of LUS score with ICU/hospital mortality, using logistic regression, adjusted for age and Acute Physiology and Chronic Health Evaluation (APACHE) II score. Results: 247 patients were included (age 62.0 ± 16.2 years; 89 female [36.0%]; APACHE II 29.7 ± 7.9; 88 sepsis diagnosis [35.6%]). ICU and hospital mortality were 16.2% and 29.6% respectively. LUS score was associated with increased ICU (OR 1.04, 95% CI 1.00-1.09, P=0.07) and hospital (OR 1.04, 95% CI 1.00-1.08, P=0.045) mortality, adjusted for age and APACHE II score. Conclusions: LUS score was associated with increased ICU/hospital mortality and may be useful for risk stratification of mechanically ventilated patients admitted to ICU. Introduction: Ventilator asynchrony results in morbidities and mortality. The aim of this study was to explore whether and how physicians used patient-ventilator interactions(PVI) to set mechanical ventilators(MV) in Thailand. Methods: Thai physicians treating MV patients were asked to respond to questionnaires distributed in conferences and to e-mails sent. Types of asynchronies encountered and frequency of MV adjustment guided by PVI were evaluated. In addition, correlations between physician's knowledge and 1)confidence to manage asynchronies and 2)their experience were analyzed. Results: Two hundred and eleven physicians answered the questionnaires. Most of them were medical residents and ICU specialists. 82% of them set and adjusted MV by asynchrony guidance and the majority used waveform analysis to more than a half of their patients. The most and the least common asynchronies encountered were double triggering and reverse triggering, respectively, while the most difficult-to-manage and the most easily managed asynchronies were periodic/?A3B2 show $132#?>unstable breathing and flow starvation, respectively. Lack of confidence and knowledge of PVI were the major reasons of physicians who did not perform asynchrony assessment. For knowledge evaluation, more than 50% of physicians incorrectly managed asynchrony. Chest and ICU fellows had the greatest skills in waveform interpretation and asynchrony management with the mean score of 2.62 from the total 5, compared with specialist(2.58), medical residents(1.85), internists(1.84) and general practitioner(0.85). There were poor correlations between years' experience in MV management and the skill in waveform interpretation (r = 0.15, p=0.034) and between physician's confidence in PVI management and the clinical skill (r = 0.27, p<0.001) Conclusions: The majority of Thai physicians realized the importance of PVI, but the skill in asynchrony management was moderate. Intensive programs should be provided to improve their clinical performance. Methods: Six deeply anesthetized swine underwent tracheostomy, thoracostomy and experimental PLEF with 10 ml/kg of radiopaque saline randomly instilled into either pleural space. Animals were ventilated at VT=10ml/kg, frequency=15bpm, I/E=1:2, PEEP=1cmH2O, and FIO2=0.5. Quantitative lung computed tomographic (CT) analysis of regional aeration and global FRC measurements by nitrogen wash-in/wash-out technique were performed in each of these randomly applied positions: semi-Fowler's (inclined 30°from horizontal in the sagittal plane); prone, supine, and lateral positions with dependent PLEF and non-dependent PLEF (Fig. 1) . Results: No significant differences in FRC were observed among the horizontal positions, either at baseline (p=0.9037) or with PLEF (p=0.58) ( Fig. 2A) . However, component sector total gas volume in each phase of the tidal cycle were different within all studied positions with and without PLEF (p=<.01). Compared to other positions, prone and lateral position with non-dependent PLEF had a more homogenous VT distribution among quadrants (p=.051, Fig. 2B ). Supine was associated with most dependent collapse (Fig. 2C ) and greatest tendency for tidal recruitment (48% vs~22%, p=0.0073, Fig. 2D ). Conclusions: Changes in body position in the setting of effusioncaused chest asymmetry markedly affected the internal distributions of gas volume, collapse, ventilation, and tidal recruitment, even when commonly used global FRC measurements provided little indication of these important positional changes. Of the respondents, 80% were affiliated with multidisciplinary ICUs, 14% with thoracic and/or cardiac ICUs and 6% with neuro-ICUs. Most respondents (79%) had completed their specialist training. Overall, arterial oxygen tension (PaO 2 ) was the preferred parameter for the evaluation of oxygenation (Fig. 1 ). The proportions of doctors' preferences for increasing, decreasing or not changing an FiO 2 of 0.50 in two (out of six) patient categories at different PaO 2 levels are presented in Table 1 and Table 2 . Conclusions: This is the largest survey of the preferred oxygenation targets among ICU doctors. PaO 2 seems to be the preferred parameter for evaluating oxygenation. The characterisation of PaO 2 target levels in various clinical scenarios provide valuable information for future clinical trials on oxygenation targets in critically ill ICU patients. Introduction: Sonographic assessment of diaphragmatic excursion and muscle thickening fraction have been suggested to evaluate diaphragm function during weaning trial [1] . The purpose of this study is to compare these two parameters to predict extubation success. Methods: This prospective study was carried out during 9 months from March to November 2017. We enrolled patients who were mechanically ventilated for more than 48h and met all criteria for extubation. The non inclusion criteria were: age < 18 years, history of neuromuscular disease or severe chronic respiratory failure. We excluded subjects who needed reintubation for upper airway obstruction, neurological or hemodynamic alteration.  The scenario involves a patient expected to receive mechanical ventilation for at least 24 hours in the ICU. All proportions are percentages of respondents with 95% confidence intervals. *p < 0.001 for comparisons of proportions of "No change" versus adjacent lower PaO2 level (McNemar's test) Introduction: Ventilator Induced Diaphragmatic Dysfunction is known to be a contributor to weaning failure. Some data suggest that assisted ventilation might protect from diaphragmatic thinning. Aims of this study are to evaluate, by ultrasound (US), the change in diaphragm thickness and thickening in patients undergoing controlled and assisted mechanical ventilation (MV) and clinical factors associated with this change. Methods: We enrolled patients who underwent either controlled MV (CMV) for 48 cumulative hours or 48 hours of pressure support (PSV) if ventilation was expected to last for at least 5 days. Patients < 18 years old, with neuromuscular diseases, phrenic nerve injury, abdominal vacuum dressing system and poor acoustic window were excluded. Diaphragm thickness and thickening were measured with US as described by Goligher and clinical data were collected every 48 hours until ICU discharge. Results: We enrolled 44 patients, 13 were excluded because they had less than 4 measurements and 2 for low quality images, leaving 29 patients for analysis. As expected, during CMV diaphragm thickening was almost absent and significantly lower than during PSV (p<0,001). Diaphragm thickness did not reduce significantly during CMV (p=0.201), but during PSV significantly increased (p<0.048) (Fig. 1 , where "day 0" represents the first day of PSV). During CMV, in 10/21 patients diaphragm thickness showed a >=10% reduction. They had a significantly higher fraction of days spent in CMV (p=0.005) and longer neuromuscular blocking drugs (NBDs) infusion (p=0.043). During PSV, 17/26 patients showed an increase in diaphragm thickness >=10%. Duration of hospital stay was significantly lower for these patients (p 0.048). Differences between the two groups are reported in Table 1 . Conclusions: Longer time spent in CMV and with NBDs infusion seems associated with a decrease in diaphragm thickness. Assisted ventilation promotes an increase in diaphragm thickness, associated with a reduction in the length of hospitalization. Prediction of intrinsic positive end-expiratory pressure using diaphragmatic electrical activity in neutrally-triggered and pneumatically-triggered pressure support F Xia Nanjing Zhongda Hospital, Southeast University, Nanjing, China Critical Care 2018, 22(Suppl 1):P199 Introduction: Intrinsic positive end-expiratory pressure (PEEPi) may substantially increase the inspiratory effort during assisted mechanical ventilation. Our purpose of the study was to assess whether electrical activity of the diaphragm (EAdi) can be reliably used to estimate PEEPi in patients undergoing conventional pneumaticallycontrolled pressure support (PSP) ventilation and neutrally-controlled Introduction: Atelectasis develops in critically ill obese patients submitted to mechanical ventilation. The pressure exerted by the abdominal weight on the diaphragm causes maldistribution of ventilation with increased pleural pressure and diminished response to PEEP. Our objective was to analyze the effects of PEEP in the distribution of ventilation in obese and non-obese patients according to BMI (obese >= 30 kg/m 2 , or non-obese: 20 to 29.9 kg/m 2 ), using electrical impedance tomography (EIT). Methods: We assessed the regional distribution of ventilation of surgical and clinical patients submitted to a decremental PEEP itration monitored by EIT. We calculated the percent ventilation to the nondependent (anterior) lung regions at the highest and lowest PEEP applied. The highest compliance of respiratory system was consistently observed at intermediate values of PEEP (between those extreme values), indicating that the highest PEEP caused pulmonary overdistension, whereas the lowest PEEP likely caused dependent lung collapse Results: Were enrolled 37 patients, with 15 non-obese patients (25,7±2 kg/m 2 ) and 22 obese patients (32.4± 1.7 kg/m 2 ). All patients presented progressively decreased ventilation to dependent (posterior) lung regions when PEEP was lowered (P<0.001). Obese patients consistently presented higher ventilation to the anterior lung zones (when compared no nonobese), Fig. 1 Introduction: Lung protective ventilation is the mainstay of mechanical ventilation in critically ill patients [1] . Extracorporeal CO2 removal (ECCO2R) can enhance such strategies [2] and has been shown to be effective in low flow circuits based on renal replacement platforms [3, 4, 5] . We show the results of a pilot study using a membrane lung in combination with a hemofilter based on a conventional renal replacement platform (Prismalung™) in mechanically ventilated hypercapnic patients requiring renal replacement therapy (NCT02590575). Methods: The system incorporates a membrane lung (0.32 m2) in a conventional renal replacement circuit downstream of the hemofilter. 26 mechanically ventilated patients requiring renal replacement therapy were included in the study. Patients had to be hypercapnic at inclusion under protective ventilation. Changes in blood gases were recorded after implementation of the extracorporeal circuit. Thereafter ventilation was intended to be decreased per protocol until baseline PaCO2 was reestablished and changes in VT and Pplat were recorded. Data from 20 patients were included in the final analysis. Results: The system achieved an average CO2 removal rate of 43.4 ±14.1 ml/min which corresponded to a PaCO2 decrease from 68.3 ±11.8 to 61.8±11.5 mmHg (p<0.05) and a pH increase from 7.18±0.09 to 7.22±0.08 (p<0.05) [ Fig. 1 ]. After adaption of ventilator settings we recorded a decrease in VT from 6.2±0.9 to 5.4±1.1 ml/kg (p<0.05) and a reduction of Pplat from 30.6±4.6 to 27.7±4.1 cmH2O (p<0.05). These effects were even more pronounced in the "per protocol" analysis [ Fig. 2 ]. Conclusions: Low flow ECCO2R in combination with renal replacement therapy provides partial CO2 removal at a rate of over 40 ml/min can significantly reduce invasiveness of mechanical ventilation in hypercapnic patients. Introduction: In ECCO2R-CRRT, efficiency of CO2 removal is higher positioning the oxygenator (OXY) up-stream than down-stream the haemofilter due to higher blood flow (BF) [1] . We tested whether this effect was due to lower pre-filter pressure (PFP). Methods: ECCO2R-CRRT circuit was tested in-vitro (n=10) with the following settings: 5 L bovine blood; BF 450 ml/min; OXY 1.81 m2 (Euroset); CVVH post mode; substitution flow 2500 ml/h; UF rate function off; 1.5 m2 haemofilter (Diapact®, B.Braun Avitum); sweep air flow 4.5 l/min. PFP was evaluated at baseline, 24, 48 and 72 hours. CO2 extraction was measured at BF of 100, 300 and 500 ml/min. Sweep air flow/blood ratio was 1:10. CO2 was add to obtain PaCO2 of 80 mmHg. CO2 removal rate calculation (2): CO2 removal rate = (CO2 ECCO2R inlet-CO2 ECCO2R outlet)* blood flow (Eq.1) CO2 molar volume at 25°C [l/mol] = 24; solubility of CO2 at 37°C = 0.03 mmol/(l*mmHg); HCO3i = inlet HCO3 concentration [mmol/l]; HCO3o = outlet HCO3 concentration [mmol/l]; Pi CO2 = inlet CO2 partial pressure [mmHg]; PoCO2 = outlet CO2 partial pressure [mmHg] equation1 becomes: CO2 removal rate=24 x ((HCO3i + 0.03 x PiCO2) -(HCO3o + 0.03 x PoCO2)) x blood flow (Eq.2) Results: BF of 450 ml/min was always reached with the up-stream configuration. BF was reduced to 400 ml/min with the down-stream configuration due to high PFP alarm (Table 1 ). CO2 removal increased to 34.5 ±13.9 to 69.1±29.5, and 126.0±28.4 ml/min, at BF of 100, 300 and 500 ml/ min (p<0.05). Conclusions: BF of 500 ml/min can be reached only with the upstream configuration due to lower circuit PFPs. BF directly correlates to CO2 removal efficiency. We may speculate that simultaneous use of CRRT and LF-ECCO2R and activation of the UF rate function with the down-stream setting may further increase PFP thus forcing to more enhanced reduction of BF and less effective CO2-removal. Introduction: We describe the use of a novel low-flow ECCO2R-CRRT device (PrismaLung-Prismaflex, Baxter Healtcare Gambro Lundia-AB-Lund, Sweden) for simultaneous lung-renal support. Methods: A retrospective review of patients submitted to PrismaLung-Prismaflex due to AKI associated to hypercapnic acidosis during the period May 2016 -August 2017 at Prato Hospital ICU was performed. Data collected were: demographic, physiologic, complications, outcome. Data were presented as mean ± DS; Anova Test was used to compare changes of parameters over time; significance was set at P< 0,05. Results: We identified 13 patients (mean age 71 ± 13 yr, mean SOFA 12 ± 3). Causes of hypercapnia were moderate ARDS (n=4) and AE-COPD (n=9). In all patients a 13fr double lumen cannula was positioned and 350 ml/min blood-flow with 10 lt oxygen sweep-gas-flow was maintained; iv-heparin aiming to double aPTT was used. Haemo-diafiltration (effluent flow 35 ml/kg/hour) was delivered. In all cases PrismaLung-Prismaflex improved respiratory and metabolic parameters (Figs. 1 and 2) without any complications. All patients survived to the treatment, nevertheless 2patients (1AE-COPD; 1ARDS) died during ICU stay due to irreversible cardiac complications. In ARDS cases: 3 patients were successfully weaned from IMV, mean duration of the treatment was 88 ± 31hours, mean duration of IMV after ECCO2R-CRRT was 2 ± 2 days. In AE-COPD cases: intubation was avoided in 3 patients at risk of NIV failure, 6 patients were successfully weaning from IMV, mean duration of the treatment was 79 ± 31 hours, mean duration of IMV after ECCO2R-CRRT was 0,1 ± 0,3 days. Fig. 1 (abstract P201) . 30 minutes after implementation of the combined renal replacement and ECCO2R circuit a moderate decrease in PaCO2 (-6.5 mmHg) corresponding to a slightly higher pH (0.04) was observed Conclusions: The use of PrismaLung-Prismaflex has been safe and effective: it may be argued that it could be due to the low-blood-flow used. The positive results of this preliminary study may constitute the rational for the design of a larger randomized control trial. Systemic IL-18 production and spontaneous breathing trial (SBT) outcome: the effect of sepsis Introduction: Spontaneous breathing trial (SBT), a routine procedure during ventilator weaning, entails cardiopulmonary distress, which is higher in patients failing the trial. An intense inflammatory response, expressed by increased levels of pro-inflammatory cytokines, is activated during SBT. Sepsis, a common condition in ICU patients, has been associated with increased levels of the pro-inflammatory cytokine IL-18. IL-18 produced among others by skeletal muscles, has been associated with severe muscle wasting and maybe by ICU acquired weakness. We hypothesised that IL-18 increases during SBT, more evidently in SBT failures. We anticipate this response to be more pronounced in formerly septic patients fulfilling the criteria for SBT. Methods: 75 SBTs of 30-min duration were performed and classified as SBT failure or success. Blood samples were drawn before, at the end of the SBT and 24 hours later. Serum IL-18 levels and other inflammatory mediators, commonly associated with distress, were determined and correlated with SBT outcome. Subgroup analysis between septic and non-septic patients was performed. 2)kg/m2) were monitored for 42.0±0.9 hours. 49985 apneas were identified ranging from 10-117s (Fig. 1A) . Apneas were observed in 99% of patients, suggesting low predictability of respiratory insufficiency. The average MV was 73±2.4%MVPRED, as patients were often sleeping or mildly sedated. We assessed the effects of each apnea on the temporally associated MV (Fig. 1B) . While apneas ranging in length from 10-18s decrease MV by as much as 30%, their effect over 1min is <10%. On a 2min time scale, even 60s apneas led to LowMV just 20% of the time (Fig. 1C) . Conclusions: While apneas were ubiquitous, they seldom led to LowMV over clinically relevant time scales. Large compensatory breaths following an apnea generally restored MV to near pre-apnea levels. Nonetheless, some apneas can become dangerous when ignored, as when subsequent sedation decreases compensatory breath size. RVM data provide a better metric of respiratory competence, driving better assessment of patient risk and individualization of care. Introduction: Diffuse alveolar hemorrhage (DAH) is an acute lifethreatening event and recurrent episodes of DAH may result in irreversible interstitial fibrosis. Identifying the underlying cause is often challenging but is needed for optimal treatment. Lung biopsy is often performed in the diagnostic evaluation of patients with suspected DAH. However, the role of lung biopsy in this clinical context is unclear. Hence, we sought to identify the spectrum of histopathologic findings and underlying causes in patients with DAH who underwent lung biopsy, surgical or transbronchial. Methods: We identified 59 patients who underwent surgical lung biopsy (n = 25) or bronchoscopic biopsy (n = 34) in the evaluation of DAH over a 19-year period from 1999 to 2017. We extracted relevant clinical pathologic and laboratory data. Results: The median age in our cohort was 67 years with 51% females. Serologic evaluation was positive in 47% of patients (n=28). Most common histopathologic findings on surgical lung biopsy included alveolar hemorrhage (AH) with capillaritis in 11 patients of whom six had necrotizing capillaritis, followed by AH without capillaritis in 7 patients. The most common histopathologic finding on bronchoscopic lung biopsy was AH without vasculitis/capillaritis in 22 patients, followed by AH with capillaritis in 11 patients. There were no procedure related complications or mortality observed with either method of lung biopsy. The clinico-pathologic diagnoses in these patients are shown in Tables 1 and 2 . Conclusions: In patients with DAH undergoing lung biopsy alveolar hemorrhage without capillaritis was found to be the most common histopathologic finding followed by pulmonary capillaritis. These histopathologic findings contributed to the final clinico-pathologic diagnoses of granulomatous polyangiitis and microscopic polyangiitis in a substantial portion of cases. Future studies are needed to ascertain the benefits vs. risks of lung biopsy in patients with suspected DAH. Note that, an apnea of 30-sec will (by definition) drive MV over a 30-sec window down to 0, but will only decrease MV over a 60-sec window down to~35% MVPRED and to less than 60% over a 2-min window. (C) Likelihood of an apnea of specific length to decrease MV below the Low MV cutoff over various time windows. Note that a single 10-sec apnea has just a 25% chance to decrease MV below 40% in a 30-sec window and less than 2% chance to decrease MV below the cutoff over a 2-min window. Even 60-sec apneas have just 20% chance of decreasing sustained MV over a 2-min window below the 40% MVPRED cutoff (4) Granulomatosis polyangitis (4) AH without capillaritis (7) Antiphospholipid syndrome (1) Microscopic polyangitis (1) AH with diffuse alveolar damage(4) Microscopic polyangitis (3) AH with Pulmonary vascular changes(1) Pulmonary hypertension(1) Introduction: Assessing the sensitivity of the peripheral chemoreflex (SPCR), we can predict the likelihood of developing respiratory and cardiovascular disorders. SPCR is one of the markers of disease progression and good prognostic marker [1] . Disturbed respiratory mechanics can make it difficult to evaluate. Breath-holding test may be helpful in such situation, the results of this test are inversely correlated with peripheral receptor sensitivity to carbon dioxide in healthy people [2] .The aim of the study was to compare the breath-holding test to single-breath carbon dioxide test in the evaluation of the sensitivity of the peripheral chemoreflex in subjects with COPD. Methods: The study involved 78 patients with COPD with FEV1/FVC <70% of predicted, all participants were divided into two groups depending of disease severity (GOLD classification, 2017). In group 1 (mild-to-moderate COPD, n=46) all patients had FEV1>=50% and in group 2 (severe-to-very severe COPD, n=32) all patients had FEV1<50%. Breath-holding test was performed in the morning before breakfast: voluntary breath-holding duration was assessed three times, with 10 min intervals [2] . A mean value of the duration of the three samples was calculated. The single-breath carbon dioxide test [3] was performed the next day. The study was approved by the local ethics committee. All subjects provided signed informed consent to both tests. and January 2017. The data was collected from the hospital electronic and paper notes, and data collected was mortality rate, APA-CHE II score, ICNARC score, type of respiratory support received and whether there was documentation of advanced decisions in case of acute deterioration. Results: There were 12 patients admitted to the ICU with acute respiratory failure as a complication of pulmonary fibrosis. The median APACHE II score was 22 and ICNARC standardised mortality ratio was 5.2. Nine patients died on ICU (75%) and hospital mortality was ten (83%). Eight patients (67%) received high flow nasal oxygen, six (50%) received non-invasive ventilation, and two (17%) received invasive ventilation. The median time to death was 3.7 days. Of 11 patients for whom paper notes were available, no patient had any documented ceiling of care or end of life decisions. Conclusions: Our study confirmed a very high mortality in this cohort of patients, supporting national guidance that invasive respiratory support has limited value. We advise that frank discussion with patients and their families should happen early after diagnosis, such that end of life plans are already in place in the event of acute deteriorations. Introduction: ARF is common in critically ill patients. We compared diaphragm contractile activity in medical and surgical patients admitted to ICU with a diagnosis of ARF. Methods: Adult medical and major abdominal laparotomic surgical patients admitted to a general ICU with a diagnosis of ARF were enrolled. ARF was defined as a PaO2/FiO2 ratio<=300 mmHg/% and need for mechanical ventilation (MV) for at least 24 hours. Diaphragmatic ultrasound was realized bedside when the patient was stable and able to perform a trial of spontaneous breathing. A convex probe was placed in right midaxillary line (8th-10th intercostal space) to evaluate right hemidiaphragm. Diaphragmatic respiratory excursion and thickening were evaluated in M-mode on 3 consecutive breaths and thickening fraction (TF) was calculated. Antropometric, respiratory and hemodynamic parameters, SAPS2, SOFA score, duration of MV, need for tracheotomy and timing, septic state and site of infection, superinfections, ICU and inhospital length of stay (LOS) and outcome were recorded. Patients with trauma and neuromuscular disorders were excluded. P<0.05 was considered significant. Results: We enrolled 30 patients: 40% medical and 60% surgical, without differences for age, sex, BMI, SAPS2, SOFA score, sepsis and superinfections. Moderate ARF was prevalent in both groups. During diaphragmatic examination, no differences were recorder for respiratory rate, hemodynamic state and fluid balance. Surgical patients showed a lower but not significant diaphragm excursion (1.6vs1.8cm), instead TF was significantly reduced (58vs90%,p<0.05). No differences emerged on duration of MV, but tracheotomy were higher in medical ones (30vs11%,p<0.05). ICU and inhospital LOS do not differ between medical and surgical patients and mortality rate was respectively 17% and 22%. Conclusions: In ARF, surgical patients showed a lower diaphragm contractility compared to medical ones, maybe due to the combination of anesthetic and surgical effects, but with no influence on outcome. (Fig. 1) . The slope of the regression line for Pes/Paw plots was consistently higher for slow compressions (0.98 ± 0.08), as compared to fast ones (0.84 ± 0.05). A good agreement between Δ Pes and Δ Paw (Fig. 2 ) was found during slow maneuvers, but not during the fast ones. Conclusions: Slow chest compressions must be used when checking position/inflation of esophageal balloon. The fast maneuver produces hysteresis and underestimation of Δ Pes (but not in direct Δ Ppl). Pes monitoring at high respiratory rates may be problematic. Methods: 20 consecutive comatose post cardiac arrest patients were ventilated with volume assist ventilation (6 mL/kg IBW, PEEP 5 cm H2O) using Elisa 800EIT (Lowenstein Medical, GE). Orogastric tube (NutriVent, Sidam, IT) was inserted, and EIT vest (swisstom AG, CH) was applied in all patients. Measurements were performed 60 min after admission and after 3 hrs (Fig. 1) . Optimal PEEP was defined as lower inflection point using PV curve (PV), positive PtPEEP (Ptp) and optimal Regional Stretch/Silent Spaces (EIT) Results: Methods to determine PEEP using PV, Ptp and EIT were comparable in non obese patients (p=NS Introduction: The driving pressure of respiratory system (DP) reflects the extent of lung stretch during tidal breathing, and has been associated with mortality in ARDS patients during controlled mechanical ventilation [1] . Aim of this study was to examine DP during assisted ventilation, and examine if and when high DP occurs in patients in assisted ventilation with PAV+. Methods: Critically ill patients hospitalized in the ICU of the University Hospital of Heraklion, on mechanical ventilation in PAV+ mode were studied. Continuous recordings of all ventilator parameters were obtained for up to three days using a dedicated software. DP was calculated from the PAV+ computed compliance (C) [2] , and the measured exhaled tidal volume (VT, DP=VT/C). Periods of sustained DP above 15 cmH2O were identified, and ventilation and clinical variables were evaluated. Results: Sixty-two patients and 3200 hrs of ventilation were analyzed. In half of the patients, DP was lower than 12 cmH2O in 99% of the recording period, while high-DP (>15cmH2O) more than 10% of the total time was observed in 10% of patients. ICU non-survivors had more time with high DP than survivors (p=0.04). Periods of sustained high-DP (>15cmH2O for >1h) were observed in 9 patients. Level of assist, minute ventilation, and respiratory rate were not different between the periods of high DP and the complete recordings, while VT was higher and C was lower during the high-DP period compared to the complete recording. The median compliance was below 30 ml/ cmH2O during the high-DP period, and above 50 ml/cmH2O during the complete recording. Conclusions: High DP is not common, but does occur during assisted ventilation, predominantly when compliance is below 30 ml/cmH2O, and may be associated with adverse outcome. Table 1 summarizes the percent of monitored time with reported data for the two devices. Figure 1 depicts MV decrease following propofol and cannula dislodgement fol- Fig. 1 (abstract P220) . Bland-Altman analysis demonstrated that CVP-derived ΔPpl and ΔPes were correlated significantly lowing a jaw thrust. Table 2) . Negative (a-ET) PCO2 was strongly associated with good outcome and were significantly associated with overall survival (Fig. 1 ) Conclusions: In conclusion, the negative arterial to end-tidal CO2 pressure gradient may predict patient survival in some subgroups. Introduction: ARDS may result from various diseases and is characterized by diffuse alveolar injury, lung edema formation, neutrophil-derived inflammation and surfactant dysfunction. Various biomarkers have been studied in diagnostics and prognostication of ARDS. The purpose of the study was to measure the expression of proinflammatory mediators like IL-8 and TNF, a cellular receptor with a role in innate immunity(TLR-2),and a biomarker of fibrogenesis (MMP-7) in different phases of ARDS patients. Methods: We studied 4 patients admitted to our ICU with diagnosis of ARDS during the month of January 2016. Six ml of blood were prospectively collected at two times: during the acute phase and in a sub-acute phase before ICU discharge. Blood samples were centrifuged to obtain the platelet-rich plasma and plasmatic RNA (cRNA) was isolated from platelets.IL-8, TNF, TLR-2 and MMP-7 expression in cRNA was determined by the Droplet Digital™ PCR as copies/ml. Results: All patient showed a decrease in IL-8, TNF, TLR2 and MMP-7 levels after the acute phase of ARDS (Fig. 1) . Patient 1 and 3 were affected by influenza A virus (H3N2), patient 2 was admitted for pneumococcal pneumonia and patient 4 was affected by Legionella. Adequate ethiologic treatment was promptly started in patients with bacterial infection. Mean duration of mechanical ventilation was 17.5 days. All patient survived ICU stay and were discharged from hospital. Conclusions: IL-8, TNF, TLR-2 and MMP-7 expression detected by extracted platelets RNA, may be a novel tool useful for clinicians indicating persistent inflammation with resulting progressive alveolar fibrosis and impaired lung function. More data are necessary to understand the real clinical significance of this biomarkers and their role in fibroproliferation and progression of ARDS. Introduction: Although mesenchymal stem cells (MSCs) transplantation has been shown to promote lung respiration in acute lung injury (ALI) in vivo, its overall restorative capacity appears to be restricted mainly because of low engraftment in the injured lung. Ang II are upregulated in the injured lung. Our previous study showed that Ang II increased MSCs migration in an Angiotensin II type 2 receptor (AT2R)dependent manner [1] . The objective of our study was to determine whether overexpression of AT2R in MSCs augments their cell migration and engraftment after systemic injection in ALI mice. Methods: A human AT2R expressing lentiviral vector was constructed and introduced into human bone marrow MSCs. We also downregulated AT2R mRNA expression using a lentivirus vector carrying AT2R shRNA to transduce MSCs. The effect of AT2R regulation on migration of MSCs was examined in vitro. A mouse model of lipopolysaccharide (LPS) induce ALI was used to investigate the engraftment of AT2R-regulated MSCs and the therapeutic potential in vivo. Results: Overexpression of AT2R dramatically increased Ang II-enhanced human bone marrow MSC migration in vitro. Moreover, MSC-AT2R accumulated in the damaged lung tissue at significantly higher levels than control MSCs 24h and 72h after systematic MSC transplantation in ALI mice. Furthermore, MSC-AT2R-injected ALI mice exhibited a significant reduction of pulmonary vascular permeability and improved the lung histopathology and had additional anti-inflammatory effects. In contrast, there were less lung engraftment in MSC-ShAT2R-injected ALI mice compared with MSC-Shcontrol after transplantation. Thus, MSC-ShAT2R-injected group exhibited a significant increase of pulmonary vascular permeability and resulted in a deteriorative lung inflammation. Conclusions: Our results demonstrate that overexpression of AT2R enhance the migration and lung engraftment of MSCs in ALI mice and may provide a new therapeutic strategy for the injured lung. Introduction: Reorganization of endothelial barrier complex is critical for increased endothelial permeability implicated in the pathogenesis of acute respiratory distress syndrome. We have previously shown hepatocyte growth factor (HGF) reduced lipopolysaccharide (LPS)-induced endothelial barrier dysfunction. However, the mechanism of HGF in endothelial barrier regulation remains to be unclear. Methods: Recombinant murine HGF with or without mTOR inhibitor rapamycin were introduced on mouse pulmonary microvascular endothelial cells (PMVECs) barrier dysfunction stimulated by LPS. Then, endothelial permeability, adherent junction protein (occludin), endothelial injury factors (Endothelin-1 and von Willebrand factor), cell proliferation and mTOR signaling associated proteins were tested. Results: Our study demonstrated that HGF decreased LPS-induced endothelial permeability and endothelial cell injury factors, and attenuated occludin expression, cell proliferation and mTOR pathway activation. Conclusions: Our findings highlight activation Akt/mTOR/STAT-3 pathway provides novel mechanistic insights into HGF protective regulation of LPS-induced endothelial permeability dysfunction. Introduction: Mechanical ventilation (MV) is a life-saving intervention for critically ill patients, but may also exacerbate pre-existing lung injury, a process termed ventilator-induced lung injury (VILI). Interestingly, we Fig. 1 (abstract P227) . Fluorescein isothiocyanate-Dextran or fluorescein isothiocyanate-BSA analysis of the effect of HGF on PMVECs permeability Fig. 2 (abstract P227) . Western blot analysis of HGF on mTOR signaling pathway discovered that the severity of VILI is modulated by the circadian rhythm (CR). In this study, we are exploring the role of the myeloid BMAL1, a core clock component, in VILI. Methods: We employed mice lacking Bmal1 in myeloid cells (LyzMCre-Bmal1-/-) and LyzMCre mice as controls. At circadian time (CT) 0 or CT12, mice were subjected to high tidal volume MV to induce VILI. Lung compliance, pulmonary permeability, neutrophil recruitment, and markers of pulmonary inflammation were analyzed to quantify VILI. To assess neutrophil inflammatory responses in vitro, myeloid cells from bone marrow of WT and Bmal1-deficient animals were isolated at dawn ZT0 (Zeitgeber time 0) and dusk (ZT12), incubated with DCFH-DA and stimulated for 15 min with PMA or PBS. Neutrophil activation (Ly6G/CD11b expression) and ROS production (DCFH-DA/Ly6G+ cells) were quantified. Results: Injurious ventilation of control mice at CT0 led to a significant worsening of oxygenation, decrease of pulmonary compliance, and increased mortality compared to CT12. LyzMCre-Bmal1-/-mice did not exhibit any significant differences when subjected to MV at CT0 or CT12. Mortality in LyzMCre-Bmal1-/-mice after VILI was significantly reduced compared to LyzMCre controls (CT0). Neutrophils isolated from control mice at ZT0 showed a significantly higher level of activation and increased ROS production after PMA-stimulation compared to ZT12. ROS production of LyzMCre-Bmal1-/-neutrophils did not differ from ZT0 to ZT12. Conclusions: The lack of the clock gene Bmal1 in myeloid cells leads to increased survival after injurious ventilation and to loss of circadian variations in neutrophil ROS production. This suggests that the internal clock in myeloid cells is an important modulator of VILI severity. Introduction: Hemodynamic resuscitation by means of fluids and norepinephrine (NE) is currently considered as a cornerstone of the initial treatment of septic shock. However, there is growing concern about the side effects of this treatment. The aim of this study was to assess the relationship between the hemodynamic resuscitation and the development of the ARDS. Methods: 18 New Zealand rabbits. Animals received placebo (SHAM=6) or lipopolysaccharide (LPS) with or without (EDX-R, n=6; EDX-NR, n=6) hemodynamic resuscitation (fluids: 20 ml/kg of Ringer's lactate; and later NE infusion titrated up to achieve theirs initial arterial pressure). Animals were monitored with an indwelling arterial catheter and an esophageal Doppler. Respiratory mechanics were continuously monitored from a sidestream spirometry. Pulmonary edema was analyzed by the ratio between lung wet and lung dry weight (W/D), and the histopathological findings. Results: SHAM group did not show any hemodynamic or respiratory changes. The administration of the LPS aimed at increasing cardiac output and arterial hypotension. In the LPS-NR group, animals remained hypotensive until the end of experiment. Infusion of fluids in LPS-R group increased cardiac output without changing arterial blood pressure, while the norepinephrine reversed arterial hypotension. Compared to the LPS-NR group, the LPS-R group had more alveolar neutrophils and pneumocytes with atypical nuclei, thicker alveolar wall, non-aerated pulmonary areas and less lymphocyte infiltrating the interstitial tissue. In addition, the airway pressure increased more in the group LPS-R, and the W/D, although slightly higher in the LPS-R, did not show significant differences. Conclusions: In this model of experimental septic shock resuscitation with fluid bolus and norepinephrine increased cardiac output and normalized blood pressure but worsened lung damage. Obese patients have been excluded from most of the clinical trials testing the effects of PEEP in ARDS. We hypothesized that in morbidly obese patients the massive load of the abdomen/chest further increases lung collapse thus aggravating the severity of respiratory failure due to ARDS. Methods: We performed a clinical crossover study to investigate the contribution of lung collapse to the severity of respiratory failure in ARDS obese patients and to determine the specific contribution of titrated PEEP levels and lung recruitment to changes in lung morphology, mechanics and gas exchange. Patients were studied at the PEEP (PEEPICU) levels selected at our institution and at PEEP levels establishing a positive end-expiratory transpulmonary pressure (PEEPINC) and at PEEP levels determining the lowest lung elastance during a decremental PEEP (PEEPDEC) trial following RM. Results: Thirteen patients were studied. At PEEPICU end-expiratory transpulmonary pressure was negative, lung elastance was increased and hypoxemia was present (Table 1) . Regardless the titration technique there was no difference in the PEEP level obtained. At PEEPINC level endexpiratory lung volume increased, lung elastance decreased thus improving oxygenation. Setting PEEP according to a PEEPDEC trial after a RM further improved lung elastance and oxygenation. At PEEDEC level after a RM lung collapse and overdistension were minimized (Fig. 1) . All patients maintained titrated PEEP levels up to 24 hours without complications. Conclusions: In severely obese patients with ARDS, setting PEEP according to a PEEPINC trial or PEEPDEC trial following a RM identifies the same level of optimal PEEP. The improvement of lung mechanics, lung morphology and oxygenation at PEEPDEC after a RM suggests that lungs of obese ARDS patients are highly recruitable and benefit from a RM and high PEEP strategy. Introduction: Lung protective ventilation (LPV) strategies, principally focused around the use of tidal volumes <6 ml/kg predicted body weight (PBW) remains an enduring standard of care for ventilated patients. However, implementation of and compliance with LPV is highly variable. We used 'nudge'-based interventions to assess if these can improve LPV. Methods: Ventilation data analysis over 2 years (186000 hours in 685 patients) showed patients had been ventilated with a median tidal volume of 7.4 ml/kg PBW with a significant proportion receiving over 8 ml/kg PBW (Fig. 1) , an effect more pronounced in female patients and those with higher BMI. Interventions: 1) Creation of a software tool to easily identify and monitor patients receiving tidal volumes that were too high for their PBW 2) Attached laminated reference guides to each ventilator to calculate PBW 3) Presentation, opportunistic education and verbal prompts to relevant clinical care staff regarding importance of LPV and use of PBW rather than actual body weight 4) Incorporating checking of tidal volumes on a daily ward rounds from junior clinical members Results: We collected hourly ventilation data of the patients over a 2-week period (2479 hours in 22 patients) following our interventions. There was, overall a statistically significant reduction tidal volume (p<0.001). There was improvement in the ventilation of male patients (p<0.001) but female patients endured higher tidal volumes. There was a mixed picture in different BMI grades. Conclusions: Reducing tidal volumes in mechanically ventilated patients can be done through a mix of behavioural and educational interventions, as well as using technological shortcuts. This helps to reduce the effort on the part of clinical staff to adhere to best practices, and ultimately improve patient outcomes. Introduction: Lung protective ventilation (LPV) using a tidal volume (VT) of 6-8mL/Kg ideal body weight (IBW) is recommended in the intensive care unit and theatres to reduce the incidence of pulmonary complications. The aim of this audit was to assess the extent to which LPV is used in theatres in a busy district general hospital and to implement measures to promote adherence to the recommendations. Methods: Anaesthetists completed questionnaires for all patients undergoing general anaesthesia at Northwick Park Hospital over 1 week. Demographics, actual body weight (ABW), height, American Society of Anesthesiologists (ASA) score, and procedural information were recorded. Ventilatory parameters included the ventilation mode, VT, and positive end expiratory pressure (PEEP (Fig. 1) . Significantly more females (75%) received VT >=8ml/kg than males (29%) (p<0.01) (Fig. 2) . VT was independent of age, ASA, BMI, ventilation mode, speciality, and patient position. Conclusions: Over half of the patients received VT >=8ml/kg IBW. Females were more likely to be over ventilated. A likely contributing factor is the disparity between ABW and IBW in this cohort. We organised staff teaching and constructed IBW charts with the appropriate corresponding tidal volumes to be displayed in all theatres to promote the use of LPV. Results: There were significant differences in ARDS incidence between groups: ARDS developed in 12.4% of protective MV groups vs. 68.3% of standard MV group (p=0.0001, Fisher's exact test). VAP patients ventilated in a protective mode presented with lower duration of MV (12.2±4.2 days) and ICU stay(16.1±3.2 days) than patients with standard MV (17.2±5.2 and 20.1±5.5 days). There were significant differences in mortality rates between patient groups: 24.1% in protective MV and 47.2% in standard MV (p=0.0043, Fisher's exact test). Conclusions: Protective MV prevents the development of ARDS in VAP septic patients. Introduction: Reduction of tidal volumes (TV) below 6 mL/kg associated with low driving pressure (dP) might improve lung protection in patients with acute respiratory distress syndrome (ARDS). The current study tests the combination of coaxial double lumen endotracheal tube (to reduce instrumental dead-space) and moderately respiratory rate (RR) (<80 bpm) to maintain CO2 at clinically acceptable levels while using ultraprotective TV. The objective is to considerably reduce dP, which has been preconized as an index more strongly associated with survival than TV, per se, Methods: 8 juvenile pigs were anesthetized, intubated and mechanically ventilated. Severe lung injury (P/F<100) was induced using a double-hit model: repeated surfactant wash-out followed by injurious mechanical ventilation using low positive end-expiratory pressure and high dP (~40cmH2O) for 3 hours. Then VTs of 6, 4, and 3 ml/kg were used in random sequence for 30 min each, both using a standard and coaxial endotracheal tube. At each VT level, RR was adjusted to achieve PaCO2=60 mmHg but not exceeding 80 bpm. Lung functional parameters and blood gas analysis were measured at each VT level. Statistical analysis was performed using mixed linear model. Results: Coaxial endotracheal tube, but not the conventional tube, allowed decreasing VT to 4 and 3 ml/kg, while keeping PaCO2 at approximately 60 mmHg and RR<80 bpm, reducing dP of 4.0 cmH2O and 6.0 cmH2O, respectively, compared to the conventional VT of 6 ml/kg (Fig. 1) . Conclusions: In this ARDS model, coaxial tube ventilation associated with moderately high RR allowed ultraprotective ventilation (VT=3 ml/kg) and reduced dP levels, maintaining PaCO2 at acceptable levels. This strategy might have a significant impact on mortality of severe ARDS patients. The Table 1 shows oxygenation and respiratory mechanics. Figure 1 : Echocardiographically measured right heart function. Conclusions: In morbidly obese mechanically ventilated patients with ARDS an increase in PEEP by 9 cmH2O (from 12.5±1.5 cmH2O to 21.2±3.3 cmH2O) did not impair right heart function, but improved respiratory mechanics and oxygenation. Introduction: Mechanical ventilation can, while being lifesaving, also cause injury to the lungs. The lung injury is caused by high pressures and mechanical forces but also by inflammatory processes which are not fully understood [1] . Heparin binding protein (HBP) released by activated granulocytes has been indicated as a possible mediator of increased vascular permeability in the lung injury associated with trauma and sepsis [2, 3] . We wanted to investigate if HBP levels were increased in bronco alveolar lavage (BAL) fluid or plasma in a pig model of ventilator induced lung injury. Methods: Anaesthetized pigs were surfactant depleted by saline lavage and randomized to receive ventilation with either tidal volumes of 8 ml/kg with a PEEP of 8 cm H2O (controls, n=6) or 20 ml/kg with a PEEP of 0 cm H2O (ventilator induced lung injury (VILI) group, n=6). Plasma and BAL samples of HBP were taken at 0,1,2,4 and 6 hours (Fig. 1) . Results: Characteristics of pigs by study group are shown in Table  1 . Plasma levels of HBP did not differ significantly between pigs in the control and VILI group at any time of sampling. HBP levels in BAL fluid were significantly higher in the VILI group after 1 (p=0.04), 2 (p=0.03), 4 (p<0.01) and 6 (p=0.02) hours of ventilation (Fig. 2) . Conclusions: In a model of ventilator induced lung injury in pigs, levels of Heparin binding protein in BAL fluid increased significantly over time compared to controls. Plasma levels however did not differ significantly between groups. (Fig. 2) . Conclusions: This meta-analysis concluded that corticosteroid treatment in ARDS provided no benefit in decreasing mortality. In addition, this treatment was not associated with increasing risk of nosocomial infection. (Fig. 1) . The change in the PaO2/FiO2 ratio was significant [RR(95%CI)=0.29(0.12-0.46), p=0.0008] (Fig. 2) . Finally, trial sequential analysis and GRADE indicated lack of firm evidence for a beneficial effect. Conclusions: Surfactant administration may improve oxygenation but has not been shown to improve mortality for adult ARDS patients. Large rigorous randomized trials are needed to explore the effect of surfactant to adult ARDS patients. Moderate to severe acute respiratory distress syndrome in a population of primarily non-sedated patients, an observational cohort study L Bentsen, T Strøm, P Introduction: Extracorporeal carbon-dioxide removal (ECCO2R) might allow ultraprotective mechanical ventilation with lower tidal volume (VT) (<6 mL/kg predicted body weight), plateau (Pplat) (<30cmH2O) and driving pressures to limit ventilator-induced lung injury. This study was undertaken to assess the feasibility and safety of ECCO2R managed with a renal replacement therapy (RRT) platform to enable ultraprotective ventilation of patients with mild-to-moderate ARDS. Methods: 20 patients with mild (n=8) or moderate (n=12) ARDS were included. VT was gradually lowered from 6 to 5, 4.5 and 4 mL/kg, and PEEP adjusted to reach 23<=Pplat<=25 cm H2O. Stand-alone ECCO2R (PRISMALUNG, no hemofilter associated with the RRT platform) was initiated when arterial PaCO2 increased by >20% from its initial value. Ventilation parameters (VT, RR, PEEP), respiratory system compliance, Pplat and driving pressure, arterial blood gases, and ECCO2R-system characteristics were collected during at least 24 hours of ultraprotective ventilation. Complications, day-28 mortality, need for adjuvant therapies, and data on weaning off ECCO2R and mechanical ventilation were also recorded. Results: While VT was reduced from 6 to 4 mL/kg and Pplat kept <25 cmH2O, PEEP was significantly increased from 13.4±3.6 at baseline to 15.0±3.4 cm H2O, and the driving pressure was significantly reduced from 13.0±4.8 to 7.9±3.2 cm H2O (both p<0.05). The PaO2/ FiO2 ratio and respiratory-system compliance were not modified after VT reduction. Mild respiratory acidosis occurred, with mean pH decreasing from 7.39 ± 0.1 to 7.32 ± 0.10 from baseline to 4-mL/kg VT. Mean extracorporeal blood flow, sweep-gas flow and CO2 removal were 421±40 mL/min, 10±0.3 L/min and 51±25 mL/min, respectively. Mean treatment duration was 31±22 hours. Day-28 mortality was 15%. Introduction: There is no consensus on the management of anticoagulation during extracorporeal membrane oxygenation (ECMO). ECMO is currently burdened by a high rate of hemostatic complications, possibly associated with inadequate monitoring of heparin anticoagulation. This study aims to assess the safety and feasibility of an anticoagulation protocol for patients undergoing ECMO based on Thromboelastography (TEG) as opposed to an activated partial thromboplastin time (aPTT)-based protocol. Methods: We performed a multicenter, randomized, controlled trial in two academic tertiary care centers. Adult patients with acute respiratory failure treated with veno-venous ECMO were randomized to manage heparin anticoagulation using a TEG-based protocol (target 16-24 minutes of the R parameter, TEG group), or a standard of care aPTT-based protocol (target 1.5-2 of aPTT ratio, aPTT group). Primary outcomes were safety and feasibility of the study protocol. Results: Forty-two patients were enrolled, 21 were randomized to the TEG group and 21 to the aPTT group. Duration of ECMO was similar in the two groups (9 (7-16) days in the TEG group and 11 (4-17) days in the aPTT group, p=0.74). Heparin dosing was lower in the TEG group compared to the aPTT group (11.7 (9.5-15.3) IU/kg/h versus 15.7 (10.9-21.3) IU/kg/h respectively, p=0.03). Safety parameters, assessed as number of hemorrhagic or thrombotic events and transfusions given, were not different between the two study groups. As for the feasibility, the TEG-based protocol triggered heparin infusion rate adjustments more frequently (p<0.01) and results were less frequently in the target range compared to the aPTT-based protocol (p<0.001). Number of prescribed TEG or aPTT controls (according to study groups) and protocol violations were not different between the study groups. Conclusions: TEG can be safely used to guide anticoagulation management during ECMO. Its use was associated with the administration of lower heparin doses compared to a standard of care aPTTbased protocol. Methods: Single-center retrospective study of patients (n=152; 45 ±11.8 years; 63% males) undergoing VV-ECMO for severe ARDS. The ACP-Score (0-4) was calculated immediately before ECMO initiation and at ECMO-Day1, -Day3 and -Day7, as follows: pneumonia as cause of ARDS -1 point; driving pressure >=18cmH2O -1 point; PaO2/FiO2 ratio <150mmHg -1 point; PaCO2 >=48mmHg -1 point. Results: Longer duration of mechanical ventilation before VV-ECMO was associated with higher ACP-Scores. Patients with higher ACP-Scores before VV-ECMO also presented longer total duration of mechanical ventilation and hospital stay. After VV-ECMO initiation, ACP-Scores significantly decreased from 3.0±0.74 to 1.5±0.84, 1.5 ±0.96 and 1.6±0.99 at ECMO-Day1, -Day3 and -Day7, respectively. At ECMO-Day7, patients with higher ACP-Scores (3-4) presented increased hospital mortality when compared with patients with lower ACP-Scores (0-2): 47.6 vs. 24.7%, respectively (p=0.038). At ECMO-Day7, high driving pressures and low PaO2/FiO2 ratios were the ACP-Score determinants that significantly associated with increased hospital mortality. Conclusions: In severe ARDS, VV-ECMO support allowed a significant and sustained ACP-Score reduction in most patients. This was achieved by artificial lung correction of low PaO2/FiO2, hypercapnia and elevated driving pressures. After an initial period of VV-ECMO support, patients with higher ACP-Scores present higher mortality rates. Our results suggest that on-going adjustment of ECMO and ventilation parameters is necessary to maximize outcome. Introduction: We sought to use mechanical power to describe "lung rest" in patients with acute respiratory distress syndrome (ARDS) supported with extracorporeal membrane oxygenation (ECMO). Mechanical power describes work done by the ventilator on the patient's respiratory system over time. This concept unifies tidal volume, rate, and total pressure delivered during the ventilatory cycle into a discrete value that may be useful to guide ventilatory support. We hypothesized that initiation of ECMO led to decreased mechanical power delivered to the patient. Methods: We reviewed the charts of the three medical intensive care unit patients at our institution supported with ECMO for severe ARDS. We collected data on plateau pressure, driving pressure, and mechanical power before initiating ECMO, then at <6 hours, 24 hours, and 72 hours after. We calculated the mechanical power delivered by the ventilator to the patient in Joules per minute as 0.098 x respiratory rate x tidal volume x (peak pressure -½ x driving pressure) [1] . Results: All patients were alive at discharge and at 90 days. Mean PaO 2 /FiO 2 at ECMO initiation was 64±38, mean plateau pressure was 37±3 cm water. All patients received neuromuscular blockade at initiation of ECMO. Following ECMO initiation, mechanical power decreased by an average of 58%±14% initially, by 69%±4% at 24 hours, and by 66%±17% at 72 hours (Fig. 1) . By comparison, driving pressure changed by an average value of -0.3±8.0, -0.3±5.5, and -2.0±4.6 cm water over those same intervals. Average plateau pressure changed by -3.3±5.7, -4.7±5.5, and -1.7±6.4 cm water during the same time period (Fig. 2) . Conclusions: In our limited case series, mechanical power decreased significantly following initiation of ECMO in patients with severe ARDS. We suggest mechanical power may be more useful than changes in driving pressure or plateau pressure when pursuing "lung rest" during ECMO. Introduction: It is not clear whether acute respiratory distress syndrome (ARDS) is independently associated with mortality after controlling for underlying risk factor and baseline severity of illness. We attempted to assess the attributable mortality of ARDS by performing a systematic review and meta-analysis. Methods: We systematically searched PubMed, EMBASE, Scopus and reference lists to identify observational studies reporting mortality rates of critically ill patients with and without ARDS. All included studies were matched for underlying risk factor. Primary outcomes were all-cause in hospital-mortality and short-term mortality (combined 28 day-mortality and intensive care unit-mortality). We calculated pooled risk ratios (RR) and 95% confidence intervals (CI) with a random-effects model. Our meta-analysis was registered with PROSPERO. Results: Of the 3119 initially retrieved articles, 41 studies (44 cohorts) involving 58408 patients were included. The underlying risk factor was sepsis, trauma and other in 15, 18 and 11 cohorts, respectively. In-hospital mortality was higher in patients with versus without ARDS (31 cohorts; 54101 patients; RR 2.63, 95% CI 2.01-3.44; P<0.001). We saw a numerically stronger association between ARDS and inhospital mortality in trauma (RR 3.15, 95% CI 2.17-4.57; P<0.001) than sepsis (RR 1.80, 95% CI 1.24-2.63; P=0.002). Short-term mortality was higher in patients with versus without ARDS (14 cohorts; 8040 patients; RR 1.88, 95% CI 1.27-2.78; P=0.002). ARDS was independently associated with mortality in approximately half of the 11 cohorts which controlled for baseline severity of illness using a multivariable analysis. Conclusions: The accumulated evidence suggests that ARDS is independently associated with mortality after controlling for underlying risk factor; the association is stronger for trauma than septic patients. Evidence is mixed as to whether ARDS is independently associated with mortality after controlling for baseline severity of illness. Introduction: Evidence is mixed as to whether acute respiratory distress syndrome (ARDS) is independently associated with mortality after controlling for baseline severity of illness, particularly in patients with sepsis. Methods: This was an observational study comparing mortality rates of septic patients with and without ARDS. Subjects for the present study were enrolled in 3 ongoing prospective cohorts of critically ill patients hospitalized in medical intensive care unit (ICU) in the United States or South Korea. ARDS was defined using the Berlin definition for cases after 2012 and the American-European Consensus Conference definition for cases before 2012. Sepsis was defined using the Sepsis-3 definition. Baseline severity of illness was assessed using a modified sequential organ failure assessment (SOFA) after exclusion of the respiratory component. The primary outcome was inhospital mortality. Results: Of the 1024 critically ill patients enrolled in the 3 cohorts, 771 (75.3%) had sepsis and comprised the population of the present study. Of the 771 septic patients, 166 (21.5%) had ARDS. Patients with versus without ARDS had higher SOFA score; both total (median 14 vs 11; P<0.001) and modified (11 vs 10; P<0.001). The unadjusted mortality of septic patients with ARDS was higher than septic patients without ARDS (46.7% vs 22.4%; P<0.001). After controlling for baseline modified SOFA score, both moderate and severe ARDS remained significant predictors for in-hospital mortality [odds ratio (OR) 2.90; 95% confidence intervals (CI) 1.66-5.03; P<0.001 and OR 3.91; 95% CI 2.33-6.58; P<0.001, respectively]. In contrast, after controlling for baseline modified SOFA score, mild ARDS was not associated with in-hospital mortality (OR 1.04; 95% CI 0.40-2.39; P=0.94). Conclusions: Among critically ill patients with sepsis, moderate and severe, but not mild, ARDS are associated with mortality after controlling for baseline severity of illness. A multicenter study on the inter-rater reliability of HEART score among emergency physicians from three Italian emergency departments Introduction: Previous studies suggested that the HEART (based on History, ECG, Age, Risk Factors, Troponin) score could be a valid tool to manage the patients with chest pain at the Emergency Department (Fig. 1 ). Our hypothesis was that there could be heterogeneity in the assignment, because of the History and ECG parameters. For this reason, our objective was to test the HEART reliability. There are no published studies on this topic. Methods: This is a multicenter retrospective study conducted in 3 Italian EDs between March and October 2017 using clinical scenarios. Twenty emergency physicians were included, provided that they had undergone a course on HEART score. We used 53 scenarios from a medical database with each scenario including demographic and clinical characteristics. Each participant assigned scores to the scenarios using the HEART. We tested the measure of interrater agreement using the kappa-statistic, the confidence intervals are bias corrected. A p-value of <0.05 was used to define statistical significance. Results: The participants' assignment is shown in Fig. 2 . The overall inter-rater reliability was good: Kappa = 0.63 (CI 95%; 0.57 -0.72); with a good agreement between the low and high class of risk but a moderate reliability in the medium class: Kappa= 0.72, 0.70 and 0.51. We have not found differences of inter-rater reliability among the senior (more than 5 yrs in ED) and junior physicians: Kappa= 0.65 (CI 95%; 0.57 -0.73) and 0.60 (CI 95%; 0.51 -0-72).The HEART score showed the worse value of inter-rater reliability in the History and ECG parameters : K inter = 0.37 (CI 95%; 0.33 -0.44) and 0.42 (CI 95%; 0.29 -0.50). Conclusions: The HEART showed a good inter-rater reliability but a fair agreement in the History parameter. The clinical experience doesn't influence the agreement in the assignment. The main limit of this study lies in using scenarios rather than real patients. Introduction: The aim of the experiment was to study the efficacy of preconditioning, based on changes in inspiratory oxygen fraction on endothelial function in a model of myocardial ischemia/reperfusion injury in conditions of cardiopulmonary bypass (CPB). Methods: The prospective study included 32 rabbits divided into four equal groups: hypoxic preconditioning; hyperoxic preconditioning (HyperP); hypoxic-hyperoxic preconditioning (HHP); control group. Animals were anesthetized and mechanically ventilated. We provided preconditioning, then started CPB, and then induced acute myocardial infarction by ligation of left anterior descending artery. After 45 minutes of ischemia we performed 120 minutes of reperfusion. We investigated endothelial function markers (endothelin-1 (ET-1), asimmetric dimethylarginine (ADMA), nitric oxide metabolites) at stages before ischemia (after preconditioning in study groups), after ischemia and after reperfusion. Results: The level of ET-1 after the stage of ischemia increased in all groups, a significant difference was between HHP and control group (p=0.006), then ET-1 increased even more after the stage of reperfusion (p=0.003 HHP vs control group). The concentration of nitrite decreased after the stages of ischemia and reperfusion in comparison with the baseline in all groups. However, the level of nitrite after all types of preconditioning was higher than in the control group (p=0.016; 0.046; 0.009). The total concentration of nitric oxide metabolites in the study groups was higher than in the control group: before ischemia (after preconditioning) p=0.034; after ischemia p=0.014; after reperfusion, p=0.022. Concentration of ADMA was lower in the HHP comparing with the control group at the stages after ischemia (p=0.006) and after reperfusion (p=0.027). Conclusions: HyperP and HHP maintain endothelial function: the balance of nitric oxide metabolites and the reduction of ET-1 hyperproduction in a model of myocardial ischemia/reperfusion injury in conditions of CPB. Upscaling hemodynamic and brain monitoring during major cancer surgery: a before-after comparison study Introduction: Hemodynamic and brain monitoring are used in many high-risk surgical patients without well-defined indications and objectives. In order to rationalize both hemodynamic and anesthesia management, we implemented monitoring guidelines for patients undergoing major cancer surgery. Methods: Early 2014, and for all eligible patients, we started to recommend (Standard Operating Procedure, SOP) cardiac output, central venous oxygen saturation, and depth of anesthesia monitoring with specific targets (MAP > 65 mmHg, SVV < 12%, CI > 2.5 l/min/ m 2 , ScvO2 > 75%, 40 < BIS < 60). Eligibility criteria were pelvic or abdominal cancer surgery expected to last > 2 hours in adult patients. Pre-, intra-, and post-operative data were collected from our electronic medical record (EMR) database and compared before (from March to August 2013) and after (from March to August 2014) the SOP implementation. Results: A total of 596 patients were studied, 313 before and 283 after the SOP implementation. The two groups were comparable in terms of age, ASA score, duration and type of surgery, The surgical POSSUM score was higher after than before (20 vs 18, p=0.045). The use of cardiac output, ScvO2 and BIS monitoring increased from 40 to 61%, 61 to 81%, and 60 to 88%, respectively (all p values < 0.05). Intraoperative fluid volumes decreased (16.9 vs 15.2 ml/kg/h, p=0.002), whereas the use of inotropes increased (6 vs 13%, p=0.022). The rate of postoperative delirium (16 vs 8%, p=0.005) and urinary track infection (6 vs 2%, p=0.012) decreased, as well as the median hospital length of stay (9.6 vs 8.8 days, p=0.032). Conclusions: In patients undergoing major surgery for cancer, despite an increase in surgical risk, the implementation of guidelines with predefined targets for hemodynamic and brain monitoring was associated with a significant improvement in postoperative outcome. Introduction: Tissue perfusion and oxygen delivery is low in patients with severe preeclampsia, which would explain multiple organ failure and death in these patients. The aim of this study was to determine the relationship between the base deficit and the risk of adverse maternal and perinatal outcomes. Methods: Retrospective multicenter cohort study included pregnant patients with severe preeclampsia admitted to six intensive care units at tertiary referral centers during a ten years period in Colombia. Clinical information was gathered from hospital medical records. The correlation of base deficit with adverse maternal outcomes was evaluated using logistic regression analysis. Outcomes were maternal death, acute kidney injury, HELLP syndrome, transfusion, eclampsia and extreme neonatal morbidity. Results: 731 patients were included in the study, we found a total of 21 (2,8%) maternal deaths, the median calculated base deficit obtained was -5.5 meq/L. Patients with base deficit greater than -8.0meq/L had significantly higher mortality rates OR 3.02 (CI 1.26-7.2) P 0,013. This group of patients was also associated with a higher probability of developing a class 1 HELLP syndrome OR 1.7 (CI 1.02-2.82) P 0,03. A more mild alteration in the base deficit (greater than -5.0meq/L) was related to the appearance of kidney injury OR 2.25 (CI 1.52-3.34) P 0.00 y complete HELLP OR 2.17 (CI 1.60-2.96) P 0.00. Conclusions: Base deficit is related to worse outcomes in patients with severe preeclampsia. According to our results, a cut-off point greater than -8meq/L, there is a higher risk of death and worse outcomes such as class 1 HELLP syndrome. Comparison of two different laser speckle contrast imaging devices to assess skin microcirculatory blood flow G Guven, Y Ince, OI Soliman, S Akin, C Ince Erasmus MC, University Medical Center Rotterdam, Rotterdam, Netherlands Critical Care 2018, 22(Suppl 1):P261 Introduction: Laser speckle contrast imaging (LSCI) is a common, non-contact and practical method used to assess blood flow of tissue surfaces. We have lack of knowledge about comparability of different LSCI devices due to the arbitrary units (AU) used to define blood flux. We sought to examine the linearity between skin blood flux, recorded using two different LSCI devices. Methods: We performed post-occlusive reactive hyperemia test (PORH) on the arm and measured blood flux on the hand using two different LSCI devices (Moor Instruments, Devon, UK and Perimed AB, Järfälla, Sweden). All volunteers were measured at baseline, during occlusion and after release of occlusion during the hyperemia phase. The third finger and fourth finger nail were selected for recording blood flux and AU were used to express values. Results: Fifteen healthy, non-smoker male volunteers participated in this study. An excellent correlation was found between the two LSCI devices (finger: R2:0.79, p<0.001 & finger nail: R2:0.68, p<0.001). Data were also assessed in terms of the variability at different stages of the PORH test (Fig. 1a-d) . Correlation of devices was still high at baseline, first minute of occlusion and in the post-occlusion hyperemia phase. However, in the period between 1 minute after start of the occlusion and the beginning of the hyperemia, correlation was lower for the whole finger (R2:0.21, p=0.002) and correlation was lost for fingernail (R2:0.05, p=0.14) between the two devices. Conclusions: Skin blood flux measured with two different LSCI devices are linearly correlated with each other. However care should be taken when assessing patients with low blood flux such as occurs during shock and ischemic organs. Introduction: The aim of this study was to evaluate the effects of hyperoxia and mild hypoxia on microcirculatory perfusion in a rat model. Methods: Spontaneously breathing anesthetized (isoflurane) male Wistar rats (n=12) were equipped with arterial (left carotid) and venous (right jugular) cannulae and tracheotomy. Rats were randomized in 3 groups: normoxiainspired oxygen fraction (FiO 2 ) of 0.21; hyperoxia -FiO 2 1; mild hypoxia -FiO 2 0.15. The following measurements were taken hourly for 4 hours: blood gases, mean arterial pressure (MAP), stroke volume index (SVI) and heart rate (echocardiography), skeletal muscle microvascular density (sidestream dark field videomicroscopy). Results: At 1 hour, arterial O2 tension was 103±19 mmHg in normoxia, 296±60 mmHg in hyperoxia, 62±8 mmHg in mild hypoxia (p<0.001). Hyperoxia induced an increase in MAP (from 109±13 to 129±8 mmHg at 1h, p<0.05) and a decrease in SVI (from 0.67±0.1 to 0.59±0.1 ml/kg at 1h, p<0.05), while in mild hypoxia MAP tended to decrease and SVI tended to increase (p>0.05). Microvascular density decreased in hyperoxia and increased in mild hypoxia (Fig. 1) . Conclusions: In anesthetized rats, microvascular density decreased with hyperoxia and increased with mild hypoxia. Introduction: The imbalance between oxygen (O2) delivery and O2 requirement in patients with sepsis can be assessed by central venous oxygen saturation (ScvO2). The low or high ScvO2 may indicate cellular hypoxia or inability to utilize the O2. This study aims to determine the relationship between high ScvO2 and mortality in patients with sepsis. Methods: A retrospective observational cohort study was done by collecting data (i.e., baseline characteristics, severity of infection and vasopressors) from medical records of >=15-year-old patients with sepsis and 1st ScvO2 measurement within 24 hours of sepsis, who were admitted in a university hospital between 2013 and 2014. The patients were stratified by 1st ScvO2 level (<70%, 70-80%, >80%) and APACHE-II score (<=25, >25). The primary outcome was inhospital mortality. Results: Among 376 patients, those with high ScvO2 (17.3%) and low ScvO2 (53.7%) were associated with adjusted hazard ratios for death of 0.79 (0.54-1.15, p=0.218) and 1.16 (0.86-1.56, p=0.325), respectively, while those with normal ScvO2 (29.0%) as control. When the patients were stratified by ScvO2 level and APACHE-II score, using patients with normal ScvO2 and low APACHE-II score as control, those with high ScvO2 and low APACHE-II score, and those with low ScvO2 and low APACHE-II score had adjusted hazard ratios of 0.54 (0.31-0.97, p=0.038) and 1.18 (0.79-1.76, p=0.432). For those with normal, high and low ScvO2, and high APACHE-II score had adjusted hazard ratios of 1.62 (1.02-2.57, p=0.041), 1.77 (1.05-2.96, p=0.031), and 1.88 (1.23-2.87, p=0.004), respectively. Conclusions: The ScvO2 >80% with APACHE-II score >25, but not only ScvO2 >80%, is independently related to increased mortality in patients with sepsis. Introduction: Serum lactic acid levels and ScvO2 are useful predictive parameters for patients with sepsis. However, little is known the differences in the impact of lactate levels and ScvO2 on the prognosis of septic patients. In this study, we investigated these differences by analysing septic patients' characteristics and prognosis. Methods: This study is a post hoc analysis of data obtained from a multicentre, prospective, randomized controlled trial, which compared two fluid management strategies for septic patients requiring mechanical ventilation. We categorised patients into the following four groups: ScvO2 >= 70% and lactic acid levels < 2 mmol/L (HH group); ScvO2 >= 70% and lactic acid levels < 2 mmol/L (HL group); ScvO2 < 70% and lactic acid levels >= 2 mmol/L (LH group) and ScvO2 < 70% and lactic acid levels < 2 mmol/L (LL group). SOFA score, SAPS II score, lactic acid levels, ScvO2 and BNP were evaluated. Primary outcome was 28-day mortality, whereas secondary outcomes were the duration of mechanical ventilation, administration of CRRT, duration of catecholamine therapy and length of ICU stay. Results: In total, 104 patients were included: HH group (n = 32), HL group (n = 31), LH group (n = 25) and LL group (n = 16). No significant differences were observed in terms of patient characteristics. Further, 28-day mortality was 32% in the LH group, 28.1% in the HH group, 25% in the LL group and 13% in the HL group, and there was no significant difference in terms of mortality among the groups. Furthermore, there were no significant differences in terms of secondary outcomes. On multivariate analysis using the HL group as reference, the odds ratios for 28-day mortality in the LH, HH and LL groups were 1.21 (95%CI, 0.5-5.8), 1.62 (95%CI, 0.36-7.2) and 2.0 (95%CI, 0.37-10.9), respectively. Conclusions: Because 28-day mortality was higher in the HH group than in the LL group, serum lactic acid levels may have bigger impact on the prognosis of septic patients. Introduction: In septic shock endothelial damage can lead to failure of microcirculation and low microcirculatory oxygen saturation. In the skin this is seen as mottling and can be quantified using hyper Fig. 1 (abstract P262) . Changes in microvascular density spectral imaging. There is insufficient data about associations between skin oxygenation, severity of illness, biomarkers of endothelial damage and mortality in patients with septic shock. Methods: This single centre observational study was performed in 24 consecutive intensive care patients with septic shock. Within 24 hours of admission hyper spectral imaging of knee area skin was performed and blood was sampled for assay of biomarkers of endothelial cell damage (plasminogen activator inhibitor -1 (PAI-1), soluble intercellular adhesion molecule (sICAM-1), soluble vascular cell adhesion molecule (sVCAM-1), thrombomodulin, angiopoetin-2). Nonlinear fitting of optical density spectra was used to calculate relative skin oxy/deoxy hemoglobin concentration and obtain oxygen saturation. The association between skin oxygen saturation, biomarkers, sepsis severity (APACHE II, SOFA) and 28-day mortality was analyzed. Results: The median (IQR) age of patients was 71 years (62 to 76), and 60% were males. The median SOFA and APACHE II scores were 9 (7 to 12) and 24 (19 to 27) and 28-day mortality rate was 29%. 7 patients (37%) had mottling. There was a relationship between skin oxygenation, plasma biomarkers (thrombomodulin and sVCAM-1) and sepsis severity assessed by SOFA and APACHE II scores, P < 0.05. Using logistic regression analysis, skin oxygenation and biomarker concentrations were not associated with 28-day mortality rate. Conclusions: In our cohort of patients with septic shock, skin oxygenation and biomarkers of endothelial injury were strongly associated with initial severity of sepsis but poorly predictive of 28-day mortality. Comparison between ultrasound guided technique and digital palpation technique for radial artery cannulation in adult patients: a meta-analysis of randomized controlled trials S Maitra, S Bhattacharjee, D Baidya All India Institute of Medical Sciences, New Delhi, New Delhi, India Critical Care 2018, 22(Suppl 1):P266 Introduction: Possible advantages and risks associated with ultrasound guided radial artery cannulation in-comparison to digital palpation guided method in adult patients are not fully known. Previous meta-analyses included both adult and pediatric patients and long axis in-plane technique and short axis out of plane technique in the same analysis, which may have incurred biases [1, 2] . Methods: PubMed and Cochrane Central Register of Controlled Trials (CENTRAL) were searched (from 1946 to 20th November 2017) to identify prospective randomized controlled trials in adult patients where 2dimensional ultrasound guided radial artery catheterization has been compared with digital palpation guided technique. For continuous variables, a mean difference was computed at the study level, and a weighted standardized mean difference (SMD) was computed in order to pool the results across all studies. For binary outcomes, the pooled odds ratio (OR) with 95% confidence interval (95% CI) was calculated using the inverse variance method. Results: Data of 1895 patients from 10 studies have been included in this meta-analysis. Overall cannulation success rate was similar between short axis out of plane technique and digital palpation [p=0.27; Fig. 1 ] and long axis in-plane technique with digital palpation. Ultrasound guided long axis in-plane approach and short axis out of plane approach provides better first attempt success rate of radial artery cannulation in comparison to digital palpation [p=0.01 and p=0.0002 respectively; Fig.  2 ]. No difference was seen in time to cannulate between long axis and short axis technique with palpation technique. Conclusions: USG guided radial artery cannulation may increase the first attempt success rate but not the over all cannulation success when compared to digital palpation technique. Introduction: Ultrasound guidance may improve the success rate of vascular cannulation. There is lack of data regarding the utility of USG guided arterial cannulation in critically ill patients in shock. We aim to compare the impact of using real time ultrasound guidance versus palpation method in achieving arterial catheterization in critically ill patients in hypotension. Methods: A single center, prospective, randomized trial was performed among 100 critically ill patients aged >18 years, with hypotension (or requiring vasopressor infusion) and on not previous cannulated radial arteries. Patients were randomized in a ratio of 1:1 to the ultrasound group or palpation group. Under aseptic precautions, arterial puncture was performed using appropriate sized Leader Cath (Vygon, Ecquen, France), under real time USG guidance using short-axis out-of-plane view with bevel down. Data were recorded and compared between two groups. The unpaired Student's t-test or Mann-Whitney U test were used for continuous variables, and the uncorrected Chi-squared or Fisher's exact test were used for proportions. Results: A total of 100 patients with hypotensive shock requiring radial artery catheterization were randomized into palpation (n = 51) and ultrasound (n = 49) groups. First pass success rate was significantly higher in ultrasound group as compared to palpation group (83% vs 41%, p<0.0001). Cannulation time was significantly shorter in ultrasound group (72.9 vs 88.7,p<0.05). Early complications were significantly higher in palpation group compared to ultrasound group (14.6% vs 5.2%, p<0.001). Conclusions: In critically ill patients with hypotension (or requiring vasopressors), ultrasound guidance improved first pass success rate, shortened the cannulation time and reduced the rate of early complications in radial artery catheterizations. Relationship between inferior vena cava diameter and variability with mean arterial pressure and respiratory effort B Kalin, K Inci, G Gursel Gazi University School of Medicine, Ankara, Turkey Critical Care 2018, 22(Suppl 1):P268 Introduction: There is no consensus on the use of vena cava inferior (IVC) diameter and variability in the assessment of fluid response (FR) in spontaneously breathing ICU patients. Influence from respiratory effort, experience requirement and measurement problems are reasons for not being preferred. The aim of the study is to investigate the relationship between IVC diameter, variability and spontaneous breathing effort and hypotension measured by ultrasonography in spontaneously breathing intensive care patients Methods: The maximum and minimum diameters of the IVC were measured and the collapsibility index (CI) was calculated. Measurements were made in 2D mode on cineloop recordings. Diaphragm thickening ratio was used as a measure of respiratory effort. Correlations between respiratory effort criteria with IVC minimum diameter and CI were calculated by Pearson's correlation coefficient. IVC measurement criterias, such as inspiratory diameter of < 1 cm, 25%, 40%, 45% of the CI were compared with Chi square test in hypotensive and non-hypotensive patients. We took two mean arterial pressure threshold for hypotension as 60 and 70 mmHg for this calculation. Results: 62 patients were included in the study. For both hypotensive threshold values, there was no significant difference in the rates of hypotensive and non-hypotensive patients with and without a minimum IVC diameter of 1 cm below. Even there was no significant relationship between the CI higher than 25%, 40% and 50% and hypotension (p>0.05). In spontaneously breathing patients, a significant correlation was found between respiratory effort and IVC CI and IVC diameter < 1 cm Conclusions: At the end of the study, there was a correlation between spontaneous breathing effort IVC diameter and CI in the intubed patients. Additionally the result that IVC CI is not different even between hypotensive and non-hypotensive patients suggests that this method should be used with caution in predicting FR. Introduction: Fluid responsiveness in ICU patients can be assessed using changes in pulse rate and blood pressure following administration of a fluid bolus, assisted if necessary by cardiac output (CO) monitors such as the LiDCOplus. This uses pulse contour analysis to estimate stroke volume (SV), with >10% change in SV following a fluid challenge (FC) signifying overall benefit. There is no evidence that the use of CO monitoring improves patient outcomes and it is unclear if it improves clinical decision making. Methods: A LiDCOplus monitor was set up with the screen covered. A 250ml FC was administered over 2 minutes. The heart rate, systolic and mean arterial pressures were recorded before and after the FC. The clinician administering the FC was asked to decide if the patient was fluid responsive. Following this decision, the SV change was revealed and the clinician asked again to assess fluid responsiveness. Results: Forty-five fluid challenges were studied. Use of the LiDCO changed the decision made on 7 occasions (Fig. 1) . In three patients (7%), this change in decision was appropriate and either corrected a misinterpretation of the haemodynamic data or represented a patient whose only marker of fluid responsiveness was a SV change. In four patients (9%), the LiDCO changed the decision inappropriately from a correct interpretation of the haemodynamic data. In six patients (13%) the SV change was ignored when it should have changed the initial decision. In the remaining 32 patients (71%) the decision made with the haemodynamic data was in agreement with the SV change and unchanged by revealing the LiDCO data. Conclusions: The use of LiDCO monitoring only appropriately changed the decision made with information from basic haemodynamic monitoring in 7% of patients. This augmentation of decision making was only seen in patients whose basic haemodynamic parameters did not respond to fluid. It changed a correct decision inappropriately in 9%. Overall, no improvement in the assessment of fluid responsiveness was seen. Introduction: There are accumulating evidences suggesting that intraoperative blood pressure affects postoperative outcome including myocardial injury, acute kidney injury, stroke, and mortality. In a patient undergoing laryngeal microsurgery (LMS), blood pressure usually rises sharply due to the stimulation on the larynx. Since pulse transit time (PTT) has been reported to reflect arterial blood pressure fairly well, it has possibility to be a marker for blood pressure which reflects beat-to-beat changes in blood pressure and is less invasive than arterial catheterization. Methods: Intraoperative noninvasive blood pressure (NIBP), electrocardiogram (ECG), and photoplethysmogram (PPG) of 26 patients undergoing LMS were recorded simultaneously. PTT was defined as a time interval between the R-wave peak on ECG and the point which the maximal rising slope appears on the PPG. The mean PTT values for one minute before and after the increase in blood pressure due to the stimulation on larynx were compared. Parameters of PPG such as width, height, maximal slope, minimal slope, and area were also compared. Then, correlation between blood pressure and each variable was calculated. Results: As the larynx was stimulated by LMS, NIBPs have surged (systolic blood pressure, 113. 6 P<0 .001) significantly in most of the patients. Systolic blood pressure and PTT were inversely correlated (r = -0.636, P < 0.001). Minimum slope of PPG also showed good negative correlation with systolic blood pressure (r = -0.537, P < 0.001). Conclusions: PPT showed good correlation with systolic blood pressure and may have potential to be used as noninvasive continuous blood pressure monitor during a surgery in which blood pressure changes abruptly. Introduction: Aim of this prospective randomized pilot study was to investigate influence of intra operative restrictive volume approach and post operative lung ultrasound (LUS)on prevention and early detection of postoperative interstitial syndrome development Methods: 42 cardiac patients who underwent non cardiac surgical procedure were randomly assigned for: group A-liberal volume approach or for group B-combination of restrictive intra operative volume approach and small dose of norepinephrine. All patients post operatively received <=1.5 ml/kg/h fluids, mostly crystalloids. LUS was performed before surgical procedure and 24 hours after their admission in ICU together with arterial blood gases measurements. The ultrasound characteristic of interstitial syndrome was development of B profile Results: Before surgery all patients had A profile. Twenty for hours later in A group significantly higher number of patients 16/22 (72.7%) vs 3/22(13.6%) in B group,had B profile (p<0.05).At the same time there were no significant difference between the groups in amount of patients with PaO2/FiO2 ratio <= 270 (3 patients with positive B lines from A group vs 0 patients from group B).(p>0.05) Conclusions: Intra operative fluid restriction is efficient in prevention of post operative cardiogenic pulmonary edema development. LUS is a simple non invasive method for early detection of interstitial syndrome even before development of signs of respiratory deterioration. Introduction: The peak rate of left ventricular (LV) pressure (dP/dtmax) has been classically used as a marker of LV systolic function. Since measuring LV dP/dtmax requires LV catheterization, other surrogates have been proposed using the peripheral arterial waveform. The aim of this study was to test the performance of LV and arterial (aortic and femoral) dP/dtmax for assessing LV systolic function against the gold-standard (the slope of the end-systolic pressure-volume relationship, Emax) during different cardiac loading and contractile conditions. Methods: Experimental study in 6 pigs. LV pressure-volume data was obtained with a conductance catheter and peripheral pressures were measured via a fluid-filled catheter into the aortic, femoral, and radial arteries. Emax was calculated during a transient occlusion of the inferior vena cava. The experimental protocol consisted in three consecutive stages with two opposite interventions each: changes in afterload (phenylephrine and nitroprusside), preload (bleeding and fluid bolus), and contractility (esmolol and dobutamine) (Fig. 1) . Measurements were obtained before and after each hemodynamic intervention. Results: Emax variations and LV, aortic, femoral and radial dP/dtmax changes throughout the study are shown in Fig. 2 . All peripheral artery-derived dP/dtmax underestimated LV dP/dtmax. Percentage changes in LV and femoral ddP/dtmax were tightly correlated (r 2 =0.77; P<0.02). Both LV and femoral dP/dtmax were affected by preload changes during fluid infusion. All peripheral dP/dtmax estimations allow to detect LV systolic function changes according to Emax during isolated variations in contractility. Conclusions: Femoral and LV dP/dtmax accurately reflected Emax changes, although both were affected by preload changes during fluid administration. Fig. 2 (abstract P272) . Emax, LV dP/dtmax and aortic, femoral and radial dP/dtmax changes. (Table 1 , Fig. 1 ). Concordance was <95% and radial LOA was ±<30°for all devices; mean polar bias was <5°for FT only (table 2, Fig. 2) . Conclusions: CS, FT and PA are not interchangeable with TPTD, because of inaccuracy [2] . When considering limitations they may be used for trending. Introduction: About 100 years ago, the German physiologist Pflüger stated that the cardio-respiratory system fulfils its physiological task by guaranteeing cellular oxygen supply and removing waste products of cellular metabolism. Methods: The study was performed in early postoperative period after major abdominal surgery in 160 patients. The physical condition of patients corresponded to 3 class of ASA. The median age was 46.0 (38.0-62.0) years. Duration of the surgery was 6,4 (4,8-9,5) hours. Surgery was performed under combined epidural anesthesia with mechanical ventilation. The study was conducted in the following stages: 1-admission from operating room; 2 -in 1-3 hours; 3 -4-7 hours; 4 -8-12 hours; 5 -after 13-24 hours after the surgery. Results: Depend on rate of oxygen extraction index (ERO2) 4 groups were revealed: group 1 (n=44)low ERO2 (<21%) followed by recovery to normal levels to stage 2-3 (ERO2 = 22-32%), group 2 (n=42)normal level ERO2 (2232%) in all the stages, group 3 (n=40)high levels ERO2 (>33%) with recovery to normal levels to stage 2, group 4 (n=34)high ERO2 (>35%) in all the stages. Oxygen extraction index at admission to ICU after surgery can be normal (26.25% of patients), reduced (27.5% of patients) or high (46.25% of patients). When oxygen extraction ratio is reduced metabolic recovery occurs classically after 4-7 hours; when ERO2 is elevated -after 812 hours. Core temperature improvement is connected with the restoration of oxygen homeostasis. So, under normal and reduced ERO2 even mild central hypothermia after surgery were not observed, and at an elevated ERO2 moderate hypothermia after surgery was observed with only to 4-7 hours post-surgery restoration. Conclusions: Maintaining an adequate tissue oxygenation is the cornerstone of metabolic response and postoperative recovery in patient after major abdominal surgery. (Fig. 1) . Patients with cSO2<50%time above 50%h had an odds ratio of hospital survival of 0.19 (95%CI 0.38-0.91, p=0.037) (Fig. 2) . Conclusions: Cerebral oxygen desaturation below 50% was significantly associated with outcome in patients undergoing vaECMO. In patients with cSO2<50%time above 50h%, prognosis was especially poor. Prospective trials are needed to evaluate if cSO2 is a viable target for therapeutic interventions. Introduction: During the second consensus meeting on microcirculatory analysis the exploration of novel parameters related to physiological function of the microcirculation was proposed. Capillary hematocrit (cHct) is a direct measure of capillary hemodilution, a potential mechanism of microcirculatory dysfunction in states of shock. Our hypothesis was that by application of advanced computer vision (I) cHct can be reliably measured in given capillaries, and (II) change in cHct reflects capillary hemodilution induced by cardiopulmonary bypass (CPB). Methods: In 11 patients undergoing coronary artery bypass surgery 3 sublingual capillary microscopy videos were recorded before and during CPB primed with HES 130/0.4. Per-capillary cHct was estimated as the product of the number of red blood cells (RBC) and an assumed volume of 90nl, divided by the capillary volume including plasma gaps. RBC number was assessed by manual counting in the first frame of a given video clip, as well as using a novel advanced computer vision algorithm employing blob detection to calculate the mean per-capillary RBC number in all frames of a given video clip (Fig. 1) . Results: 100 capillaries were analyzed, within a total of 100 and 322000 frames using manual and algorithmic analysis. A good correlation was found between both methods for cHct (r=0.79, p<0.01, Fig. 2 ). CPB initiation resulted in an decrease in cHct from (mean±SEM) 0.23±0.02 to 0.18±0.01, p<0.001 and 0.23±0.02 to 0.18±0.01, p=0.05 in manual and algorithm. Conclusions: Accurate measurement of cHct is possible using advanced computer vision, and it reflects hemodilution induced by initiation of CPB. cHct further is a determinant of capillary delivery of oxygen. Combined with the assessment of functional capillary volume, blood flow velocity, and capillary hemoglobin saturation, cHct may enable direct optical quantification of capillary delivery of oxygen as an integrated functional parameter of the microcirculation. Fig. 2 (abstract P277) . Prognosis of patients with cSO2<50%time above 50%h was poor Fig. 1 (abstract P278) . Detection of single erythrocytes using a novel advanced computer vision algorithm in a representative capillary ribbon extracted from a video frame of the sublingual microcirculation Fig. 1 (abstract P277) . The area under cSO2<50% was significantly lower in survivors Introduction: Cardiac function is known to be impacted by sepsis. Passive Leg Raise (PLR) is an effective method to predict fluid responsiveness (FR) or cardiac response to preload expansion. Preload functional status and trending cardiac output may identify patient phenotypes with varying cardiac reserve, dysfunction and outcome. Methods: Patient data were analyzed from a currently enrolling prospective randomized controlled study, evaluating the incidence of FR in critically ill patients with sepsis or septic shock (FRESH study, NCT02837731). Patients randomized to PLR guided resuscitation were classified as PLR+ (fluid responsive/preload dependent) if stroke volume (SV) increased >= 10% when measured with a non-invasive bioreactance device (Starling SV, Cheetah Medical). Patients were categorized into 5 different phenotypic cohorts based on changing physiology exhibited on PLR and trending cardiac output over the initial 72 hours of therapy. Results: A total of 269 PLR assessments were performed in 31 patients. Overall, 36% (96/269) of assessments indicated a patient was PLR+ after receiving initial resuscitation fluid of~3L. Most patients (71%) demonstrated a dynamic physiology with changing PLR Status occurring > 1 time over 72 hours. There were no differences among the 5 groups with respect to age, gender, or QSOFA score (Fig. 1) . Patients in Group 1 exhibited a significantly decreased ICU stay (113.8 hours) compared to Group 3 (271.1 hours, p=0.024) (Fig. 2) . Patients in Group 3 exhibited significantly increased ECHO evidence of LV/RV cardiac dysfunction (77%), compared to Group 1 (16%, p=0.02) ( Table 1) . Patients in Group 4 exhibited 100% evidence of ECHO based LV/RV cardiac dysfunction. Conclusions: Physiological based resuscitation phenotypes identify significantly different patient groups. Patients who are initially not PLR+, but then become PLR+ with no improved CO are significantly more likely to have confirmed LV/RV dysfunction and a significantly longer ICU stay. Introduction: Accurate measurement of a patient's intravascular volume status remains an unsolved clinical problem in the ICU setting. In particular, septic and cardio-renal patients often receive volume challenges or diuresis, respectively, with little appreciation of baseline BV or the resulting response. This can lead to volume overload and/or depletion and associated increases in morbidity, mortality and hospital length of stay. Methods: We tested the performance of a novel, rapid, minimally invasive technique capable of measuring PV, BV and glomerular filtration rate (mGFR) in 32 human subjects. The method consists of a single IV injection of a large (150 kDa) carboxymethyl dextran conjugated to a rhodamine-derived dye and a small (5 kDa) carboxymethyl dextran conjugated to fluorescein. Plasma and blood volumes were quantified 15 minutes following the injection of the dye based on the indicatordilution principle. Results: This phase 2b study included 16 normal subjects, 8 chronic kidney disease (CKD) stage III and 8 CKD stage IV subjects. PV and BV varied according to weight and body surface area, with PV ranging from 2115 to 6234 mls, and both were stable for greater than six hours with repeated measurements. There was excellent agreement ( Fig. 1) with Nadler's formula for PV in normal subjects. A 24 hour repeat dose measurement in 8 healthy subjects showed PV variability of less than +/-5%. Following an intravenous bolus of 350 ml 5% albumin solution the mean +/-(SD) measured increase in PV was 326.8 ml +/-49.9 ml post infusion (Fig. 2) . Conclusions: This novel bedside approach allowed for rapid and accurate determination of PV, BV, mGFR (data not shown) and dynamic monitoring following clinical maneuvers such as fluid administration, with a high level of safety, accuracy and reproducibility. This approach should assist the Intensivist especially with volume administration and removal in septic and cardiorenal patients. Introduction: Accumulating evidence shows that fluid overload is independently associated with adverse outcome in children and adults with acute lung injury. Fluid restriction initiated early in the disease process may prove beneficial, potentially by diminishing the formation of interstitial edema. The main goal of this study was to determine the short-term biophysical effects of intravenous (IV) fluid restriction during acute lung injury in relation to age. Methods: Infant (2-3 weeks) and adult (3-4 months) Wistar rats were mechanically ventilated (MV) 24 hours after intratracheal inoculation with lipopolysaccharide to model acute lung injury. Both age groups were randomized to either a normal or restrictive IV fluid regimen during 6 hours of MV. Thereafter the rats were sacrificed and studied for markers of interstitial edema formation (wet-dry weight ratios), lung permeability (total protein and alpha-2 macroglobulin (A2M) in bronchoalveolar lavage; BAL) and local inflammation (cell counts and cytokines in BAL). Results: Restrictive fluid therapy was not associated with worsening of hemodynamic indices during the period of MV in either infant or adult rats. However, as compared to the normal fluid regimen, restrictive fluid therapy led to lower wet-dry weight ratios of the lungs and kidneys in adult rats (p < 0.05), but not in infants (Figs. 1 and 2). No difference was found in total protein and A2M in BAL between the two fluid regimens in both age groups. Also, neutrophil influx in the lungs did not differ between fluid regimens in both age categories, nor did the influx of inflammatory cytokines IL-6 and MIP-2 in BAL fluid. Conclusions: There is an age-dependent effect of early fluid restriction on the formation of interstitial edema in local and distant organs in the disease process of acute lung injury. Further investigation of the effects of fluid therapies in experimental models may help steering towards better treatment in critically ill patients. . 1) . In a multivariate analysis FB was independently associated with: group C (p<0.001), a history of diabetes (p=0.03), the Acute Physiology and Chronic Health Evaluation III score (<0.001) and the duration of aortic-cross clamp (p<0.001). The main findings of this study substantiated the hypothesis that the introduction of continuous FB-tracking throughout the entire care process, is associated with a significant reduction in the administration of fluids in post-cardiac surgery patients, independent of differences in their baseline characteristics. Demonstrating that certain organizational changes can influence medical behavior beyond the scope of teaching and instruction, and therefore serves to provide awareness to the current issue known as 'knowledge-to-care gap'. Using a protocol for fluid resuscitation: how well is it followed? Introduction: Positive fluid balance in ICU patients has been correlated with worse outcomes [1] . Consequently, we developed a protocol to guide fluid resuscitation. The protocol was introduced in 2011 and mandates that fluid responsiveness is assessed when administering fluid boluses. Once a patient becomes fluid unresponsive, no further resuscitation fluid should be administered. To assess responsiveness, the protocol advises the use of haemodynamic data such as heart rate and blood pressure as well as the change in stroke volume (SV) measured by a LiDCOplus monitor. After years of use and a rolling education program this protocol was felt to be well ingrained in our unit culture. We then assessed how well it was being followed. Methods: Staff performing fluid challenges were asked to fill out a form recording the haemodynamic and SV data measured before and after a fluid challenge. They were also asked to record their interpretation of just the haemodynamic data and then this data combined with the SV data. Results: Forty five forms were completed. The protocol was not followed on 16 occasions (36%). Four patients who should have been assessed as responsive were deemed to be unresponsive. Six patients who should have been assessed as unresponsive were assessed as being responsive. The remaining deviations from the protocol represent misinterpretation of the haemodynamic data but correct use of the SV data to reach a correct final assessment. Conclusions: Despite being a longstanding ingrained practice in our ICU, this review suggests that the protocol for fluid resuscitation is being followed incorrectly approximately a third of the time. This could result in inappropriate under or over administration of IV fluid. We plan to review the educational programme and raise awareness of the protocol to try and improve future compliance. Introduction: Understanding the effects of therapeutics on the left ventricular (LV) loading conditions is of utmost importance in critically ill patients. The effective arterial elastance (Ea=ESP/SV, where ESP is aortic end-systolic pressure and SV stroke volume) is a lumped parameter of arterial load that has been proposed as an index of LV afterload. We aimed at comparing the effects of fluid administration on ESP (i.e., the LV afterload in the pressure-volume phase-plane according to the classic "cardiocentric" framework) and on Ea. Methods: In 30 mechanically ventilated patients, we recorded Ea from the femoral peripheral systolic arterial pressure SAP (Ea=(0.9×femoral SAP)/SV) before and after the infusion of 500-mL of saline. Patients in whom fluid administration induced an increase in cardiac index (PICCO-2) >=15% were defined as "responders". Introduction: The respiratory variations of the inferior vena cava (IVC) diameter in mechanically ventilated patients with preload responsiveness could be explain by a higher compliance of the IVC and/or higher respiratory variations of the IVC backward pressure, i.e., the central venous pressure (CVP).We aimed at determining the respective weight of these two phenomena. Methods: In 25 mechanically ventilated patients, haemodynamic, respiratory and the intra-abdominal pressure (IAP) signals were continuously computerised. CVP, IAP and the IVC diameter (transthoracic echocardiography) were recorded during end-inspiratory and endexpiratory occlusions, before and after the infusion of 500-mL of saline. Patients in whom fluid administration induced an increase in cardiac index (PICCO-2) >=15% were defined as "responders". The respiratory variations of the IVC diameter, CVP and IAP were calculated as (end-inspiratory -end-expiratory values)/mean value. The compliance of the IVC was estimated by the ratio between (end-expiratoryend-inspiratory) values of IVC diameter and CVP. Results: Fluid administration increased cardiac index by more than 15% in 9 patients. The respiratory variations of the IVC diameter predicted fluid responsiveness (area under the ROC curve: 0.799 (95%CI: 0.591-0.931), p<0.05). Before fluid administration, the compliance of the IVC was not different between responders and non-responders (0.75±0.32 vs. 0.79±1.14 mm/mmHg, p=0.91), whereas the respiratory variations of the CVP were higher in responders than in nonresponders (36±24 vs. 20±10 %, p=0.03). The respiratory variations of the IVC diameter were associated with the respiratory variations of CVP (r=0.49, p=0.01) but not of IAP (r=-0.12, p=0.56). Conclusions: The respiratory variations of the IVC diameter rather depend on the respiratory variations of the CVP than on the IVC compliance. The IAP seems to not be involved in the respiratory variations of the IVC diameter. hours and GEDI measured at the same time was examined. Since the dataset used in this study consists of repeated measurement data, the analysis used the general linear mixed effect model (GLMM). The multivariate analysis adjusted with age, Cr, and cardiac index was also conducted. Results: Of the 143 patients with the total BNP measurements conducted for 412 times and GEDI measurements for 171 times, the median of age and SAPS2 were 73 (IQR 62-80) and 53 (IQR 43-67), and the hospital mortality rate was 25%. The univariable analysis and the multivariable analysis using GLMM respectively found statistically significant differences, with regression coefficient at 0.03 95%CI 0.01-0.06 (p=0.02), and 0.06 95%CI 0.03-0.09 (p<0.001). Conclusions: While a positive correlation between GEDI and BNP was statistically identified, its effect may be minor in clinical terms, and its significant clinical difference remains unclear. Introduction: Fluids are a cornerstone of the management of critically ill patients who are at risk of multiple organ dysfunction syndrome. However positive fluid balance (FB) is associated with worse morbidity and mortality in this population, so fluid administration needs to be carefully titrated and the nutritional support products must be taken in consideration. Objective: Evaluate the impact of nutritional support in the fluid balance in a Intensive Care Unit Methods: Observational prospective study, conducted in eleven Portuguese ICUs of nine general hospitals. Patients with 18 years of age or older were eligible if they were ventilated and had a length of stay (LOS) in ICU greater than 7 days. Demographic data, fluid balance along type of nutritional support used in the first 7 days and were collected from the selected patients. Results: 130 patients were enrolled, 63.8% were male, the median age -64±16 (19-91), ICU LOS -15.4±6.1 days, mortality rate of 26.9% (35). 70 % of patients were admitted for medical reasons, 31.5% had normal weight, the remaining patients were either overweight or obese. The average daily FB in the eight days was 258 ± 464 ml, being the maximum at day 1 with +1152 ml, slowly trending down reaching a neutral balance at day 4 and reaching -224 ml at day 7. In the first days the majority of the intake is due to resuscitation driven fluids, however the nutritional support contribution rises as the days passes, reaching 25% at day 4 and 35% at day 7 ( Fig. 1) . Regarding the administration route, the enteral route was responsible to 28,9% of fluids at day 7 compared to 6,5% of parenteral route. The nutritional support is an factor to take into account regarding fluid balance in Intensive Care Units. In this study after the 4th day the nutritional support, it was responsible for more than 25% of the total volume that was delivered to the patient and with an higher impact with the increase in LOS Results: We included 64 patients with mean age 65 years, 53% male, APACHE 28 ± 7, SAPS II 56 ± 20, SOFA in admission 8 ± 3, mechanical ventilation 76%, continuous renal replacement techniques 38%. The mean total volume administered during the first 7 days was 26 ± 8L with a mean DCB of 16 ± 8L and a mean fluid accumulation of 21% ± 13. Regarding Fluid accumulation: 17% have <10%, 35% between 10-20% and 47.5% > 20%. 28th-day mortality and ICU mortality were 17% i 28% respectively. During the first week, the percentage of fluid accumulation was significantly higher in non-survivors than in survivors (28.5 ± 10.7 L vs. 18.7 ± 13.1 L, p 0.046) (Fig. 1) . Cumulative survival was significantly lower (logRank = 6.05, p=0.01) in patients with >20% of volume gain since the 6th day (Fig. 2) . >20% volume gain in the 6th day is a independently associated variable to mortality after adjusting by age, APACHE and haemodialysis (OR = 7.3; CI 95% 1.2-43.9; p = 0.02) ( Table 2) . Conclusions: In septic shock patients, Fluid overload more than 20% since 6-day of evolution is associated with a higher 28th-day mortality. Its early detection may influence the prognosis and survival. Introduction: Sepsis is defined as a life-threatening organ dysfunction due to a deregulated host response to infection [1] . Fluid infusion is one of the cornerstones of sepsis resuscitation therapies. One of the major adverse effects reported is fluid overload (FO). The objective of this study was to assess influence of FO on SOFA score changes from day 0 to day 5. Methods: This study is a retrospective, multicenter, epidemiologic data analysis. It was performed in three French ICUs. All adult patients admitted for septic shock, caused by peritonitis or pneumonia and mechanically ventilated, were enrolled. Delta SOFA score was defined as the SOFA score measured on admission minus SOFA score measured on day 5. Results: 129 patients met the inclusion criteria of the study. FO occurs in about 40% of the patients. Cumulative fluid balance at day 5 was greater in the FO group (2.738 versus 8.715 ml, p < 0.001) ( Table 1) . Delta SOFA score was higher in the no FO group than in the FO group (4.52 versus 2.15, p = 0.001) (Fig. 1 ). There was a stepwise decrease of delta SOFA score when duration of fluid overload was greater (p = 0.001) (Fig. 2) . In linear modelling, association between FO status and delta SOFA score was confirmed with an adjusted RR of 0.15 [0.03-0.63] (p = 0.009) ( Table 2) . Conclusions: 1) FO patients had more prolonged multi-organ failure during septic shock; 2) The longer the FO is the longer the more multi-organ failure last. , 100 (T120) and 160 (T180) minutes later. Cardiovascular parameters were also measured at above time points. Biomarker change from baseline (fold-change), indexed to hemoglobin, was compared between groups using mixed effects models (Bonferroni-Holm corrected P<0.05). Results: Minor differences in measures of shock between groups after fluid administration resolved by T120. CRYST showed increased fold-change in hyaluronan compared to other groups at T20 (FWB P=0.019, HES P<0.001, GELO P<0.001), T60 (FWB P<0.001) and T120 (FWB P<0.001) (Fig. 1) . GELO had increased fold-change in hyaluronan compared to other groups at T20 (HES P=0.009), T60 (FWB P<0.001) and T120 (FWB P<0.001, CRYST P=0.006), as did FWB at T20 (HES P=0.008). CRYST showed increased fold-change in IL10 compared to other groups at T20 (HES P<0.001, GELO P=0.002), T60 (HES P=0.001, GELO P=0.005,), T120 (HES and GELO P<0.001) and T180 (HES and GELO P<0.001) (Fig. 2) , of IL8 at T60 (GELO P=0.006), and of KC at Shock (FWB P=0.002, GELO P=0.007), T20 (FWB P=0.009, GELO P=0.007), and T120 (GELO P=0.002). Conclusions: Rapid large-volume crystalloid given for hemorrhagic shock was associated with increased hyaluronan, a biomarker of endothelial glycocalyx damage, and inflammation, including increased IL10, IL8 and KC. Introduction: A bi-center randomized controlled trial has recently been published that investigates the impact of the type of fluid (crystalloid versus colloid) on patient outcome following major surgery [1] . The study used a closed-loop fluid delivery system to eliminate the clinician bias when determining when to deliver fluids. The goal of the current analysis is to compare the immediate hemodynamic response to 100 ml fluid boluses of either a crystalloid or a colloid solution. Methods: Patient consent was obtained prior to transferring the data from [1] to Edwards Lifesciences for further post-hoc analysis. The percent change in stroke volume (DSV) following each 100mL bolus was tabulated and cross-referenced to the type of fluid. The responder rate and the DSV cumulative distribution function (CDF) were determined for each type of fluid administered. A responder was defined as a DSV >= 5% for a 100mL fluid challenge. The mean DSV was compared between the two groups using a student t-test. Results: From the 160 datasets reported in [1] , 119 were used in the analysis. Descriptive statistics are summarized in Table 1 and the CDFs are plotted in Fig. 1 . More crystalloid boluses were administered. In both groups, the responder rate was around 50%. Mean DSV was not significantly different between groups (p = 0.57). We observed similar responder rates and CDFs with the two fluid types, suggesting that the immediate hemodynamic response to 100 ml fluid boluses is independent from the fluid type. We therefore hypothesized that it is the longer intra-vascular persistence of the colloid that explain the lower number of boluses required to achieve the hemodynamic endpoints targeted in the clinical study [1] . Fig. 1 (abstract P294) . Cumulative distribution functions of delta stroke volume for crystalloid and colloid fluid boluses The reduction projected to an average annual saving of 100,272 USD ( Introduction: Colloids are widely used for volume resuscitation. Among synthetic colloids, hydroxyethyl starch (HES) is commonly administered. In cardiac surgery, priming of the cardiopulmonary bypass (CPB) circuit with colloids minimizes resuscitation volume and results in less pulmonary fluid accumulation. However, the use of HES has been associated with a higher incidence of renal damage and a higher occurrence of coagulopathy. The aim of this study was to investigate the effect of low dose (5 -10 ml/kg) HES 6% (130/0,4) in CPB pump priming on fluid balance, blood loss, transfusion requirement and occurrence of acute kidney injury. Methods: In a pre-post design, data from 1120 patients undergoing cardiac surgery with CPB were analyzed. In 560 patients, priming solution consisted of 1250 ml balanced crystalloids, 250 ml mannitol 15%, tranexamic acid 2g and 500 I.E. heparin. For the other 560 patients, 500 ml of the crystalloids were replaced with HES 6% (130/ 0.4), the other components were the same. Patients were matched 1:1 with propensity score method. The primary endpoint was intraoperative fluid balance. Secondary endpoints were perioperative blood loss, transfusion requirement and the occurrence of acute kidney injury. Results: In total, 866 patients were analyzed. The HES group showed less positive fluid balance than the crystalloid group (p< 0.001). There was no difference in intraoperative blood loss (p=0.426) and transfusion requirement (p=0.442). The occurrence of acute kidney injury was not significantly different between the two groups (p=0,147). Conclusions: Low-dose administration of 5-10 ml/kg HES 6% (130/ 0.4) to CPB pump priming decreased intraoperative fluid accumulation without increasing perioperative blood loss and transfusion requirement. There was no effect on the incidence of acute kidney injury. Priming CPB pumps with a low-dose of HES 6% (130/ 0.4) is an important component for a restrictive volume strategy and might safely be used in patients with preexisting renal dysfunction. Introduction: Most crystalloid solutions used in critically ill patients have a greater chloride (Cl) concentration than plasma, which may be detrimental. Replacing some Cl with bicarbonate (HC03) reduces Cl, but may increase partial pressure of carbon dioxide (PC02) in blood. Such an increase in PC02 may be harmful [1] . The main objective was to determine if a HCO3 balanced fluid resulted in increased PaCO2 compared to a conventional balanced fluid. Methods: Single center randomized controlled trial in an adult ICU, comparing balanced fluid (sodium,Na=142mmol/l, Chloride,Cl=99mmol/l, HCO3=49mmol/l) vs conventional fluid (Na=130mmol/l, Cl=110mmol/l, HC03<=27mmol/l). University ethics committee approval:M080932. We used the absolute difference between the PCO2 and 40mmHg as a comparison for the 2 fluid groups. Betweengroup comparisons of PC02 from D1-D7 was done by repeated measures ANOVA. A p value <0.05 was considered significant. Results: 46 patients were allocated to the conventional group and 40 to the balanced group. At baseline the 2 groups were well matched (p>0.05) for age, weight, gender, severity of illness and organ support. There were no significant differences in PC02 between the two fluid groups, overall or at D1, D5 or D7. The balanced group showed a significant improvement in eGFR (sCr), between D0 and D5 (p=0.02) while the conventional group exhibited a significant decline (p=0.00). There were no significant differences between the 2 groups with respect to fluid requirements, number of positive blood cultures, ICU renal replacement utilization, ICU length of stay, ICU mortality and 28 day mortality. Conclusions: The use of a balanced fluid did not result in an increase in PCO2 and appears to be safe. A beneficial effect on renal function was observed. Introduction: The effects of crystalloids and colloids on macro-and microcirculation is controversial. Our aim was to compare their effects on microcirculation during free flap surgery when management was guided by detailed hemodynamic assessment. Methods: Patients undergoing maxillo-facial tumour resection and free flap reconstruction were randomized into a crystalloid (ringerfundin, RF, n=15) and a colloid (6% hydroxyethyl starch, HES, n=15) groups. Cardiac index (CI), stroke volume (SVI) and pulse pressure variation (PPV) were continuously monitored by a non-calibrated device (PulsioFlex -PULSION, MAQUET). Central venous oxygen saturation (ScvO2), venous-to-arterial pCO2-gap (dCO2), lactate levels and hourly urine output was also measured, and a multimodal, individualized approach based algorhithm was applied [1] . Microcirculation was assessed by laser Doppler flowmetry (PeriFlux 5000 LDPM, Perimed Jarfalla, Sweden). Measurements were performed at baseline and from the start of reperfusion hourly for 12 hours. For statistical analysis, two-way RM ANOVA was used. Results: There was no difference between the groups regarding age, sex, length of surgery (whole population: 348 ± 69 min). Patients in the RF-group required significantly more fluid in total (RF: 2581±986, HES: 1803±497 ml, p=0.011). Both groups remained hemodynamically stable (CI, SVI, PPV, ScvO2, dCO2, lactate and urine output) throughout the study. There was no difference between the RF-, and HES-groups in the laser Doppler measurements neither on the control site nor in the flap (Fig. 1) . Conclusions: We found that when hemodynamic management is guided by a multimodal assessment and stability is maintained, there was no difference between crystalloids and colloids in macrocirculation and microcirculatory perfusion. Introduction: Our aim is to evaluate the impact of crystalloid fluids on immune cells. Intensive care unit (ICU) patients' inflammatory status can switch from an early pro-inflammatory to a late anti-inflammatory phase, which favors infections. They can receive different crystalloids, either Normal Saline (NS), Ringer's Lactate (RL) or Plasma-Lyte (PL). High chloride concentration present in NS has been associated with various complications [1] , whereas high doses of NaCl have inflammatory effects on immune cells [2] . However, the immune consequences of crystalloids in humans are ill-defined. Methods: Using our comprehensive immunemonitoring platform, we assessed the immunological phenotype of peripheral blood mononuclear cells (PBMC) in humans. 11 healthy subjects received a liter of NS, RL and PL. Blood samples were taken before and 6h later. PBMC phenotypes were assessed by flow cytometry and cytokine concentrations were measured by a multiplex assay. 9 off-pump cardiac surgery patients were also randomized to receive either NS, RL or PL during surgery and their stay in the ICU. Blood samples were drawn at various time-points. All leucocytes were analyzed in a similar fashion. We are still recruiting. Results: Study of healthy subject's PBMC suggested that RL reduced classical monocytes, whereas NS increased lymphocyte activation and IL-17 and MIP-1b levels. In cardiac surgery patients, our preliminary results suggested that RL and PL reduced classical monocytes and increased non-classical monocytes compared to NS. Neutrophils were also affected differently by crystalloids, where NS seemed to activate them more. Conclusions: Our results suggest that crystalloids have different immune consequences. A better understanding of their immune modulation will lead to personalization of their use according to the inflammatory status of patients to restore their immune homeostasis.  This randomised controlled open-label pilot study included 65 patients presenting to an emergency department with suspected infection requiring a fluid bolus. Patients received either a single bolus of 10mL/kg of 0.9% NaCl (isotonic group) or 5mL/kg of 3% NaCl (hypertonic group). Blood biomarker concentrations of glycocalyx shedding (syndecan-1, hyaluronan), endothelial activation (sICAM-1, sVCAM-1) and inflammation (interleukin-6, -8, -10, NGAL, resistin) were measured at T0 (before fluid) and 1 hour (T1), 3 hours (T3) and 12-24 hours (T24) later. Changes in biomarker concentrations were compared between study groups using mixed regression models, with fold-change from T0 reported. Differences in fluid volumes were compared using the Wilcoxon rank sum test. Significance was set at P<0.05. Results: Syndecan-1 concentration in the isotonic group decreased from T0 to T1 (fold-change 0.8, 95% CI 0.7-0.9), which was significantly different to the hypertonic group (fold-change 1.0, 95% CI 0.9-1.1)(P=0.012)( Table 1) . Interleukin-10 concentration decreased in the isotonic group from T0 to T24 (fold-change 0.1, 95% CI 0.0-0.3), which was significantly different to the hypertonic group (fold-change 0.8, 95% CI 0.3-2.6)(P=0.006). Otherwise, there were no significant differences in change over time between groups for measured biomarkers. Total fluid volume administered between T0 and T1 was significantly higher in the isotonic group (P<0.001) ( Fig. 1) but not different for subsequent time periods. Conclusions: Biomarkers of glycocalyx shedding, endothelial activation and inflammation were not different between patients receiving either 0.9% or 3% saline. Also, 3% NaCl did not reduce administration of additional fluids. Introduction: Acute changes in PCO2 are buffered by non-carbonic weak acids (ATOT), i.e., albumin, phosphates and hemoglobin. Aim of the study was to describe acid-base variations induced by in-vitro PCO2 changes in critically ill patients' blood and isolated plasma, compare them with healthy controls and quantify the contribution of different buffers. Methods: Blood samples were collected from patients admitted to the ICU and controls. Blood and isolated plasma were tonometered at 5 and 20% of CO2 in air. Electrolytes, pH, blood gases, albumin, hemoglobin and phosphates were measured. The Strong Ion Difference (SID) was calculated [1] and non-carbonic buffer power was defined as β=-ΔHCO3-/ΔpH [2] . T-tests and linear regression were used for analysis. Results: Seven patients and 10 controls were studied. Hemoglobin, hematocrit and albumin were lower in patients (p<0.001), while SID and phosphates were similar. PCO2 changed from 29±4 to 108±13 mmHg, causing different blood pH variations in patients and controls (0.43±0.06 vs. 0.36±0.02, p=0.03). Patients had lower blood and plasma β (20±5 vs. 30±4, p<0.001 and 2±2 vs. 4±1, p=0.03, respectively). Figure 1 shows changes in [HCO3-] and SID induced in blood by PCO2 variations. In both populations, 82±12% of [HCO3-] change was due to SID variations, while only 18±12% to changes in ATOT dissociation. A significant correlation between hematocrit and ΔSID was observed in the whole study population (Fig. 2) . Conclusions: The β of ICU patients was lower, likely due to reduced albumin and hemoglobin concentrations. Similar PCO2 increases caused therefore greater pH variations in this population. Electrolyte shifts, likely deriving from red blood cells [3] , were the major buffer system in our in-vitro model of acute respiratory acidosis. Introduction: There is an increasing trend in the incidence of aneurysmal subarachnoid haemorrhage in Hong Kong and the disease carries high morbidity and mortality rate. Electrolyte disturbance is one of the known complications of SAH and the outcomes associated with this are not fully understood. The objective of this retrospective local study is to evaluate the pattern of electrolyte disturbances in patients with SAH and their impact on the prognostic functional outcome. Methods: Patients with spontaneous aneurysmal SAH who were admitted to ICU at Pamela Youde Nethersole Eastern Hospital, Hong Kong between 1st January 2011 and 31st December 2016 were included into this retrospective local study. Collected data include demographic details, comorbidities, serum electrolyte levels (sodium and potassium) from day 1 to 11 of admission into ICU, radiographic intensity of haemorrhage using Fisher scale and the clinical grading of SAH using WFNS. Prognosis of these patients was estimated using the Glasgow Outcome Scale at 3 months after initial insult (Fig. 1) . Results: A total of 244 patients were included in this study. The mean age was 58, with the majority of patients being female (63.6%). The most common aneurysm location was in anterior communicating artery, though poor outcomes were shown significant in patients with posterior circulation aneurysms. Whilst early-onset hyponatremia was not correlated with poor outcome, late-onset hyponatremia was associated with better outcome. Logistic regression analysis identified 9 independent predictors of poor outcome (Table 1) . Patients who underwent interventional radiological procedure treatment was shown to have better outcome. Conclusions: Hypernatremia after SAH is associated with poor outcome. There does not appear to be significant evidence that hyponatremia has an effect on short-term mortality or certain outcome measures such as GOS, and its longer-term effects are not well characterized. Fig. 1 note logarithmic transformation of LOS data). We found a statistically significant difference between the two groups when comparing the length of stay (p < 0.001). Conclusions: Dean et al demonstrated no significant difference in the mean length of stay using the same definitions of hypo and eunatraemia as in this study [1] . Even though our data appears to contradict their findings, regarding the statistical significance seen, we feel that this is not significant clinically, given the very similar median times for LOS between the two groups; the unbalanced design may contribute to the statistical significance. Fig. 1 (abstract P306) . Length of stay between the two groups (note logarithmic scale for LOS) Fig. 1 (abstract P305 ). GOS at 3 months group consisted of 1477 patients with mean age 64.3 (SD 17.8) years and mean sodium 139.8 (SD 2.8) mmol/l with a median LOS of 3.8 (IQR 1.9 -7.1) days. We found no statistically significant difference (p = 0.0636) between the two groups when comparing the length of stay (Fig. 1) . Conclusions: Darmon et al demonstrated prognostic consequences of an admission sodium greater than 145, eliciting hypernatraemia as a factor independently associated with 30-day mortality [1] . In contrast, our study suggests that hypernatraemia (as defined) is not associated with the length of stay, however this result is limited by the unbalanced design of this small study. Introduction: Our aim is to determine whether auscultation for bowel sounds helps in clinical decision making in ICU patients with ileus. Ileus can be the consequence of an operation, a side effect of drugs or the result of an obstruction requiring direct operative correction. Although auscultation for bowel sounds is routinely performed in the ICU and a well-established part of the physical examination in patients with suspected ileus, its clinical value remains largely unstudied. Methods: A literature search of PubMed, Embase and Cochrane was performed to study the diagnostic value of auscultation for bowel sounds. Results: Auditory characteristics (tinkling, high pitched and rushes) were highly variable in postoperative ileus, mechanical ileus and healthy volunteers. The inter-observer variability for the assessment of the quantity, volume and pitch of bowel sounds was high, with a moderate interobserver agreement for discerning postoperative ileus, bowel obstruction and normal bowel sounds (kappa value 0.57). The intra-observer reliability of duplicated recordings for distinguishing between patients with normal bowels, obstructed bowels or postoperative ileus was 54% [1] . No clear relation between bowel sounds and intestinal transit was found (Table 1) . Sensitivity and positive predictive value were low: respectively 32% and 23% in healthy volunteers, 22% and 28% in obstructive ileus, and 22% and 44% in postoperative ileus ( Table 2) . Conclusions: Auscultation with the aim to differentiate normal from pathological bowel sounds is not useful in clinical practice. The low sensitivity and low positive predictive value together with a poor inter-and intra-observer agreement demonstrate the inaccuracy of utilizing bowel sounds for clinical decision-making. Given the lack of evidence and standardization of auscultation, the critically ill patient is more likely to benefit from abdominal imaging. Introduction: Stress ulcer prophylaxis has become a standard of care in Intensive Care Unit (ICU). However, it has been proposed that enteral nutrition (EN) could play preventive role for gastrointestinal bleeding and some studies revealed no added benefit of acid suppressive drugs to patients on EN. Based on these backgrounds, we use proton pump inhibitor (PPI) as stress ulcer prophylaxis during starvation period, and discontinue it within 24 hours after commencing meals or EN. The aim of this study is to evaluate the applicability of our protocol by reviewing the incidence of upper gastrointestinal bleeding (UGIB) in our ICU. Methods: We conducted a retrospective observational study. All consecutive patients admitted to our ICU between April 2016 and March 2017 were reviewed. Patients who had UGIB within 24 hours after admission, had previous total gastrectomy, or underwent upper gastrointestinal surgery were excluded. The primary outcome was the incidence of overt or clinically important UGIB, and the secondary outcome was protocol adherence. We presented descriptive data as number (percentage) and median (interquartile range). Results: A total of 521 patients were included. Of those, 315 (60.5%) were male, median age was 73 (57-81), and median SOFA score was 5 (2) (3) (4) (5) (6) (7) (8) . Of all 521 patients, 16 (3.37%) had overt bleeding, and 2 (0.38%) had clinically important bleeding. Both 2 patients who Introduction: Patients requiring operative procedures admitted under non-surgical specialties typically experience delays in treatment and fail to meet peri-operative standards with regards to the timing of operative intervention. Patients admitted from medicine requiring an emergency laparotomy have an increased mortality when compared to those patients admitted from surgery (20.4% v 13.6%) [1] . Methods: We undertook a retrospective case note review of patients requiring a non-elective laparotomy at our hospital during a sixmonth period in 2016. Patients were identified using the emergency theatre booking system. Data were gathered on admission details, peri-operative care and post-operative stay. Results: Two main investigators reviewed 104 patients to standardise data extraction. Six patients presenting with inflammatory bowel disease were excluded from analysis. Most patients (59.1%) were admitted through the Emergency Department; 17 (29.3%) of whom were initially admitted under medicine, with only 37.5% of these reviewed by a senior clinician prior to admission (Table 1 ). There was no statistically significant difference in mortality between the medicine and surgery groups. There was a trend to increased length of stay in ICU and in hospital in the medical group (Table 2) . Conclusions: Lack of senior decision making may have a direct impact on patient care due to the inappropriate streaming of patients to medicine. The increased mean length of stay in those patients admitted to medicine may reflect a delay in surgical intervention and therefore a prolonged recovery period. We are introducing an Acute Abdominal Pain Screening and Immediate Action Tool to improve identification of these high-risk patients and early involvement of senior decision makers. Introduction: Biomarkers reflecting the extent of surgical tissue trauma should be investigated in an effort to predict and prevent postoperative complications. The aim of the present study was to investigate blood concentrations of selected alarmins in patients after colorectal surgery in comparison to healthy individuals. The secondary aim was to analyze the relationship between alarmins and inflammatory biomarkers during early postoperative period. Methods: The prospective, single-center, observational study consisted of non-surgical (NS) group (n=35) and surgical (S) group (n=38) undergoing colorectal surgery. Serum levels of selected alarmins (S100A8 and S100A12) and inflammatory biomarkers (leukocytes; C-reactive protein, CRP; interleukin-6, IL-6) were analyzed. Results: Proteins S100A8 an S100A12 had significantly higher serum values in the S-group during all three days after the surgery. The multidimensional model taking into account age, sex, weight, group and days revealed significant differences between study groups for both proteins S100A8 and S100A12 (p<0.001, p=0.001, respectively). Biomarkers (leukocytes, CRP, and IL-6) showed significant differences between study subgroups (p<0.001, p<0.001, and p<0.001, respectively). In S-group, moderate positive correlations were found between S100A8 and all biomarkers: leukocytes (r=0.6), CRP (r=0.5), and IL-6 (r=0.6). S100A12 had moderate positive correlation with leukocytes (r=0.5). Levels of S100A8 also positively correlated with intensive care unit and hospital length of stay (r=0.6, r=0.5, respectively) Conclusions: Protein S100A8 might be considered as early biomarker of first wave of immune activation elicited by surgical injury after colorectal surgery. The increase of the alarmins is reflected by the elevation of routine inflammatory biomarkers. Introduction: Critical illness-induced liver test abnormalities are associated with complications and death in adult ICU patients, but remain poorly characterized in the pediatric ICU (PICU). In the PEPaNIC RCT, delaying initiation of parenteral nutrition to beyond day 7 (late PN) was clinically superior to providing PN within 24h (early PN), but resulted in a higher rise in bilirubin. We aimed to document prevalence and prognostic value of abnormal liver tests and the impact of withholding early PN in the PICU. Methods: We performed a preplanned secondary analysis of 1231 of the 1440 PEPaNIC patients aged 28 days to 17 years, as neonatal jaundice was considered a confounder. Plasma concentrations of total bilirubin, ALT, AST, γ GT, ALP were measured systematically during PICU stay. Analyses were adjusted for baseline characteristics including severity of illness. Results: During the first 7 PICU days, the prevalence of cholestasis (>2mg/dl bilirubin) ranged between 3.8%-4.9% and of hypoxic hepatitis (>=20-fold ULN for ALT and AST) between 0.8%-2.2%, both unaffected by the use of PN. Throughout the first week in PICU plasma bilirubin concentrations were higher in late PN patients (p<0.05), but became comparable to early PN patients as soon as PN was started on day 8. Plasma concentrations of γ GT, ALP, ALT and AST were unaffected by PN. High day 1 plasma concentrations of γ GT, ALT and AST (p<=0.01), but not ALP, were independent risk factors for PICU mortality. Day 1 plasma bilirubin concentrations displayed a Ushaped association with PICU mortality, with higher mortality associated with bilirubin concentrations <0.20mg/dl and >0.76mg/dl (p<=0.01). Conclusions: In conclusion, overt cholestasis and hypoxic hepatitis were rare and unrelated to nutritional strategy. However, accepting a large macronutrient deficit during week 1 increased plasma bilirubin. A mild elevation of bilirubin on the first PICU-day was associated with lower risk of death and may represent an adaptive stress response rather than true cholestasis. Positive fluid balance is an independent risk factor for intensive care unit mortality in patients with acute-on-chronic liver failure Introduction: Muscle wasting is a common consequence of disuse and inflammation during admission to intensive care with critical illness. Limb muscles are known to decrease in size during critical illness, but less is known about muscles of the trunk. In this study, we tracked how psoas muscle area changes at multiple levels, in a group of patients with acute severe pancreatitis. Methods: Paired computed tomography (CT) scans were obtained from 21 patients admitted to the Royal Liverpool University Hospital's ICU with acute severe pancreatitis. The first scan was within 3 days of admission, and the second took place between 8 to 16 days later. For each scan, three slices were identified: the top and bottom plates of L4, and the mid-point of L4 vertebral body. On each slice, the cross sectional area (CSA) of the left and right psoas muscle was calculated using ImageJ. The difference and percentage change in CSA between both scans was calculated. White cell counts and C-reactive protein results were obtained, with peak levels correlated against change in muscle size. Results: Combined CSA of the left and right psoas muscle increased from top to bottom plates and was positively correlated with height (r=0.74, p<0.001 mid L4 level)) and weight (r=0.57, p=0.014, mid L4 level) at all three levels. At all three levels, there were significant losses of CSA between the two scans (see Table 1 ). CRP was moderately correlated with percentage change in CSA (r= -0.55, p=0.014). Increasing weight on admission was associated with greater percentage losses in CSA (r= -0.78, p<0.001). WCC did not correlate with change in size. In critically ill patients with acute severe pancreatitis, there are significant losses in both psoas muscles throughout the L4 level. Further prospective studies are required to determine if inflammatory markers and cytokines have a role in these losses, and to determine the functional effects of these losses. Introduction: The evidence for penta-therapy for hyperlipidemic severe acute pancreatitis (HL-SAP) is anecdotal. The purpose of our study is to evaluate the efficacy of penta-therapy for HL-SAP in a retrospective study. Methods: Retrospective study between January 2007 and December 2016 in a hospital intensive care unit.HL-SAP patients were assigned to conventional treatment alone (the control group) or conventional treatment with the experimental protocol (the penta-therapy group) consists of blood purification, antihyperlipidemic agents, lowmolecular-weight heparin, insulin, covering the whole abdomen with Pixiao (a traditional Chinese medicine).Serum triglyceride, serum calcium, APACHE II score, SOFA score, Ranson score, CT severity index, and other serum biomarkers were evaluated. The hospital length of stay, local complications, systematic complications, rate of recurrence, overall mortality, and operation rate were considered clinical outcomes. Results: 63 HL-SAP patients received conventional treatment alone (the control group) and 25 patients underwent penta-therapy combined with conventional treatment (the penta-therapy group). Serum amylase, serum triglyceride, white blood cell count, C -reactive protein, and blood sugar were significantly reduced, while serum calcium was significantly increased with penta-therapy. The changes in serum amylase, serum calcium were significantly different between the penta-therapy and control group on 7th day after the initiation of treatment. The reduction in serum triglyceride in the pentatherapy group on the second day and 7th day were greater than the control group. Patients in the penta-therapy group had a significantly shorter length of hospital stay. Conclusions: This study suggests that the addition of penta-therapy to conventional treatment for HL-SAP may be superior to conventional treatment alone for improvement of serum biomarkers and clinical outcomes. Average energy expenditure (EE) for all patients was 26 ± 4 kcal/kg (mean ± SD). There was no difference in the average EE between the patients who survived and those who died: 27 ± 1 and 25 ± 1 kcal/ kg (mean ± SD) respectively (p > 0.05). However, there was a negative correlation between EE and SAPS 3 score in the non-survivors groupcorrelation coefficient -0.679, p < 0.05. The energy deficit (computed by subtracting caloric intake from EE measurement) was similar among survivors and non-survivors, 5.5 ± 1 vs 6.5 ± 2 kcal/kg, respectively (mean ± SD) (p > 0.05). The patients who survived had received 21 ± 1 kcal/kg while those who died -18 ± 1 kcal/kg (mean ± SD) (p > 0.05). The provision of protein was also similar for both groups: 0.9 ± 0.1 g/kg for survivors and 1 ± 0.04 g/kg for nonsurvivors (mean ± SD) (p > 0.05). There was no statistically significant correlation between provision of calories and protein and outcomes such as length of hospital and ICU stay or duration of mechanical ventilation. Conclusions: Average energy expenditure in critically ill patients with acute severe pancreatitis roughly equals to ASPEN estimation of 25 kcal/kg and does not differ among survivors and non-survivors. Outcomes such as survival, length of hospital and ICU stay and duration of mechanical ventilation were unaffected by caloric nor protein provision in this sample. Introduction: Disturbances in gastrointestinal motility are common in critically ill patients receiving enteral nutrition. Slow gastric emptying (GE) is the leading cause of enteral feeding intolerance (EFI), which compromises nutritional status and is associated with increased morbidity and mortality. This phase 2a study evaluated the efficacy, safety and tolerability of acute TAK-954 (previously TD-8954), a selective agonist of the 5 hydroxytryptamine receptor 4 (5HT4), compared with metoclopramide in critically ill patients with EFI. Methods: This was a double-blinded, double-dummy study conducted in mechanically ventilated patients with EFI (>200 mL gastric residual volume) randomized to receive either intervention (TAK-954 0.5 mg over 1 hour and 0.9% saline 10 ml injection QID) or control (0.9% saline over 1 hour and metoclopramide 10 mg injection QID). Within 1 hour of the first dose, patients received a test meal of 100 mL Ensure® and GE was measured using scintigraphy. Primary objectives were to evaluate the safety and tolerability of TAK-954 and its effect on GE (% retention at 180 mins) vs control. Results: A total of 13 patients (intervention, n = 7; control, n = 6) were studied. The median ages were 47 and 57 years in these groups, respectively. Post-treatment, a 2-fold greater number of patients had normal gastric retention (<13% at 180 mins) in the intervention group vs the control group (6 vs 3; Fig. 1 ). In the intervention and control groups, (Table 1) . No AEs led to treatment discontinuation. Conclusions: A greater proportion of patients receiving TAK-954 had normal gastric retention after a single dose compared with those receiving metoclopramide. Treatment with TAK-954 was not associated with an increase in AEs. These results support further evaluation of TAK-954 in critically ill patients with EFI. Method to assess gastric emptying in the fed state in enterally tube fed patients: comparison of the paracetamol absorption test to scintigraphy J James 1 Introduction: The paracetamol absorption test (PAT) is the most common and practical approach for assessing gastric emptying (GE) in critically ill patients. However, current methods require that paracetamol be administered to an empty stomach, removing gastric contents and depriving patients of feeding for several hours. The objective of this study was to develop methods to assess gastric emptying in these patients without interrupting feeding. Methods: Gastric emptying was assessed in the fed state using PAT and scintigraphy in 12 healthy volunteers. Paracetamol 1g in 30mL was ingested immediately before consumption of a test meal of 250mL Ensure Plus containing 375kcal, 15.6g protein, and 12.3g fat plus 4mBq of 99mTc-DPTA as a scintigraphic agent. Comparisons were made between paracetamol absorption and the time to 25% and 50% gastric emptying by scintigraphy at baseline and after administration of ulimorelin 600μg/ kg, a prokinetic agent known to enhance gastric emptying. Blood samples for paracetamol were collected for up to 4h post administration. Values for normal gastric emptying were based on the 95% confidence intervals for PK parameters. Sensitivity and specificity were assessed by receiver operating characteristic (ROC) analysis before and after treatment. Results: The PAT correlated with scintigraphy and PK parameters for normal emptying were determined. Cmax and AUC2 were the most sensitive and specific parameters for assessing GE with lowest variability and areas under the ROC curve of 0.8981 and 0.8889, respectively. A 2h sampling period appeared sufficient to distinguish normal from abnormal emptying. Conclusions: The PAT can be used to distinguish normal versus abnormal GE in the fed state. Under the conditions used, patients can receive up to 250mL enteral feeding over a 2h test period (125 mL/hr). This method can be used to distinguish normal from abnormal gastric emptying in enterally tube fed patients without interrupting feedings. Introduction: For mechanically ventillated critically ill patients, the effect of full feeding on mortality is stil controversial. We aimed to investigate the relationship of energy intakes with 28-day mortality, and nutritional risk status influenced this relationship. Methods: This prospective observational study was conducted among adult patients admitted to ICU and required invasive mechanical ventilation (IMV) for more than 48 h. Data on baseline characteristics and the modified Nutritional Risk in Critically ill [mNUTRIC] score was collected on day 1. Energy intake and nutritional adequacy was recorded daily until death, discarge or until twelfth evaluable days. Patients were divided into 2 groups: a)received < 75% of prescribed energy b) received >= 75% of prescribed energy. Results: 150 patients (65% male, mean age 51.0±15.3 years, mean body mass index 27.9±6.2 kg/m2, mean mNUTRICscore 5.8± 1.7) were included. In the univariate analysis, mNUTRİC score was associated with 28-day mortality. In the multivariable logistic regregression analysis, mNUTRIC score(Odds ratio, OR 1.65, CI 1.20-1.70, P < O.OO1) was associated with 28-day mortality. Nutritional adequacy was assessed, median nutritional adequacy was 0.40 (0.17-0.75). In patients with high mNUTRİC score (5-9), received >= 75% of prescribed energy was associated with a lower predicted 28-day mortality; this was not observed in patients with low mNUTRİC score (0-4). Conclusions: Nearly 60 % of IMV required patients admitted to ICU were at nutritional risk, mNUTRİC score is associated with 28-day mortality. Energy adequacy of >= 75% of prescribed amounts were associated with decreased mortality in patients with a high mNUTRİC score. Results: Patients included in the study were ASA IV. Four patients died in the first few days after surgery (2÷16 days). Mean length of stay in ICU was 5.2 ±3.4 days. Univariate analysis showed a correlation between hypoalbuminemia and the onset of MOF (p = 0.004); reduction of the lymphocyte count and risk of MOF (p = 0.008). SOFA score showed a significant correlation with occurrence of pneumonia (p = 0.035) and MOF (p = 0.04). Including the 30-day mortality among confounders, albumin and lymphocyte count were the strongest predictors of MOF. Length of stay in ICU and ventilation days did not have statistical significance. BMI showed no predictive value of any outcome. Conclusions: Our sample was poor but results of our study seem to indicate malnutrition as an independent risk factor for elderly patients undergoing emergency surgery. Early multidisciplinairy screening of dysphagia at admission to the emergency departmenta pilot study D Melgaard, L Sørensen, D Sandager, A Christensen, A Jørgensen, M Ludwig, P Leutscher North Denmark Regional Hospital, Hjørring, Denmark Critical Care 2018, 22(Suppl 1):P324 Introduction: Dysphagia increase the risk of aspiration pneumonia, malnutrition, dehydration and death. This combined with the fact that patients with dysphagia have a longer stay in the hospital makes early prognosis and appropriate treatment important. Knowledge about effect of early dysphagia screening is limited. The aim of this study is to examine the prevalence of dysphagia in the Emergency Department (ED) population. Methods: This study included consecutively hospitalized patients in 10 days from 2pm-10pm at the ED of North Denmark Regional Hospital. The screening took place within 2 hours of admission. Inclusion criteria were any of the following: age ≥65 years, neurological disorders, alcoholism, COPD, pneumonia, dyspnoea, diabetes or unexplained weight loss. A nurse screened patients with a water test and with signs of dysphagia tested by an occupational therapist with the V-VST and the MEOF-II. Results: Of 140 eligible patients (56% male, median age 75 years) 95 (68%) were screened. It was impossible to screen 12 patients (9%) to limited time and 30 patients (21%) due to poor health condition and 5 patients (4%) declined participation. The prevalence of dysphagia in the study population was 16% (15 patients). Results from the water test were confirmed with V-VST and MEOF-II. In patients with lung related diseases or circulatory diseases was the prevalence respectively 25% and 24%. Patients, not screened due to poor health condition, were tested during hospitalisation and the prevalence of dysphagia was 75% in this group of patients. Conclusions: The prevalence in ED patients was 16%. Patients transferred to other departments due to poor health condition had a prevalence of 75%. It is possible to screen patients in the ED. The water test is a useful screening tool in an acute setting. Introduction: To improve protein and energy delivery in a nutrition delivery bundle was introduced to a Level 3 ICU. Greater protein and energy intake is associated with improved outcomes in the critically ill [1] [2] [3] [4] , but only 50% of prescribed protein and energy is delivered in ICUs worldwide [5, 6] . Methods: Percentage of target protein and energy delivery was measured via participation in the International Nutrition Survey (INS) before and after a "nutrition delivery bundle" was introduced by the ICU dietitian. The nutrition delivery bundle involved all stakeholders in ICU nutrition care (Fig. 1) and included the following quality improvement measures: increased ICU dietetic staffing, update of ICU Enteral Feeding Protocol with staff education, use of higher protein formulations, earlier patient nutrition assessment, daily calculation of percentage nutrition delivery, increased nutrition communication through more regular discussion of patient care with medical team, expansion of choice of nasojejunal tube available, 6 monthly reporting of key nutrition performance indicators, improved resources for cover dietitian(s) when ICU dietitian on leave (Fig. 2) . Results: Prior to a nutrition delivery bundle being introduced the Mater Misericordiae University Hospital (MMUH) ICU achieved 59% of protein and 62% of energy targets over the first 12 admission days of 20 consecutive mechanically ventilated patients in ICU >72hrs enrolled in the International Nutrition Survey. This increased to 75% of protein and 79% of energy targets in 2014 (Table 1) . Conclusions: A 27% improvement in protein and energy delivery to critically ill patients was seen after the introduction of a dietitian-led nutrition delivery bundle. Introduction: The critically ill polytrauma patient with sepsis presents with variable energetic necessities characterized by a proinflammatory, pro-oxidative and hypermetabolic status. One of the challenges the ICU doctor faces is adapting the nutritional therapy based on the individual needs of each patient. Through this paper we wish to highlight the trend of energy needs in the case of critically ill polytrauma patients with sepsis by using non-invasive monitoring of respiratory gases based on indirect calorimetry (GE Healthcare, Helsinki, Finland). Methods: This is a prospective observational study carried out in the Anesthesia and Intensive Care Unit "Casa Austria", Emergency County Hospital "Pius Brinzeu", Timisoara, Romania. We monitored VO2, VCO2, energy demand (ED), and specific clinical and paraclinical data. We measured energy demand values monitored by direct calorimetry with values calculated based on standard formulas. Results: 21values have been recorded in the study. The mean VO2 was 3.3 ± 0.4 ml/min/kg, the mean VCO2 was 2.3 ± 0.3 ml/min/kg. In regard with energy demand, the mean ED obtained through direct calorimetry was 2393.2 ± 912.9 kcal/day, and those obtained by using mathematic formulas were 1988.6 ±1100 kcal/day (p < 0.05). Moreover, statistically significant differences have been observed regarding the mean difference between energy demand determined using indirect calorimetry and that determined mathematically, respectively between the enteral and parenteral administered ED. Conclusions: Continuous monitoring of the energy demand in critically ill patients with sepsis can bring important benefits in regard with the clinical prognosis of these patients through the individualization and adaption of intensive therapy for each patient. Introduction: Cachexia is defined as a complex metabolic syndrome associated with underlying illness, characterized by loss of muscle with or without loss of fat. In cancer cachexia, reduction in muscle size has been demonstrated to be an independent risk factor for mortality. Loss of muscle in ICU patients is rapid and extensive and is also associated with mortality risk, but methods to measure muscle mass in these patients are lacking. Surrogate methods (DEXA, CT, ultrasound, total body water) do not measure muscle mass directly Methods: The D3-creatine (D3-Cr) dilution method takes advantage of the fact that 98% of Cr is found in muscle and that muscle mass can be assessed by Cr pool size. Cr is transported into muscle against a concentration gradient and irreversibly converted to creatinine (Crn), which is excreted in urine. A single oral dose of D3-Cr is transported to skeletal muscle, and measurement of D3-Crn enrichment in a spot urine sample provides an accurate estimate of skeletal muscle mass. Results: The method has been validated in preclinical and clinical studies; in a large longitudinal observation study in older men, D3-Cr muscle mass was strongly associated with habitual walking speed, risk of falls, and incident mobility limitation; DEXA failed to show these relationships. The D3-Cr method is being used in a NICU study to measure changes in muscle mass in neonates (Gates Foundation Grant). Further, this method has been incorporated into a trial assessing the treatment effects of a ghrelin agonist in ICU patients with enteral feeding intolerance (NCT02784392). In this trial, the D3-Cr dose is delivered intravenously and a spot urine sample is collected at baseline and postdose. Conclusions: The D3-Cr method provides a non-invasive, accurate way to assess therapeutic agents that may mitigate the loss of skeletal muscle mass; it is of particular utility in clinical settings where changes in muscle mass are consequential, such as muscle loss during an ICU admission. Introduction: Vitamin C, an enzyme cofactor and antioxidant, could hasten the resolution of inflammation, which affects most intensive care unit (ICU) patients. While many observational studies have demonstrated that critical illness is associated with low levels of vitamin C, randomized controlled trials (RCTs) of high-dose vitamin C, alone or in combination with other antioxidants, yielded contradicting results. The purpose of this systematic review and meta-analysis is to evaluate the clinical effects of vitamin C when administered to various populations of ICU patients. Methods: Eligible trials: RCTs comparing vitamin C, by enteral or parenteral routes, to placebo in ICU patients. Data Collection and Analysis: We searched MEDLINE, EMBASE, and the Cochrane Central Register of Controlled Trials. After assessing eligibility, data was abstracted in duplicate by two independent reviewers. Overall mortality was the primary outcome; secondary outcomes were infections, ICU length of stay (LOS), hospital LOS, and ventilator days. Pre-specified subgroup analyses were conducted to identify more beneficial treatment effects. Results: Pooling 9 RCTs (n=1322) reporting mortality, vitamin C was not associated with a lower risk of mortality (risk ratio [RR]: 0.84, 95 % confidence interval [CI]: 0.48-1.37, P=0.44, I2=59%). In a subgroup analysis, trials of lower quality (n= 5) were associated with a reduction in mortality (RR 0.50, 95% CI 0.32, 0.77, P= 0.002), whereas high quality trials (n= 4) were not. No statistical difference existed between subgroups (P= 0.22). In addition, no effect was found on infections, ICU or hospital length of stay, and ventilator days. Conclusions: Current evidence does not support the hypothesis that vitamin C supplementation improves clinical outcomes of ICU patients. Introduction: The protein intake for patients who met adequacy for energy was assessed within our cardiothoracic intensive care. Nutritional support should aim to provide at least 80% of calorie requirements to achieve nutritional adequacy with suggested protein requirements of 1.2-2 g/kg/day [1] . Guidelines highlight the difficulty achieving the correct protein:energy ratio from nutritional support to meet this target especially in the obese population. Methods: The audit was registered with clinical governance. Data was collected prospectively from patients requiring tube feeding for three or more days from January 2016 -October 2017 (Table 1 ). Data included type and volume of feed and calories from other sources. Patients who met adequacy for energy (Fig. 1) Introduction: Patients admitted to the intensive care unit (ICU) are usually at high risk of malnutrition [1, 2] . The purpose of our study was to compare the accuracy of Nutric score, NRS 2002 and SGA in predicting LOS-ICU, LOS-HOSP and in-hospital mortality. Methods: A total of 348 consecutive patients admitted between March to June 2016 in a mixed (medical/surgical) ICU were assessed on day of admission using the three screening tools to classify them into high-risk and low-risk of malnutrition. Day 1 APACHE 2 scores and demographic data were recorded. LOS-ICU, LOS-HOSP inhospital mortality and secondary outcomes studied were need for supplemental nutritional support, need for ventilation and need for dialysis in high-risk and low-risk patients by each nutrition assessment tool. Results: Of the 348 patients studied, 221 (63.5%) were males and 127 (36.5%) were females. 67.87% males and 71.65% females were found to be at a high risk of malnutrition by at least one of the scores. The mean APACHE 2 score for patients at high risk (using any one screening tool) was 15.11 (SD 6.10) and 8.04 for the low risk group (SD 3.34; p <0.01). The NRS 2002 and SGA demonstrated statistically significant correlation(p=0.001) for length of ICU stay for both the high risk and low risk group whereas only the NRS 2002 correlated significantly for the length of hospital stay(p=0.002). Mortality was significantly higher in high risk patients identified using all 3 scores. Conclusions: There was a wide difference in the percent of patients identified as high-risk using each of the 3 scores. Introduction: Nitrogen Balance (NB) may be an important tool in the nutritional management of critically ill patients. Cancer patients present a special challenge regarding nutrition, due to its peculiar characteristics related to neoplasia and adjuvant treatments. Objectives: To evaluate NB in patients with solid cancer in the postoperative period in the ICU, analyzing the correlation between NB and the mortality outcome in the ICU. Methods: Retrospective cohort study. We evaluated adult patients (>18 years) admitted to the ICUs of two different hospitals, with diagnosis of current cancer in postoperative period (elective or emergency surgeries). Patients were excluded if the diagnosis of cancer was not confirmed. NB (measured through analysis of dietary protein intake subtracted from 24-hour urinary urea plus an estimate of nonurinary losses) was calculated on the 1st, 3rd and 5th ICU day. NB was measured only while the patient was in the ICU. Results: During the study period, 125 patients were included (mean age 58.1, mean APACHE 17.4, 65.6% male). Admission APACHE II and abdominal-site surgery were predictors of mortality. The NB of all patients was negative on the 1st ICU day. In the patients who survived, NB of the 3rd and 5th day remained stable (negative), whereas in patients who died NB was more positive (Fig. 1) . There was no difference in the amount of protein ingested on the 1st day between survivors and deceased patients. Conclusions: Among adult patients with solid cancer in the postoperative period in the ICU, NB was persistently negative in the survivors between 1st and 5th ICU day, but among the patients who died NB tended to be more positive on the 3rd day. NB monitoring could allow a more adequate individualization of nutritional management in this group of patients. Fig. 1 (abstract P335) . Nitrogen Balance in 1st, 3rd and 5th ICU day Introduction: Nutritional therapy plays an important role in the treatment of critically ill patients. Caloric and protein goals are defined, and artificial nutrition tailored to the targets which are related to outcome [1] . Questions rise about the mean caloric and protein needs of patients, once discharged from ICU, and the evolution of body weight, and nutritional adequacy. The aim is to know the ratios between caloric needs and intake of patients with a minimum stay at ICU of 5 days. Methods: After evaluation of 146 critically ill patients, 12 patients were prospectively followed during their entire hospitalization. Data concerning nutritional needs, prescriptions and delivery were collected from the electronic medical file. Nutritional calculations of oral intake were done by Nubel. Ratios were made during the entire stay and body weight was followed up. Results: In 5 female and 7 male patients, median age 63.5 years (range 26-84 year), estimated body weight of 74.8 ± 21 kg and actual body weight of 73.3± 17 kg, a mean caloric need of 1795 ± 479 kcal/ day and an effective delivery of 1348 ± 508 kcal/day was observed. Body weight increased in two patients and decreased in 10 (83%). In ten out of twelve patients, underfeeding was present. One patient with a caloric need of 1125 kcal/day received a mean caloric load of 230 kcal/day (20.4%). Conclusions: The overall observed evolution in body weight was negative in most of the patients. Nutritional adequacy was low after ICU discharge and never reached target. Introduction: Severe burn injury can create a rapid-onset, sustained proinflammatory condition that can severely impair all major organs. This massive systemic response has been documented clinically by associated biomarker measurements including dramatic elevations in cytokines such as IL-6. The severity of multi-organ injury and subsequent development of other systemic complications in burn patients have been well-correlated with IL-6 levels, including the increased risk of sepsis/multi-organ failure and associated morbidity and mortality. Considering that estrogen is a powerful and easy to use anti-inflammatory agent, an experimental burn model was created to test the potential value of parenteral 17β-estradiol (E2) as a feasible and inexpensive early intervention to mitigate the the profound pro-inflammatory response associated with severe thermal injury. Methods: Male rats (n = 28) were assigned randomly into three groups: 1) controls/no burn (n = 4); 2) burn/placebo (n = 12); and 3) burn/E2 (n = 12). Burned rats received a 40% 3°TBSA dorsal burn, fluid resuscitation and one dose of E2 or placebo (0.5 mg/kg intra-peritoneal) 15 minutes post-burn. Eight animals from each of the two burn groups (burn/placebo and burn/E2) were sacrificed at 30 minutes (sham group at 7 days only), with four each of the two burn groups sacrificed at 45 days. Tissue samples from 9 major organs and serum were obtained and analyzed by ELISA for IL-6 at each of these intervals. Results: In the burned rats, 17β-estradiol decreased the organ levels of IL-6 significantly as measured at both early (30 min.) and late (45 day) phases post-burn (Figs. 1 & 2 . Also, sham animal levels were comparable to the estradiol group, Conclusions: Experimentally, a single, early post-burn dose of estrogen significantly mitigates the associated detrimental inflammatory response in all major organs up to 45 days. In turn, this may present a promising potential therapy to decrease the widespread multipleorgan dysfunction seen in severe burn injury patients. Early, single-dose estrogen increases levels of brain-derived neurotrophic factor (BDNF), a neurotrophin for neuronal survival and neurogenesis following indirect brain inflammation caused by severe torso burns Introduction: Prior studies have found that patients with severe burns may suffer significant neurocognitive changes. While frequently attributed to psycho-social issues, we have found a substantial, rapid and sustained (30 min -45 day) increase in rat brain inflammatory markers (for example, IL-6) following remote torso burns that is blunted by a single post-burn dose of estrogen. Brain-derived neurotrophic factor (BDNF), one of the most active neurotrophins, protects existing neurons and encourages the growth and differentiation of new neurons and synapses. As estrogens not only blunt inflammation but also exert an influence on CNS growth factors, we hypothesized that 17β-estradiol (E2) might affect levels of BDNF in the post-burn rat brain. Methods: Male rats (n = 44) were assigned randomly into three groups: controls/no burn (n = 4); burn/placebo (n = 20); and burn/E2 (n = 20). Burned rats received a 40% 3°TBSA dorsal burn, fluid resuscitation and one dose of E2 or placebo (0.5 mg/ kg intraperitoneally) 15 minutes post-burn. Eight animals from each of the two burn groups (burn/placebo and burn/E2) were sacrificed at 24 hours and at 7 days, respectively (sham group at 7 days only), with four each of the two burn groups sacrificed at 45 days. Brain tissue samples were analyzed by ELISA for BDNF. Results: Mean levels of BDNF were significantly elevated within 24 hours and continued to increase up to 45 days post-injury in burned animals receiving the 17β-estradiol (>300 pcg/mg) as compared with the placebo-treated burned animals (<160 pg/mg) and controls (<120. pcg/mg). See Fig. 1 . Conclusions: Early, single-dose estrogen administration following remote severe burn injury significantly elevated levels of BDNF in brain tissue. This finding may represent an extremely novel and important pathway to enhance both neuroprotection and neuroregeneration in burn patients. The value of cortisol in patients with the infection and multiple organ dysfunction. S Tachyla, A Marochkov Mogilev Regional Hospital, Mogilev, Belarus Critical Care 2018, 22(Suppl 1):P339 Introduction: Hormones changes in patients with infection and multiple organ dysfunction is a topic that hasn't been adequately studied. Goal of study: to establish the value of cortisol in patients with infection and multiple organ dysfunction. Methods: After approval the ethics committee of the Mogilev Regional Hospital a prospective observational study was performed. The study included 181 patients aged 18 to 87 years. All patients were hospitalized in the Intensive Care Unit with the infection and multiple organ dysfunction. Patients with endocrine diseases and receiving glucocorticoids were excluded. Cortisol levels were measured on admission and during the course of treatment by radioimmunoassay. In Group L (n = 16) patients had a low levels of cortisol, in the M group (n = 96) -normal cortisol, in group H (n = 69) -high cortisol. Results: Cortisol level was in L-group 91.9 (8.28, 131.7) nmol/L, in Mgroup 410.9 (292.8; 504.7) nmol/L, in H-group 934.2 (763, 6; 1495.5) nmol/L. It is found that the mortality was higher in the groups L -43.8% (p = 0.33) and H -47.8% (p = 0.03), than in the M-group -31.3%. The Mgroup odds ratio equals 2.02 at 95% confidence interval 1.06 -3.82 when compared with the H-group. In the M-group in survivors patients (n = 36) showed a decrease cortisol with 1281 (1033.8, 1702.5) nmol/L to 912.3 (801.5, 1068.8) nmol/L (p = 0.01). While the no survivors patients (n = 33) showed increase cortisol with 732 (657.1, 749.2) nmol/L to 1491.2 (1000; 1600) nmol/L (p = 0.008). Thus itself cortisol level is not a marker of mortality. Receiver operating curve analysis for cortisol was performed: area under the curve equals 0.56 at 95% confidence interval of 0.47 -0.65 (p = 0.19), sensitivity 48.4%, specificity 70.6%. Conclusions: In patients with infection and multiple organ dysfunction may be observed disorders in cortisol levels. These disorders require correction to prevent the increased mortality. Introduction: The hypothalamic-pituitary-adrenal (HPA) axis is a key regulator of critical illness. Cortisol and adreno-corticotrophic hormone (ACTH) are pulsatile, which emerges from the feed forwardfeedback of the two hormones [1] . Different genes are activated by continuous or pulsatile activation of the glucocorticoid receptor, even when the total amount is the same [2] . We aimed to characterise the ACTH and cortisol profiles of patients who were critically ill after cardiac surgery and assess the impact of inflammatory mediators on serum cortisol concentrations. Methods: 20 patients with >2 organ system failure, >2 days after cardiac surgery were recruited. Total cortisol was assayed every 10 min, ACTH every hour and IL1, IL2, IL4, IL6, IL8, TNF-α every 4 hours. Cortisol binding globulin (CBG) was assayed at 0 and 24hrs. The relationship between cortisol and the inflammatory mediators was quantified in individual patients using a mixed regression model. Results: All profiles showed pulsatility of both cortisol and ACTH and there was concordance between the two hormones (See Fig. 1 ). One patient died after 23 hours (see Fig. 2 ). This patient lost pulsatility and concordance of cortisol and ACTH. Mean CBG was 26.89μ g/ml at the start of sampling and 28.13μ g/ml at the end. There was an association between IL6 (p=0.0002), IL10 (p<0.0001), IL4 (p=0.029) and serum cortisol levels. There was no association between the other mediators and cortisol. Conclusions: Cortisol and ACTH are both pulsatile in critical illness. Because pulsatility emerges from the interaction between the two hormones[2]the premise of a 'disconnect' between the pituitary and adrenal gland is refuted. IL6, IL10 and IL4 may have roles in the control of cortisol during critical illness. Introduction: Elevation in plasma cortisol is a vital response to sepsis and partially brought about by reduced cortisol breakdown in which bile acids (BAs) may play a role. Vice versa, cortisol can also upregulate BAs. We hypothesized a central role for the hepatic glucocorticoid receptor (hGR) in cortisol and BA homeostasis and in survival from sepsis. Methods: In a mouse model of sepsis, we documented hGR expression and investigated the impact of hepatocyte-specific shRNAknockdown of GR on markers of corticosterone (CORT), BA and glucose homeostasis, inflammation and survival. We also compared hGR expression in human septic ICU and elective surgery patients. Results: In mice, sepsis reduced hGR expression with 21% (p=0.04), elevated plasma CORT, BAs and glucose and suppressed A-ringreductases. Also in human patients, sepsis reduced hGR expression (p<0.01), further suppressed by treatment with steroids (p=0.04). In septic mice, further and sustained hGR-inhibition increased mortality from 12% to 60% (p<0.01). At 30h, hGR-inhibition prevented the rise in total plasma CORT, but did not affect A-ring-reductases expression. However, it further reduced CORT binding proteins, resulting in elevated free CORT equal to septic mice without modified hGR. After 3 days of hGR-inhibition in sepsis, total and free CORT were comparable to septic mice without modified hGR, now explained by further reduced A-ring-reductase expression, possibly driven by higher hepatic BA content. HGR-inhibition blunted the hyperglycemic sepsis response without causing hypoglycemia, markedly increased hepatic and circulating inflammation markers and caused liver destruction (p<0.05), the severity of which explained increased mortality. Conclusions: In conclusion, sepsis partially suppressed hGR expression, which appears to upregulate free CORT availability via lowered CORT binding proteins and A-ring-reductases. However, further sustained hGR suppression evoked lethal excessive liver and systemic inflammation, independent of CORT availability. Introduction: Cortisol levels have been found to be increased in sepsis patients, and high cortisol levels have been correlated with increased mortality. The purpose of this project is to assess the association of plasma cortisol levels with severity of coagulopathy in a population of patients with sepsis and clinically confirmed DIC. Methods: Citrated, de-identified plasma samples were collected from 52 adults with sepsis and suspected DIC at the time of ICU admission. Platelet count was determined as part of standard clinical practice. PT/INR and fibrinogen were measured using standard techniques on the ACL-ELITE coagulation analyzer. Cortisol, D-dimer, PAI-1, CD40L, NLRP3, and microparticles were measured using commercially available ELISA kits and were performed. DIC score was calculated using ISTH scoring algorithm. Results: Cortisol showed significant variation based on DIC status (Kruskal-Wallis ANOVA, p < 0.0001). Patients with non-overt DIC and overt DIC exhibited significantly elevated cortisol levels compared to healthy controls (p < 0.0001 for both groups). Cortisol levels showed DIC based variations. Patients with sepsis and overt DIC had elevated cortisol compared to patients with sepsis and no DIC (p = 0.0069) (Fig. 1) . Correlations were evaluated between cortisol and hemostatic markers platelets, fibrinogen, INR, D-Dimer, and PAI-1 as well as with the inflammatory marker, NLRP3 and the platelet markers CD40L and microparticles. Cortisol Conclusions: Cortisol showed a significant association with hemostatic status in a population of patients with sepsis and welldefined coagulopathy. Cortisol levels were significantly elevated in patients with overt or non-overt DIC compared to healthy individuals and in patients with overt DIC compared to those with sepsis without DIC. Introduction: In most cases presenting with hypoglycemia in emergency departments (EDs), the etiology of the hypoglycemia is almost identified. However, about 10% of cases, the etiology of hypoglycemia cannot be determined. Methods: This is a 2-year prospective observational study. A total of 232 patients were transported to our ED with hypoglycemia. After the investigation, a rapid ACTH loading test (synthetic 1-24 ACTH 250 μg iv.) was performed on 21 patients with unexplained hypoglycemia; i.e., 250 μg ACTH was administered intravenously and blood specimens were collected before loading, at 30 min and 60 min after ACTH administration. We adopted a peak serum cortisol level < 18 μg/dl or a delta cortisol of < 9 μ g/dl for the diagnosis of adrenal insufficiency. Results: Among the patients, 163 of 232 (70.3%) were using antidiabetic drugs, 15 (6.5%) were using hypoglycemia-relevant drugs, 12 (5.2%) suffered from digestive absorption failure including malnutrition, 10 (4.3%) had been consuming alcohol, 9 (3.9%) suffered from malignancy, and 2 (0.9%) suffered from insulin autoimmune syndrome. Initially, an etiology was unknown in 21 of 232 (9.1%) patients. Rapid ACTH test revealed the adrenal insufficiency in 19 (8.2%) among them. Administration of hydrocortisone in adrenal insufficiency patients promptly improved hypoglycemia. In those patients, serum sodium level was lower (Na; 134 vs. 139 mEq/l, P<0.001) and serum potassium level was higher (K; 4.7 vs. 3.9 mEq/l, P<0.001) than in the other hypoglycemic patients, respectively. There was no significant difference in baseline plasma glucose level on ED between the groups of patients (28 vs. 26 mg/dl, P=0.34). Conclusions: The probability of adrenal insufficiency was much greater than that of the better-known insulinoma as a cause of hypoglycemia. When protracted hypoglycemia of unknown etiology is recognized, we recommend that the patient is checked for adrenal function using the rapid ACTH loading test. Introduction: Sepsis caused have showed serious alternations of thyroid hormones releasing, causing a nonthyroidal illness syndrome. The aim of the study was to measure thyroid hormone levels in septic patients and analyse its relation with clinical state and outcome. Methods: Prospective study in a cohort of 150 consecutive septic patients. We studied thyrotropin (TSH), free triiodothyronine fraction (fT3) and free thyroxin fraction (fT4) serum levels, APACHE II and SOFA score. Statistical analysis was performed using SPSS 15.0. Results: We analysed 150 episodes of sepsis (16%) and septic shock (SSh) (84%), the median age of the patients was 64 (inter-quartile range, 48.7-71) years; the main sources of infection were: respiratory tract (39%) and intra-abdomen (30%); 70.7% had medical diseases. APACHE II score was 25 [21-30], SOFA score was 10 [7.75-11] and 28day mortality was 22.7%. Our data shown 18.3% with low levels of TSH (<0.2uUI/mL), 20.3% had low levels of fT4 (<0.75 ng/dL) and 71.4% low levels of fT3 (<2 pg/mL). The TSH (0.89 vs. 1.46 uUI/mL) and fT3 (1.3 vs 1.8 pg/ml) concentration of SSh group were significantly lower than those of sepsis group, whereas FT4 (1.10 vs 1.18 ng/dL) it was not statistically significantly. Correlation of FT3 to APA-CHE II (r = −0.342, p = 0.035) and SOFA score (r = −0.409, p = 0.017). The profile of death patients were men (64.7%, n =22), with significantly older (63 vs. 57 years; p=0,049), as well as clinical severity scores, APACHE II (29.8 vs. 24.1; p<0.001) and SOFA (12.1 vs 8.9; p<0,001). Non-survivors had significantly lower TSH 0.85 vs. 1.4 uUI/ mL; p=0.042, and fT3 1.2 vs. 1.39 pg/mL, p=0.031, however fT4 did not show statistical significance 0.42 vs. 0.58ng/dL, p=ns. Conclusions: Conclusions: Most of our septic patients present an altered thyroid function. Our data suggest that TSH and specially fT3 may be used as a marker of disease severity and a mortality predictor. Observational study to evaluate short and long-term bone metabolism alteration in critical patients. Introduction: Reduction of bone mineral density and/or muscle mass can be short and long-term complications in critical patients admitted in Intensive Care Unit (ICU). The study aims to evaluate, during a 12-month period, the following parameters: 1) the alterations of bone metabolism and quantitative and qualitative parameters of bone tissue, 2) the proportion of subjects with bone fragility, and 3) the identification of specific risk factors. Methods: An observational-longitudinal monocentric study is being conducted in adult patients hospitalized in ICU. The evaluations performed at baseline, 6 and 12 month visits include analysis of biochemical and instrumental exams. Results: A specific clinical-care pathway was created between Bone Metabolic Diseases Unit and ICU, in order to perform specific anamnestic collection, biochemical analysis of bone metabolism, and instrumental exams. 31 patients were enrolled and evaluated at the baseline visit. Biochemical exams, performed within 72 hours of hospitalization, showed that 64% (N:20) of subjects had a deficit of 25OHvitaminD <20 ng/dl, associated with normal corrected serum calcium levels and of these 42% (N:13) had high PTH levels. Bone alkaline phosphatase was increased in 26% (N:8) of patients. Conclusions: Critical patients are "fragile" subjects, which should be monitored with a short and long-term follow-up. The creation of a clinical pathway that includes specialists of bone metabolism may be a virtuous way to identify patients who report bone mass loss and increased fracture risk. This study will allow to implement the knowledge regarding specific risk factors of bone fragility and the most appropriate therapeutic choices as prevention and treatment. A retrospective analysis of predictors for length of intensive care stay for patients admitted with diabetic ketoacidosis A Fung, TL Samuels, AE Myers, PG Morgan East Surrey Hospital, Redhill, UK Critical Care 2018, 22(Suppl 1):P346 Introduction: Diabetic ketoacidosis (DKA) is one of the most common metabolic causes of admission to the intensive care unit (ICU). The incidence of DKA is quoted as between 4.6-8 episodes per 1000 patients with diabetes mellitus (DM) [1] . We aim to establish the factors that affect length of stay (LOS) on ICU. Methods: We undertook an analysis of patients admitted to ICU over the last 7 years with a primary diagnosis of DKA. We assessed whether there was an association between the following factors and an increased length of ICU stay: age, gender, body mass index (BMI), systolic blood pressure, heart rate, sodium, potassium, haemoglobin and pH. These factors were assessed using multiple linear backward stepwise regression. Results: Overall, 94 admissions were identified over the time period from the ward watcher database. The median LOS was 2.4 days (IQR 1.3 -4.7). Our analysis demonstrated that length of ICU stay (alpha level <0.05) was significantly associated with BMI, low systolic blood pressure, and the presence of hyponatraemia or hypernatraemia. Conclusions: We found the variables that affect the LOS for patients presenting to our unit with DKA are BMI, elow systolic BP, low sodium and high sodium. We intend to extend this work to include survival analysis with the same subgroup of patients. Maximal glycemic gap is the best glycemic variability index correlated to ICU mortality in medical critically ill patients T Issarawattna, R Bhurayanontachai Prince of Songkla University, Songkla, Thailand Critical Care 2018, 22(Suppl 1):P347 Introduction: Several evidences shown a correlation of glycemic variability (GV) and ICU mortality. However, there have been no report of the correlation between various parameters of GV and mortality in medical ICU patients. The aim was to determine the correlation between various parameters of GV and medical ICU mortality, as well as, to identify the best GV index to predict ICU mortality. Methods: A retrospective chart review was then conducted in medical ICU at Songklanagarind hospital. The patient characteristics, causes of admission, APACHE II, blood glucose within the first 24 hours of ICU admission and ICU mortality were recorded. Glycemic variability parameters including maximal glycemic gap, standard deviation, coefficient of variation and J-index of blood glucose were calculated. The correlation of those GV index to ICU mortality was determined. The ROC and AUROC of each GV index were then compare to identify the best GV index to predict ICU mortality. Results: Of 538 patients, 442 patients (82.2%) were survived ( Table 1 ). All GV indexes were significantly higher in non-survival group (p < 0.05) ( Table 2 ). Maximal glycemic gap was independently correlated to ICU mortality and give a highest AUROC compared to others GV. (Maximal glycemic gap AUROC 0.69 (95%CI 0.64-0.75 vs. coefficient of variation AUROC 0.68 (95%CI 0.62-0.74) vs standard deviation AUROC 0.67 (95%CI 0.61-0.73) vs J-index AUROC 0.63 (95%CI 0.57-0.7), (p< 0.001) (Fig. 1) . Conclusions: Maximal glycemic gap independently correlated to ICU mortality and was the best GV to predict ICU mortality in medical critically ill patients. Reliability of capillary blood glucose measurement for diabetic patients in emergency department H Ben Turkia, S Souissi, A Souayeh, I Chermiti, F Riahi, R Jebri, B Chatbri, M Chkir Regional Hospital of Ben Arous, Ben Arous, Tunisia Critical Care 2018, 22(Suppl 1):P348 Introduction: Acute glycemic disorders should be early diagnosed and treated in Emergency Department (ED), especially hypoglycemia. Can capillary blood glucose (CG) replace plasmatic glucose (PG). The objective of this study was to compare capillary blood glucose with venous blood glucose Methods: Patients with type 2 diabetes were included. We realize a capillary blood glucose with a glucose meter (acu-check active-Roche) and a concomitant determination of venous blood glucose with laboratory machine (synchrony CX3 delta system beckman coulter). A correlation study (Pearson correlation) between the two measurements was evaluated and linear fitting equation was established. The concordance was checked with Bland and Altman method. Results: During the 4 months of the study, 258 patients were included. The average age was 55+/-19 years old, with a sex ratio =1. Majority of patients (70%,n=182) had type 2 diabetes and 58% was treated with insulin. We found an excellent correlation between the two techniques with a Pearson correlation coefficient r= 0.96.Topredict the PG from CG, we can use this equation: PG(g/l)=0.9979 CG(g/l)+ 0.08128 (R2=0.9207 ; p=0.0001). We noticed a good concordance between the two techniques especially in case of hypoglycemia and moderate hyperglycemia (Fig. 1) . However, 11 releases were noted with a PG higher than 4g/l. Conclusions: In ED, the measurement of capillary glucose can exempt from venous blood glucose especially in case of hypoglycemia and moderate hyperglycemia. is frequently found in critically ill patients in ICU, especially patients who are treated for a long time. This study aims to analyse the comparison between length of stay and DVT incidents in critically ill patients. Methods: A cross-sectional study was employed. We include all patients who were 18 years or older and were treated in ICU of Dr Soetomo public hospital for at least 7 days. Data were collected from June 2016 until June 2017. The patients were examined with Sonosite USG to look for any thrombosis in iliac, femoral, popliteal, and tibial veins and Well's criteria were also taken. Results: Thirty patients were included in this study. This study shows that length of stay is not the only risk factor for DVT in patients treated in ICU. In our data, we found out that the length of treatment did not significantly cause DVT. Other risk factors such as age and comorbidities in patients who are risk factors may support the incidence of DVT events. The diagnosis of DVT is enforced using an ultrasound performed by an expert in the use of ultrasound to locate thrombus in a vein. Conclusions: Length of treatment is not a significant risk factor for DVT. Several other factors still need to be investigated in order for DVT events to be detected early and prevented. [2] was used to retrospectively study trends and outcomes of cancer patients admitted to the ICU between 2002 and 2011. Logistic regression analysis was performed to assess predictors of 28-day and 1-year mortality. Results: Out of 41,468 ICU admissions, 1,100 hemato-oncological, 3,953 oncological and 49 patients with both a hematologic and solid malignancy were analyzed. Hematologic patients had higher critical illness scores, while oncological patients had similar APACHE-III and SOFA-scores. In the univariate analysis, cancer was strongly associated with mortality (OR 2.5, Table 1 ). Over the 10-year study period, 28-day mortality of cancer patients decreased by 30% (Fig. 1) . This trend persisted after adjustment for covariates, with cancer patients having significantly higher mortality (OR=2.49, 95%CI: 2.3, 2.7). Between 2002 and 2011, the adjusted 28-day mortality decreased by 8% every year. Over the decade, 1-year mortality decreased by 27%. Having cancer was the strongest individual predictor of 1-year mortality in the multivariate model (OR=4.40, 95%CI: 4.1, 4.8) (Fig. 2) . Conclusions: Between 2002 and 2011, the number of cancer patients admitted to the ICU increased steadily and significantly, while longitudinal clinical severity scores remained overall unchanged. Although hematological and oncological patients had higher mortality rates than patients without cancer, both 28-day and 1-year mortality decreased significantly over the study period. Introduction: Sepsis was redefined in 2016 with the introduction of an increase in Sequential Organ Failure Assessment ΔSOFA) score of >= 2 and the quickSOFA (qSOFA) as screening tools for sepsisrelated mortality. However, the implementation of these criteria into clinical practice has been controversial and the applicability for hematological patients is unclear. Methods: We therefore studied the diagnostic accuracy of different sepsis criteria for sepsis and mortality according to definition criteria in a retrospective analysis of hematological patients in an academic tertiary care hospital. Patient characteristics and variables were collected in ICUand non-ICU patients to determine the Systemic Inflammatory Response Syndrome (SIRS), ΔSOFA and qSOFA. By applying the definition of sepsis as "life-threatening organ dysfunction caused by a dysregulated host response to infection" [1] as reference, the scores were evaluated. In patients with sepsis who died, 5/22 were SIRS-negative, 4/24 ΔSOFA-negative and 14/20 qSOFA-negative ( Fig. 1 and Table 2 ). Conclusions: In conclusion, these findings suggest that criteria proposed in the Sepsis-3 definition might have limitations as screening Fig. 2 (abstract P351) . Results of the logistic regression analysis for (A) 28-day and (B) 1-year mortality. All covariates were statistically significant except for white race in the 1-year mortality model. ***p-value<10-16, **p-value<0.001, *p-value<0.01 Fig. 1 (abstract P351) . Longitudinal change in 28-day mortality for cancer patients (yes) compared with controls (no) over the 10-year study period. Mortality in the cancer group decreased from 36% to 25% (-30%), while mortality in the control group decreased from 14 to 12% (-21%). Enoxaparin pharmacokinetics in patients with augmented renal clearance, preliminary results of a single center study Introduction: Augmented renal clearance (ARC) has being described in some groups of critically ill patients. The aim was to investigate the impact of ARC on the pharmacokinetics of enoxaparin. Methods: This is a prospective study in a surgical and medical intensive care unit (ICU) carried out from August to November 2017. Patients <65 years old, under prophylactic treatment with enoxaparin and normal plasma creatinine, were included. Anti-Xa activity was measured at second day under treatment. Creatinine clearance was calculated from urine sample collected during 24-hours. ARC was defined by a creatinine clearance >=130 mL/min/1.73 m2. Results: Thirteen patients aged 43 years old (±16.4) were included. Six patients developed ARC and 5 of them were in therapeutic range. Seven patients did not develop ARC and 6 of them were in therapeutic range. There was no differences between the two groups in achieving therapeutic range (Fisher test, p=0.5). We did not observe thromboembolic events. Conclusions: We found no relationship between ARC and therapeutic failure in patients under prophylactic treatment with enoxaparin. Introduction: This study reviewed argatroban use in patients in a tertiary hospital critical care unit. Argatroban is a direct thrombin inhibitor approved for use in proven or suspected heparin-induced thrombocytopenia (HIT) in patients with renal dysfunction. Methods: This was a retrospective cohort study in a medical and surgical ICU in a tertiary teaching hospital. Data was collected for adult patients treated with argatroban for proven or suspected HIT April-August 2016, excluding patients requiring ECMO. We scored patients using the 4T score and compared this to an ELISA immunoassay optical density score which quantifies the PF4/H antibody level. Also noted was use of continuous haemodialysis and organ failure using the Sequential Organ Failure Assessment (SOFA), scoring >=3 defines failure. Results: 16 patients were treated with argatroban for proven or suspected HIT. 15/16 patients had a positive ELISA. There was no relationship between 4T score and ELISA optical density (Fig. 1) . Infusions were commenced at either the manufacturer recommended dose of 2 μg/kg/min or a reduced dose of 0.5 μg/kg/min. Patients receiving the reduced dose had a median of 2 organs failing compared to 1 in the standard regimen. The time taken to the first APTR in range was longer with the reduced dose regimen, however, the time to a stable APTR was less (Table 1 ). In 2 patients the dose of argatroban never stabilised. 1 died and 1 was very sensitive to argatroban and required cessation of the infusion for interventions. In the reduced regimen group, there were 2 episodes of bleeding, 1 minor PR bleed in a patient with 3 organs failure and 1 upper GI bleed. Conclusions: In this population of ICU patients the 4T score did not correlate with the ELISA optical density score, as found previously. Patients with multi-organ failure mostly received the reduced starting dose. However, the bleeding events were still confined to this group. This correlates with previous studies that organ dysfunction necessitates a dose reduction for argatroban. Results: The mean age in our study group was 54±22 years. The effects of TPE on standard coagulation were increased aPTT (24±2 to 36 ± 6 s, p=0.005) and decreased fibrinogen levels (286±76 to 242 ±48 mg/dL, p=0.008). A non-significant decrease in platelet count was observed (160333±23091 to 151133±22244/mm 3 , p=0.662). On ROTEM parameters TPE was associated with increased CT in ExTEM (57±8 to 73±12 s, p=0.030) and InTEM (156±15 to 194 ±52 s, p=0.003) and increased MaxVt on ExTEM (90± 27 to 128 ± 37 s, p=0.031) and InTEM (177±17 to 225±71 s, p=0.003). All other ROTEM parameters changed non-significantly. The decrease observed in fibrinogen levels was not associated with a decrease in FibTEM MCF (15±2 to 14±2 mm, p=0.414). Conclusions: Our results demonstrate that TPE is associated with minimum changes in clot kinetics initiation that do not result in either pro-or anti-coagulant changes. Therefore, TPE with fresh frozen plasma can be safely used in normal subjects. Introduction: Acutely ill patients are prone to critical illness anaemia, a multifactorial condition with potential contribution of iatrogenic anaemia defined as lowered Hb due to large/frequent venepunctions. Decline in Hb is most pronounced in the first 3 days of ICU stay. It correlates with the need for RBC transfusion, but the impact on patient outcome is uncertain. The aim of this study was to determine impact of phlebotomy on change in Hb (ΔHb), and correlation of ΔHb with need for transfusion, presence of central venous catheter (CVC) and patient outcome. Conclusions: Critical illness anaemia is an unexplained phenomenon. Impact of phlebotomy is hard to unequivocally determine since there are many confounders. The change in Hb levels during ICU stay correlates with the need for transfusion that could cause immunomodulation and potentially adverse outcome. Every effort should be made to maintain adequate Hb levels and lower the risk of iatrogenic anemia. Introduction: Anemia is prevalent in critically ill traumatic brain injury (TBI) patients and red blood cell (RBC) transfusions are often required. Over the years, restrictive transfusion strategies have been advocated in the general critically ill population. However, considerable uncertainty exists regarding optimal transfusion thresholds in critically ill TBI patients due to the susceptibility of the injured brain to hypoxemic damages. Methods: We conducted an electronic self-administered survey targeting all intensivists and neurosurgeons from Canada, Australia and the United Kingdom working caring for TBI patients. The questionnaire was developed using a structured process of domains/items generation and reduction with a panel of experts. It was validated for clinical sensibility, reliability and content validity. Results: The response rate was 28.6% (217/760). When presented with a scenario of a young patient with severe TBI, a wide range of transfusion practices was noted among respondents, with 47% favoring RBC transfusion at a hemoglobin level of 7g/dL or less in the acute phase of care, while 73% would use this trigger in the plateau phase. Multiple trauma, neuromonitoring data, hemorrhagic shock and planned surgeries were the most important factors thought to influence the need for transfusion. The level of evidence was the main reason mentioned to explain the uncertainty regarding RBC transfusion strategies. Conclusions: In critically ill TBI patients, transfusion practices and hemoglobin thresholds for transfusion are said to be influenced by patients' characteristics and the use of neuromonitoring in critical care physicians and neurosurgeons from Canada, Australia and the UK. Equipoise regarding optimal transfusion strategy is manifest, mainly attributed to lack of clear evidences and clinical guidelines (1-year) . No significant associations were found between FFP:RBC ratio and mortality rates. Patients with higher APACHE II score received more platelet transfusions and mortality rates were higher in those who received Platelets:RBC ratio >1. On multivariate analysis, higher APA-CHE II score was an independent predictor of increased mortality. Conclusions: The compliance with the recommended 1:1:1 ratio of blood products was poor. There was no association between transfusion ratios and mortality after adjusting for APACHE II score. Introduction: The lack of evidence-based medicine supporting the transfusion decision is illustrated by the wide range of blood product use during first-time coronary artery bypass grafting (CABG). Use of red blood cells (RBC) ranges from 3 to 83 percent, while the use of platelets range from 0 to 40 [1] . Approximately 20 percent of CABG patients suffer abnormal bleeding, with platelet dysfunction thought to be the most common culprit [2] . Methods: The objective of this study was to evaluate the use of allogeneic blood and blood products among patients undergoing first-time CABG over the past 15years. The first 50 patients who underwent CABG (on-pump and off-pump) from 1 st of March each year were included for analysis. The percentage of patients receiving RBC, fresh frozen plasma (FFP), platelet and cryoprecipitate during the first 48 hours intra-and postoperatively were analysed. Linear regression analysis was performed in each group. Results: Our analysis shows that the use of RBC decreased over the last 15 years, in contrast to the use of the other 3 investigated products. (see Fig. 1 ) The increase of platelets was the most pronounced with a direction coefficient of 0.022 and had the least variability (r 2 =0.59). (see Fig. 2 ) The decrease in RBC was less obvious than the rise in platelet use (direction coefficient of 0.015) and had a higher variability (r 2 =0.32). The consumption of FFP and cryoprecipitate stayed constant (direction coefficient of 0.004 and 0.001 respectively). The higher incidence of semi-urgent CABG in recent years, which involves continuation of anti-platelet therapy until the day before surgery, can be an explanation for our observed increased use of platelets. The observed decrease in RBC transfusion over the past 15 years might be due to rising awareness of complications associated with red cell transfusion. Introduction: Red blood cells (RBC) transfusion is frequently required in cardiac surgery and is associated with increased morbidity and mortality rates. The aim of this study is to identify predictors of RBC transfusion for patients undergoing cardiac surgery, emphasizing the use of bioelectrical impedance analysis (BIA). Methods: This was a retrospective study of patients who underwent elective cardiac surgery between years 2013 and 2014 in a tertiary reference center. Patients' demographic and clinical variables, preoperative BIA measurements and postoperative data were analyzed. The univariate and multivariate logistic regression analyses were used to identify the predictors of postoperative RBC transfusion. All of the calculations were performed with IBM SPSS v. 24. Introduction: Red blood cells (RBC) transfusion is a common intervention in cardiac surgery and is associated with higher mortality rates and predisposes serious adverse events. The aim of this study was to determine whether red blood cells (RBC) transfusion is linked to long-term results after cardiac surgery. Methods: This observational retrospective study included all of the patients who underwent any of the STS defined elective cardiac surgery types from 2013 to 2014. We evaluated 3-5 year all-cause mortality rates and secondary postoperative outcomes defined by the STS risk prediction model. Patients were categorized according to whether they received RBC transfusions postoperatively; long-term results were compared using Cox-regression analysis and Kaplan-Meier method. Introduction: Transfusion of packed red cells (PRCs) is an important treatment option for patients requiring intensive care but, like all treatments, it is not without risk. These patients, although may be more sensitive to anaemia, are also at increased risk of transfusionrelated complications. We conducted an audit of blood prescribing and administering practices in our intensive care unit. Methods: Audit proformas were placed in blood prescribing forms for a 1-month period. All transfusions of PRCs were logged over this time, and transfusion triggers, post-transfusion Haemoglobin (Hb) and whether Hb was checked between units was recorded, in addition to other supplementary information. Results: Over a 1-month period, 25 transfusion events were recorded, with an average age of the transfused patients of 60 years old (range 35 -87 years). 76% of transfusion events were for low Hb, 8% for bleeding and in 16% of cases the indication was not documented. For patients transfused for a low Hb, the mean transfusion trigger was 75 g/L (range: 66 g/L -86 g/L). Only 12% had a transfusion trigger of 70g/L or less, and a further 12% who were transfused for a low Hb had a Hb of 80g/L or more. 36% of transfusion events involved transfusing 2 or more units and, in only 22% of these cases the Hb was checked between units. Excluding the two bleeding patients, the mean increase in Hb following a single unit transfusion was 11.4 g/L (range 2 g/L -18 g/L), whilst in patients transfused two units, the average increase in Hb was 10 g/L per unit transfused (range 7 g/L -14.5 g/L), suggesting single unit transfusions may have greater Hb yields. Conclusions: Our audit demonstrated variability in transfusion triggers and progress needed with administering practices when transfusing multiple units of blood in the non-bleeding patient. We have since implemented measures to meet guidelines in both prescribing PRCs with restrictive triggers and in the administration and assessment of Hb between units, and will be re-auditing. Introduction: There is a perceived increased risk of bleeding in cirrhosis patients undergoing invasive procedures. This lead to a high rate of empirical prophylactic transfusion, which has been associated to increased complications and cost. The best strategy to guide transfusion in these patients remains unclear. Our aim was to compare three strategies to guide blood component transfusion prior to central venous catheterization (CVC) in critically ill cirrhosis patients. Methods: Single center, randomized, double-blinded, controlled clinical trial conducted in Brazil [1] . All cirrhosis patients admitted to the ICU with indication for a CVC were eligible. Participants were randomized 1:1:1 to three transfusion strategies based on: (1) standard coagulation tests (SCT), (2) rotational thromboelastometry (ROTEM) and (3) restrictive. The primary outcome was proportion of transfusion of any blood component prior to CVC. Secondary outcomes were incidence of major and minor bleeding, ICU length of stay (LOS), and 28-day mortality. Analysis was intention-to-treat. Results: 57 participants (19 in each group) were enrolled between September 2014 and December 2016. Most were male (64.9%) and listed for liver transplantation. The study ended after reaching efficacy in first interim analysis. There was no significant difference in baseline characteristics among groups. Regarding primary endpoint, there was 14 (73.7%), 13 (68.4%), and 3 (15.8%) events in SCT, ROTEM and restrictive groups, respectively (p <0.001). There was no difference between SCT and ROTEM groups (p >0.99). Overall 28-day mortality was 33.3% and was similar between groups. ICU LOS did not differ between groups. There was no major bleeding. Overall minor bleeding occurred in 10.53% with no difference between groups. Conclusions: A restrictive strategy is safe and effective in reducing the need of blood component transfusion prior to CVC in critically ill cirrhosis patients. A ROTEM-based strategy was no different from transfusion guided by SCT. Introduction: Desmopressin (DDAVP) is a vasopressin analogue which improves platelet function. Its general use as a haemostatic agent is still controversial. The aim of study was to evaluate the effect of prophylactic desmopressin in blood coagulation in patients undergoing heart valve surgery. Methods: Prospective, randomized, double-blind clinical trial performed at the Heart Institute of the University of São Paulo. A total of 108 adult patients undergoing heart valve surgery were enrolled from February 2015 to November 2016. Immediately after cardiopulmonary bypass weaning and heparin reversal, patients were randomized in ratio 1:1 to intervention group: DDAVP (0.3 μg/kg) or control group. Blood samples were drawn at three different times, at baseline (T0), 2 hours (T1) and 24 hours (T2) after study medication. Blood coagulation and perioperative bleeding were analysed using laboratorial tests and thromboelastometry, chest tube drainage and requirement of allogenic transfusion within 48 hours. Results: A total of 54 patients were allocated to intervention and 54 to control group. Blood levels of factor VIII at T2 (236. 5 Conclusions: Prophylactic use of desmopressin in heart valve surgery does not influence coagulation and thromboelastometric parameters. Identifying the impact of hemostatic resuscitation on development of multiple organ failure using factor analysis: results from a randomized trial using first-line coagulation factor concentrates or fresh-frozen plasma in major trauma (RETIC study) P Innerhofer 1 Introduction: to clarify how hemostatic resuscitation affects occurrence of multiple organ failure. Methods: analysis of secondary endpoints of the RETIC study [1] (coagulation factors, activated protein C (APC), thrombin generation, ROTEM parameters, syndecan-1, thrombomodulin (TM) and D-Dimer) measured at randomization, and after patients had received FFP or coagulation factor concentrates (CFC) at admission to ICU, 24 and 48 hours thereafter. We used factor analysis to reduce the highly interrelated variables to a few main underlying factors and analysed their relation to MOF before and after hemostatic therapy. Results: The factors Concentration, Clot and Hypoperfusion representing trauma-induced coagulopathy (Table 1) were comparable between groups at baseline (Fig. 1) and only high Hypoperfusionscore predicted MOF, while after therapy a low Clot-score also predicted MOF. Only the changes of the Clot-score independently affected occurrence of MOF (p=0.0002, adjusted OR 3.40, CI 2.46-4.71), while changes of Concentration (p= 0.8979, adjusted OR 0.96, CI 0.68-1.34) and Hypoperfusion (p=0.8098, adjusted OR 1.06, CI 0.84-1.33) did not. A lower Clot-score occurred after FFP transfusion than use of CFC, mainly through persistent thrombocytopenia (platelet count R2-4 FFP vs CFC p<0.02) (Fig. 2) . The higher Concentration-score after FFP did not affect MOF and FFP had no beneficial effect on fibrinolysis, syndecan-1, TM or APC. Conclusions: Hemostatic resuscitation should augment the factor Clot, which is feasible with early fibrinogen administration but not with FFP. The found platelet-saving effect of early fibrinogen administration is important as platelets play a major role in inflammation and transfusion of platelets did not correct thrombocytopenia. Introduction: The Trauma Induced Coagulopathy Clinical Score (TICCS) was developed to be calculable on the site of injury with the objective to discriminate between trauma patients with or without the need for Damage Control Resuscitation (DCR) and thus transfusion [1] . This early alert could then be translated to in-hospital parameters at patient arrival. Base excess (BE) and ultrasound (FAST) are known to be predictive parameters for emergent transfusion. We emphasize that adding this two parameters to the TICCS could improve its predictability. Methods: A retrospective study was conducted in the University Hospital of Liège. Based on the available data in the register (from January 1st 2015 to December 31st 2016), the TICCS was calculated for every patient. BE and FAST results were recorded and points were added to the TICCS according to the TICCS.BE definition (+3 points if BE < -5 and + 3 points in case of a positive FAST). Emergent transfusion was defined as the use of at least one blood product in the resuscitation room. The capacity of the TICCS, the TICCS.BE and the Trauma Associated Severe Hemorrhage (TASH) to predict emergent transfusion were assessed. Results: A total of 328 patients were included in the analysis. 46 (14 %) needed emergent transfusion. The probability for emergent transfusion grows with the TICCS.BE value (Fig. 1) . Positive Predictive Values (PPV) and Negative Predictive Values (NPV) of the three scores are displayed in Table 1 . Conclusions: Our results confirm that BE and FAST results are relevant parameters that can be added to the TICCS for better prediction of the need for emergent transfusion after trauma. Fig. 1 (abstract P367) . Probability for emergent transfusion with TICCS.BE values. Fig. 2 (abstract P366) . Boxplots show available measurements of extrinsically activated clot firmness at 10 min (ExA10), fibrin polymerization at 10 min (FibA10) and platelet count at baseline (R1) and after therapy at admission to ICU, 24 and 48 hours thereafter (R2 to R4) for the CFC (blue, n=46) and the FFP (yellow, n=42) group as well as for patients without (white, n=40) and with (grey, n=48) multiple organ failure. Table 1 ) for the CFC (blue, n=46) and the FFP (yellow, n=42) group, as well as for patients without (white, n=40) and with (grey, n=48) multiple organ failure. Each factor is given at the measurement time point baseline (R1) and following haemostatic resuscitation at admission to ICU, 24 and 48 hours thereafter (R2 to R4). Introduction: The management of the critically ill polytrauma patient is complex and is often a challenge for the intensive care team. The objectives of this study is to analyze the oxidative stress expression in polytrauma cases as well as to evaluate the impact of antioxidant therapy on outcomes. Methods: This prospective study was carried out in the Clinic for Anaesthesia and Intensive Care "Casa Austria", form the "Pius Brînzeu" Emergency County Hospital, Timisoara, Romania, with the approval of the hospital's Ethics Committee. ClinicalTrials.gov identifier NCT03095430. The patients' selection criteria included an Injury Severity Score (ISS) of 16 or higher, and age of 18 or higher. 67 patients were eligible for the study. They were divided in two groups, group A (antioxidant free, control, N=32), and group B (antioxidant therapy, study group, N=35). The antioxidant therapy consisted in continuous IV administration of 7500 mg/24 h of vitamin C until discharge from ICU. The patients included in the study presented with similar characteristics, and no statistically significant differences were shown between group A and B regarding age (p > 0.05), sex (p > 0.05), ISS upon admission (p > 0.05), percentage of patients admitted in the ICU more than 24 hour post-trauma (p > 0.05), and associated trauma (p > 0.05). Among patients in group B statistically significant differences were identified regarding the incidence of sepsis (p < 0.05), multiple organ dysfunction syndrome (p < 0.05), mechanical ventilation time (p < 0.05), and mortality (p < 0.05). No statistically significant differences were shown regarding the time spent in the ICU (p > 0.05). Conclusions: Following this study we can state that the administration of substances with a strong antioxidant character has positive influences on the outcome of critically ill patients, decreasing the incidence of secondary pathologies as well as mortality rates. ICC increased by 22.62%, ICD increased by 17.57%, slightly increased MA, and IRCL was nearly in the normal range. Conclusions: Rapid and accurate diagnosis of the coagulation system by LPTEG method at different stages of traumatic disease allows for more accurate selection and adjustment of the therapy, which allows improving the prognosis of the disease. Introduction: Evidence for tranexamic acid (TXA) in the pharmacologic management of trauma is largely derived from data in adults [1] . Guidance on the use of TXA in pediatric patients comes from studies evaluating its use in cardiac and orthopedic surgery. There is minimal data describing TXA safety and efficacy in pediatric trauma. The purpose of this study is to describe the use of TXA in the management of pediatric trauma and evaluate efficacy and safety endpoints. Methods: This retrospective, observational analysis of pediatric trauma admissions at Hennepin County Medical Center from August 2011 to November 2017 compares patients who did and did not receive TXA. The primary endpoint is survival to hospital discharge. Secondary endpoints include surgical intervention, transfusion requirements, length of stay, thrombosis, and TXA dose administered. Results: There were 35 patients [<=] 16 years old identified for inclusion using a massive transfusion protocol order. Twenty patients (57%) received TXA. Baseline characteristics and results are presented as median (IQR) unless otherwise specified, with statistical significance defined as p < 0.05. Patients receiving TXA were more likely to be older, but there was no difference in injury type or injury severity score (ISS) at baseline (Table 1) . There was no difference in survival to discharge, need for surgical intervention, or thrombosis (Table 2) . Patients who did not receive TXA had numerically higher transfusion requirements and longer length of stay, but these did not reach significance. Conclusions: TXA was utilized in 57% of pediatric trauma admissions at a single level I trauma center, more commonly in older patients. Though limited by observational design, we found patients receiving TXA had no difference in mortality or thrombosis. Introduction: The risk of venous thromboembolism (VTE) in trauma is greatly increased and one of the leading causes of morbidity and mortality after an accident [1] . Prophylactic measures to prevent VTE primarily consist of anticoagulants. In instances in which anticoagulation is contraindicated or inadequate, inferior vena cava (IVC) filters can be used [2] . However, insertion of IVC filter as a prophylactic measure is controversial as filter-related complications are well documented and increase with treatment time [3] . The objectives of our study were to evaluate IVC filter insertion indications and filter related complications in pelvic trauma patients. Methods: 250 patients with pelvic fractures were operated during the study period 1/01/2011-31/12/2015. All patients who received IVC filter during the period were included into analysis. Relevant data was collected from electronic patient journal. Results: Thirty four patients received retrievable filters during the study period (22 males and 12 females) ( Table 1) . Median age of patients was 59 years (range, 21-80). The predominant indication (79%) was prophylactic insertion. The median indwell time was 26 days (range 1-410 days). Despite IVC filter insertion one patient experienced lung embolism and another -DVT. In eleven cases IVC filters were tried to be removed at the treating hospital. In two cases filter extraction was unsuccessful and in another two cases filters were left in place due to IVC thrombosis. Conclusions: Majority of IVC filters were inserted outside guidelines [4] and proportion of prophylactic indications is significantly higher (80% vs 24%) than seen in registry studies [5] . Filter related complications were observed in 18% of patients. More restrictive approach to prophylactic IVC insertion should be exercised. The impact of preinjury antiplatelet and anticoagulant pharmacotherapy on outcomes in patients with major trauma admitted to intensive care unit ( Conclusions: Patients on preinjury anticoagulants and antiplatelet agents showed an increased mortality; this may be the result of the greater incidence of bleeding, the older age and more comorbidities in this groups. Is enzymatic debridement better in critically burned patients? Introduction: Early debridement of burned tissue reduces infection rate, ICU stay and mortality. The use of proteolytic enzymes such as bromelain allows a faster, more effective and selective debridement of denatured tissue, preserving and exposing healthy tissues, reducing debridement times compared to standard of care. Methods: Retrospective observational study performed in the Critical Burn Unit (March 2016 to September 2017) including 27 patients >18 years old with a total body surface area (TBSA) burned > 15% and < 75%, or > 65 years old with a TBSA burned > 10%, who underwent enzymatic debridement. Mean and standard deviation were used for normal quantitative variables and median and interquartile range in the opposite case. Qualitative variables were presented by absolute and relative frequencies. Results: Mean age was 47.6 ± 17.8 years old, 74% males, APACHE II 11 (RI 5-18), ABSI 7 (RI 5-9). Median TBSA burned was 29% (RI 18-50%), 21% (RI 16-39) were deep dermal or full thickness. Time until debridement was 21 hours (RI 8-35). 7.4% (n=2) had incomplete debridement after first application, 33% (n=9) received regional anesthesia, 91% (n=25) didn't need blood transfusion. 25% of patients who didn't have vasopressors prior debridement, needed the use of it with a mean dose of 0,6mcg/Kg/min. 25% of patients with vasopressors prior treatment, required an increase of dose by a mean of 0.9 mcg/Kg/min. Median ICU stay was 19 days. Mortality was 22%. Conclusions: Topical bromelain allows a fast start of tissue debridement with a low rate of failure. The need for fasciotomy and blood transfusion was very low. Topical treatment involved a fast and simultaneous debridement of the TBSA burned, generating an inflammatory response that in some cases required vasopressors. 837.539.15/10307). The BChE activity was measured by using point-ofcare-test system (Securetec Detektions-Systeme AG, Neubiberg, Germany). Levels of the routine inflammation biomarkers, i.e. C-reactive protein (CRP) and the white blood cell count (WBCC), were measured during the initial treatment period. Measurements were performed at the admission, followed by 12, 24 and 48-hour time points. Injury Severity Score (ISS) was used to assess the trauma severity. Results: The observed reduction in the BChE activity was in accordance with the change in the CRP concentration and the WBCC. The BChE activity measured at the hospital admission negatively correlated with the length of the ICU stay in patients with polytrauma (r = -0.5, Spearman's rank correlation coefficient). Conclusions: The BChE activity might be used as an early indicator for the magnitude of the systemic inflammation following polytrauma. Moreover, the BChE activity, measured at the hospital admission, might predict the patient outcome and therefore prove useful in early identification of the high-risk patients. Pharmacological interventions for agitation in traumatic brain injury: a systematic review Introduction: Among TBI complications, agitation is a frequent behavioural problem [1] . Agitation causes potential harm to patients and caregivers, interferes with treatments, leads to unnecessary chemical and physical restraints, increases hospital length of stay, delays rehabilitation, and impedes functional independence. Pharmacological treatments are often considered for agitation management following TBI. However, the benefit and safety of these agents in TBI patients as well as their differential effects and interactions are uncertain. Methods: The major databases and the grey literature were searched. We included all randomized controlled, quasi-experimental, and observational studies with control groups. The population of interest was all patients, including children and adults, who have suffered a TBI. Studies in which agitation was the presenting symptom or one of the presenting symptoms, studies where agitation was not the presenting symptom but was measured as an outcome variable and studies assessing the safety of these pharmacological interventions in TBI patients were included. Results: We identified 14 226 references with our search strategy. Two authors screened 12 899 after removal of duplicates. After searching the grey literature and secondary databases, a total of 170 potential articles were identified. Eleven studies in which agitation or an associated behavior was the presenting symptom, 11 studies where agitation was not the presenting symptom but was measured as an outcome variable, and 3 studies assessing the safety of these pharmacological interventions were identified. Overall, the quality of studies was weak. In studies directly addressing agitation, pindolol and propranolol may reduce assaults and agitation episodes. Amantadine and olanzapine may reduce aggression, whereas valproic acid may reduce agitated behavior. Conclusions: There is weak evidence to support the use of pharmacological agents for the management of agitation in TBI. Impact of decompressive craniectomy on neurological functional outcome in critically ill adult patients with severe traumatic brain injury: a systematic review and meta-analysis P Bonaventure, JA Jamous, F Lauzier, R Zarychanski, C Francoeur, A Turgeon CHU de Québec -Université Laval, Québec, Canada Critical Care 2018, 22(Suppl 1):P377 Introduction: Severe traumatic brain injury is associated with high mortality and functional disability. Several interventions are commonly used to control the intracranial pressure to prevent secondary cerebral injuries. Among them, decompressive craniectomy (DC) is widely performed; however, its impact on functional outcome is still under debate. Our objective was to assess the efficacy and safety of this procedure in adult patients with severe traumatic brain injury. Methods: We systematically searched in MEDLINE, EMBASE, CEN-TRAL, Web of Science, conference proceedings and databases of ongoing trials for eligible trials. We included randomized controlled trials of adult patients with severe traumatic brain injury, comparing DC to any other intervention. Our primary outcome was the neurological function based on the Glasgow Outcome Scale. Secondary outcomes were mortality, intensive care unit (ICU) and hospital length of stay, intracranial pressure control, and complications. Two reviewers independently screened trials for inclusion and extracted data using a standardized form. We used random effect models to conduct our analyses and the I2 index to assess heterogeneity. Results: We identified 5360 citations, from which we included 3 trials for a total of 573 patients. We observed no impact on the [3] . Univariate logistic regression analyses were performed to identify predictors associated with the decision for ICP monitoring. Results: A total of 857 adult patients were included (Tables 1  and 2 ). The risk of poor outcome estimated by the IMPACT model was associated to the decision to monitor ICP (Fig. 1) . ICP was more often monitored in patients with severe TBI, with one dilated pupil at admission and positive CT findings (in particular, high Marshall scores). Conclusions: According to our results, the clinician follows a multifactorial reasoning: the main determinants for the decision to monitor ICP are GCS, pupils' abnormalities and, above all, CT findings. Future studies will be needed to clarify specific indications for the clinicians in the identification of patients who would benefit from invasive monitoring. Trajectories of early secondary insults after traumatic brain injury: a new approach to evaluate impact on outcome. Introduction: Secondary insults (SI) occur frequently after traumatic brain injury (TBI). Their presence is associated with a worse outcome. We examined the early trajectories of hypotension (SBP<90mmHg), hypoxia (SpO2<90%) and pupillary abnormalities from the prehospital settings to the Emergency Department (ED), and their relationship with 6-months outcome. Methods: In this retrospective, observational study we included all TBI patients admitted to our Neuro Intensive Care Unit (NICU) from January 1997 to December 2016. We defined the trajectories of SI: -"sustained" if present on the scene of accident and at hospital admission, -"resolved" if present on the scene but resolved in ED, -"new event" if absent on the scene and present in ED, -"none" if no insults were recorded. We investigated the association of SI trajectories with 6-months dichotomized outcome (Glasgow Outcome Scale (GOS); favorable=4-5; unfavorable=1-3). Results: 967 patients were enrolled in the final analysis. Hypoxia and hypotension were related with unfavourable outcome when Introduction: Guidelines for management of pediatric traumatic brain injury recommend maintaining intracranial pressure (ICP) <20 mmHg [1] . Use of 23.4% sodium chloride (NaCl) is considered safe and effective for management of ICP in adults, but evidence for concentrations >3% in pediatrics is limited. This study will describe the safety and efficacy of 23.4% NaCl in reducing ICP among pediatric patients. Methods: This retrospective study evaluated patients <=18 years old who received 23.4% NaCl and had continuous ICP monitoring. Cerebral perfusion pressure (CPP), mean arterial pressure (MAP), ICP, and brain tissue oxygenation (PbtO2) were recorded hourly and were compared to baseline for 6 hours after each dose. Safety outcomes included peak serum sodium, peak serum chloride, and the incidence of stage 1 acute kidney injury (AKI) (serum creatinine elevation >=0.3 mg/dL or >=50%) [2] . Results: Between August 2007 and July 2017, 45 eligible pediatric patients received 235 doses of 23.4% NaCl; 215 doses were included in the analysis of perfusion parameters. Mean age was 11.6 +/-6 years (2 months to 18 years), and the median initial Glasgow Coma Scale score was 4. Subjects received a median of four 23.4% NaCl boluses, with a mean dose of 0.5 +/-0.18 mL/kg. Significantly lower ICP and higher CPP (p<0.001) were observed at all post-treatment time points (Fig. 1) ; PbtO2 was also significantly increased during 3 of the 6 hours recorded (p<0.05). There was no difference in MAP. Peak post-treatment serum sodium and chloride were 157 +/-6 mEq/L and 122 +/-7 mEq/L, respectively (Fig. 2) . Stage 1 AKI was observed in 15.6% of patients, and in-hospital mortality was 24.4%. Conclusions: Our data suggests that 23.4% NaCl is a safe and effective therapy for elevated ICP in pediatric patients. Methods: We performed a prospective study in adult patients with mild head trauma (GCS 14 and 15) qualified for acquisition of urgent head CT scan. The clinical symptoms potentially related to intracranial lesion including abnormal vitals, vomiting, headache, persistent dizziness were recorded. ONS as well as head CT were then performed. All ONS examinations were executed by an experienced sonographer to eliminating interrater bias. Head CT findings were dichotomized as positive or negative finding for ICH based on formal radiology reports. The patients' disposition including admission, surgery and safe discharge were followed. Results: 78 patients were enrolled for the survey. 16 patients had at least one symptom related to potential intracranial lesion (20.5%). The mean ONSD was 44±9mm. 25 patients were found to have ICH and 6 underwent neurosurgery thereafter. No significant difference of ONSD was found between the groups with and without ICH, as well as the group receiving surgery or conservative treatment. With introducing a conventional 5mm threshold of ONSD, the sensitivity, specificity, PPV and NPV was 0.36, 0.83, 0.50 and 0.73, respectively. While incorporating occurrance of at least one positive clinical symptom with the ONSD measurement greater than 5mm as a composite threshold, the sensitivity, specificity, PPV and NPV was 0.32, 1.00, 1.00 and 0.76, respectively. Conclusions: The diagnostic value of ONS in mild head trauma is defective. Nevertheless, with the supplemental aid of recognition of clinical symptoms relevant to potential intracranial lesion, the overall accuracy would improve. A correlation between YKL-40 concentrations in cerebrospinal fluid and Marshall classification in traumatic brain injurypreliminary results G Pavlov 1 , M Kazakova 1 , P Timonov 1 , K Simitchiev 2 , C Stefanov 1 , V Sarafian 1 1 Medical University -Plovdiv, Plovdiv, Bulgaria, 2 University of Plovdiv, Plovdiv, Bulgaria Critical Care 2018, 22(Suppl 1):P382 Introduction: Establishment of prognostic models in traumatic brain injury (TBI) would improve the classification based on predictive risks and will better define treatment options [1] . In recent years, one of the most intensively studied glycoprotein is YKL-40. It is expressed as a consequence of broad spectrum of inflammatory and malignant diseases [2] . This is study aimed to investigate the level of YKL-40 in TBI patients and its relationship with several clinical models. Methods: We determined plasma and cerebrospinal fluid (CSF) YKL-40 levels in six (6) patientson the 24th and 96th hour after the TBI. Each patient was examined by physical and instrumental methods for somatic and neurological status, clinical assessment and prognostic scales (GCS, Marshall Classification, APACHE III). Routine haematological and biochemical tests were also performed. As control served the CSF of age-matched suddenly deceased healthy individuals (n = 11), which was examined post mortem for YKL-40 levels. Results: We found no significant difference between plasma YKL-40 levels till 24th and 96th in all patients (mean difference ± SD: 57 ± 237 ng/ml 1 ) and calculated Cerebral Autoregulation (AR) as correlation coefficients (Pearson) for each IH Wave. Z-Ratios were divided according to binary AR outcome and correlation calculated with intracranial pressure before, during and after the IH waves. Results: Our preliminary analysis demonstrated a negative correlation between Intracranial pressure and Z-Ratio in the grouped 6 IH waves with preserved AR, but no correlation in the grouped 9 IH waves with impaired AR (Table 2 and Fig. 2 ). This indicates a decrease in power in the EEG low frequencies (0-7Hz) and/or an increase in the EEG high frequencies (7-30Hz) for increased values of intracranial pressure when AR is preserved. Conclusions: Features of IH waves differ depending on the ability of the injured brain to autoregulate cerebral blood flow. These features might include different signature of EEG frequency changes. The causative links and clinical significance of the different EEG patterns remain unexplored and might represent a signature of neurovascular coupling. Introduction: Targeted temperature management of patients who have suffered a traumatic brain injury is often used in the hope of preventing further insult to the brain; however, there is no uniform approach to managing temperature either locally, nationally or internationally, and maintenance of goal temperature in this patient population is often challenging due to hypothalamic injury. We sought to evaluate the feasibility and safety of an esophageal heat transfer device (EnsoETM, Attune Medical, Chicago, IL) to perform temperature management of patients suffering from traumatic brain injury. Methods: This was an IRB-approved prospective study of patients undergoing temperature management after traumatic brain injury. Patients were treated with an esophageal heat transfer device connected to an external heater-cooler, and maintained at target temperature for at least 24 hours. Patient temperature obtained via Foley catheter was recorded hourly, and the deviation from goal temperature during treatment reported. Results: A total of 12 patients were treated from August 2015 to May 2016. Temperature targets during treatment ranged from 34.0 to 36.8 degrees C. Maintenance of target temperature was successful, with 85% of readings within +/-1 degrees C of target, and 75% of readings within +/-0.5 degrees C of target. One patient developed a small hydrothorax, not attributed to device use. All patients survived to discharge from the ICU, with median CPC of 2 (range 1 to 4). Conclusions: Targeted temperature management of patients with traumatic brain injury using an esophageal heat transfer device was feasible and safe, providing a tight maintenance of goal temperature in this challenging patient population. Introduction: Traumatic brain injury (TBI) represents a serious problem in Europe. It still is the principal cause of death in US and Europe. Every year in Italy 250 people on 100,000 suffers of TBI and 17 on 100,000 dies. Disability and incapacity from TBI provides "strong ethicals, medicals, social and health economy imperative to motivate a concerted effort to improve treatment and preventions" Methods: Our hospital is the hub for Modena's county for TBI and we took part in the past 3 year on European project CREACTIVE (Collaborative ResearcE on ACute Traumatic brain Injury in intensiVe Care medicine in Europa) as branch of Italian group GIVITI (Gruppo Italiano per la Valutazione degli Interventi in Terapia Intensiva). Our study concerned about patients with TBI dismissed from ICU that "personally" or by the family will accepted the consensus to be included in our follow up conducted after 6 months from the dismissal. We collected clinical data from the admission to the dismissale and measured impact of TBI on all day life with GOS-E and QOLIBRI-OS using telephonical interview. Results: We collected data about 63 patients, 33 answered to the telephonical follow-up and only 10 compilated the QOLIBRI-OS. We found out that patients admitted with lower GCS has worst outcome in terms of quality of life. It also appears that anisocoria during ICU staying represents an odds ratio for death and is connected with worst quality of life after 6 months from the dismissal (Tables 1 & 2) . Inability to re-start a normal work-activity appeared to be the most important factor on the perception that our patient have of their new lives. Conclusions: Anisocoria seems to be an indicator of severe brain damage. GCS, despite it's simplicity, still represent the best and easiest way to score TBI. Work impairment appear to be the most important disability to determine subjective perception of quality of life after TBI, so efforts have to be made to improve work rehabilitation after the dismissal from hospital. Introduction: Hyperventilation (HV) reduces elevated intracranial pressure (ICP) by changing autoregulatory functions connected to cerebrovascular CO2 reactivity. Criticism to HV is due to the possibility of developing cerebral ischemia and tissue hypoxia because of hypocapnia-induced vasoconstriction. We aimed to investigate the potential adverse effects of moderate HV of short duration in the acute phase in patients with severe traumatic brain injury (TBI), using concomitant monitoring of cerebral metabolism, continuous brain tissue oxygen tension (PbrO2), and cerebral hemodynamic with transcranial color-coded duplex sonography (TCCD). Methods: A prospective trial was conducted between May 2014 and May 2017 at the University Hospital of Zurich. Adults (>18 years), with non-penetrating TBI, first GCS < 9, ICP-monitoring, PbrO2 and/or microdialysis (MD)-probes were included within 36 hours after injury. Data collection and TCCD measurements took place at baseline (A), at the begin of moderate HV (PaCO2 4-4.7 kPa) (C), after 50 minutes of moderate HV (PaCO2 4-4.7 kPa) (D), and after return to baseline (E) (Fig. 1) . Repeated measures ANOVA was used to compare variables at the different time points followed by post hoc analysis with Bonferroni adjustment as appropriate. P-value < .05 was considered significant. Results: Eleven patients were included (64% males, mean age 36 ± 14 years). First GCS was 7 (3-8: median and interquartile range). Data concerning PaCO2, ICP, PbrO2, mean flow velocity (MFV) in the middle cerebral artery, and MD values are presented in Table 1 . During HV, ICP and MFV decreased significantly. PbrO2 presented a trend of reduction. Glucose, lactate and pyruvate did not change significantly ( Table 2) . Conclusions: Short episodes of moderate HV have a potent effect on the cerebral blood flow, as assessed by TCCD, reduce ICP and PbrO2, and do not induce significant changes in cerebral metabolism. Outcome of pediatric patients six months after moderate to severe TBI -results of CREACTKids study from three PICU in Israel PaCO2 arterial partial pressure of CO2, CPP cerebral perfusion pressure (mmHg), ICP intracranial pressure (mmHg), PbrO2 brain tissue oxygen tension (mmHg), MFV mean flow velocity in the middle cerebral artery Introduction: Delirium is a major cause of complications in postoperative patient in ICU. Risk factors for delirium include poor cerebral hemodynamics and peri-operative cerebral desaturations. Intraoperative target cerebral oximetry monitoring may decrease the incidence of postoperative delirium in elective major abdominal surgery patients. Methods: A single-blinded, randomised controlled trial in patients undergo elective major abdominal surgery who received postoperative care in surgical ICU with age more than 65 years were randomised into two groups. The intervention group was received intra-operative target cerebral oxygen monitoring using cerebral oximetry whereas the control group was not. Delirium was assessed in both group at 24, 48, 72 hour postoperatively. Other risk factors for delirium, mechanical ventilator day, length of ICU stay, length of hospital stay and post-operative complication were recorded. Results: From August 2015-March 2016, 37 patients who met the criteria were randomised to 19 patients in intervention group and 18 patients in control group. Overall incidence of delirium was 27.03% (Intervention 21.05% VS Control 33.34%, p=0.401). Baseline cerebral oxygen in intervention group was 66.79 ± 3.11%. Desaturation below 10% from baseline was found in 8 from19 patients (42.1%) and was the only significant risk factor associated with delirium (p=.008, odd ratio 1.68). There was no significant different in mechanical ventilator day, ICU length of stay, hospital length of stay and postoperative complication between both groups. There was no complication associated with application of the cerebral oximetry probe in the intervention group. Conclusions: From this preliminary report can not demonstrated the significant different of intra-operative target cerebral oxygen monitoring by using cerebral oximetry in prevention of delirium. However the reduction of cerebral oxygen more than 10% from baseline in intervention group showed significantly associated with delirium postoperatively. The SET score as a predictor of ICU length of stay and the need for tracheotomy in stroke patients who need mechanical ventilation Introduction: SET score was initially developed as an in-house screening tool based on tracheotomy predictors identified in several retrospective studies. It combined the categories of neurological function, neurological lesions, and general organ function/ procedure, and weighed by allocation of certain point values [1] . In our study it was very interesting to us to find a tool to judge application of early tracheotomy, and as we have a good culprit number from stroke cases so we decided to try to apply this score in our ICU after discussion with the inventor of this score. Methods: 164 stroke patients were prospectively included in the study as they were ventilated or were very little potential for ventilation and assessed by the stroke-related early tracheotomy score (SET score, Table 1 ) within the first 24 h of admission (Table 2) . Endpoints were length of stay and ventilation time (VT) after doing early tracheotomy. We examined the correlation of these variables with the SET score using standard analytical methods. Results: The SET score with a value cutoff point of 8 had a significant effect on decision of making tracheotomy and hence decreasing ventilation time and length of stay in ICU, which affected outcome (Figs. 1 & 2) . Conclusions: All efforts must be exhausted in neuro intensie care to decrease the secondary changes of brain injury after stroke,early tracheotomy is a good tool to decrease length of stay in ICU and ventilation time in these patients.Inventing a tool to judge these decisions of doing tracheotomy was a challenge. SET score proved to be valuable.Further multi center trials are needed. Fig. 2 (abstract P390) . Specificity for the cutoff point of SET SCORE. Cut point of 9 is the best to predict tracheostomy with sensitivity of 85.0% and specificity of 80.4%. Cut point of 9 is the best to predict early tracheostomy with sensitivity of 86.1% and specificity of 81.5%. Since no patients had score 9 so the previous analysis that consider cut-point of 8 should remain the same but just change the number in the text to 9 contraindication for pharmacological VTE prophylaxis (65.4%). Overall, NCC patients were more likely to receive mechanical (90.3% ICU days) than pharmacological VTE prophylaxis (74.1% ICU days), however pharmacologic was more likely among younger patients with lower APACHE II scores. Guideline concordant care varied by recommendation; lower for pharmacological and higher for mechanical VTE prophylaxis. Conclusions: NCC patients uncommonly receive guideline concordant pharmacological VTE prophylaxis. Collectively, our findings suggest that current VTE prophylaxis prescribing practices may reflect uncertainty around risks associated with VTE prophylaxis among NCC patients. Methods: We retrospectively analysed prospectively collected data from 134 consecutive ICH patients that received DVT prophylaxis in a tertiary hospital. HE was defined as an increase of >6mL measured using the ABC/2 method or the semiautomatic software based volumetric approach. Using multivariate analysis, we analysed risk factors including early DVT prophylaxis for HE>24h, hospital mortality and poor 3-month functional outcome (3m modified Rankin Score>3). Results: Patients presented with a median GCS of 14 (IQR 10-15), hematoma volume of 11mL (IQR 5-24) and were 71y old (IQR 61-76). 56% received early DVT prophylaxis, 37% late DVT prophylaxis and 6% had unclear bleeding onset. Hematoma volume was smaller in the early DVT prophylaxis group with 9.5mL (IQR 4-18.5) vs 17.5mL (IQR 8-29) in the late prophylaxis group (p=0.038) without any other significant differences in disease severity. Delayed HE (N=5/134, 3.7%) was associated with higher initial hematoma volume (p=0.02) and lower thrombocyte count (p=0.03) but not with early DVT prophylaxis (p=0.36) in a multivariate analysis adjusted for known risk factors. Early DVT prophylaxis was not independently associated with 3m outcome. Conclusions: Although limited by the retrospective design, our data suggest that early DVT prophylaxis (<48h) may be safe in patients presenting with primary ICH, which supports the recommendations given by the Neurocritical Care Society. Introduction: There is a paucity of literature describing the relationship between clevidipine and its impact on intracranial pressure (ICP). The safety of clevidipine in patients with intracranial hemorrhage is often extrapolated from studies using nicardipine, which has demonstrated a neutral effect on ICP [1] . The objective of this study was to determine if there was a relationship between clevidipine initiation and changes to cerebral hemodynamic parameters. Methods: This study was a retrospective analysis of adults admitted to Hennepin County Medical Center between July 2012 and July 2017. Individuals were included if they had intracranial bleeding and ICP data recorded prior to initiation of a clevidipine infusion. Baseline demographic data was collected, including age, gender, type of injury, and initial Glasgow Coma Score (GCS Introduction: Aneurysmal subarachnoid hemorrhage (SAH) is an acute cerebrovascular event with high mortality and is an important cause of neurologic disability among survivors. Many complications in the course of SAH, such as hydrocephalus, also play a role in the poor outcome. The aim of the study was to describe the characteristics of patients with SAH admitted to the ICU to evaluate the factors associated with outcome. Methods: This study was conducted in two reference centersone in Rio de Janeiro and one in Porto Alegre. From July 2015 to September 2017, every adult patient admitted to the ICU with aneurysmal SAH was enrolled in the study. Data were collected prospectively during hospital stay. The primary endpoint was mortality and dichotomized functional outcome (poor outcome defined as Glasgow Outcome Scale 1 to 3) at hospital discharge and 12 months. Dichotomous variables were analyzed using two-tailed Fisher's exact test. Results: A total of 261 patients were included. Demographic characteristics are presented in Table 1 . Frequency of clinical and neurological complications are presented in Table 2 . In univariate analysis, factors most frequently seen in patients with unfavorable outcome were seizure (17% vs 3%, p=0.0003), hydrocephalus (51% vs 17%, p<0.0001), meningitis (30% vs 12%, p=0.0004), rebleeding (17% vs 4%, p= 0.0008), vasospasm (46% vs 26%, p=0.001), pneumonia (37% vs 7%, p<0.0001), sepsis/septic shock (21% vs 4%, p<0.0001), postsurgical neurological deterioration (37% vs 18%, p=0.001) and delayed cerebral ischemia (37% vs 12%, p<0.0001). At 12 months, out of 74 patients with follow-up, 40% had poor outcome. Conclusions: SAH is associated with high morbidity. Both neurological complications as clinical complications were associated with unfavorable outcomes. Therapeutic interventions to prevent those may have an impact on clinical outcomes. Introduction: Brain tissue hypoxia (brain tissue oxygen tension, PbtO2<20mmHg) is common after subarachnoid hemorrhage (SAH) and associated with poor outcome. Recent data suggest that brain oxygen optimization is feasible and may reduce the time with brain tissue hypoxia to 15% in patients with severe traumatic brain injury [1] . Little is known about the effectiveness of protocolized treatment approaches in poor-grade SAH patients. Methods: We present a retrospective analysis of prospectively collected data of 105 poor-grade SAH patients admitted to 2 tertiary care centers where PbtO2<20mmHg was treated using an institutional protocol. Treatment options were left to the discretion of the treating neuro-intensivists including augmentation of cerebral perfusion pressure (CPP) using vasopressors if necessary, treatment of anemia and targeting normocapnia, euvolemia and normothermia. The dataset used for analysis was based on routine blood gas analysis for hemoglobin data matched to 2 hourly averaged data of continuous CPP, PbtO2, temperature and cerebral microdialysis (CMD) samples over the first 10 days of admission. Results: Patients were admitted with a GCS of 3 (IQR 3-4) and were 58 (IQR 48-66) years old. Overall incidence of brain tissue hypoxia was 25%. During this time we identified associated episodes of CPP<70mmHg (27%), hyperglycolysis (CMD-lacta-te>4mmol/L, CMD-pyruvate>120μmol/L; 26%), pCO2<35mmHg (19%), metabolic distress (CMD-lactate-to-pyruvate-ratio>40; 18%), PaO2<80mmHg (14%), Hb<9g/dL (10%), and temperature>38.3°C (4%) (Fig. 1 ). Of these variables only hyperglycolysis was significantly more common (37%) during episodes of normal PbtO2 (75% of episodes). Conclusions: Underutilization of IVT despite the overwhelming evidence that support the effectiveness of such therapy can be partly attributed to the fear of hemorrhagic complications. This fear is not justified by current data. The estrangement of the emergency medicine community regarding IVT and the domination of stroke experts in decision making is also a barrier. Regional wall motion abnormalities and reduced global longitudinal strain is common in patients with subarachnoid hemorrhage and associated with markedly elevated troponin K Dalla Sahlgrenska University Hospital, Gothenburg, Sweden Critical Care 2018, 22(Suppl 1):P402 Introduction: Stress-induced cardiomyopathy after subarachnoid hemorrhage (SAH) is a life-threating condition associated with poor outcome. Regional wall motion abnormalities (RWMA) is a frequent finding, however, assessment of RWMA is known to be difficult. In the present study we hypothesized that global and regional longitudinal strain (GLS and RLS) assessed with speckle tracking echocardiography can detect myocardial dysfunction indicated by increased levels of the cardiac biomarker troponin (TnT). Methods: This prospective study comprised 71 patients with SAH. The TnT was followed daily from the admission up to 3 days postadmission and elevated TnT was defined as > 80 ng/l. A transthoracic echocardiography examination was performed within 48 hours after the hospitalization. The peak GLS was determined using the three apical projections and presented as the mean of the 18 segments. Reduced GLS was defined as > -15% and reduced RLS was considered present when segmental strain was > -15% in > 2 adjacent segments. Introduction: Deviations from strict eligibility criteria for intravenous thrombolysis (IVT) in ischemic strokes regarding either license contraindications to alteplase or relative contraindications to thrombolysis have been reported in international literature, with conflicting results on patients' outcome.The aim of our study was to evaluate safety and efficacy for patients receiving IVT outside standard inclusion criteria. Methods: Retrospective analysis of our department's thrombolysis database.We compared 83 patients with strict protocol adherence (strict protocol group) [mean age 63 years and National Institutes of Health Stroke Scale (NIHSS) at admission 12/range 5-28] and 41 patients with protocol deviations (off-label group) [mean age 68 years and NIHSS at admission 10/range 2-24],in particular 10 patients >80 years old, 13 patients with mild stroke-NIHSS< 5,and 22 with symptom-to-needle time 3-4.5 hours (4 patients had 2 deviations). Results: Patients in the off-label group were older but had no difference in baseline severity scores (SAPSII, NIHSS). They had no statistically significant difference on short-term (NIHSS at 7 days, need for critical care support, primary adverse event) and long-term (mortality,functional outcome at 3 months) outcome measures when compared to standard protocol patients. Conclusions: In accordance with international literature,off-label thrombolysis is save and equally effective to standard protocol thrombolysis.Thrombolysis strict protocol needs expansion of inclusion criteria. Introduction: Most scales (GCS,NIHSS) don't consider the pathway of secondary acute brain failure (sABF). Neuron-specific-enolase (NSE) could be usefull in diagnostic and treatment pts. with sABF [1, 2] . Methods: Prospective study incl. 35 pts. with ABF. Pts. were identical in condition, age and comorbidies. In main group, NSE examed and choline alfoscerate was used, pts. was divided into 2 subgroups Ia (n=12) with acute ischemic stroke(AIS) and Ib (n=10) pts. with posthypoxic encephalopathy. The control group (n=13) pts. with AIS treated by Loc.Protocol №602. Clinical, laboratory, and imaging variables were fully compared. Pts. examed by ABCDE algorithm, GCS and NIHSS. Brain CT, Carotid Doppler performed. Considering criteria:NSE(days 1,3,5), neurological status, length of stay in ICU (ICU LOS). "SS-6.0"was used. Results: The baseline NSE was higher and correlated to NIHSS (16.3 ±2.2, ÷2=1.08) in all pts. In Ia, Ib sbgroups NSE decreased for 3-5 days vs. control group 10-12days (÷2=7.93) and correlated with regression neurological deficit. ICU LOS in main group was 3.8±0.9 days vs. control group 5.9±0.9 days. Sensitivity and specificity of NSE as a marker of brain injury in pts. with AIS were 65 and 83% and in posthypoxic pts. were 90 and 90%, respectively, which showed NSE as eligible diagnostic criterion of posthypoxic cerebral edema. In Ia (AIS) pts. and Ib (posthypoxic edema) were confirmed by increasing NSE in 4fold and 9-fold respectively more vs. pts. who had only Brain CT at first day. NSE also correlated with regression neurological deficit and improving of the neurological status. Although, two pts. In IIb group died with NSE 150-220 ìg/ml Conclusions: 1. NSE is an effective marker of the severity of damages even in the sABF, and shoved efficacy efficacy of treatment. 2. Negative outcome can be in pts. with sABF and more 3-fold increasing NSE and increasing up to 150-220 ìg/ml is a mortality predictor. 3. We included NSE in local protocols P405 N-terminal pro-brain natriuretic peptide as a bio-marker of the acute brain injury Introduction: The detection of biomarkers levels facilitates an early diagnosis of brain tissues damage, allows assessing the prognosis of the disease and its outcome, and performs the monitoring of the patient treatment. Methods: We studied 64 patients (36 m, 28 f.). 1st group comprised 12 patients with severe brain trauma: 1asurvivors with good outcome (on Glasgow outcome scale groups I-II) (n=8), 1bdead or severely disabled (on Glasgow outcome scale groups III-V) (n=4). 2nd group comprises 37 patients with intracranial and sub-arachnoid hemorrhages: Assignment to groups 2a (n=14), 2b (n=22) was done using the same criteria as group 1. 3rd group comprises 16 patients operated in conjunction with brain tumor. Assignment to groups 3a (n=6) and 3b (n=10) was done using the same criteria as groups 1 and 2. We tested the level of N-terminal Pro-Brain natriuretic peptide in blood (0-125 pg/ml) between 1st and 3rd days after severe brain injury and then every 2-12 days for the total duration of 21 days. Results: : Statistical analysis failed to demonstrate noticeable difference in the level of NTproBNP between groups 1,2,3. We detected the differences between subgroups (p<0.01). Patients from groups 1a,2a,3a (n=28) NTproBNP level stayed below 700 pg/ml in 20 cases (71%), in the 8 cases (29%) the level was above 700 pg/ml, but by 14-21th day decreased to the normal values. For patients in subgroups 1b,2b,3b (n=36) in 28 cases (78%) NTproBNP level was above 700 pg/ml at least once, in 8 cases (12%) level stayed below 700 pg/ ml but remain high for the entire duration of the study without significant decrease. Conclusions: All the patients with acute brain injury show the increased level of NTproBNP above normal values, irrespective of ethiology of injury. In case when NTproBNP level increases above 700 pg/ml and/or does not decrease to the normal values it is possible to predict a negative outcome. Introduction: Cerebrovascular and coronary artery diseases share many of the same risk factors [1] . Cardiac mortality accounts for 20% of deaths and is the second commonest cause of death in the acute stroke population, second only to neurologic deaths as a direct result of the incident stroke. Methods: This is a prospective observational study from July 2015 to April 2016 done on 80 adult patients (groupI: 50 pts acute ischemic strokes & group II:30 pts as control) in Kafr-Elsheikh general hospital ICU. Inclusion criteria: All patients with acute ischemic stroke while Exclusion criteria: Patients with heart or renal failure/sepsis&septic shock/Ischemic heart disease or Hemorrhagic stroke,full clinical examination&labs including admission quantitative serum cardiac Troponin I ELISA immunoassay,ECG,2D echocardiography&CT brain on day 0&3,Alberta stroke program (ASP) early CT (ASPECT) to predict neurological outcomes and mortality in patient with acute ischemic stroke within 28 days so survivors Vs non-survivors in group 1 were divided to G1A & G1B respectively. Results: Dyslipidemia, hypertension, diabetes mellitus were significant comorbidities in all ischemic stroke pts.TLC, Urea, INR and Troponin were significantly higher in case group Vs control group.GCS was found to be lower in non-survivors at day 0&at 3rd day follow up while ASPECT was significantly lower only at 3rd day follow up.Troponin level was significantly higher in non-survivors G1B, it was also higher in patents who developed convulsion later during their ICU stay& it was significantly inversely correlated to GCS and ASP. troponin had sensitivity 53% and specificity 87% (ROC curve analysis) Conclusions: Troponin level was predictor for mortality in patient with acute ischemic stroke.it is well correlated to GCS and ASP on admission.it was a predictor for occurence of convulsions later in ICU stay. Introduction: Based on examination and treatment of hyperkinetic disorder in patients with UWS and MCS, we supposed that hyperkinesis manifesting the formation of the generator of pathologically enhanced excitation in cerebral cortex, basal ganglia, which subsequently causes the formation of hyperkinesis. Halogencontaining anesthetic sevoflurane had a good clinical effect in patients with prolonged impairment of consciousness. Methods: The study included 5 patients with UWS (4 -hypoxia, 1 -encephalitis) and 3 patients with MCS (2 -hypoxia, 1-encephalitis). Hyperkinetic disorder presenting as permanent myoclonus of arms and legs, face. All patients were performed head MRI and EEG (before, during and after anesthesia), CRS-R assessment, 3 patients -[18F]-FGD PET. Initial anesthesia: propofol 2-3 mg/kg, rocuronium bromide (Esmeron) 0, 6 mg/kg, fentanyl 3-5 mg/kg and clonidine (clophelin) 0.5-0.7 mg/kg. Maintenance of anesthesia is carried out due to the following scheme: inhalation anesthesia using Sevoflurane (2.0-3.0 vol%, MAC 0.8-0.9). Additionally, during the 2nd -4th hours of medical anesthesia was prescribed the intravenous injection using Ketamine 1-2 mg/kg/hr. The anesthesia is used during 24 hours. The patients were nurtured by balanced mixtures through nasogastric tube. After 24 hours the patients were gradually transferred to the autonomous breathing. The control clinical and instrumental studies to evaluate the therapy effectiveness (EEG, CRS-R) were performed. Results: In 5 patients (2 MCS, 3 UWS) was observed the hyperkinetic disorder regression as decrease of hyperkinesis manifestation, 3 patients didn't have a significant dynamics. Conclusions: The artificially formed "pharmacological dominant" (using sevoflurane and Ketamine) may decrease the activity of pathological system of the brain, which clinically presented as significant decrease of hyperkinesis manifestation in 5 out 8 patients. 15-year experience of using benzodiazepines in predicting outcomes and targeted treatment of patients in unresponsive wakefulness syndrome (UWS). Introduction: We accepted a hypothesis that in some patients UWS is a consequence of a pathologic system (PS), that limits the brain functional activity. Identification of a PS allow to predict consciousness recovery. EEG registration under benzodiazepines test (BT) has become the method of PS identifying in UWS patients. Methods: We examined 145 UWS patients (74 -traumatic, 71 -non traumatic). CRS scales assessment, EEG with BT, MRI of brain were performed for all patients. The midazolamum was administered iv 0.04 mg/kg,.In 3-4 min after BZD was recorded EEG for 15 min. The test was considered to be positive if against the background of BZD EEG pattern restructuring was observed: the low-amplitude EEG activity was rebuilt with the advent of alpha-and beta-spectrum.In patients with slowwave activity of theta-and delta-spectrum appeared stable fast forms, and in patients with baseline polymorphic EEG pattern was recorded prevalence of alpha activity and (or) the alpha rhythm. In order to confirm the correlation between the BZD effect and EEG pattern restructuring, Flumazenil was administrated at rate of 0.1 mg every 1 to 2 minutes until the original EEG pattern has been registered again. Results: The BT was true positive (recovery consciousness in 3-12 month later) in 22 traumatic and 19 non traumatic patients. True negative (permanent UWS 12 month later) in 27 traumatic and 43 non traumatic patients. False positive -11 traumatic, 4 non traumatic. False negative 14 traumatic, 5 non traumatic patients. Sensitivity BT to VS/UWS = 74.6% Sensitivity to MCS = 43.1% Conclusions: Our data confirmed the correctness of hypothesis that a PS limits the activity of the brain in patients in a UWS. We proposed diagnostic method of a PS activity and suppression. Apparently, BZD are the drugs of first stage examination choice in the treatment of UWS patients. Early identification of sepsis-associated encephalopathy with EEG is not associated with short-term cognitive dysfunction Introduction: Septic-associated encephalopathy (SAE) affects approximately 75% of septic patients. Recent studies showed SAE is associated with short-term mortality and long-term cognitive disability. However, diagnosis of SAE is one of exclusion and its association with short-term cognitive deficit is uncertain. The aim of this study is to evaluate the sensitivity of clinical examination in detecting SAE. The association between SAE and short-term cognitive impairment is also assessed. Methods: Prospective observational study enrolling adult septic patients admitted to a mixed ICU. Exclusion criteria were: encephalopathy from another cause, history of psychiatric/neurologic disease, cardiac surgery. All patients received continuous EEG monitoring and were assessed for SAE for up to 7 days after inclusion. We performed a comprehensive consciousness assessment twice daily during the ICU (GCS; Full Outline of UnResponsiveness, FOUR; Coma Recovery Scale-Revised, CRS-R; Reaction Level Scale 85, RLS85; Confusion Assessment Method for the ICU, CAM-ICU). We defined altered brain function as GCS<15, No correlation between cognitive function at hospital discharge and severity of EEG alteration was found. Conclusions: EEG was more sensitive than clinical assessment in detecting SAE. Altered EEG was not associated with short-term cognitive function. Analysis of the training needs in Italian centers that use brain ultrasound in their daily practices: a descriptive, multicenter study R Aspide 1 Introduction: As mission of SIAARTI Neuroanesthesia and NeuroICU group of study, we are mapping out the Brain Ultrasound training needs in our Centers. Although Brain Ultrasound is widely used to study the intracranial vessels and other issues, it is still not clear the homogeneity of the skills required in both Neuro and General ICU in Italy. The aim of this study is to explore the use of US-TCD and validate a collection of criterea which would prove useful in any future national wide survey. Methods: Starting from Sept. 2017 the seven Center involved (Bologna, Catania, Pisa, Verona, Bergamo, Cesena, Roma) collected clinical and sonographic data, basing on a CRF of twenty criteria such as: kind of hospital and ICU, number of beds and neuro-patients/year, the physicians specialization trained to perform US-TCD, the kind of US doppler device used and the kind of training course followed. As a second step, data were analyzed by coordination team, as third step, during annual SIAARTI conference, these Centers had a deep discussion on these selected items, further modifying and adapting the content of the items. Results: The result is a ready list of 20 items, an available tool for all the participant Centers, that are going to start with an internal test survey for a final validation. Conclusions: There is more than one path to train a physician on Brain US in Italy and there are new possible applications, even outside of the Neuro sub-speciality. From the preliminary discussion, it is clear that in Italy we have a inhomogeneous frame of training and use. This group of study believes that among the anesthesiologists/intensivists, it is possible to find a useful number of physicians interested in training on this topic. The main aim is the production of a validated criterea collection, available for eventually future national survey, useful to help map out the real national training needs in Italy on US Brain. Perinatal neurosurgical admissions to intensive care C Nestor, R Hollingsworth, K Sweeney, R Dwyer Beaumont Hospital, Dublin, Ireland Critical Care 2018, 22(Suppl 1):P411 Introduction: Beaumont Hospital is the Neurosurgical centre for Ireland serving a population of 3.6 million. We present data on all perinatal patients who required ICU admission for Neurosurgical conditions over an 8 year period. Our data presents an insight into the incidence and outcome of Neurosurgical conditions during pregnancy Methods: Searching our database identified 11 pregnant and 8 recently pregnant patients admitted to ICU with neurosurgical conditions. Patient data was collected retrospectively by review of charts and of an electronic database. A further 12 pregnant patients were admitted for Neurosurgical intervention but did not require critical care. Results: Intracranial haemorrhage was the most common diagnosis (5 subarachnoid haemorrhage and 5 had intra-cerebral haemorrhage). 6 patients presented with intracranial tumours and 1 patient had a traumatic brain injury. 1 patient was admitted post spinal tumour resection. 1 patient was referred with an ischemic stroke after iatrogenic injury to the carotid and vertebral artery. The requirement for organ support in this cohort of patients was high; 64% required ventilation and 45% inotropes. 13 patients underwent neurosurgical intervention & 6 medical treatment. 2 maternal deaths occurred at 16 & 37 weeks gestation. The modified Rankin Score (mRS) on discharge from hospital was <= 2 for 9 of the 17 surviving patients (median=3). Of the 11 pregnancies (all singleton) there were 3 foetal deaths. 1 patient miscarried spontaneously at 4 weeks, 1 had a medical termination of pregnancy at 12 weeks to facilitate chemotherapy and 1 foetus died after maternal death at 16 weeks. The 8 remaining patients delivered normal babies. Conclusions: Neurosurgical disease requiring ICU admission during pregnancy is rare; our data suggest an incidence of 1 case per 2 million population. Maternal outcomes were mixed with more than half having a mRS>2 on discharge. Foetal outcomes were good with only one miscarriage and good neurological outcome in all surviving infants. Stepwise multivariable analyses that included interaction between time of day and arrest location were performed in a stepwise manner. Results: Prehospital ALS (adjusted OR, 1.63; 95%CI, 1.38-1.93) but not good-quality of bystander-performed CCs (1.02, 0.84-1.26) was associated with sustained return of circulation (ROSC). Neither provison of good-quality CCs nor prehospital ALS was a major factor associated with on-month survival. However, good-quality of bystanderperformed CCs (2.44, 1.81-5.00) in addition to shockable rhythm (13.3; 8.70-20.4) and bystander-witnessed OHCA (4.79; 2.98-8.00) were associated with higher chances of neurologically favourable one-year survival, whereas prehospital ALS (0.21; 0.10-0.39) and elderly OHCA (0.47; 0.31-0.73) were associated with lower chances of the survival (Fig. 1) . The impact of good quality CCs on survival were preserved in bystander-witnessed OHCAs with shockable initial rhythm. Noncentral region (adjusted OR for good-quality, 0.46; 95%CI, 0.39-0.54), lack of BLS training experience (0.47; 0.36-0.62), elderly-only rescuers (0.53; 0.44-0.65), CC initiation following dispatcher-assisted cardiopulmonary resuscitation (0.71; 0.55-0.91), and female-only rescuer (0.77; 0.65-0.90) were associated with poor-quality CCs. CC quality in athome OHCAs remained low throughout the day, whereas that in outof-home OHCAs decreased during night-time. Conclusions: Provision of good-quality CCs before EMS arrival but not prehospital ALS was essential for neurologically favourable survival. New protocol for start of chest compressions before definitive cardiac arrest improved survival from out-of-hospital cardiac arrest witnessed by emergency medical service Introduction: Healthcare providers including emergency medical service (EMS) personnel usually confirm absence of carotid pulse before starting chest compressions. At the end of 2011, Ishikawa Medical Control Council implemented new criteria for start of chest compressions encouraging EMS to start chest compressions when carotid pulse was week and/or <50/min in comatose adult patient with respiratory arrest or agonal breathing. Methods: Data were prospectively collected for out-of-hospital cardiac and respiratory arrests during the period of 2008-2015. Definitive cardiac arrest was recorded when loss of carotid pulse was confirmed by pulse checks performed every 2 min after the early start of chest compressions. The effect of early chest compressions on the proportions of definitive cardiac arrest was analysed in 243 cases with respiratory arrest and circulatory depression in initial patient evaluation. Before/after comparison of neurologically favourable 1-Y survival was performed in 619 cases with EMS-witnessed OHCA. Results: The early start of chest compressions did not significantly prevent definitive cardiac arrest that followed respiratory arrest with circulatory depression in the initial patient evaluation (Fig. 1) . Time interval between start of chest compressions and definitive cardiac arrest confirmation (median; IQR) was 2; 1.5-3 min. The survival rate of all EMS-witnessed OHCAs after the implementation of new criteria was significantly higher than that before the implementation: adjusted OR; 95% CI, 1.86; 1.02-3.40 (Fig. 2) . No complications related to early chest compressions were reported during the study period. Conclusions: Start of chest compressions before definitive cardiac arrest improved survival from out-of-hospital cardiac arrest witnessed by emergency medical service. Healthcare providers including EMS personnel should be encouraged to provide chest compressions on cases with respiratory arrest and severe cardiovascular depression. Introduction: Our study sought to determine if there is a difference in time to tracheal intubation between direct and video laryngoscopy during cardiac compressions. Guidelines suggest no more than 5 seconds should be taken to perform intubation to minimise any delay in compressions [1, 2] . It is unclear if use of video laryngoscopes results in faster intubation times during cardiac arrest. Methods: Observational trial involving Emergency, Anaesthesia and Intensive Care doctors. Participants' baseline data obtained by questionnaire. Resusci-Anne™ manikin with Airway Trainer™ head [Laerdal] with grade 1 airway was utilised. Participants intubated the manikin 3 times, once with each of: MacIntosh size 3 blade, C-Mac video laryngoscope (Karl Storz, Germany) with size 3 blade and portable McGrath MAC enhanced video laryngoscope (Medtronic, USA) with size 3 blade. Order of laryngoscopes was randomised by computer generated sequence. Continuous cardiac compressions were performed throughout attempts. Results: Total 54 participants. There was a statistically significant difference in time to intubation between the 3 devices using Friedman test (p<0.01). Wilcox signed-rank test demonstrated time to intubation with videolaryngoscopy was longer, C-Mac (p=0.032) and McGrath (p=0.011) compared with direct laryngoscopy. There was no significant difference between the two videolaryngoscopes (p = 0.401). When controlled for participants level of seniority and previous experience with device, direct laryngoscopy was still significantly faster than C-Mac (p = 0.009) and McGrath (p = 0.017) Conclusions: Our study showed a disadvantage of video laryngoscopy during cardiac compressions. Faster intubation times with direct laryngoscopy could result in less pause in compressions and decrease periods without perfusion. Direct laryngoscopy is an appropriate first choice for tracheal intubation during cardiac arrest. Introduction: The aim of this study was to describe the coronary angiographic findings in relation to specific ECG changes and comorbidity in survivors after cardiac arrest. Methods: A retrospective cohort study of out-of-hospital cardiac arrest patients with data retrieved between 2008-2013 from national registries in Sweden. Unconscious patients with coronary angiography performed within 28 days after return of spontaneous circulation and available ECG were included (Fig. 1) . Results: After exclusion, 1133 patients were analyzed (Fig. 1) , (Table 1) . 249 (22%) were women and mean age were 64 years. Patients without ST-elevation were separated into groups with specified ECG changes and comorbidities. Differences were observed in the incidence of any significant stenosis, total occlusion and PCI performed, between the specified ECG changes, as well as between the comorbidity groups ( Introduction: Fewer women after return of spontaneous circulation from Out-of-Hospital Cardiac Arrest (OHCA) are undergoing coronary angiography (CAG) with possible Percutaneous Coronary Intervention (PCI). The aim was to investigate gender differences in comorbidity, CAG findings and outcome after OHCA in comatose patients with a shockable first ECG rhythm. Methods: A retrospective cohort study of out-of-hospital cardiac arrest patients with data retrieved between 2008-2013 from national registries in Sweden (Fig. 1) . Results: There was no difference in age or comorbidity except for men having more ischemic heart disease, 21.5 vs 15.0% (p=0.006). Rates of previous myocardial infarction did not differ, 8.2 vs 6.3%. No difference was seen in rates of ECG indicating prompt CAG according to guidelines. Still, more men underwent CAG but no difference in numbers of CAG leading to PCI was seen (Table 1) . Furthermore, in patients with ST elevation or LBBB, no gender difference in CAG and subsequent PCI was found. Men had lower rates of normal CAG findings but more triple vessel and left main coronary artery disease ( Table 2) . 1 year survival did not differ, 49.1 vs 45.0%. Conclusions: Our study suggests, that despite no gender differences in rate of ECG findings indicating a prompt CAG, men seems to have a more severe coronary artery disease while women have more frequently normal angiograms. However, this did not influence 1 year survival. Introduction: The circadian clock influences a number of cardiovascular physiological processes. A time-of-day variation in infarct size has recently been shown in patients with ST segment elevation myocardial infarction. However, there is no clinical evidence of circadian variation in patients with out-of-hospital cardiac arrest (OHCA) of cardiac etiology. Methods: We performed retrospective analysis using data from Japan's nationwide OHCA registry from January 2005 through December 2012, which includes all OHCA patients presented with ventricular fibrillation as first documented rhythm, and consequently confirmed cardiac etiology. In order to eliminate the night and weekend effects, we enrolled only patients suffered OHCA in the morning We conduct a retrospective cohort study focusing on the association between OHCA outcome and ICU bed availability. The OHCA data was acquired from a regional emergency operation center, and the ICU bed information was obtained from a regional sur it exceeds physiological levels in order to avoid insufficient oxygenation [1] . Hyperoxia has been associated with increased in-hospital mortality, though uncertainty remains about this association. Multiwave pulse co-oximetry has safely been studied intraoperatively as a guide to monitor hyper-and hypoxia by calculating an oxygen reserve index (ORI) which could add information to pulse oximetry measures when SpO2 is >98% [2] . Methods: This is a monocentric prospective study including 12 patients with successful resuscitation following OHCA. The aim of our study is to evaluate the feasibility and assess the availability of novel non invasive oxygen and hemodynamic variables. Collected data principally concern blood oxygen and circulation such as ORI, SpO2, total Hb, perfusion index and pulse rates. Recording is ideally started at time of ROSC. Results: We monitored 12 consecutive patients for a total time of 456.8 min during transport from OHCA place to the ER. SpO2 signal was present for 82.3% of transport time.Oxygen Reserve Index signal was present for 58.5% of the total transport time. Pleth variability index (PVI) signal was present 59.8% of the total transport time. SpHb signal was present 44.7% of total time from ROSC to hospital. The confidence interval for each variable is given in Fig. 1 . Conclusions: Our pilot study shows that noninvasive measurements of hyperoxia, fluid responsiveness and hemoglobin are readily available from the prehospital phase of post-ROSC care allowing for early tailored and goal directed interventions. increase in SOFA score was associated with 170€ (95% CI 150-180€) increase in the cost per day alive in the first 12 months after CA. The SOFA score is a good indicator of disease severity but the overlap between outcome groups does not allow its use for early prognostication in CA patients. The association of SOFA and its sub-scores with 12-month outcome and healthcare costs highlights that in addition to neurologic damage the full spectrum of multiple organ failure affects the survival and morbidity of CA patients. Public opinion on cardiopulmonary resuscitation decision and outcome in out-of-hospital cardiac arrest patientsquestionnaire study TY Li 1 Introduction: Metabolomics is a novel approach that can characterize small molecules (metabolites) and has the potential to explore genotype-phenotype and genotype-environment interactions, delivering an accurate snapshot of the subject's metabolic status. In this context, the aim of metabolomics is to improve early diagnosis, classification, and prediction over the development of a pathological condition. To this end, metabolomics have not been used in the characterisation of cardiac arrest (CA), cardiopulmonary resuscitation (CPR) and return of spontaneous resuscitation (ROSC). The aim of the present study was to explore whether metabolomics can characterize the CA versus ROSC in a swine model of ventricular fibrillation (VF). Methods: Ten animals were intubated and instrumented and VF was induced with the use of a cadmium battery. VF was left untreated for 6min and the animals were then resuscitated according to the 2010 guidelines. Defibrillation was attempted in all animals. Venous blood was drown at baseline, 2 min, 4 min, 6 min during untreated CA and finally at 2min, 30min, 2h, 6h after ROSC in order to determine the metabolomic profile during CA and during the early post-resuscitation period. ROSC was defined as the presence of an organized cardiac rhythm with a mean arterial pressure of at least 50 mmHg for >5 min. Blood was centrifuged and serum was analysed by high resolution 1H-NMR spectroscopy. NMR spectral data were submitted to multivariate discriminant analysis. Results: Eight animals survived the experiment and were included in the analysis. Metabolites upregulated in the immediate ROSC versus CA were succinate, hypoxanthine, choline and lactate. Metabolites upregulated in the 2 hour ROSC versus CA were ornithine and alanine. The 3 measured phases are shown in Fig. 1 Introduction: Early outcome prognostication in successfully resuscitated out-of-hospital cardiac arrest (OHCA) patients remains challenging. Prediction models supporting the early decision to continue with full supportive treatment could be of major interest following OHCA. We constructed prognostic models able to predict good neurologic outcome within 48 hours after ICU admission. Methods: Upon ICU admission, targeted temperature management at 33°C, hemodynamic and neuromonitoring (cerebral oxygen saturation measured with near-infrared spectroscopy and bispectral index (BIS)) was initiated. Prediction models for good neurologic outcome at 180 days post-CA were constructed at hour 1, 12, 24 and 48 after admission using variables easily collectable and known to be predictive for outcome. After multiple imputation, variables were selected using the elastic-net method. Each imputed dataset was divided into training and validation sets (80% and 20% of patients, respectively). Cut-off probabilities yielding a sensitivity above 90% were determined and performance of all logistic regression models was assessed using misclassification rates. Introduction: In many venues, EMS crews limit on-scene care for pediatric out-hospital cardiac arrest (POHCA), attempting treatment during transport. Hypothesizing that neuro-intact survival can be improved by prioritizing on-site care, strategies were effected to expedite on-scene drug delivery and intubation (with controlled ventilation). Methods: From 1/1/2012 to 4/30/2017, data for POHCA cases were collected. In 2014, new training prioritized on-site resuscitation (Phase I) using expedited drug delivery and intubation with controlled ventilation (~6 breaths/min). Training included psychological and skills-enhancing tools to boost confidence in providing on-scene care. In 2016, drugs were prepared while responding (Phase II). 2010 American Heart Association guidelines were used throughout and no other modifications were made. Neuro-intact survival in 2012-13 was compared to Phase I & II outcomes. Results: Over the 5.33-years, EMS faced 143 consecutive POHCA cases. The great majority presented in asystole throughout. In those resuscitated, mean time from on-scene arrival to the 1st epinephrine infusion fell from 16.5 min (2012-13) to 7.3 min (Phase I) and 5.0 min (Phase II). By 2017, it was 2 min. for resuscitated patients and 3.33 min. for all patients. Intubation and intraosseous insertion occurred more frequently in Phase I/II, but there were no significant differences in age, sex, etiology, response times, bystander CPR or drug sequencing. Neuro-intact survival improved significantly from 0/38 in 2012-13 to 23.2% (13/56) in Phase I and 34.7% (17/49) in Phase II (p < 0.0001; 2-tailed Fisher's exact test) (Fig. 1) . Conclusions: Although historically-controlled, the sudden appearance of neuro-intact survivors following a renewed focus on rapid on-site care was profound, immediate and sustained. Beyond skillsenhancing strategies, physiologically-driven techniques and supportive encouragement from leadership, pre-arrival psychological and clinical tools were also likely contributors to the observed outcomes. Fig. 1 (abstract P429) . Effecting neurologically-intact survival for children with out-of-hospital cardiac arrest P430 Improved outcomes with a bundled resuscitation technique to enhance venous return out of the brain and into the heart during cardiopulmonary resuscitation PE Pepe 1 , KA Scheppke 2 , PM Antevy 2 , D Millstone 2 , C Coyle 2 , C Prusansky 2 , S Garay 2 , JC Moore Introduction: Lowering intracranial pressure to improve brain perfusion during CPR has become a focus for our team. Combined with devices that enhance venous return out of the brain and into the thorax during CPR, outcomes have improved using head/chest elevation in the laboratory (Fig. 1) . This study's purpose was to confirm the safety/clinical feasibility of this new approach involving mechanical CPR at an angle. Methods: 2,285 consecutive out-of-hospital cardiac arrest (OOHCA) cases (all rhythms) were studied for 3.5 years (1/1/14 to 30/6/17) in an expansive, socio-economically-diverse U.S. county (pop. 1.4 mill). In 2014, EMS crews used the Lucas© and impedance threshold (ITD) devices on such patients, but, after April 2015, they also: 1) applied O2 and deferred +-pressure ventilation several min; 2) raised the backboard~20°; and 3) solidified a pit-crew approach to expedite Lucas© placement. Neuro-intact survival was not recorded until 2015, so resuscitation by EMS to hospital admission was used for consistency. Quarterly reports were run to identify any periodic variations or incremental effects during protocol transition (Quarter 2, 2015). Results: No problems were observed with head/torso-up positioning (n=1,319), but rates of resuscitation rose steadily during the transition period with an ensuing sustained doubling (Fig. 2 ) over the ensuing 2 years when compared to those studied (n=806) prior to the head-up approach (mean 35.2%; range 30-40% vs. 17.9%, range 15-20%; p < 0.0001). Outcomes improved across subgroups. Response intervals, indications for attempting CPR and bystander CPR rates were unchanged. Resuscitation rates in 2015-17 remained proportional to neuro-intact survival. Conclusions: The head/torso-up CPR bundle was not only feasible, but also associated with an immediate, steady rise in resuscitation rates during the transition phase with a sustained doubling of resuscitation rates, making a compelling case that this bundled technique may improve OOHCA outcomes in future clinical trials. Introduction: Cardiac arrest (CA) often requires intensive care unit (ICU) treatment, which is costly. While there are plenty of data regarding post-CA outcomes, knowledge of cardiac arrest associated healthcare costs is virtually non-existent. Methods: We performed a single-center registry-based study to determine expenditure data for ICU-treated CA patients between 2005 and 2013. Healthcare cost evaluation included costs from the initial hospital treatment, rehabilitation costs and social security costs up to one year post-CA. We calculated mean healthcare costs for one year survivors and for hospital survivors who died within the first year after cardiac arrest. We calculated effective costs per independent survivor (ECPIS) as an indicator of cost-effectiveness. We adjusted all costs according to consumer price index (CPI) in Finland as of 2013. All costs are presented as 2013 euros (€). Results: We identified 1,314 CA patients eligible for the analyses. At one year after CA 52% of the patients were alive and 40% were alive and independent in daily activities. One year survival stratified by cardiac arrest location group was 59% for out-ofhospital CA patients, 47% for in-hospital CA patients and 27% for in-ICU CA patients. For the whole study population, mean healthcare costs were 50,211€ per patient. Healthcare costs for hospital survivors were 67,928€ per patient and for hospital non-survivors 22,100€ per patient. Healthcare costs for those who survived to hospital discharge but died within the first year were 56,490€ per patient, while for one year survivors they were 70,148€ per patient. Healthcare costs stratified by CA location are presented in Fig. 1 . Mean ECPIS were 65,684€. Conclusions: For ICU-treated cardiac arrest patients, the mean ECPIS were close to 65,000€. The best prognosis and the lowest costs were observed for out-of-hospital CA patients. Introduction: In Lithuania the incidence of out-of-hospital cardiac arrest (OHCA) is unknown, as there is no official coding for OHCA as a cause of death in the national death registry. Kaunas Emergency Medical Service (EMS) underwent major stepwise changes since 2011, including implementation of Medical Priority Dispatch System and dispatcher-assisted CPR instructions. We sought to describe the epidemiology and outcomes from OHCA in Kaunas, the second largest Lithuanian city. Methods: The incidence, demographics and outcomes of patients who were treated for an OHCA between 1st January 2016 and 31st December 2016 in Kaunas EMS, serving a population of almost 0.3 million, were collected and are reported in accordance with 2014 Utstein recommendations. Results: In total, 313 OHCA cases of EMS treated cardiac arrests were reported (105 per 100.000 of resident population). The mean age was 67.7 (SD=15.7) years and 64.9% were male. 70% OHCA cases occurred at home and 52.7% were witnessed by either EMS or a bystander. In non-EMS witnessed cases, 43.8% received bystander CPR, whilst public access defibrillation was not used. Medical dispatcher identified OHCA in 71.3% of all cases and provided over-the-phone CPR instructions in 60.2% of them. Average EMS response time (90% fractile) was 13 min. Cardiac aetiology was the leading cause of cardiac arrest (84.3%). The initial rhythm was shockable (VF or pVT) in 26% and non-shockable (asystole or EMD) in 70.5% of all cases. Return of spontaneous circulation (ROSC) at hospital transfer was evident in 24.9% and survival to hospital discharge was 8.6%. Conclusions: ROSC and survival to hospital discharge in Kaunas were similar to those reported in United Kingdom in 2014 [1] . Routine OHCA data collection and analysis will allow us to track the efficiency of service improvements and should become a standard practice in all Lithuanian regions. Outcomes of patients admitted to intensive care following cardiac arrest J McLoughlin, E Landymore, P Morgan East Surrey Hospital, Surrey, UK Critical Care 2018, 22(Suppl 1):P433 Introduction: Patients who have return of spontaneous circulation following a cardiac arrest are haemodynamically unstable and require critical care input. Outcomes are often poor, with unadjusted survival to hospital discharge at 18.4%, following an in hospital cardiac arrest [1] . The aim of the study was to assess the survival of patients admitted to intensive care following a cardiac arrest, reviewing whether age and gender impacted on their outcome. Methods: The INARC database for a general intensive care unit (ICU) at a district general hospital was reviewed. Since 1993, 519 patients were admitted following a cardiac arrest (both in and out of hospital). Their age, gender and survival to ICU discharge and overall hospital discharge were recorded. Results: 210 female patients and 309 male patients of varying ages were admitted to our ICU following a cardiac arrest. The mortality for both genders increased with increasing age. Overall survival to the time of ICU discharge following a cardiac arrest was similar for both females (44.3%) and males (48.5%). Figures 1 (female) and 2 (male) below show the number of patients who survived or died on ICU discharge, by age and gender. Mortality rates increased when reviewing hospital outcome, as some patients died following discharge to the ward. Conclusions: Overall mortality in our ICU following a cardiac arrest at any age is at least 50%, with the general trend appearing to rise with increasing age. More male patients were admitted to ICU following a cardiac arrest than female, with similar survival rates for both male and female patients. More research could be undertaken to assess whether survival rates following a cardiac arrest have improved since 1993 and also to compare outcomes following either an in or out of hospital arrest. Introduction: Raw simplified EEG tracings obtained by a bispectral index (BIS) device significantly correlate with standard EEG [1] . This study aimed to investigate whether simplified BIS EEG tracings can predict poor neurologic outcome after cardiac arrest (CA). Methods: Bilateral BIS monitoring (BIS VISTATM, Aspect Medical Systems, Inc. Norwood, USA) was started following ICU admission. Six, 12, 18, 24, 36 and 48hrs after targeted temperature management (TTM) at 33°C was started, raw simplified BIS EEG tracings were extracted and reviewed by two neurophysiologists for the presence of burst suppression, cerebral inactivity and epileptic activity. At 180 days post-CA, neurologic outcome was determined using the CPC scale, where a CPC1-2 and CPC3-5 corresponded to good and poor neurologic outcome, respectively. Results: Of the 75 enrolled CA-patients enrolled, 40 had good and 35 poor neurologic outcome. With a positive predictive value (PPV) of 1.000 and a negative predictive value (NPV) of 0.606, epileptic activity within 6-12hrs predicted a CPC3-5 with the highest accuracy. Epileptic activity within time frames 18-24hrs and 36-48hrs showed a PPV for poor outcome of 0.917 and 0.938, respectively. Cerebral inactivity within 6-12hrs had a poor predictive power (PPV=0.545, NPV=0.566). In contrast, cerebral inactivity between 36-48hrs predicted a CPC3-5 with a PPV of 1.000 and a NPV of 0.597. The pattern with the worst predictive power at any time point was burst suppression with a PPV of 0.363, 0.529 and 0.500 at 6-12hrs, at 18-24hrs and at 36-48hrs, respectively. Conclusions: Based on simplified EEG derived from a BIS device, both the presence of epileptic activity at any time as well as cerebral inactivity after the end of TTM can be used to assist with poor outcome prognostication in successfully resuscitated CA patients. The helicopter as first response tool -Rio de Janeiro Fire Department experience. (interquartile range=28) min, followed by TIH with 552 flights (34%) and median time of 65 (IQR=70) min, and 270 (17%) were NEO missions with median time of 120 (IQR=92) min. Total time of aircraft usage was higher for TIH (39%), followed by NEO (32%). EVAM was the most frequent mission, however it accounted for 29% of aircraft utilization time, where most victims had traumatic brain injury (TBI) followed by other traumatic injuries (216 and 187 cases respectively). TBI victims were predominantly males (83%) with a median age of 30(IQR=23) years. Most commonly, TBI is a consequence of transportation accident (75%), where a motorcycle was involved in 31%, car collision in 24% and pedestrian run over 24% of the cases. Conclusions: GOA utilizes the air ambulance helicopter as a first response tool in 49% of total missions, where respect for the Trauma Golden Hour is paramount. Traumatic brain injury is the most prevalent diagnosis at the scene of event. Therefore, GOA training and equipment must be tailored to meet this demand, which translates in stabilization of critical patients outside hospital environment with limited resources. Introduction: The intra-hospital transport of critical patients is associated with adverse events and worse outcomes. The objective of this study was to evaluate the safety profile of intrahospital transport after the creation of a specific group for this purpose. Methods: Evaluated all the transports of critical patients from October 2016 to September 2017, in a large hospital, after the creation of a group consisting of intensive care physician, nurse and physiotherapist. Clinical and non-clinical complications related to the transport and outcome of the patients were evaluated. Results: A total of 1,559 transports were performed, 54.7% of the male patients and 60.9% of the patients being hospitalized. 10.6% were under mechanical ventilation and 19.8% under vasoactive drugs. At the time of transport, 78.8% were clinically stable. During transport, 7.5% presented clinical complications, being more frequent hemodynamic instability (43 patients) and respiratory failure (21 patients). Non-clinical complications occurred in 125 patients (8.0%), and communication failures were responsible for 79.2% of the occurrences. In 29 cases (1.9%) there was worsening of the clinical conditions during transportation, and in only one case this worsening resulted in an increase in the length of stay in the ICU and in the hospital, with no correlation with deaths. Conclusions: The implantation of a group specialized in critical patients to carry out in-hospital transport made the process safer with complications rates lower than literature and guarantee better quality of care. Clinical profile of patients admitted to ICU due to acute poisoning MP Benitez Moreno 1 , E Curiel Balsera 1 , MC Martínez González 1 , S Jimenez Jimenez 2 1Intensive Care Unit, Hospital Regional Universitario Carlos Haya, Malaga, Spain; 2 Hospital Regional Universitario Carlos Haya, Málaga, Spain Critical Care 2018, 22(Suppl 1):P438 Introduction: Patients suffering from acute intoxication, whether voluntarily for autolytic or accidental purposes, often require life support in intensive care units. Methods: Retrospective observational study of all patients admitted for acute intoxication who required admission to the ICU of the Regional Hospital of Malaga between January 2012 and August 2016, older than 14 years with admission to the ICU for intoxication of any kind. We study patient characteristics in terms of age, sex and medical history, type of toxicity, severity and evolution in our unit. Results: We found 70 cases of patients who required admission to the ICU due to acute intoxication, of which 55.6% were women. The average age was 47.36 (standard deviation 18.22). The average stay in ICU was 5.04 (standard deviation 8.09). 54.2% of patients had a psychiatric history. As other background highlights, 19.4% were addicted to illegal drugs and 25% were hypertensive. Most patients took more than one toxic 83.3% and intoxication was voluntary in 84.7% versus accidental in 12. 5% of cases. The toxic was known in 68%. The most used benzodiazepines in 26.4% of the total. The main cause of admission to the ICU was due to neurological deterioration in 49 of the cases registered and mechanical ventilation was necessary in 44 patients. The maximum time in mechanical ventilation was 34 days. The infection occurred in 24.3%, with the majority being respiratory infection. The 4.7% died in ICU. The hospital stay presented an average of 9.3 days. Conclusions: The profile of a patient admitted to the ICU due to acute intoxication is that of a woman of middle age and psychiatric history, with voluntary intoxication of several toxic substances and requiring mechanical ventilation for a low level of consciousness for an average of 3 days. The survival is very high and it would be necessary to analyze the possible relapses of these patients. Mushroom that break hearts: a case report E Karakoc, K Demirtas, S Ekemen, A Ayyildiz, B Yelken Eskisehir Osmangazi University, Eskisehir, Turkey Critical Care 2018, 22(Suppl 1):P438A Introduction: Because of the high mortality and morbidity mushroom poisoning is a significiant medical emergency [1] . Amanita phalloides (A. phalloides) is responsible for the 20% of the mortality in adults caused by mushroom poisoning. It causes damage in liver, kidneys and rarely pancreas, causing encephalopathic coma, disseminated intravascular coagulation, hemorrhage, hypovolemic shock and death but its effect on cardiac functions has not been established yet. There are three main groups of toxins;phallotoxins, virotoxins and amatoxins;amatoxin is the common responsible toxin from the fatality. We aimed to present a 44-year-old woman poisoned by mushroom complicated with hepatic,renal and cardiac toxicity Methods: Patient with nausea and vomiting started 48 hours after mushroom eating,creatine kinase MB 2.73 ng/mL and cardiac troponin I 0.02 ng/mL Her blood urea nitrogen, creatinine levels and liver enzymes were higher than upper limits in lab tests (Table 1) ; she was admitted to ICU, treated for acute renal failure by hemodialysis.Plasmapheresis was applied against potent mushroom toxins. At 5.day in ICU, hypoxemia and severe swelling resistant to ultrafiltration was evaluated as a global left ventricular hypokinesia with ejection fraction(EF) 20%, end-diastolic diameter of 5.9 cm, and systolic pulmonary artery pressure (SPAP) of 40 mmHg. Oxygen was administrated to treatment.Urine output improved at 6.day, three more plasmapheresis sessions were performed. hypoxemia was recovered,liver enzymes and creatinin levels decreased Results: At control EF measured was 44%, end-diastolic diameter of 4.9 cm, SPAP of 25 mmHg.than at the 15.day patient discharged from the ICU.After a year follow up assessment she has no complaints Conclusions: One of the major problems for amanita poisoning is diagnosis. Patients who had mushroom poisoning should also be evaluated especially in terms of cardiac dysfunction with clinic signs, ECG, cardiac enzyme tests and ECO Introduction: The characterization of clinical and/or biological variables found in the emergency room predictive of a secondary admission in ICU would help to improve the identification of patients at risk of aggravation in order to avoid the associated consequences, such as, an increased mortality and increased hospital stay. Methods: This is a retrospective monocentric study of 3 years with patients admitted secondarily to a medical ICU within 48 hours of admission to the general wards from the emergency department in the Pitié-Salpêtrière hospital in Paris. Each case was matched to 2 controls. 62 different variables were collected in the emergency room. Results: 319 patients, of whom 107 were cases and 212 controls were studied. Pneumonia is the diagnosis the most frequent in cases followed by sepsis (in 23 and 16%, respectively). 6 Conclusions: The risk of being admitted secondarily to intensive care is higher if patients consult for dyspnea or fever, if they are old smokers, if they have a high IGS2 score, if an arterial blood gas is requested and if an ICU medical advice is taken. The MEDS score under 7 and being an active smoker seems to be protects for the unexpected transfer. Introduction: Managing the special needs of patients who present with agitation or psychosis can pose a greater challenge to an already busy emergency department as their symptoms can escalate rapidly. Traditional antipsychotics used in the ED, such as haloperidol or ziprasidone often do not fully relieve patient's symptoms and may require administration of repeat doses or additional medications such as benzodiazepines to achieve effective results. This can induce excess sedation which can lead to longer length of stay in the ED and requires additional time at the bedside by the ED physicians and staff to manage these patients. Adasuve® is an antipsychotic drug that works in a single-use device providing an aerosol form of Loxapine that is rapidly absorbed by the lungs which may offer faster symptom relief, allowing subsequent earlier psychiatric evaluation and disposition. Methods: To test this hypothesis, data including time of physician assignment and time physician documented discharge disposition and number of hours physician was assigned to the patients was retrospectively collected from 407 patients who arrived to the emergency department presenting with agitation or psychosis that received Adasuve or other types of antipsychotic medication such as ziprasidone, haloperidol and benzodiazepines or a combination of the three. Results: We found that physicians who administered Adasuve spent an average of 8.30 hours assigned to their patient compared to 11.42 hours when the physician administered any other type of antipsychotic medication. This resulted in a significant 3.12-hour difference (p < 0.002) between the two groups. Conclusions: In conclusion, less time spent assigned to a patient that received Adasuve can be attributed to faster symptom relief which allowed the physicians to complete their psychological evaluations and develop dispositions more rapidly than with patients that received other antipsychotic agents. Clinical work in language-discordant Emergency Department Introduction: Emergency residents are particularly vulnerable to sleep deprivation due to persistent conflicts between work schedule and the biological clock. Recent approaches to address fatiguerelated risk mainly focused on reducing work hours and ensuring sufficient recuperation time. Such approach has demonstrated its limits due to growing emergency rooms visits and emergency residents' shortage. Dawson & McCulloch (2005) introduced the notion of proofing as a complementary approach to manage fatigue-related risk [1] . Fatigue proofing strategies (FPS) aim to reduce the likelihood a fatigued operator will make an error, in contrast of reduction strategies (FRS) aiming to reduce the likelihood a fatigued operator is working. Most formal risk control systems do not encompass the notion of proofing and FPS typically develop as informal practices. In this study, we aim to 1) identify informal reduction and proofing strategies used by residents and 2) to investigate how they relate to fatigue-related risk indicators. Methods: First, we organized 4 focus-group with a total of 25 residents in order to identify informal strategies used to manage fatigue-related risk. Second, we designed a questionnaire assessing the frequency of use of each reported strategy. Introduction: This randomized controlled study assessed the impact of a 3-hour intravenous medication safety simulation-based learning (SBL) on self-efficacy, stress, knowledge and skills of nursing students. Medication administration error is a worldwide concern [1], that has been linked with a lack of knowledge and skills in safe medication administration among new graduate and student nurses [2] [3] [4] . Preventing medication errors could therefore involve training through simulation. Methods: Participants (n=99) were randomly assigned either to the control group (CG, n=50) or the experimental group (EG, n=49). While CG and EG both had a traditional clinical internship, EG beneficiated in addition the 3-hour SBL, using standardized patients in the context of an intensive care unit. The two groups were assessed twice: at T0 and T1 (four weeks later), through an Objective Structured Clinical Examination (OSCE) and questionnaires. Two blinded experts rated the students OSCE with an evaluation grid. Results: Mean participants age was 21,2. There were no statistically differences between groups at T0. Compared to the CG (0%), the EG increased its self-efficacy (+19.35%) with a significantly difference (p<0.001) at T1. The SBL conducted to a greater increase of knowledge and skills in the EG (respectively +150%, +128%) than in the CG (respectively +46% and +47%), with a statistically significant difference (p<0.0001). Conclusions: Results reinforce the interest of a short SBL using standardized patients to improve medication administration. Clinical impact of these observations requires further evaluation to determine potential transfer in clinical settings and retention over time. Introduction: Medication errors occur frequently in the intensive care unit (ICU) and during care transitions. Medication reconciliation by a pharmacist could be useful to prevent such errors. Therefore, the aim of this study was to determine the effect of medication reconciliation at the ICU. Methods: A prospective 8-month intervention study with a pre-and post-phase was performed in Haga Teaching Hospital (2013) and Erasmus University Medical Center (2014). The intervention consisted of medication reconciliation by pharmacists at ICU admission and discharge. The severity of potential harm of the medication transfer errors (MTE) (pADE=0; 0.01; 0.1; 0.4; 0.6) was scored. Primary outcome measures were the proportions of patients with >= 1 MTE at ICU admission and ICU discharge. Secondary outcome measures were the proportions of patients with a pADE score >= 0.01, the severity of the pADEs and a cost-benefit analysis. Odds ratio and 95% confidence intervals were calculated. Results: Table 1 shows patient characteristics. Figure 1 shows the primary outcome measures (ORadj admission =0.18 [95% CI 0.11-0.30] and ORadj discharge = 0.24 [95% CI 0.15-0.37]). The proportion of patients with a pADE >=0.01 at ICU admission reduced from 34.8% to 8.0% and after ICU discharge from 69.5% to 36.2%. The pADE reduction resulted in a potential net cost benefit of € 103 per patient. Conclusions: Medication reconciliation by pharmacists at ICU transfers is an effective safety intervention, leading to a significant decrease in the number of errors and a cost effective reduction of potential adverse drug events. Introduction: In intensive care unit, administration of numerous drugs in ICU patients via a central venous catheter provide a high risk of drugs incompatibilities. It has been reported in experimental studies [1] that particles issued of drug incompatibilities could induce thrombogenesis, microcirculation impairment and inflammatory response which could aggravate the occurrence of organ dysfunctions [2] . The objective of this study was to evaluate the occurrence of particles by reproducing in vitro the intravenous system and the drugs combination used in ICU for patients suffering either septic shock or Acute Respiratory distress Syndrome (ARDS). Methods: First, we registered during a period of 6 months the most common central venous catheter system used in patients admitted for septic shock or ARDS in three University Hospital in Lille. The second part of the study was to reproduce in vitro the previous infusion system in order to quantify the amount of particles generated during a simulated period of 8 hours infusion. The egress of the IV line was connected to a dynamic particle counter QicPIC analyser (Sympatec Inc ; Clausthal Zellerfeld, Germany) (Fig. 1) . Results: The most common intravenous system observed was a three lumen central catheter. The proximal lumen was dedicated for vasoactive agents, the medial lumen for sedation and the distal lumen for the other drugs infused continuously and discontinuously..Among the drugs infused via the distal lumen of the central venous catheter, Introduction: Insufficient identification of possible organ donors in the ICU is one of the main factors contributing to the loss of donors after brain death [1] . Up to 50% of potential donors might not be identified [2] . The aim of this study was to evaluate how active search of possible brain dead donors affect the potential deceased donor pool. Methods: The strategy implemented at university hospital with 5 specialized ICUs from December 2016 to October 2017 and data compared to the matching period of the previous year. Donor coordinator visited all ICUs every day and selected patients who met possible brain dead donor criteria: 1) GCS <= 5; 2) severe brain injury. All data registered in original color coded follow-up system according to the patient status. Results: A total of 237 patients were identified as possible donors. There was no significant difference of potential donor numbers in study period comparing to previous year (32 vs 31). Main causes of brain death remain intracranial hemorrhage and subarachnoid hemorrhage. The length of hospital stay of potential donors was significantly longer in study period comparing to previous year (4±4.86 vs 2.29±2.2, P=0.004). There was no significant difference of donor's demographic data, conversion rates to actual donor or frequency of family refusals and medical contraindications. Conclusions: Active search of brain dead donors neither increased total number of potential donors nor increased conversion rates and did not change a donor profile in our donor center. Longer observational period and more sophisticated follow-up system might be required. A fast hug bid a day keeps the patient ok! E Sousa, T Leonor, R Pinho Centro Hospitalar de Entre Douro e Vouga, Santa Maria da Feira, Portugal Critical Care 2018, 22(Suppl 1):P461 Introduction: Regardless the underlying diagnose, providing meticulous supportive care is essential to critically ill patients management. In 2005, Vincent JL introduced the FAST HUG (Feeding, Analgesia, Sedation, Thromboembolic prophylaxis, Head of bed elevation, Ulcer prevention, Glucose control) mnemonic for recalling what he considered the key issues to review in daily clinical practice. Our Intensive Care Unit (ICU) decided to add BID (Bowel regimen; Indwelling catheter removal; De-escalation of antibiotics) indicators following some published data. Since 2013, the adequate use of this mnemonic became an instrument for quality of care evaluation. Objectives for each variable were designed; regular annual audits done. The present study aims to audit the use of this mnemonic in a portuguese tertiary hospital ICU, in 2017. Methods: A prospective observational study was performed. Admissions in ICU staying at least one 00h00min and 23h59min period, during the first six months of 2017 were included. All mnemonic variables were recovered from ICU medical record database, as well as demographics, severity scores and clinical information. Data was analyzed with Microsoft Office Excel software. Results: We included 119 admissions. The predictable global FAST HUG BID assessment was 1086 entries [one per each full day (00h00-23h59) in the unit, per patient]. The mnemonic was used in about 65% of the opportunities. The target thresholds were considered as achieved in 95% of entries (concordance equal or superior to 80%). Looking to individual variables, the best performance was achieved in H and U; worse performance was seen in S. The daily use of this mnemonic aims to revisit important intervention sectors in critical patient. Applying the "Plan-Do-Check-Act" policy, this study allowed us to identify growth opportunities, reviewing or creating protocols, adopting more frequent training measures and seeking to take this model to other hospital areas. Impact of incidents and adverse events in intensive care unit and its characteristics on outcomes E Siqueira, L Taniguchi, J Vieira Junior Hospital Sírio LIbanês, Sao Paulo, Brazil Critical Care 2018, 22(Suppl 1):P462 Introduction: Critically ill patients are usually exposed to adverse events (AE) due to acuity and complexity of care. AE might potentially result in disability or death, and increase in length of stay. Our aim was to assess the incidents and AE in a general intensive care unit (ICU). Methods: This is a prospective cohort study conducted in a private tertiary hospital (Hospital Sírio-Libanês) in São Paulo, Brazil. All consecutive patients who were admitted to the ICU and all incidents and AE reported in the study period were evaluated. Univariate and multivariate analysis were used to identify risk factors associated with hospital mortality. Results: Between May to November 2016 we studied 890 patients and 533 reported incidents and AE. Overall, 267 patients (30%) experienced some incident or AE during ICU stay. We found higher severity of illness (SAPS3 of 48 versus 44; p<0.001), mechanical ventilation (MV), use of vascular lines, drains and catheters, physical restraints, delirium and also an increased length of ICU (4 vs 2 days; p<0.001) and hospital stay (20 vs 11 days; p<0.001) and hospital mortality (24% vs 11%; p<0.001) among patients who experienced any incident or AE. Independent risk factors for hospital mortality in our logistic model were: higher SAPS3, MV and at least one adverse event during the ICU stay. Mortality was higher among patients who experienced late AE (>48 hours after ICU admission) compared to patients who experienced early AE (37% vs 19%; p<0.003). SAPS3, SOFA and MV were predictors of moderate and/or severe AE and a negative correlation between these events and ICU occupancy rate was found. Conclusions: Patients who experienced incident or adverse event during ICU stay had poorer outcome. AE, mainly moderate or severe, MV and severity of illness were independent risk factors to mortality. There was a negative correlation between moderate or severe adverse event and ICU occupancy rate. Monte Carlo modelling of patient flow can aid complex intensive care bed and workforce capacity planning. Introduction: Models for ICU populations based on the Queuing model use arrival rate, length of stay, and bed number [1, 2] . These models lack the complexity of specialised ICUs with different admission types, and patient subpopulations. Results: >98% of patients reported satisfaction on all areas except noise, patient facilities for hand hygiene and being informed about timing of operations. Staff survey results revealed confusion regarding the interventions that are provided. Baseline capacity for new patients was 53%, bed occupancy varied between 1 and 12 per day (overflow to recovery) with overall capacity at 93.5% and mean length of stay (LOS) was 1.3 days (SD=0.7, n=481, =range 1-5). Following intervention, the LOS was reduced to 1.18 days (SD=0.4, n=112, range 1-3). New patient capacity was increased to 62% with a bed occupancy range 1-8. Introduction: In clinical practice, when harm or potential harm occurs to patients, this can adversely impact upon the morale of staff involved and thereby affect clinical care delivered to subsequent patients. The personal narratives behind clinical incidents contain learning opportunities and individuals involved may reflect on the course of events and make changes to their practice to avoid recurrence. The aim of this study was to evaluate whether sessions enabling trainees to discuss their mistakes in a confidential environment improved trainee morale and safe clinical practice in an anaesthetic trainee cohort. Methods: We conducted a survey amongst anaesthetic trainees in a London teaching hospital before and after a monthly, hour long, confidential, semi-structured, trainee lead "confession session" was introduced. Results: Initial results demonstrated that 68% of respondents (N=30) had made a mistake resulting in patient harm with 84% of these individuals describing negative feelings about themselves as a consequence. Additionally, 97% of respondents had made a mistake causing a near miss, with 96% of these describing negative feelings as a result. Of note, only 55% of respondents felt comfortable discussing errors with more senior colleagues, whilst 78% felt comfortable discussing errors with their peers. A follow-up survey identified that 100% of respondents (N=13) agreed that the session had the potential to improve clinical practice and trainee morale with 77% agreeing that their own clinical practice had improved from attending the sessions. Conclusions: Clinical mistakes leading to harm and "near misses" are common and provide opportunities to improve care. This trainee lead "confession session" appears to improve trainee morale and may improve patient care by encouraging trainees to engage in a process that seeks to understand error through sharing stories in a non-judgmental setting. Funnel plots for quality control of the Swiss ICU -minimal data set Introduction: A clinical database should be representative of the labelled population and guarantee completeness and accuracy of collected data. Without explicit permission of the patients, Swiss laws regarding data protection do not allow external audits based on periodic checks of random samples, supposed to give a general pattern of accuracy. To test alternative methods for quality control we introduced the principles of statistical process control to derive funnel plots from the Swiss ICU -Minimal Data Set (MDSi). The MDSi from all certified adult Swiss ICUs (2014 and 2015) was subjected to quality assessment (completeness and accuracy). For the analysis of accuracy, a list of logical rules and cross-checks was developed as e.g. range of SAPS II according to age. Errors were classified in coding errors (e.g. NEMS score > 56 points) or implausible data (NEMS without basic monitoring). We also checked for ICUs producing significantly more errors -outliers -(> mean ± 3 standard deviations [SD] or > 99.8% confidence interval [CI] of an adapted version of the funnel plots, which allows the presence of trends depending of the ICU's size. Results: A total of 164'415 patient MDSi (31 items/patient; 32 items for trauma patients) from the 77 certified ICUs.were investigated. We detected 15'572 patients (9.5%) with an overall sum of 3121 coding errors and 31'265 implausible situations. Implausible situations related to supposedly inaccurate definitions (diagnostic and patient's provenance prior to ICU admission) and discrepancies in the logical rules between diagnostics and treatments. Figure 1 is an example for imprecise coding of the diagnostic: 11 ICUs declared having treated 14-61% of their patients without a defined diagnosis. Conclusions: Accuracy of data in MDSi needs further improvement. Funnel plots may be useful for meaningful interpretation of data quality and permit to identify ICUs disproportionately generating inaccurate and/or implausible data. Introduction: Lung cancer is the leading cause of intensive care unit (ICU) admission in patients with the advanced solid tumors. This study was aimed to elucidate the clinical factors associated with ICU mortality of advanced lung cancer patients and the effect of intensivist's contribution on their clinical outcomes. Methods: We included patients with advanced lung cancer including non-small cell lung cancer (NSCLC) with stage IIIB or IV and small cell lung cancer (SCLC) with extensive stage who admitted to ICU from 2005 to 2016. Multivariate logistic regression analysis was performed to find the variables associated with ICU mortality and in-hospital mortality. We applied autoregressive integrated moving average (ARIMA) for time-series analysis of the intenvention of intensivists. Results: Among total 264 patients with advanced lung cancer, 85 patients (32.2%) were admitted ICU before introduction of organized intensive care at 2011, and 179 (67.8%) were admitted after 2011 (Fig.  1) . The leading cause of admission was the respiratory failure (77.7%) and cancer-related event (34.5%) in terms of intensivist's and oncologist's perspective. Before and after 2011, the 30-day ICU mortality rate was 43.5% and 40.2% (p = 0.610), and the hospital mortality rate changed from 82.4% to 65.9% (p = 0.006) (Fig. 2) Introduction: Decisions when to refer and to admit patients to the intensive care unit (ICU) care are very challenging. Demand typically exceeds supply in ICU beds, which results in a constant need for evaluation of the processes involved in ICU referral and admission with a view to optimising resource allocation and patient outcomes. The aim of this study was to evaluate the theoretical impact of a newly designed triage tool for ICU referrals on a cohort of patients referred to ICU (Fig. 1) . Methods: We reviewed all patients consecutively referred to our ICU, whether admitted or not, in February 2017. Demographics, referring speciality, role of the referrer, comorbidities, the presence of advanced disease or terminal illness, the presence of acute organ failure, DNR status, reason for not admitting, and ICU mortality were recorded. A retrospective analysis of ICU referrals using a pilot triage tool was carried out independently by three authors. Results: Forty-six patients were referred to our ICU over the study period. Of these, 34 (74%) were admitted. Patients were declined ICU if their admission was deemed unnecessary (50%), futile (33%), or were transferred due to bed shortage (16%). Of the patients referred, 25 (54%) had an advanced disease or a terminal illness. Of those, 18 (72%) were admitted, DNR status was unclear in 22 (88%), family was involved in 12 (48%) and their ICU mortality was 48%. By analysing retrospectively these referrals with the aid of a triage tool, we propose that the overall referrals could have decreased from 46 to 30 (42% percentage difference). DNR status and family involvement would have been clarified in all patients with advanced disease or terminal illness before ICU referral. Kappa score for inter-rater agreement was 0.78. Conclusions: Adopting a triage tool for ICU referrals could reduce the overall proportion of inappropriate referrals and admissions. End-of-life discussion would also be proactively clarified prior to ICU admission. Introduction: Intensive care unit (ICU) admission triage occurs frequently worldwide and often involves decisions with high subjectivity, possibly leading to potentially inappropriate ICU admissions. In this study, we evaluated the effect of implementing a decision-aid tool for ICU triage on ICU admission decisions. Methods: Urgent ICU referrals before (May, 2014 to November, 2014, phase 1) and after (November, 2014 to May, 2015, phase 2) the implementation of a decision-aid tool were prospectively evaluated. Our primary outcome was the proportion of potentially inappropriate ICU referrals (defined as priority 4B or 5 patients, as described by the 1999 or 2016 Society of Critical Care Medicine [SCCM] guidelines) that were admitted to the ICU in 48 hours following referral. We conducted multivariate analyses to adjust for potential confounders, and evaluated the interaction between phase and triage priorities to assess for differential effects in each priority strata. Results: Of 2374 urgent ICU referrals, 110 (5%), 161 (7%), 284 (13%), 726 (33%) and 928 (42%) were categorized as priorities 4B, 4A, 3, 2 and 1 (SCCM 1999) or 110 (4.6%), 115 (4.8%), 887 (37%), 169 (7%) and 928 (39%) were categorized as priorities 5, 4, 3, 2 and 1 (SCCM 2016), respectively. Overall, 1184 (54%) patients were admitted to the ICU in 48 hours following referral. The implementation of the decision-aid tool was associated with a reduction of admission of potentially inappropriate ICU referrals [adjOR (95% CI) = 0.36 (0.13-0.97), p = 0.043] (Fig. 1) . There was no difference on hospital mortality for the overall cohort between phase 1 and phase 2. Conclusions: The implementation of a decision-aid tool for ICU triage was associated with a reduction of potentially inappropriate ICU admissions. Introduction: The aim was analyze the ICU bed rotation pattern, the epidemiological characteristics of patients and to correlate them with prognostic score after software implementation Methods: This is an epidemiological and retrospective study. Data were collected between June 2016 and November 2017, using EPIMED® monitor software, applied in an adult ICU of a public hospital in Bahia/Brazil. Authorization for collection and use of data was granted by the institution. All patients hospitalized in the period were included regardless of other exclusion criteria. Results: During the period evaluated, there were 1.011 new hospitalizations, 649 men (64.19%) and 362 women (35.81%). 46.38% (469) were in the age group of 18 to 44 years, followed by 28.28% of the patients (286), who were between 45 and 64. The mean duration of hospitalization in our unit was approximately 8,45 days. During the period covered, 1.009 exits occurred: 701 patients (69.47%) were Introduction: Early debriefing after stressful events holds great value in reflection on both an individual and team-based level. Our objective was to implement routine structured debriefing sessions for doctors working in intensive care in order to optimise learning and develop strategies to improve practice. Methods: 100% of junior doctors (n=10, pre-implementation questionnaire) on the intensive care unit expressed a need for regular debriefing sessions to discuss challenging and complex cases. Weekly sessions were implemented and structured using the SHARP performance tool [1] . Key learning points were collected and added to a debrief list to track progress and assimilate learning. Informal feedback was obtained on a weekly basis with formal feedback assessed following one month of implementation. Results: 30min sessions occurred on a weekly basis supported by a consultant intensivist. Desired outcomes included assessment of team performance, identification of key learning points and psychological support. Following one month, 100% doctors involved felt that debriefing sessions were important and should continue. 75% felt that they left every session with a key learning point applicable to future clinical practice. Common themes in perceived benefits included improved team communication and creation of an open environment to address concerns. Conclusions: Working in intensive care exposes doctors to challenging and stressful situations. Implementation of a regular structured debrief session provides an opportunity for clinicians to address concerns, consolidate learning and develop strategies to improve clinical practice. Nurse staffing patterns, outcomes and efficiency in resource use in the context of icus with a "low-intensity" nurse staffing: a multicenter study in brazilian icus M Soares 1 Introduction: Studies investigating nurse staffing and outcomes were often conducted in high-income countries with low bed/nurse ratios. Our objective was to investigate the association between nurse staffing patterns, outcomes and resource use in Brazilian ICUs. Methods: Retrospective cohort study in 129,680 (68% medical) patients admitted to 93 medical-surgical ICUs during 2014-15. We retrieved patients' data from an ICU registry (Epimed Monitor System) and surveyed participating ICUs about characteristics related to ICU organization. We used multilevel logistic regression analysis to identify factors associated with hospital mortality. We evaluated efficiency in resource use using standardized mortality rates (SMR) and resource use (SRU) based on SAPS 3. Results: SAPS 3 score was 44 (34-54) points and hospital mortality was 18.2%. Intensivists were present 24/7 in 83% ICUs. Median bed/ nurse ratio was 5.8 (4.2-7.3) and at least the chief nurse was boardcertified in critical care (BCCC) in 47% ICUs. Bed/nurse technicians ratio was 1.9 (1.8-2.1). Adjusting for relevant characteristics at patientlevel (age, admission type, SOFA, performance status, comorbidities, hospital days before ICU) and ICU-level (hospital type, checklist use, 24/7 intensivist, protocols), bed/nurse ratio was not associated with mortality [OR=0.99 (95% CI, 0.95-1.03)]. However, mortality was lower in ICUs with at least the chief nurse BCCC [OR=0.78 (0.65-0.74)]. In multivariate analysis, bed/nurse ratios <=6 [OR=3.53 (1.19-10.53)] and having the chief nurse BCCC [OR=6.36 (2.13-19.02)] were associated with higher efficiency. Conclusions: In a "low intensity" nurse staffing scenario, bed/nurse ratios were not associated with mortality. However, having at least the nurse chief BCCC was associated with higher survival. Moreover, bed/nurse ratios <=6 and presence of chief nurse BCCC were associated with higher efficiency in resource use. Methods: A systematic search on the value of acute non-physician provider on the ICU was conducted. The methodological quality of the included studies was rated using the Newcastle Ottawa scale (NOS). The agreement between the reviewers was assessed with Cohen's kappa. Results: In total 145 studies were identified. Twenty comparative cohort studies were identified which compared non-physicians with either residents or fellows. All studies comprised adult intensive care. Most of the included studies were moderate to good quality. A random effects meta-analysis from all studies regarding length of stay and mortality showed no differences between non-physicians and physicians, although there was a trend to better survival when implementing acute non-physician providers in the ICU (Figs. 1 & 2) . Mean difference for length of stay on the ICU was 0.36 (95% CI -0.07 -0.79; I2=88%) and for in hospital -0.15 (95% CI = -0.90 -0.61; I2=83%); while the odds ratio for ICU mortality was 0.94 (95% CI = 0.73 -1.20; I2=60%) and for hospital mortality 0.94 (95% CI 0.89 -1.00; I2=0). Conclusions: The acute care non-physician provider in the ICU seems a promising clinician on the ICU with regard to quality and continuity of care. Whether they also can reduce mortality remains to be determined by designing studies, which adequately measure the contribution of the non-physician providers in ICU care overall and per task. Their role in Europe remains to be elucidated. Burnout and depression in ICU staff members N Bahgat Menoufia University hospital, Shibin Elkom, Egypt Critical Care 2018, 22(Suppl 1):P479 Introduction: Family and success in work are the most important sources of person satisfaction in life, Chronic prolonged exposure to stressful high workload in intensive care units (ICU), create a bad psychological state named burnout syndrome in which person is depressed, exhausted and thinks to leave job. In this study we made a survey on ICUs staff members in Egypt Menoufia university hospital to explore and find risk factors increase depression and burnout among nurses and doctor. Methods: Questionnaires were given to all intensive care staff for estimating the prevalence and associated risk factors of burnout using Maslach Burnout Inventory (MBI) with its three subscales emotional exhaustion (EE), lack of accomplishment (LA), and depersonalization (DP). Depressive symptoms using the Beck Depression Inventory Scale. Blood sample was taken for assessing depression biomarkers including IL-6, tumor necrosis factor (TNF)-alpha, and coenzyme Q10 (CoQ10), which appears to be one of the most reliable peripheral biomarkers. Results: 100 participants were respond in our survey from 127 ICU members the response rate was 78.7%, The depression symptoms found increased in nurses more than physicians in ICU with more desire to leave the job. There was strong correlation between the degree of depression symptoms and decrease percent of personal accomplishment. Impaired personal relationships at work and increased night shifts were major risk factors of burnout syndrome. Levels of the proinflammatory cytokine (IL6 and TNF alpha) were elevated in members who recorded sever degree of depression score with decrease in concentration of Co-enzyme Q10. Conclusions: The health workers in ICU had high liability for depression and burnout syndrome. The risk factors differ between nurses and doctors. IL6, co-enzyme Q10 and TNF alpha concentrations had god correlation with degree of severity of symptoms. Impact of a tailored multicomponent program to reduce discomfort in the ICU on post-traumatic stress disorder: a casecontrol study P Kalfon 1 , M Alessandrini 2 , M Boucekine 2 , M Geantot 3 , S Renoult 4 , S Deparis-Dusautois 5 , O Mimoz 6 , J Amour 7 , E Azoulay 8 , C Martin 9 , T Sharshar 10 , M Garrouste-Orgeas 11 , K Baumstarck 2 , P Auquier 2 Introduction: Reducing discomfort during the ICU stay should be beneficial on long-term outcomes. The aim of this study was to assess the impact of the implementation of a tailored multicomponent program to reduce discomfort in the ICU [1] on the occurrence of posttraumatic stress disorder (PTSD) 12 months after discharge from the ICU. Methods: Design: case-control study; the cases were patients hospitalized in the ICUs which implemented the tailored multicomponent program; the controls were patients hospitalized in the ICUs which did not implement the program. Exposition: the tailored multicomponent program consisted of assessment of ICU-related self-perceived discomforts by using the IPREA questionnaire, immediate and monthly feedback to healthcare teams, and tailored site-targeted measures under control of a duo of local champions. General procedure: eligible patients were recalled 12 months after the ICU stay. Data collection: sociodemographics, clinical data related to the ICU stay, discomfort's levels assessed the day of discharge from the ICU, life situation (home/care center), PSTD (IES-R) and anxiety-depression symptoms (HADS) 12 months after the ICU discharge. Results: From the 617 eligible cases and 847 eligible controls, 344 cases and 475 controls were included (reason for exclusion: deaths after discharge from the ICU, lost to follow-up, patient refusal, cognitive incapacity). A total of 6.1% of the cases and 12.2% of the controls presented certain symptoms of PTSD at12 months (p=0.004). After adjustment for age, gender, IPREA score, McCabe score, presence of invasive devices during the ICU stay and considering anxietydepression symptoms at 12 months, cases are less likely to have PTSD symptoms than controls. Conclusions: Our tailored multicomponent program for discomfort reduction in the ICU can reduce long-term outcomes as PTSD. Diffusion of such a program should be enhanced in the ICUs paving the way for a new strategy in care management. Introduction: Cognitive dysfunction is a major factor leading to disability and poor quality of life in ICU survivors. In order to identify patients at risk for developing cognitive dysfunction due to critical illness or ICU treatment, one has to discriminate between patients with pre-existing cognitive dysfunction and those developing new cognitive dysfunction or worsening of cognitive function during ICU treatment. We investigated the incidence of pre-existing cognitive dysfunction in ICU patients using the Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE) and its relation with delirium during ICU treatment. Methods: Patients relatives were asked to fill in the IQCODE on admission. An overall score on cognitive dysfunction was calculated by the average of the score on each item of the questionnaire. The incidence of delirium was based on the CAM-ICU score. Statistical analysis was performed using the Fisher's exact test. P-values of less then 0.05 were deemed significant. Results: In total 452 consecutive patients admitted to our ICU were analyzed, of whom 47.8% (n=216) showed decline in cognitive function prior to ICU admission. Cognitive function was divided in four groups; no change 52.2% (n=236), slight decline 34.1% (n=154), moderate decline 9.7% (n=44) and severe decline 4.0% (n=18) (Fig. 1 ). Incidence of delirium is shown in Fig.  2 . Patients with moderate to severe cognitive dysfunction showed significant more delirium during ICU treatment than patients with no change in cognition (44.2% and 21.1% respectively, (p=0.023)). Conclusions: Almost half of the patients admitted to the ICU have cognitive dysfunction prior to ICU admission. To assess ones cognitive function after ICU treatment one has to take in to account the patients pre-existing cognitive functioning. Patients with a moderate to severe pre-existing cognitive dysfunction develop significantly more delirium during ICU treatment. Introduction: Our aim was to identify and analyse patients treated for POCD admitted to a thoracics/urology intensive care unit at University College London, UK. POCD is rising in the ageing high-risk surgical patient. Early identification of those at risk and timely intervention could help reduce associated morbidity and mortality [1] . Methods: We identified patients treated with haloperidol, midazolam, lorazepam, olanzapine, clonidine or chlordiazepoxide from our electronic data system. These pharmacological interventions were used as surrogate markers of primarily hyperactive POCD, acknowledging other forms of delirium may be unaccounted for. 111 of 808 admissions (13.7%) were shortlisted from August 2016 to July 2017. Patients were excluded if the drugs had been used for other indications. Prevalence of known POCD risk factors were then detailed. On these data we performed a cluster analysis using R. Results: Of the 58 patients (7.17%) suitable for analysis, the mean age was 72. 41 patients underwent elective procedures. 39 were male and 19 were female. 75% patients had thoracic surgery. The mean pain score in the first 24 hours post-op was 1.6 (SD=1.1), (with 0= no pain, 4= very severe pain). 62% had evidence of poor sleep and 14% evidence of anxiety. In the 24 hours prior to evidence of POCD, the mean pain score remained 1.6 (SD=0.99), 76% had evidence of poor sleep and 22% had evidence of anxiety. 66% of our population was septic during their ITU admission. Conclusions: Our analysis demonstrates POCD is highly prevalent in male patients over 70 undergoing thoracic procedures. We will now develop a POCD pathway targeting improved postoperative management of pain, sleep, anxiety and infection in this patient population. Introduction: Our objective was to determine the feasibility of employing family-administered tools to detect delirium in the critically ill. The use of family-administered delirium detection tools has not been assessed in the ICU where patients are critically ill and frequently intubated. Family members may be able to detect changes in patient cognition and behavior from pre-illness levels earlier than unfamiliar providers. These tools may be a valuable diagnostic adjunct in the ICU. Methods: Consecutive patients and family members (dyads) in the largest adult ICU in Calgary, Canada were recruited (Aug. 9-Sept. 11, 2017). Inclusion criteria were: patients with a Richmond Agitation Sedation Scale (RASS) >=-3; no primary brain injury and Glasgow Coma Scale score of <9; ability to provide informed consent (patient/ surrogate); and remain in ICU for 24 hours. Data were collected for up to 5 days. Family-administered delirium assessments were completed once daily (Family Confusion Assessment Method & Sour Seven). To assess feasibility, we assessed proportion of eligible patients and percent family member enrollment. Barriers to enrollment were categorized. Results: Of 99 admitted patients with family, 37 (37%) met inclusion criteria and 17 (46%) dyads consented. 20% of admitted patients did not have family and were thus ineligible. 73% of enrolled dyads assessed delirium at least once, with a median of 5 (of 10 total) assessments. The most common reason for non-enrollment was refusal by the family, who commonly reported feeling overwhelmed by the ICU environment. Barriers with nursing staff were encountered, including not providing access to patients and patient exclusion. Conclusions: These data suggest that employing family-administered delirium detection tools in the ICU is feasible for a subset of the population. Future studies will validate the use of these tools in the ICU, decrease modifiable barriers to enrollment, and test strategies to overcome attitudinal barriers towards employing these tools. Introduction: Psychological impact of critical illness and ICU stay on patients can be severe and frequently results in acute distress as well as psychological morbidity after discharge [1] . However, the stressful experience in ICU and its influence on patient recovery, remain relatively understudied. We assessed patients in ICU for acute distress and psychological symptoms with validated tools. Methods: We conducted an observational study in a group of awake ICU adult patients admitted in a tertiary centre for at least 48 hours, from January 2017 until October 2017, with mixed diagnosis on admission. We collected demographic factors, SAPS II at admission, mechanical ventilation, day of sedation, history of psychopathological disorder. Un-sedated and alert, critical care patients were assessed with tools such as Intensive Care Delirium Screening Checklist (ICDSC), Hospital Anxiety and Depression Scale (HADS) and Intensive Care Psychological Assessment Tool (IPAT). Results: 68 patients were recruited, (mean age 51.2±17.9 years, 66.2% males). SAPS II at admission was 32.2±16.7, 60.3% was mechanically ventilated (mean duration 6.1±14), mean duration of sedation was 2.8±3.9 days and a rate of 22.05% had an history of psychopatological disorder. 10.3% of the sample had clinical delirium (ICSDC>3) and was not assessed with others tools, 20.6% had subclinical delirium (ICSDC <=3). Regarding psychological outcomes, 26.2 % (mean score 6.1±2.5) reported a score (>=8) on HADS that indicates a possible diagnosis of anxiety and 54.1% (mean score 7.9 ± 3.7) of depression. A rate of 24.6% reported a score >= 7 on IPAT suggesting an acute distress. Conclusions: The study's key finding was that acute psychological distress was high in awake ICU patients. Further work is needed to determine the efficacy of early psychological interventions to reduce the incidence of acute distress and psychological outcomes after ICU stay. Introduction: A high percentage of polytrauma patients require surgery within the first 24 hours to stabilize primary traumatic injuries. One of the main intraoperative complications in this type of patients is due to hemodynamic instability [1] . Thus, it is necessary to implement multimodal monitoring involving both hemodynamic monitoring and monitoring of general anesthesia. The objectives of this study were to identify the possible implications of Entropy monitoring on hemodynamic stability in critically ill polytrauma patients. Methods: Prospective Observational Study, Deployed in the Clinic of Anesthesia and Intensive Care, Emergency County Hospital "Pius Brinzeu" Timisoara, Romania. ClinicalTrials.Gov Identifier. There were two groups, Group A (N = 37), in which the depth of hypnosis was monitored through Entropy (GE Healthcare, Helsinki, Finland) and Group B (N = 35). Results: The incidence of hypotension and tachycardia episodes was statistically significantly lower in Group A, unlike the control group (p <0.05). Moreover, a statistically significant (p <0.05) consumption of inhaled anesthetic agent was recorded in Group A compared with Group B. Consumption of vasopressor was also lower in Group A (p <0.0001, difference between means 0.960 ± 0.063, 95% confidence interval 0.8334 -1.0866) Conclusions: Deploying monitoring for the depth of hypnosis in general anesthesia using Entropy can significantly increase the hemodynamic stability of critically ill polytrauma patients. Introduction: The use of methadone as a potent analgesic has been gaining ground in the intensive care setting, such as where it is possible to properly select the group of patients who will benefit from the drug, as well as monitoring of possible complications. The objective of this study is to evaluate the safety of the use of methadone in critically ill patients in a large hospital. Methods: A retrospective analysis of all patients who used methadone in a neurological intensive care unit for a period of four months and the results were evaluated. Results: In the four-month period, 52 patients used methadone during intensive care. 65 % of the patients were male, with a medical age of 59.7 ± 17.4 years. The main indication for the use of the medication was for analgesia in patients who were weaned from mechanical ventilation. The mean time of use was 6.1 days. In all cases evaluated, analgesia was effective, with methadone being used alone or in combination with other drugs, according to an institutional protocol. Among the complications found, 20 patients presented hypotension (38 %); 20 presented bradycardia (38 %); 15 presented constipation (29%); 4 had excessive sedation (8 %) and 7 had other complications. All complications were reversible. 10 patients of the studied population died, however, without correlation with the use of methadone. Conclusions: The use of methadone, in the studied group, was effective in the control of analgesia, with no impact on patient safety when used in a monitored way. Introduction: Renal colic is a common disorder which presents with dramatic acute pain. Providing rapid relief, using effective pain control medications is the clinical priority to treat the patients. This study aims to compare the effect of IV Ketorolac versus Morphine in releasing renal colic pain by measuring pain severity and duration and also the need for additional doses. Methods: We performed a clinical pilot cohort study from during 2014 on patients with the clinical diagnosis of renal colic who recruited from the emergency department (ED) of Rasool-e-Akram Hospital and Firoozgar Hospital. Participants who were candidate to receive either Morphine or Ketorolac were divided into two groups who received either 30 mg Ketorolac IV or 5 mg Morphine. The pain was evaluated using the visual analog scale (VAS) at four time points: before drug injection (VAS-1), 20 minutes (VAS-2), 40 minutes (VAS-3), and 60 minutes (VAS-4) after injection. In cases when the pain was not controlled with the first injection of drug beyond 60 minutes; additional doses (rescue) were injected. Statistical analyses were performed using SPSS 21. Results: One-hundred-fifty patients treated with Morphine and 150 ones with Ketorolac were studiedThe group treated with Morphine scored on average 9.91 before the injection, which was roughly 2.4 points higher than Ketorolac. Morphine reduced patients' VAS scores more intensely (median: 10, IQR: 0 versus median: 6, IQR: 1; p value<0.001). In general, patients treated with Morphine were more likely to need a second (rescue) dose, when compared to Ketorolac group (38.6% vs 20%, p value= 0.001). Conclusions: Morphine is a better option for pain release in cases of renal colic. Ketorolac released the pain to an acceptable level; but, because of its slower action time, we recommend it in cases with moderate than severe pains. Effect of analgesics on cardiovascular and hormonal response to operative trauma D Loncar Stojiljkovic, MP Stojiljkovic SGH, 11000, Serbia Critical Care 2018, 22(Suppl 1):P493 Introduction: Objective of this study was to compare the effects of two analgesic regimens, one opioid and one non-opioid, on cardiovascular and hormonal reaction of patients undergoing elective surgery under general endotracheal anaesthesia. Methods: A total of 40 elderly patients, ASA 2, scheduled for elective abdominal surgery were assigned to receive on induction a single dose of either fentanyl (0.2 mg, +0.1mg) or a fix combination of etodolac and carbamazepine (Novocomb, dose 100mg+100mg iv bolus). Haemodynamic parameters and concentrations of prolactin cortisol and growth hormone (GH) [1] were determined at critical points and 24 h after operation. Results: Both fentanyl and Novocomb blocked the hypertensivetachycardic response to surgical trauma. Cortisol was a more appropriate endocrine marker of stress than prolactin or GH since fentanyl as an opioid analgesic increased secretion of prolactin [2] , while carbamazepine from Novocomb did the same with GH [3] (Figs. 1 & 2) . Conclusions: Cortisol plasma concentration correlates positively with cardiovascular parameters in patients undergoing elective abdominal surgery who received fentanyl or Novocomb as intraoperative analgesic. Its suppression is better marker of analgesia than prolactin and GH. Volatile anaesthetic consumption and recovery times after long term inhalative sedation using the mirus system -an automated delivery system for isoflurane, sevoflurane and desflurane Introduction: The new MIRUS system as well as the AnaConDa uses a reflector to conserve volatile anaesthetics (VA) [1, 2] . Both systems can be paired with ICU ventilators, but MIRUS features an automated control of end-tidal VA concentrations (etVA). We compare feasibility and recovery times for inhalational long term sedation with isoflurane (ISO), sevoflurane (SEVO) or desflurane (DES). Methods: 30 ASA II-IV patients undergoing elective or emergency surgery under general anaesthesia were included. Patients were randomized into three equal groups ISO, SEVO and DES. The MIRUS system was started with a targeted etVA of 0.5 MAC. We used the Puritan Bennett 840 ICU ventilator and performed a spontaneous breathing trial. If successful, the target concentration was set to 0 MAC and recovery times measured. Results: Patients were comparable in demographics, tidal volume, respiratory rate and sedation time (total 696h: ISO 19±9h; SEVO 22 ±19h; DES 29±29h; p=0.55). In all patients, a MAC of 0.5 was reached. Conclusions: MIRUS could automatically control end-tidal VA concentrations in ventilated and spontaneously breathing patients. The recovery times are only prolonged in the ISO group and could be shortened by removing the reflector. The higher etVA required for a 0.5 MAC using DES and SEVO were associated with an increased VA consumption. Introduction: Intranasal analgesia is increasingly used in order to relieve pain in the emergency department. This non-invasive approach avoids discomfort, stress and risks related to the parenteral route of administration. The objective is to compare intranasal (IN) fentanyl versus any parenteral opioid (intravenous, subcutaneous, intramuscular) for the effectiveness of acute pain relief in an adult population. Methods: The systematic review was registered in Prospero (CRD42016052976). The research of articles was conducted through Embase, Central, and Medline databases. Randomized clinical trials comparing the effectiveness of IN fentanyl to any parenteral opioid for acute pain relief (<= 7 days) in an adult population (>= 18 years old) were considered for inclusion. Studies on breakthrough cancer pain were excluded. Two different reviewers extracted data and analyzed the quality of the selected articles. The main outcome was the difference between pain levels before and after analgesia. The effect size was approximated using the inverse of variance of standardized mean differences, based on a random-effect model. Heterogeneity was quantified using a test of I2. Results are presented with 95% confidence interval. Results: Eight randomized clinical trials with 11 cohorts and a total of 613 patients were selected (320 IN fentanyl vs 293 control group). Selected articles contained a low risk of bias. There is no significant difference between the average levels of pain before and after analgesia comparing the two groups (SMD 0.12 [IC 95% -0.04 à 0.28], p=0.14; I2 = 0%) (Fig. 1) . Conclusions: IN fentanyl is as effective as other parenteral opioid to relieve pain during the first hour of treatment. Introduction: The aim of this study is to underline the importance of sedation protocol when performing the PEGJ procedure in advanced Parkinson's Disease (PD) patients. Research about the use of sedation in endoscopy is getting more and more widespread as to answer to the increasing grade of complexity and duration of endoscopic procedures as to offer comfort to the patient in terms of analgesia, tolerability, and amnesia. Sedation is also a way to assure quality and safety examination and to improve its outcome [1] . Methods: This observational retrospective study includes 40 PD patients scheduled for PEGJ procedure (Fig. 1 ) in order to start therapy with Duodopa gel. We propose an anesthetic technique (Table 1) to support PEGJ with both local anesthesia and moderate sedation so as to provide analgesia and patient's comfort. This technique ensures Mean duration of PEGJ procedure was 35'±5'. Mean stay time in recovery room 12'±2'. Compared to our old experience, we collected lack of patient's discomfort, anxiety, and memory, high procedure compliance and improvement of the quality of procedure without use of opioids. Conclusions: Based on our experience, we consider this sedation protocol effective for different reasons: to relieve or abolish patient's discomfort, anxiety, and memory, to ensure compliance with the procedure, to ensure patient's analgesia and patient's safety and, finally to assure procedure's quality and rapid discharge. Anyways, a multicentric study should be done to test our protocol. Introduction: Studies have shown that ICU survivors exhibit longterm neurocognitive impairment and perceived reduction in quality of life after ICU discharge, but studies examining sleep architecture and sleep disordered breathing (SDB) in ICU survivors after ICU discharge are scanty. The aim of our study was to assess sleep architecture and SBD in ICU survivors. Methods: ICU survivors were screened for eligibility. Inclusion criteria were: age 18 -80 yrs, mechanical ventilation >= 48 hours, GCS of 15 at the time of hospital discharge. Patients with a history of SBD, chronic neuromuscular disorders, chronic restrictive lung disease, congestive heart failure and respiratory failure at hospital discharge were excluded. Patients were evaluated within one week after hospital discharge and 6 months later. At both visits patients completed health related quality of life questionnaires (SF36 and Epworth Sleepiness Scale), underwent a physical examination, lung function tests including maximum inspiratory and expiratory mouth pressures, and an overnight full polysomnography (PSG). Results: Sleep quality at 7 days of hospital discharge is poor, characterized by severe disruption of sleep architecture and excessive SDB, mainly of obstructive type which in 76% of patients was classified as moderate or severe. Although at six months after hospital discharge sleep quality remained relatively poor, significant improvement in N3 stage and AHI was observed, with more patients to be classified as normal or mild SDB. Both at hospital discharge and 6 months later quality of life was reduced but there was no relationship between the health related quality of life and sleep disturbances. Conclusions: ICU survivors experience significant deterioration in their quality of life status with minor improvement 6 months later and a variety of sleep disturbances that seems to start getting better 6 months later. Introduction: Disrupted sleep in critically ill patients may be associated with delirium, prolonged stay in ICU and increased mortality. Polysomnography (PSG), the criterion standard method of sleep monitoring, is challenging in ICU due to interpretation difficulties, as the patterns defined by the standard classification for scoring sleep are absent in many critically ill patients. The aim of this study was to investigate if the presence of atypical patterns in critically ill patients' PSG is associated with poor outcome measured by 90-days mortality in conscious critically ill patients on mechanical ventilation. Methods: 70 PSGs (median duration 20 hours) recorded in conscious critically ill mechanically ventilated patients were scored by an expert in sleep medicine blinded to patient characteristics. Standard sleep scoring classification was used if possible. Otherwise, modified classification for scoring sleep in critically ill patients proposed by Watson et al. was applied [1] . The association of sleep patterns (normal or atypical) and micro-sleep phenomena (sleep spindles and Kcomplexes) with 90 days mortality was assessed using Weibull model by calculation of Hazard Ratios (HR). Results: HR analysis showed twice as high mortality risk in case of atypical sleep compared to normal sleep; this was however not significant (HR 2.5; 95% CI 0.95-4.44; p=0.08). The presence of sleep spindles in PSG significantly reduced mortality risk to 1/3 (HR 0.33; 95% CI 0.13-0.86; p=0.02). The presence of K-complexes in PSG reduced mortality risk to ½, though not significantly (HR 0.52; 95% CI 0.24-1.12; p=0.1). Conclusions: The absence of normal sleep characteristics in PSG in conscious critically ill patients on mechanical ventilation is associated with poor short-term outcome. Antipsychotics (APs) prescribing in critically ill delirious patients, the reported versus the perceived practice E Almehairi 1 , G Davies 1 , D Taylor Introduction: APs are the most commonly prescribed drugs in hyperactive/mixed delirium and agitation in critical care (CC) [1] . Yet evidence in CC is scant, there are known adverse effects (ADE) and prescription is out with the European license. Meticulous observation of AP selection, prescribing and safety, alongside delirium assessment/plan is essential to gain new knowledge and patients. When accompanied by prescribing clinicians perspective of delirium AP treatment results are more interpretable. We conducted a two-part single centre cohort study that aimed to describe/compare real to perceived delirium assessment/plan, APs prescribing and safety in CC adult patients at GSTT. Methods: Part 1: a prospective survey, of CC prescribing clinician's beliefs and attitudes to delirium assessment/plan, APs prescribing and safety over previous 12 months. Part 2: a meticulous audit of APs prescribing and safety and delirium/agitation assessment and plan, over period of 4 months. Results: Part 1 Survey. 43 of 82 prescribers (53.6%) completed survey. 88% of reported using APs to treat delirium, with 83% selecting atypical APs as first option. Part 2 Audit. There were 2400 admissions to CC. APs were prescribed in 6.4% (188 prescription), 3.8% (113 prescription) were in delirium/agitation patients (Table 1) . Survey (vs.) audit: In the survey 67% reported daily delirium screening whereas only 12.3% undertook daily screening in audit (Fig. 1) . Higher quetiapine and lower IV haloperidol maximum daily dose were prescribed in audit in comparison with survey reported doses ( Table 2) . 12 Lead ECG was used to monitor AP ADE. In survey 34% reported assessing ECG once or more daily. Audit revealed only 19% actually did so (Fig. 2) . Conclusions: Authors believe perceived vs actual can identify key areas for Quality Improvement (QI). Major differences were in delirium assessment/plan and safety monitoring sedation practices in Turkish ICUs, the aim was to provide knowledge on this matter. Methods: An electronic survey form was generated with google forms. First part of the form included questions about demographics, and choices and routines of sedation administration. This part mostly contained multiple choice questions, which more than one choice could be indicated. Second part was comprised of some statements to investigate the attitudes of physicians, which were indicated on a five-point Likert scale. The link for the survey was posted to all email addresses registered in the Turkish Society of Intensive Care member database. Results: Of 1700 members, 429 (25%) completed the survey form. Demographics are given in Table 1 . Sedation was generally applied by the physicians (96%). The indications were mechanical ventilation (94%), agitation (87%), seizures (77%), anxiety (61%), delirium (59%). Drug choices of the respondents are shown in Fig. 1 . Sedation level was evaluated daily by 73% of respondents, mostly using Ramsay Scale (57%). Daily established sedation level was indicated in 63.8%, and daily interruption of sedation was indicated in 71.4% answers. Sedation protocol was not used in 62.7% of the answers. Analgesics applied commonly, while 63% routinely evaluated pain and visual analogue scale (VAS) was the preferred method in 69% of the answers. 77.2% of physicians indicated routine use of neuromuscular blockers. In 50.5% answers routine evaluation for delirium was indicated, mostly using CAM-ICU.When the knowledge of 2013 SCCM guideline pain, agitation and delirium management, 38% indicated a positive answer.The respondents indicated their opinion for some comments on sedation, the answers are shown in the Table 2 . Conclusions: It may be concluded sedation practices may need to be improved by increasing awareness on novel concepts in this area. Fig. 1 (abstract P504) . The prediction-corrected VPC plots for dexmedetomidine PK. The VPC plots show the simulation-based 95% confidence intervals around the 10th, 50th, and 90th percentiles of the PK data in the form of blue (50th) and gray (10th and 90th) areas. The corresponding percentiles from the prediction corrected observed data are plotted in black color Methods: A prospective multinational cohort study was performed in 10 ICUs in Sweden, Denmark and The Netherlands. All adult patients with an ICU stay >= 12 hours were screened for inclusion. Primary outcome was psychological problems three months after discharge from the ICU, assessed with the questionnaires Hospital Anxiety and Depression Scale (HADS) and Post-Traumatic Stress Symptoms Checklist-14 (PTSS-14). A subscale score >10 in the HADS and a score >45 in the PTSS-14 part B indicate clinically significant symptoms of depression, anxiety and PTS and was considered an adverse outcome. We collected data on 21 known risk factors for psychological problems post-ICU. Univariable and multivariable logistic regression modelling of risk factors was performed in order to create an instrument to be used bedside, predicting individual risk for adverse psychological outcome. Results: 573 patients were included and 404 (71%) returned follow-up questionnaires. 14% of patients scored above the predefined cut-offs having symptoms of depression, anxiety or PTS. Age, lack of social support, depressive symptoms and traumatic memories at discharge remained significant after multivariable modelling and constituted the screening instrument ( Table 1) . The predictive value of the instrument was fairly good with an area under the receiver operating characteristics curve (AUROC) of 75% (Fig. 1) . We developed an instrument to be used at ICU discharge, predicting individual patients' risk for psychological problems three months post-ICU. The instrument can be used as a screening tool for ICU follow-up and enable early rehabilitation. Improving the patients hospitalization experience in an intensive care unit by contact with nature W Yacov 1 , Y Polishuk 2 , A Geal-dor 2 , G Yosef-hay 2 1 Kaplan Medical Center, Rehovot, Israel; 2 Kaplan medical center, Rehovot, Israel Critical Care 2018, 22(Suppl 1):P508 Introduction: The intensive care unit is characterized by a noisy and threatening work environment using multi tecnologic equipment.the staff works very intensively caring for very complicated and unstable patients.whilst caring for the patients physical needs one must not forget the patients mentally needs.the improvement of the patients hospitalization experience by changing the environment improves the mood and responsiveness to treatment gives hope for healing to the patient and family. Methods: a quality questionare with open questions relating to the subjective sensory experience of the patients and their families. the patients were transferred to the "sun balcony" for a period of 30-60 minutes having their families alongside. music was transmitted and the patients were offered food and drinks if their condition allowed. Results: the patients reported a significant improvement of hospilizaton experience following their exposure to the "nature environment". patients described the sensory experience as a positive, pleasant, quiet and relaxing experience. the contact with the sun, wind, sky and grass and being outside on the "sun balcony" allows a disconnection from the threatening ICU environment. Conclusions: The "sun balcony" gave the patients a sense of hope and wish for healing. mobilizing complicated patients to the "sun balcony" is a big challenge which requires planning and preparation by the staff. yet by the proactive and creative thinking of the staff the patients are tranferred to the "sun balcony" to give them encouragement, a feeling of well being and hope for recovering. This intervention is costless and a routine procedure in the intensive care unit. Introduction: Long-term psychological outcomes of patients(pts) discharged from ICU represent an emergent relevant matter of concern.Systematic reviews refer prevalence of 23%-48% for anxiety,17%-43% for depression and 8%-35% for posttraumatic symptoms in ARDS patients.The onset of psychiatric symptoms after discharge, might be associated with patient's competence to process memories related with hospitalization and with memories. Methods: We selected 35 ARDS pts in ICU of a tertiary centre (Jan 2014-Dec 2016) at least 72 hour, for 6 months follow-up and 26 pts for 12 months follow-up after discharge. The psychopathological assessment was performed using scale as: Impact Event Scale-Revised (IES-R), Hospital Anxiety and Depression Scale (HADS), ICU Memory Tool (ICU-MT). Results: Mean age was 53.11±14.32 at 6 months follow-up and 51,19±14,83 at 12 months. PTSD symptoms was fund respectively in 24% and 34.6% pts at 6 and 12 months; anxiety symptoms 24% and 23.1% of pts;depression symptoms in 24% and 30.8%. Significant correlations were fund between psychopathology at 6 months and memories of ICU: HADS anxiety with delusion memories (r 0.45,p<0.01); HADS depression with factual (r 0.46,p<0.05), feeling (r 0.49,p<0.01) and delusion memories (r= 0.59,p<0.01); feeling (r 0.45,p<0.05). At 12 months significant correlations was fund between HADS anxiety and feeling memories (r 0.48,p<0.05); IES-R and factual (r 0.45,p<0.01), feeling (r 0.68,p<0.01) and delusion memories (r 0.64,p<0.01). The results of the study confirmed the importance of assessing psychopathology after discharge from ICU. The onset of these symptoms appeared to be mediated by specific traumatic memories related with ICU hospitalization. The main clinical recommendation emerging from this study is to investigate psychiatric history and develop psychological strategies to manage frightening or delusional experiences during ICU stay. Introduction: The aging of the population is a fact. The subgroup of very old (>= 80 years (ys)) is the one that increases the most rapidly. Intensive Care Unit (ICU) admission of these patients is an ongoing discussion worldwide. Our ICU has designed the VOOLCAno aiming its characterization and reviewing outcomes, to find some predictive indicators. The purpose of this first analysis is to evaluate specifically the group of very old patients (VOLDs) admitted to a tertiary Portuguese hospital ICU. Methods: Retrospective observational study was preformed, included all VOLDs admitted in ICU during 15 years (2002) (2003) (2004) (2005) (2006) (2007) (2008) (2009) (2010) (2011) (2012) (2013) (2014) (2015) (2016) . Demographic data, admission diagnosis, severity scores, Charlson comorbidity index, length of stay and outcomes were considered. Data analysis used SPSS software. Results: We found a total of 460 admissions. The median age was 83.0 ys with IQR 4; Mostly male with medical admission diagnosis (sepsis and respiratory failure due to infection). There was a median Acute Physiology and Chronic Health Evaluation II of 18 (IQR 8) and Simplified Acute Physiology Score II of 49 (IQR 16). Median Charlson comorbidity index was 6.0 (IQR 2). Median length of stay was 3.9 days (IQR 8.2). Concerning outcomes, we found intra-ICU mortality of 36%; intra-hospital after ICU discharge mortality of 12% and mortality after hospital discharge of 41%. Identified as predictors of intra-hospital mortality the use of mechanical ventilation (p < 0.001), urgent surgical admission or medical admission versus schedule surgical admission (p < 0.001) and the absence of oncologic disease (p = 0.024). On multivariate analysis, only mechanical ventilation (p = 0.002, HR 0.35, 95% C.I. 0.18-0.68) and urgent surgical admission versus schedule surgical admission (p = 0.002, HR 0.29, 95% C.I. 0.14-0.63) remain significant. Conclusions: Recognizing the need to understand what is the biologic|funcional age (opposed to chronologic age) would be beneficial in the selection of VOLDs to ICU admission. Organ failure and return to work after intensive care S Riddersholm 1 , S Christensen 2 , K Kragholm 1 , CF Christiansen 2 , BS Rasmussen 1 1 Aalborg university hospital, Aalborg, Denmark; 2 Aarhus Univeristy hsopital, Aarhus, Denmark Critical Care 2018, 22(Suppl 1):P515 Introduction: Organ failure is associated with an unfavorable prognosis. Nevertheless, the association with capability to return to work remains unclear. Therefore, we investigated the association between organ support therapy as a proxy for organ failure and return to work in a nationwide cohort of ICU survivors. Methods: We linked Danish registry-data on ICU-and hospitalsurvivors working prior to hospital admission during 2005-2014, 18-65 years of age, with an ICU length of stay > 24 hours and not previously treated with dialysis, to data on return to work. We reported cumulative incidences (chance) of return to work with death as competing risk, and compared rate of return to work in adjusted Cox regression-models by number of organ support therapies including renal replacement therapy, cardiovascular support and mechanical ventilation and stratified on primary hospital-admission diagnosis. Results: Of 24,795 patients 18-65 years of age, 35% (8,660) survived to hospital discharge (Tables 1 and 2 ). Among these, the chance of return to work was 72.2% (95% CI [71.3-73.2]) within two years (Fig. 1 (Fig. 2) . When stratified an increasing number of organ support was associated with a decreased chance of return to work among patients with infection, respiratory failure or trauma but not among patients with neoplasms or endocrine, gastrointestinal and cardiovascular diagnoses. Introduction: Mortality rates among people with moderate to severe learning disabilities (LD) are 3 times higher than in the general population [1] [2] [3] . This study was designed to examine Critical Care admissions with learning disabilities in terms of mortality, demographics and reason for admission. Methods: Data was retrieved for adult patients (>16 years old) between Sept 1993 and 2016. The Ward Watcher database for ICUs within Surrey and Sussex Healthcare NHS Trust was interrogated using search words including, learning disability, cerebral palsy, Down's syndrome and autism. Results: There were 154 episodes (1.4% of all admissions) of patients admitted with LD. 10% of the LD patients had more than 1 admission. Respiratory is the most common system affected (46%). Logistic regression suggests survival is highest in those with a neurological reason for admission (p=0.007). Proportionally LD patients were young compared to the total population (Fig. 1) . We found that mortality appears to increase rapidly in those over 60 years of age and overall mortality is greater in those with LD (Fig. 2) . Conclusions: From April 2018 all UK Trusts will be required to complete a detailed review for patients with LD who die whilst in hospital care. This follows MENCAP's report 'Death by Indifference' which exposed deficiencies in the care of 6 people with LDs who died whilst in NHS care and the subsequent Confidential Inquiry into premature deaths of people with learning disabilities. In our population, LD patients have an earlier death than the general population and the overall mortality from critical illness is greater. A multidisciplinary approach at the emergency department to admit potential organ donors for Introduction: The aim of the present study is to improve the recognition of potential organ donors by implementing a multidisciplinary approach for organ donation at the emergency department (ED) [1] . Methods: In a prospective intervention study, we implemented this approach in six hospitals in the Netherlands. When the decision to withdraw life sustaining treatment was made at the ED in patients with a devastating brain injury without contra indications for organ donation, an Intensive Care Unit (ICU) admission for end-of-life care was considered. Every ICU admission for end-of-life care was evaluated. Interviews were conducted with emergency physicians, neurologists and ICU physicians according to a standardized questionnaire. This interview focused on medical decisions that were made and difficulties arising during hospitalization. Results: From 1 January 2016 to November 2017 data were collected on the number of patients admitted to the ED with acute brain injury. In total, 50 potential organ donors were admitted to the ICU for end-of-life care. Donation was either requested in the ED (12%), ICU (78%), neurology department (4%), or donation was not requested (6%). Out of 48 donation requests, 26 families (51%) consented to donation. This led to 21 successful organ transplantations. In four of these 21 patients family consent was obtained to intubate them solely for the purpose of organ donation. The most important points raised during the interviews were: explaining the non-therapeutic ICU admission to the family, the location where donation should be requested (ED/ICU) and utility of ICU resources. Conclusions: A close collaboration between the ED, neurology department and ICU is necessary and achievable in order not to miss potential organ donors in patients with acute brain injury with a futile prognosis in the ED. Introduction: There is a relationships between Intensive care patients losing the ability to speak and negative emotions [1] . Nursing care is challenging when patients are unable to verbalise and factors like pain and comfort are misjudged.. Our Intensive Care Unit has introduced a communication tool Intelligaze grid 3 which enables patients with primary motoric disorders to communicate their needs. A quality improvement study reviewed the methods of communication and interactions that our nurses use for patients who are ventilated. The objective of the study was to promote areas of improvement with communication in the ICU. Methods: We used a mixed-methods qualitative and quantitative study to evaluate the communication tools used by our nursing staff to interact with ventilated patients. A convenient data sample for all nurses working on particular dates was collected which is 66% of the nursing workforce. The study has been approved as a Quality Assurance project by the Human Research Ethics Committee of Nepean Hospital. Results: Sixty registered nurses (66%) participated in the study. The most common communication tool used with patients was closed YES/NO questions(27%), followed by hand gestures(21%), magnetic writing board(19.8%), lip reading(14.4%) and alphabet board(7.2%). The descriptive analysis identified challenges were levels of sedation, weakness, non-English speaking patients and delirium. A significant finding was that only 6% of nurses identified the patients message being understood and 5% acknowledged listening as effective communication. Conclusions: Communication is a vital aspect of ICU nursing and is achieved through dialogue and specialised skills. The study concluded that ICU nurses find it difficult to communicate effectively with ventilated patients. The introduction of Intelligaze Grid 3 has improved patient communication and promotes holistic nursing care. P525 Withdrawn Introduction: Substantial variability in EOLP occurs around the world [1] . Differences in EOLP were previously reported in Europe in the Ethicus I study [2] . Methods: ICUs worldwide were invited to participate through their country societies. Consecutive admitted ICU patients who died or had treatments limitations during a 6 month period from 1.9.2015 to 30.9.16 were prospectively studied. Regions included North, Central and Southern Europe (NE, CE, SE), North and Latin America (NA, LA), Asia (As), Australia (Au) and Africa (Af). Previous EOLP definitions were used [2] . Results: 199 ICUs in 36 countries participated enrolling 12,857 patients. Figure 1 shows differences in EOLP by region and Figure 2 in patient competency by region. Conclusions: Worldwide differences included more CPR in Af, LA, and SE and less CPR in NE, Au and NA. There was more withdrawing (WD) in NE and Au and less WD in LA and Af. More patients were competent in Au and NE and less were competent in Af, SE and LA. Introduction: The decision of end-of-life care in the ICU is very tough issue because the law, ethics, traditions and futility should be concerned involving family's will. Especially, stop or withdraw therapy is a quite difficult operation in Japan because of our traditions. Recently there are few legal issues due to some guidelines. Our hypothesis is some difference over time exists in thoughts about end-of-life care in the ICU. The purpose of this study is to know changing Methods: A questionnaire survey, which consists of 11 questions with 5 optional answers related to the thoughts of participants about end-of-life care of hopeless or brain death patients, was performed to nurses and doctors in our ICU. The questions were; whether accept to withdraw therapy or not and with family's will, whether positive or not to donate of organs from brain death patient, necessary of ICU care for brain death patient, feel guilty and stress for doing stop or withdraw therapy. The optional answer has 5 gradations from 'Yes' to 'No' for all questions. It was guaranteed to be anonymous for them in the data analysis. We conducted entirely same survey in 2012. The answers between in 2012 and in 2017 were Fig. 2 (abstract P528) . Patient mental compentency by region  
